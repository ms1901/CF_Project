{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ms1901/CF_Project/blob/main/Analysis/Analysis_CARL_GoodReads.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4f364b6b",
        "outputId": "2af105ae-01a6-49d7-ee69-54d372753fdb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.7/dist-packages (2.8.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.15.0)\n",
            "Requirement already satisfied: tensorboard<2.9,>=2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.8.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (14.0.1)\n",
            "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.0.0)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.21.6)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.17.3)\n",
            "Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (4.2.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.0)\n",
            "Requirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.5.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tensorflow) (57.4.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.14.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.44.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.25.0)\n",
            "Requirement already satisfied: tf-estimator-nightly==2.8.0.dev2021122109 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.8.0.dev2021122109)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.2)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: keras<2.9,>=2.8.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.8.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow) (0.37.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow) (1.5.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (3.3.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (0.6.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (2.23.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (0.4.6)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (1.8.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (1.35.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (1.0.1)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (4.2.4)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (4.8)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow) (4.11.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow) (3.8.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (0.4.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (2021.10.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow) (3.2.0)\n"
          ]
        }
      ],
      "source": [
        "pip install --upgrade tensorflow"
      ],
      "id": "4f364b6b"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ac9149d"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import scipy.sparse as sp\n",
        "import math\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import sys\n",
        "from time import time\n",
        "import os\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ],
      "id": "2ac9149d"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q5LbbrqP0Ilv",
        "outputId": "b12e87c5-b30d-438e-c9c3-6472aeb95661"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "id": "Q5LbbrqP0Ilv"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RVxfHDx8tGaF",
        "outputId": "82922849-7eb0-4153-ca31-dd785580e9a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<module 'tensorflow._api.v2.version' from '/usr/local/lib/python3.7/dist-packages/tensorflow/_api/v2/version/__init__.py'>\n"
          ]
        }
      ],
      "source": [
        "print(tf.version)"
      ],
      "id": "RVxfHDx8tGaF"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZdenhkrjwCSg",
        "outputId": "cb4654c5-cfe9-482a-bbe5-7c6db3c66c27"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/CF_end_proejct/Goodreads\n"
          ]
        }
      ],
      "source": [
        "%cd /content/drive/MyDrive/CF_end_proejct/Goodreads"
      ],
      "id": "ZdenhkrjwCSg"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RGS6qYUmtLs_",
        "outputId": "be3ebf25-0e90-4915-c28c-dac37978174c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['/device:CPU:0', '/device:GPU:0']\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.python.client import device_lib\n",
        "\n",
        "def get_available_devices():\n",
        "    local_device_protos = device_lib.list_local_devices()\n",
        "    return [x.name for x in local_device_protos]\n",
        "\n",
        "print(get_available_devices())"
      ],
      "id": "RGS6qYUmtLs_"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "590f617f"
      },
      "source": [
        "ExtractData"
      ],
      "id": "590f617f"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4c63801c"
      },
      "outputs": [],
      "source": [
        "class Dataset(object):\n",
        "    \"'extract dataset from file'\"\n",
        "\n",
        "    def __init__(self, max_length, path, word_id_path):\n",
        "        self.word_id_dict = self.load_word_dict(path + word_id_path)\n",
        "        print(\"wordId_dict finished\")\n",
        "        \n",
        "        self.userReview_dict = self.load_reviews(max_length, len(self.word_id_dict), path + \"dataPreprocessingUserReviews.out\")\n",
        "        self.itemReview_dict = self.load_reviews(max_length, len(self.word_id_dict), path + \"dataPreprocessingItemReviews.out\")\n",
        "        print(\"load reviews finished\")\n",
        "        \n",
        "        self.num_users, self.num_items = len(self.userReview_dict), len(self.itemReview_dict)\n",
        "        \n",
        "        self.trainMtrx = self.load_ratingFile_as_mtrx(path + \"dataPreprocessingTrainInteraction.out\")\n",
        "        self.valRatings = self.load_ratingFile_as_list(path + \"dataPreprocessingValInteraction.out\")\n",
        "        self.testRatings = self.load_ratingFile_as_list(path + \"dataPreprocessingTestInteraction.out\")\n",
        "\n",
        "    def load_word_dict(self, path):\n",
        "        wordId_dict = {}\n",
        "\n",
        "        with open(path, \"r\") as f:\n",
        "            line = f.readline().replace(\"\\n\", \"\")\n",
        "            while line != None and line != \"\":\n",
        "                arr = line.split(\"\\t\")\n",
        "                wordId_dict[arr[0]] = int(arr[1])\n",
        "                line = f.readline().replace(\"\\n\", \"\")\n",
        "\n",
        "        return wordId_dict\n",
        "\n",
        "    def load_reviews(self, max_doc_length, padding_word_id, path):\n",
        "        entity_review_dict = {}\n",
        "\n",
        "        with open(path, \"r\") as f:\n",
        "            line = f.readline().replace(\"\\n\", \"\")\n",
        "            while line != None and line != \"\":\n",
        "                review = []\n",
        "                arr = line.split(\"\\t\")\n",
        "                entity = int(arr[0])\n",
        "                word_list = arr[1].split(\" \")\n",
        "\n",
        "                for i in range(len(word_list)):\n",
        "                    if (word_list[i] == \"\" or word_list[i] == None or (not self.word_id_dict[word_list[i]] in self.word_id_dict.keys())):\n",
        "                        continue\n",
        "                    review.append(self.word_id_dict.get(word_list[i]))\n",
        "                    if (len(review) >= max_doc_length):\n",
        "                        break\n",
        "                if (len(review) < max_doc_length):\n",
        "                    review = self.padding_word(max_doc_length, padding_word_id, review)\n",
        "                entity_review_dict[entity] = review\n",
        "                line = f.readline().replace(\"\\n\", \"\")\n",
        "        return entity_review_dict\n",
        "\n",
        "    def padding_word(self, max_size, max_word_idx, review):\n",
        "        review.extend([max_word_idx]*(max_size - len(review)))\n",
        "        return review\n",
        "\n",
        "    def load_ratingFile_as_mtrx(self, file_path):\n",
        "        mtrx = sp.dok_matrix((self.num_users, self.num_items), dtype=np.float32)\n",
        "        with open(file_path, \"r\") as f:\n",
        "            line = f.readline()\n",
        "            line = line.strip()\n",
        "            while line != None and line != \"\":\n",
        "                arr = line.split(\"\\t\")\n",
        "                user, item, rating = int(arr[0]), int(arr[1]), float(arr[2])\n",
        "                if (rating > 0):\n",
        "                    mtrx[user, item] = rating\n",
        "                line = f.readline()\n",
        "\n",
        "        return mtrx\n",
        "\n",
        "    def load_ratingFile_as_list(self, file_path):\n",
        "        rateList = []\n",
        "\n",
        "        with open(file_path, \"r\") as f:\n",
        "            line = f.readline()\n",
        "            while line != None and line != \"\":\n",
        "                arr = line.split(\"\\t\")\n",
        "                user, item = int(arr[0]), int(arr[1])\n",
        "                rate = float(arr[2])\n",
        "                rateList.append([user, item, rate])\n",
        "                line = f.readline()\n",
        "\n",
        "        return rateList"
      ],
      "id": "4c63801c"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "77e2f302"
      },
      "source": [
        "GetTest"
      ],
      "id": "77e2f302"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "523a402f"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "def get_test_list(batch_size, test_rating, user_reviews, item_reviews):\n",
        "    user_test_batchs, item_test_batchs, user_input_test_batchs, item_input_test_batchs, rating_input_test_batchs = [], [], [], [], []\n",
        "    for count in range(int(math.ceil(len(test_rating) / float(batch_size)))):\n",
        "        user_test, item_test, user_input_test, item_input_test, rating_input_test = [], [], [], [], []\n",
        "        for idx in range(batch_size):\n",
        "            index = (count * batch_size + idx)\n",
        "            if (index >= len(test_rating)):\n",
        "                break\n",
        "            rating = test_rating[index]\n",
        "            user_test.append(rating[0])\n",
        "            item_test.append(rating[1])\n",
        "            user_input_test.append(user_reviews.get(rating[0]))\n",
        "            item_input_test.append(item_reviews.get(rating[1]))\n",
        "            rating_input_test.append([rating[2]])\n",
        "        user_test_batchs.append(user_test)\n",
        "        item_test_batchs.append(item_test)\n",
        "        user_input_test_batchs.append(user_input_test)\n",
        "        item_input_test_batchs.append(item_input_test)\n",
        "        rating_input_test_batchs.append(rating_input_test)\n",
        "        #print count, len(item_input_test_batchs[count])\n",
        "    return user_test_batchs, item_test_batchs, user_input_test_batchs, item_input_test_batchs, rating_input_test_batchs"
      ],
      "id": "523a402f"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "74573912"
      },
      "source": [
        "CARL"
      ],
      "id": "74573912"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "70fbf310"
      },
      "outputs": [],
      "source": [
        "def ini_word_embed(num_words, latent_dim):\n",
        "    word_embeds = np.random.rand(num_words, latent_dim)\n",
        "    return word_embeds\n"
      ],
      "id": "70fbf310"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "15e0a2c1"
      },
      "outputs": [],
      "source": [
        "def word2vec_word_embed(num_words, latent_dim, path, word_id_dict):\n",
        "    word2vect_embed_mtrx = np.zeros((num_words, latent_dim))\n",
        "    with open(path, \"r\") as f:\n",
        "        line = f.readline()\n",
        "        while line != None and line != \"\":\n",
        "            arr = line.split(\"\\t\")\n",
        "            row_id = word_id_dict.get(arr[0])\n",
        "            vect = arr[1].strip().split(\" \")\n",
        "            for i in range(len(vect)):\n",
        "                word2vect_embed_mtrx[row_id, i] = float(vect[i])\n",
        "            line = f.readline()\n",
        "\n",
        "    return word2vect_embed_mtrx"
      ],
      "id": "15e0a2c1"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cabc3fef"
      },
      "outputs": [],
      "source": [
        "def get_train_instance(train):\n",
        "    user_input, item_input, rates = [], [], []\n",
        "\n",
        "    for (u, i) in train.keys():\n",
        "        # positive instance\n",
        "        user_input.append(u)\n",
        "        item_input.append(i)\n",
        "        rates.append(train[u,i])\n",
        "    return user_input, item_input, rates\n",
        "\n"
      ],
      "id": "cabc3fef"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e6abcbc9"
      },
      "outputs": [],
      "source": [
        "def get_train_instance_batch_change(count, batch_size, user_input, item_input, ratings, user_reviews, item_reviews):\n",
        "    users_batch, items_batch, user_input_batch, item_input_batch, labels_batch = [], [], [], [], []\n",
        "\n",
        "    for idx in range(batch_size):\n",
        "        index = (count*batch_size + idx) % len(user_input)\n",
        "        users_batch.append(user_input[index])\n",
        "        items_batch.append(item_input[index])\n",
        "        user_input_batch.append(user_reviews.get(user_input[index]))\n",
        "        item_input_batch.append(item_reviews.get(item_input[index]))\n",
        "        labels_batch.append([ratings[index]])\n",
        "\n",
        "    return users_batch, items_batch, user_input_batch, item_input_batch, labels_batch"
      ],
      "id": "e6abcbc9"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "38ffbde1"
      },
      "outputs": [],
      "source": [
        "#review.py\n",
        "def  cnn_model_average(filters, user_reviews_representation_expnd, item_reviews_representation_expnd, W_u, W_i, W_u_1, W_i_1, rand_matrix):\n",
        "    #convolution layer\n",
        "    convU = tf.nn.conv2d(user_reviews_representation_expnd, W_u, strides=[1, 1, word_latent_dim, 1], padding='SAME')\n",
        "    convI = tf.nn.conv2d(item_reviews_representation_expnd, W_i, strides=[1, 1, word_latent_dim, 1], padding='SAME')\n",
        "\n",
        "    hU = tf.nn.relu(tf.squeeze(convU, 2))\n",
        "    hI = tf.nn.relu(tf.squeeze(convI, 2))\n",
        "\n",
        "    # attentive layer\n",
        "    sec_dim = int(hU.get_shape()[1])\n",
        "    tmphU = tf.reshape(hU, [-1, filters])\n",
        "    hU_mul_rand = tf.reshape(tf.matmul(tmphU, rand_matrix), [-1, sec_dim, filters])\n",
        "    f = tf.matmul(hU_mul_rand, hI, transpose_b=True)\n",
        "    f = tf.expand_dims(f, -1)\n",
        "    att1 = tf.tanh(f)\n",
        "\n",
        "    pool_user = tf.reduce_mean(att1, 2)\n",
        "    pool_item = tf.reduce_mean(att1, 1)\n",
        "\n",
        "    user_flat = tf.squeeze(pool_user, -1)\n",
        "    item_flat = tf.squeeze(pool_item, -1)\n",
        "\n",
        "    weight_user = tf.nn.softmax(user_flat)\n",
        "    weight_item = tf.nn.softmax(item_flat)\n",
        "\n",
        "    weight_user_exp = tf.expand_dims(weight_user, -1)\n",
        "    weight_item_exp = tf.expand_dims(weight_item, -1)\n",
        "\n",
        "    hU = tf.expand_dims(hU * weight_user_exp, -1)\n",
        "    hI = tf.expand_dims(hI * weight_item_exp, -1)\n",
        "\n",
        "    #abstracting layer\n",
        "    hU_1 = tf.nn.relu(tf.nn.conv2d(hU, W_u_1, strides=[1, 1, 1, 1], padding='VALID'))\n",
        "    hI_1 = tf.nn.relu(tf.nn.conv2d(hI, W_i_1, strides=[1, 1, 1, 1], padding='VALID'))\n",
        "\n",
        "    sec_dim = hU_1.get_shape()[1]\n",
        "\n",
        "    oU = tf.nn.avg_pool(hU_1, ksize=[1, sec_dim, 1, 1], strides=[1, 1, 1, 1],\n",
        "        padding='VALID')\n",
        "    oI = tf.nn.avg_pool(hI_1, ksize=[1, sec_dim, 1, 1], strides=[1, 1, 1, 1],\n",
        "        padding='VALID')\n",
        "\n",
        "    att_user = tf.squeeze(oU)\n",
        "    att_item = tf.squeeze(oI)\n",
        "    #print \"attention\", att_user.get_shape(), att_item.get_shape()\n",
        "\n",
        "    return att_user, att_item"
      ],
      "id": "38ffbde1"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6967e9a6"
      },
      "outputs": [],
      "source": [
        "def eval_model(users, items, users_inputs, items_inputs, dropout_rate, predict_rating, sess, user_batch, item_batch, user_input_batch, item_input_batch, rate_tests, rmses, maes):\n",
        "    print(sess)\n",
        "    print(\"user batch. \"+str(user_batch))\n",
        "    print(\"item batch \"+str(item_batch))\n",
        "    print(\"user_input_batch. \"+str(user_input_batch))\n",
        "    print(\"item_input_batch \"+str( item_input_batch))\n",
        "\n",
        "    print(\"before session run\")\n",
        "    predicts = sess.run(predict_rating, feed_dict={users: user_batch, items: item_batch, users_inputs: user_input_batch, items_inputs: item_input_batch, dropout_rate:1.0})\n",
        "    print(\"after session run\")\n",
        "    #print(predicts)\n",
        "    row, col = predicts.shape\n",
        "    for r in range(row):\n",
        "        rmses.append(pow((predicts[r, 0] - rate_tests[r][0]), 2))\n",
        "        maes.append(abs((predicts[r, 0] - rate_tests[r][0])))\n",
        "    print(rmses)\n",
        "    return rmses, maes\n"
      ],
      "id": "6967e9a6"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fvDJvR-tr2Ap"
      },
      "source": [
        "## Just interaction based"
      ],
      "id": "fvDJvR-tr2Ap"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "55f18043"
      },
      "outputs": [],
      "source": [
        "#combination of interaction.py and review.py\n",
        "def train_model():\n",
        "    mae_list=[]\n",
        "    mse_list=[]\n",
        "    tf.compat.v1.disable_eager_execution()\n",
        "    #I and R-----\n",
        "    users = tf.compat.v1.placeholder(tf.int32, shape=[None])\n",
        "    items = tf.compat.v1.placeholder(tf.int32, shape=[None])\n",
        "    users_inputs = tf.compat.v1.placeholder(tf.int32, shape=[None, max_doc_length])\n",
        "    items_inputs = tf.compat.v1.placeholder(tf.int32, shape=[None, max_doc_length])\n",
        "    ratings = tf.compat.v1.placeholder(tf.float32, shape=[None, 1])\n",
        "    \n",
        "    #R-----\n",
        "    dropout_rate = tf.compat.v1.placeholder(tf.float32)\n",
        "    \n",
        "    #I-----\n",
        "    user_entity_embedding = tf.Variable(tf.random.normal([num_users, latent_dim], mean=0, stddev=0.02), name=\"user_entity_embeddings\")\n",
        "    item_entity_embedding = tf.Variable(tf.random.normal([num_items, latent_dim], mean=0, stddev=0.02), name=\"item_entity_embeddings\")\n",
        "    user_entity_embeds = tf.nn.embedding_lookup(user_entity_embedding, users)\n",
        "    item_entity_embeds = tf.nn.embedding_lookup(item_entity_embedding, items)\n",
        "    \n",
        "    #R-----\n",
        "    user_bias = tf.Variable(tf.random.normal([num_users, 1], mean=0, stddev=0.02), name=\"user_bias\")\n",
        "    item_bias = tf.Variable(tf.random.normal([num_items, 1], mean=0, stddev=0.02), name=\"item_bias\")\n",
        "    user_bs = tf.nn.embedding_lookup(user_bias, users)\n",
        "    item_bs = tf.nn.embedding_lookup(item_bias, items)\n",
        "    \n",
        "    text_embedding = tf.Variable(word_embedding_mtrx, dtype=tf.float32, name=\"review_text_embeds\")\n",
        "    padding_embedding = tf.Variable(np.zeros([1, word_latent_dim]), dtype=tf.float32)\n",
        "    text_mask = tf.constant([1.0] * text_embedding.get_shape()[0] + [0.0])\n",
        "    word_embeddings = tf.concat([text_embedding, padding_embedding], 0)\n",
        "    word_embeddings = word_embeddings * tf.expand_dims(text_mask, -1)\n",
        "    user_reviews_representation = tf.nn.embedding_lookup(word_embeddings, users_inputs)\n",
        "    user_reviews_representation_expnd = tf.expand_dims(user_reviews_representation, -1)\n",
        "    item_reviews_representation = tf.nn.embedding_lookup(word_embeddings, items_inputs)\n",
        "    item_reviews_representation_expnd = tf.expand_dims(item_reviews_representation, -1)\n",
        "    \n",
        "    # CNN layers\n",
        "    W_u = tf.Variable(tf.random.truncated_normal([window_size, word_latent_dim, 1, num_filters], stddev=0.3), name=\"review_W_u\")\n",
        "    W_i = tf.Variable(tf.random.truncated_normal([window_size, word_latent_dim, 1, num_filters], stddev=0.3), name=\"review_W_i\")\n",
        "    W_u_1 = tf.Variable(tf.random.truncated_normal([window_size, num_filters, 1, num_filters], stddev=0.3), name=\"review_W_u_1\")\n",
        "    W_i_1 = tf.Variable(tf.random.truncated_normal([window_size, num_filters, 1, num_filters], stddev=0.3), name=\"review_W_i_1\")\n",
        "    # b = tf.Variable(tf.constant(0.1, shape=[self.num_filters]))\n",
        "    rand_matrix = tf.Variable(tf.random.truncated_normal([num_filters, num_filters], stddev=0.3), name=\"review_rand_matrix\")\n",
        "\n",
        "    user_embeds, item_embeds = cnn_model_average(num_filters, user_reviews_representation_expnd, item_reviews_representation_expnd, W_u, W_i, W_u_1, W_i_1, rand_matrix)\n",
        "    \n",
        "    #shared MLP layer\n",
        "    W_mlp = tf.Variable(tf.random.normal([num_filters, latent_dim], mean=0, stddev=0.02), name=\"review_W_mlp\")\n",
        "    W_mlp = tf.nn.dropout(W_mlp, dropout_rate)\n",
        "    b_mlp = tf.Variable(tf.constant(0.1, shape=[latent_dim]), name=\"review_b_mlp\")\n",
        "\n",
        "    user_embeds = tf.nn.relu(tf.matmul(user_embeds, W_mlp) + b_mlp)\n",
        "    item_embeds = tf.nn.relu(tf.matmul(item_embeds, W_mlp) + b_mlp)\n",
        "\n",
        "    embeds_sum = tf.concat([tf.multiply(user_embeds, item_embeds), user_embeds, item_embeds], 1, name=\"concat_embed\")\n",
        "    #I\n",
        "    entity_embeds_sum = tf.concat([tf.multiply(user_entity_embeds, item_entity_embeds), user_entity_embeds, item_entity_embeds],1)\n",
        "\n",
        "    #FM layer\n",
        "    #R---\n",
        "    # w_0 = tf.Variable(tf.zeros(1), name=\"review_w_0\")\n",
        "    # w_1 = tf.Variable(tf.random.truncated_normal([1, latent_dim * 3], stddev=0.3), name=\"review_w_1\")\n",
        "    # v = tf.Variable(tf.random.truncated_normal([latent_dim * 3, latent_dim * 3], stddev=0.3), name=\"review_v\")\n",
        "    \n",
        "    #I---\n",
        "    w_entity_0 = tf.Variable(tf.zeros(1), name=\"entity_w_0\")\n",
        "    w_entity_1 = tf.Variable(tf.random.truncated_normal([1, latent_dim*3], stddev=0.3), name=\"entity_w_1\")\n",
        "    v_entity = tf.Variable(tf.random.truncated_normal([latent_dim*3, v_dim], stddev=0.3), name=\"entity_v\")\n",
        "    \n",
        "    #I---\n",
        "    J_e_1 = w_entity_0 + tf.matmul(entity_embeds_sum, w_entity_1, transpose_b=True)\n",
        "    #R---\n",
        "    # J_1 = w_0 + tf.matmul(embeds_sum, w_1, transpose_b=True)\n",
        "    \n",
        "    #I---\n",
        "    entity_embeds_sum_1 = tf.expand_dims(entity_embeds_sum, -1)\n",
        "    entity_embeds_sum_2 = tf.expand_dims(entity_embeds_sum, 1)\n",
        "    #R---\n",
        "    # embeds_sum_1 = tf.expand_dims(embeds_sum, -1)\n",
        "    # embeds_sum_2 = tf.expand_dims(embeds_sum, 1)\n",
        "    \n",
        "    #I---\n",
        "    J_e_2 = tf.reduce_sum(tf.reduce_sum(tf.multiply(tf.matmul(entity_embeds_sum_1, entity_embeds_sum_2), tf.matmul(v_entity, v_entity, transpose_b=True)),2), 1, keepdims=True)\n",
        "    #J_e_3 = tf.trace(tf.multiply(tf.matmul(entity_embeds_sum_1, entity_embeds_sum_2), tf.matmul(v_entity, v_entity, transpose_b=True))) \n",
        "    \n",
        "    #R---\n",
        "    # J_2 = tf.reduce_sum(tf.reduce_sum(tf.multiply(tf.matmul(embeds_sum_1, embeds_sum_2), tf.matmul(v, v, transpose_b=True)),2), 1, keepdims=True)\n",
        "    #J_3 = tf.trace(tf.multiply(tf.matmul(embeds_sum_1, embeds_sum_2), tf.matmul(v, v, transpose_b=True)))\n",
        "    #R---\n",
        "    # J_total = (J_1 + 0.5 * (J_2))\n",
        "    \n",
        "    #I---\n",
        "    J_e_total = (J_e_1 + 0.5 * (J_e_2))\n",
        "    \n",
        "    \n",
        "    # numerator = J_total + J_e_total\n",
        "    numerator = J_e_total\n",
        "    # predict_rating = tf.divide(J_total, numerator) * J_total + tf.divide(J_e_total,numerator) * J_e_total + user_bs + item_bs\n",
        "    predict_rating = tf.divide(J_e_total,numerator) * J_e_total + user_bs + item_bs \n",
        "    loss = tf.reduce_mean(tf.math.squared_difference(predict_rating, ratings))\n",
        "    loss += lambda_1 * (tf.nn.l2_loss(W_i_1) + tf.nn.l2_loss(W_u_1) + tf.nn.l2_loss(user_entity_embedding) + tf.nn.l2_loss(item_entity_embedding) + tf.nn.l2_loss(W_u) + tf.nn.l2_loss(W_i) + tf.nn.l2_loss(v_entity) + tf.nn.l2_loss(rand_matrix) + tf.nn.l2_loss(user_bs) + tf.nn.l2_loss(item_bs))\n",
        "    train_step = tf.compat.v1.train.RMSPropOptimizer(learning_rate).minimize(loss)\n",
        "    saver = tf.compat.v1.train.Saver()\n",
        "    \n",
        "    init = tf.compat.v1.global_variables_initializer()\n",
        "    with tf.compat.v1.Session() as sess:\n",
        "        sess.run(init)\n",
        "\n",
        "        for e in range(epochs):\n",
        "            t = time()\n",
        "            loss_total = 0.0\n",
        "            count = 0.0\n",
        "            \n",
        "            for i in range(int(math.ceil(len(user_input) / float(batch_size)))):\n",
        "                user_batch, item_batch, user_input_batch, item_input_batch, rates_batch = get_train_instance_batch_change(i, batch_size,user_input, item_input, rateings, user_reviews,item_reviews)\n",
        "                _, loss_val, words = sess.run([train_step, loss, word_embeddings], feed_dict={users: user_batch, items: item_batch, users_inputs: user_input_batch, items_inputs: item_input_batch, ratings: rates_batch, dropout_rate:drop_out})\n",
        "                loss_total += loss_val\n",
        "                count += 1.0\n",
        "\n",
        "            t1 = time()            \n",
        "            #LOOK FROM HERE \n",
        "            val_mses, val_maes = [], []\n",
        "            for i in range(len(user_input_val)):\n",
        "                #eval_model(users, items, users_inputs, items_inputs, dropout_rate, predict_rating, sess, user_vals[i], item_vals[i], user_input_val[i], item_input_val[i], rating_input_val[i], val_mses, val_maes)\n",
        "                #predicts = sess.run(predict_rating, feed_dict={users: user_batch, items: item_batch, users_inputs: user_input_batch, items_inputs: item_input_batch, dropout_rate:1.0})\n",
        "                predicts = sess.run(predict_rating, feed_dict={users: user_vals[i], items: item_vals[i], users_inputs: user_input_val[i], items_inputs: item_input_val[i], ratings: rating_input_val[i], dropout_rate: drop_out})\n",
        "                #print(predicts)\n",
        "                row, col = predicts.shape\n",
        "                for r in range(row):\n",
        "                    val_mses.append(pow((predicts[r, 0] - rating_input_val[i][r][0]), 2))\n",
        "                    val_maes.append(abs((predicts[r, 0] - rating_input_val[i][r][0])))\n",
        "            val_mse = np.array(val_mses).mean()            \n",
        "            t2 = time()            \n",
        "            mses, maes = [], []\n",
        "            for i in range(len(user_input_test)):\n",
        "                #eval_model(users, items, users_inputs, items_inputs, dropout_rate, predict_rating, sess, user_tests[i], item_tests[i], user_input_test[i], item_input_test[i], rating_input_test[i], mses, maes)\n",
        "                predicts_test = sess.run(predict_rating, feed_dict={users: user_tests[i], items: item_tests[i], users_inputs: user_input_test[i], items_inputs: item_input_test[i], ratings: rating_input_test[i], dropout_rate: drop_out})\n",
        "                #print(predicts)\n",
        "                row, col = predicts_test.shape\n",
        "                for r in range(row):\n",
        "                    mses.append(pow((predicts_test[r, 0] - rating_input_test[i][r][0]), 2))\n",
        "                    maes.append(abs((predicts_test[r, 0] - rating_input_test[i][r][0])))\n",
        "                    \n",
        "            mse = np.array(mses).mean()\n",
        "            mae = np.array(maes).mean()\n",
        "            t3 = time()\n",
        "            mae_list.append(mae)\n",
        "            mse_list.append(mse)\n",
        "            print(\"epoch%d train time: %.3fs test time: %.3f  loss = %.3f val_mse = %.3f mse = %.3f mae = %.3f\"%(e, (t1 - t), (t3 - t2), loss_total/count, val_mse, mse, mae))\n",
        "        \n",
        "        avg_mae=sum(mae_list) / len(mae_list)\n",
        "        avg_mse=sum(mse_list)/len(mse_list)\n",
        "        avg_mae_list.append(avg_mae)\n",
        "        avg_mse_list.append(avg_mse)\n",
        "        print(\"MAE \"+str(avg_mae))\n",
        "        print(\"MSE \"+str(avg_mse))"
      ],
      "id": "55f18043"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a924fe38",
        "outputId": "797c542f-c8be-4961-92a6-1d6ddb2a6c49"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "wordId_dict finished\n",
            "load reviews finished\n",
            "load data: 2.044s\n",
            "506 2581\n",
            "15\n",
            "shape (19930, 300)\n",
            "18880 18880 18880\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/training/rmsprop.py:188: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "epoch0 train time: 3.969s test time: 0.456  loss = 199.018 val_mse = 15.116 mse = 15.267 mae = 3.719\n",
            "epoch1 train time: 2.377s test time: 0.452  loss = 132.983 val_mse = 13.845 mse = 14.038 mae = 3.559\n",
            "epoch2 train time: 2.507s test time: 0.499  loss = 70.851 val_mse = 12.473 mse = 12.710 mae = 3.375\n",
            "epoch3 train time: 2.412s test time: 0.483  loss = 36.328 val_mse = 10.469 mse = 10.794 mae = 3.069\n",
            "epoch4 train time: 2.478s test time: 0.561  loss = 18.892 val_mse = 7.892 mse = 8.399 mae = 2.606\n",
            "epoch5 train time: 2.401s test time: 0.499  loss = 11.255 val_mse = 5.982 mse = 6.626 mae = 2.230\n",
            "epoch6 train time: 2.151s test time: 0.381  loss = 8.543 val_mse = 4.777 mse = 5.485 mae = 1.987\n",
            "epoch7 train time: 2.085s test time: 0.518  loss = 7.537 val_mse = 4.014 mse = 4.730 mae = 1.830\n",
            "epoch8 train time: 2.672s test time: 0.463  loss = 6.893 val_mse = 3.547 mse = 4.235 mae = 1.729\n",
            "epoch9 train time: 2.166s test time: 0.365  loss = 6.389 val_mse = 3.266 mse = 3.907 mae = 1.664\n",
            "epoch10 train time: 1.983s test time: 0.518  loss = 5.958 val_mse = 3.100 mse = 3.684 mae = 1.622\n",
            "epoch11 train time: 1.858s test time: 0.381  loss = 5.563 val_mse = 3.003 mse = 3.526 mae = 1.595\n",
            "epoch12 train time: 1.971s test time: 0.587  loss = 5.181 val_mse = 2.944 mse = 3.406 mae = 1.577\n",
            "epoch13 train time: 1.995s test time: 0.377  loss = 4.801 val_mse = 2.903 mse = 3.306 mae = 1.563\n",
            "epoch14 train time: 2.038s test time: 0.203  loss = 4.418 val_mse = 2.862 mse = 3.209 mae = 1.549\n",
            "epoch15 train time: 1.211s test time: 0.188  loss = 4.030 val_mse = 2.795 mse = 3.094 mae = 1.527\n",
            "epoch16 train time: 1.219s test time: 0.207  loss = 3.648 val_mse = 2.662 mse = 2.925 mae = 1.483\n",
            "epoch17 train time: 1.218s test time: 0.214  loss = 3.295 val_mse = 2.471 mse = 2.707 mae = 1.420\n",
            "epoch18 train time: 1.225s test time: 0.197  loss = 2.984 val_mse = 2.277 mse = 2.492 mae = 1.353\n",
            "epoch19 train time: 1.218s test time: 0.193  loss = 2.710 val_mse = 2.106 mse = 2.301 mae = 1.291\n",
            "epoch20 train time: 1.229s test time: 0.190  loss = 2.468 val_mse = 1.958 mse = 2.135 mae = 1.236\n",
            "epoch21 train time: 1.242s test time: 0.194  loss = 2.256 val_mse = 1.832 mse = 1.993 mae = 1.186\n",
            "epoch22 train time: 1.226s test time: 0.191  loss = 2.071 val_mse = 1.724 mse = 1.871 mae = 1.141\n",
            "epoch23 train time: 1.221s test time: 0.195  loss = 1.909 val_mse = 1.634 mse = 1.767 mae = 1.100\n",
            "epoch24 train time: 1.207s test time: 0.194  loss = 1.767 val_mse = 1.557 mse = 1.679 mae = 1.064\n",
            "epoch25 train time: 1.234s test time: 0.198  loss = 1.643 val_mse = 1.494 mse = 1.605 mae = 1.032\n",
            "epoch26 train time: 1.231s test time: 0.195  loss = 1.535 val_mse = 1.441 mse = 1.543 mae = 1.004\n",
            "epoch27 train time: 1.231s test time: 0.202  loss = 1.440 val_mse = 1.398 mse = 1.491 mae = 0.979\n",
            "epoch28 train time: 1.219s test time: 0.192  loss = 1.358 val_mse = 1.363 mse = 1.449 mae = 0.958\n",
            "epoch29 train time: 1.212s test time: 0.205  loss = 1.287 val_mse = 1.334 mse = 1.414 mae = 0.939\n",
            "epoch30 train time: 1.225s test time: 0.190  loss = 1.225 val_mse = 1.311 mse = 1.386 mae = 0.922\n",
            "epoch31 train time: 1.215s test time: 0.204  loss = 1.171 val_mse = 1.293 mse = 1.362 mae = 0.907\n",
            "epoch32 train time: 1.237s test time: 0.191  loss = 1.126 val_mse = 1.279 mse = 1.346 mae = 0.895\n",
            "epoch33 train time: 1.228s test time: 0.201  loss = 1.086 val_mse = 1.270 mse = 1.333 mae = 0.884\n",
            "epoch34 train time: 1.220s test time: 0.189  loss = 1.055 val_mse = 1.263 mse = 1.323 mae = 0.874\n",
            "epoch35 train time: 1.193s test time: 0.192  loss = 1.027 val_mse = 1.259 mse = 1.317 mae = 0.866\n",
            "epoch36 train time: 1.222s test time: 0.190  loss = 1.006 val_mse = 1.257 mse = 1.314 mae = 0.860\n",
            "epoch37 train time: 1.215s test time: 0.197  loss = 0.988 val_mse = 1.254 mse = 1.310 mae = 0.854\n",
            "epoch38 train time: 1.226s test time: 0.190  loss = 0.974 val_mse = 1.253 mse = 1.309 mae = 0.851\n",
            "epoch39 train time: 1.232s test time: 0.195  loss = 0.963 val_mse = 1.252 mse = 1.307 mae = 0.847\n",
            "epoch40 train time: 1.220s test time: 0.190  loss = 0.954 val_mse = 1.251 mse = 1.308 mae = 0.845\n",
            "epoch41 train time: 1.224s test time: 0.189  loss = 0.947 val_mse = 1.248 mse = 1.305 mae = 0.843\n",
            "epoch42 train time: 1.219s test time: 0.203  loss = 0.942 val_mse = 1.243 mse = 1.301 mae = 0.842\n",
            "epoch43 train time: 1.214s test time: 0.191  loss = 0.937 val_mse = 1.250 mse = 1.307 mae = 0.841\n",
            "epoch44 train time: 1.229s test time: 0.202  loss = 0.932 val_mse = 1.252 mse = 1.309 mae = 0.841\n",
            "epoch45 train time: 1.211s test time: 0.190  loss = 0.929 val_mse = 1.248 mse = 1.305 mae = 0.839\n",
            "epoch46 train time: 1.226s test time: 0.191  loss = 0.927 val_mse = 1.244 mse = 1.302 mae = 0.839\n",
            "epoch47 train time: 1.214s test time: 0.191  loss = 0.925 val_mse = 1.242 mse = 1.301 mae = 0.838\n",
            "epoch48 train time: 1.210s test time: 0.192  loss = 0.921 val_mse = 1.248 mse = 1.305 mae = 0.837\n",
            "epoch49 train time: 1.222s test time: 0.186  loss = 0.920 val_mse = 1.242 mse = 1.300 mae = 0.837\n",
            "epoch50 train time: 1.228s test time: 0.190  loss = 0.918 val_mse = 1.240 mse = 1.299 mae = 0.836\n",
            "epoch51 train time: 1.223s test time: 0.193  loss = 0.916 val_mse = 1.240 mse = 1.300 mae = 0.836\n",
            "epoch52 train time: 1.210s test time: 0.190  loss = 0.914 val_mse = 1.239 mse = 1.298 mae = 0.835\n",
            "epoch53 train time: 1.232s test time: 0.216  loss = 0.913 val_mse = 1.237 mse = 1.298 mae = 0.835\n",
            "epoch54 train time: 1.216s test time: 0.193  loss = 0.910 val_mse = 1.244 mse = 1.303 mae = 0.834\n",
            "epoch55 train time: 1.211s test time: 0.206  loss = 0.909 val_mse = 1.238 mse = 1.299 mae = 0.834\n",
            "epoch56 train time: 1.216s test time: 0.188  loss = 0.909 val_mse = 1.236 mse = 1.295 mae = 0.832\n",
            "epoch57 train time: 1.199s test time: 0.210  loss = 0.907 val_mse = 1.235 mse = 1.297 mae = 0.833\n",
            "epoch58 train time: 1.226s test time: 0.188  loss = 0.906 val_mse = 1.235 mse = 1.294 mae = 0.831\n",
            "epoch59 train time: 1.215s test time: 0.191  loss = 0.904 val_mse = 1.234 mse = 1.296 mae = 0.833\n",
            "epoch60 train time: 1.245s test time: 0.188  loss = 0.903 val_mse = 1.234 mse = 1.293 mae = 0.830\n",
            "epoch61 train time: 1.214s test time: 0.194  loss = 0.903 val_mse = 1.233 mse = 1.296 mae = 0.832\n",
            "epoch62 train time: 1.228s test time: 0.199  loss = 0.902 val_mse = 1.230 mse = 1.290 mae = 0.830\n",
            "epoch63 train time: 1.732s test time: 0.376  loss = 0.900 val_mse = 1.237 mse = 1.299 mae = 0.831\n",
            "epoch64 train time: 1.682s test time: 0.197  loss = 0.900 val_mse = 1.231 mse = 1.292 mae = 0.829\n",
            "epoch65 train time: 1.203s test time: 0.192  loss = 0.898 val_mse = 1.233 mse = 1.296 mae = 0.831\n",
            "epoch66 train time: 1.233s test time: 0.208  loss = 0.901 val_mse = 1.227 mse = 1.289 mae = 0.828\n",
            "epoch67 train time: 1.220s test time: 0.193  loss = 0.897 val_mse = 1.229 mse = 1.294 mae = 0.832\n",
            "epoch68 train time: 1.210s test time: 0.201  loss = 0.899 val_mse = 1.228 mse = 1.289 mae = 0.829\n",
            "epoch69 train time: 1.205s test time: 0.206  loss = 0.896 val_mse = 1.228 mse = 1.292 mae = 0.831\n",
            "epoch70 train time: 1.218s test time: 0.195  loss = 0.898 val_mse = 1.228 mse = 1.290 mae = 0.828\n",
            "epoch71 train time: 1.235s test time: 0.203  loss = 0.895 val_mse = 1.223 mse = 1.288 mae = 0.831\n",
            "epoch72 train time: 1.205s test time: 0.195  loss = 0.896 val_mse = 1.228 mse = 1.290 mae = 0.828\n",
            "epoch73 train time: 1.213s test time: 0.191  loss = 0.896 val_mse = 1.219 mse = 1.286 mae = 0.830\n",
            "epoch74 train time: 1.212s test time: 0.193  loss = 0.895 val_mse = 1.228 mse = 1.289 mae = 0.828\n",
            "epoch75 train time: 1.233s test time: 0.196  loss = 0.893 val_mse = 1.222 mse = 1.289 mae = 0.831\n",
            "epoch76 train time: 1.220s test time: 0.194  loss = 0.893 val_mse = 1.227 mse = 1.290 mae = 0.828\n",
            "epoch77 train time: 1.205s test time: 0.194  loss = 0.894 val_mse = 1.217 mse = 1.286 mae = 0.831\n",
            "epoch78 train time: 1.214s test time: 0.195  loss = 0.893 val_mse = 1.226 mse = 1.290 mae = 0.828\n",
            "epoch79 train time: 1.205s test time: 0.194  loss = 0.892 val_mse = 1.216 mse = 1.285 mae = 0.831\n",
            "epoch80 train time: 1.234s test time: 0.195  loss = 0.893 val_mse = 1.224 mse = 1.289 mae = 0.827\n",
            "epoch81 train time: 1.202s test time: 0.188  loss = 0.891 val_mse = 1.216 mse = 1.288 mae = 0.832\n",
            "epoch82 train time: 1.197s test time: 0.206  loss = 0.891 val_mse = 1.226 mse = 1.290 mae = 0.827\n",
            "epoch83 train time: 1.214s test time: 0.196  loss = 0.891 val_mse = 1.213 mse = 1.285 mae = 0.830\n",
            "epoch84 train time: 1.210s test time: 0.203  loss = 0.890 val_mse = 1.227 mse = 1.290 mae = 0.827\n",
            "epoch85 train time: 1.213s test time: 0.199  loss = 0.890 val_mse = 1.215 mse = 1.286 mae = 0.831\n",
            "epoch86 train time: 1.219s test time: 0.191  loss = 0.891 val_mse = 1.223 mse = 1.289 mae = 0.826\n",
            "epoch87 train time: 1.236s test time: 0.190  loss = 0.889 val_mse = 1.214 mse = 1.287 mae = 0.831\n",
            "epoch88 train time: 1.208s test time: 0.191  loss = 0.888 val_mse = 1.226 mse = 1.290 mae = 0.827\n",
            "epoch89 train time: 1.224s test time: 0.194  loss = 0.889 val_mse = 1.211 mse = 1.286 mae = 0.831\n",
            "epoch90 train time: 1.201s test time: 0.194  loss = 0.888 val_mse = 1.226 mse = 1.290 mae = 0.827\n",
            "epoch91 train time: 1.209s test time: 0.193  loss = 0.888 val_mse = 1.211 mse = 1.285 mae = 0.831\n",
            "epoch92 train time: 1.224s test time: 0.196  loss = 0.889 val_mse = 1.224 mse = 1.288 mae = 0.826\n",
            "epoch93 train time: 1.240s test time: 0.204  loss = 0.886 val_mse = 1.212 mse = 1.286 mae = 0.830\n",
            "epoch94 train time: 1.225s test time: 0.198  loss = 0.886 val_mse = 1.225 mse = 1.289 mae = 0.827\n",
            "epoch95 train time: 1.207s test time: 0.204  loss = 0.887 val_mse = 1.208 mse = 1.283 mae = 0.829\n",
            "epoch96 train time: 1.204s test time: 0.193  loss = 0.887 val_mse = 1.223 mse = 1.288 mae = 0.826\n",
            "epoch97 train time: 1.227s test time: 0.207  loss = 0.885 val_mse = 1.210 mse = 1.286 mae = 0.829\n",
            "epoch98 train time: 1.218s test time: 0.187  loss = 0.884 val_mse = 1.225 mse = 1.290 mae = 0.828\n",
            "epoch99 train time: 1.202s test time: 0.193  loss = 0.886 val_mse = 1.208 mse = 1.283 mae = 0.829\n",
            "epoch100 train time: 1.222s test time: 0.191  loss = 0.884 val_mse = 1.227 mse = 1.289 mae = 0.826\n",
            "epoch101 train time: 1.240s test time: 0.190  loss = 0.884 val_mse = 1.205 mse = 1.285 mae = 0.830\n",
            "epoch102 train time: 1.240s test time: 0.188  loss = 0.883 val_mse = 1.224 mse = 1.288 mae = 0.826\n",
            "epoch103 train time: 1.205s test time: 0.193  loss = 0.884 val_mse = 1.204 mse = 1.282 mae = 0.829\n",
            "epoch104 train time: 1.211s test time: 0.193  loss = 0.882 val_mse = 1.226 mse = 1.289 mae = 0.826\n",
            "epoch105 train time: 1.211s test time: 0.190  loss = 0.882 val_mse = 1.202 mse = 1.283 mae = 0.830\n",
            "epoch106 train time: 1.228s test time: 0.200  loss = 0.881 val_mse = 1.225 mse = 1.288 mae = 0.827\n",
            "epoch107 train time: 1.213s test time: 0.193  loss = 0.883 val_mse = 1.202 mse = 1.283 mae = 0.829\n",
            "epoch108 train time: 1.213s test time: 0.203  loss = 0.880 val_mse = 1.222 mse = 1.288 mae = 0.826\n",
            "epoch109 train time: 1.215s test time: 0.190  loss = 0.881 val_mse = 1.202 mse = 1.284 mae = 0.828\n",
            "epoch110 train time: 1.215s test time: 0.192  loss = 0.878 val_mse = 1.221 mse = 1.286 mae = 0.826\n",
            "epoch111 train time: 1.239s test time: 0.192  loss = 0.881 val_mse = 1.200 mse = 1.282 mae = 0.828\n",
            "epoch112 train time: 1.225s test time: 0.189  loss = 0.877 val_mse = 1.224 mse = 1.288 mae = 0.828\n",
            "epoch113 train time: 1.222s test time: 0.191  loss = 0.880 val_mse = 1.201 mse = 1.282 mae = 0.828\n",
            "epoch114 train time: 1.209s test time: 0.192  loss = 0.876 val_mse = 1.220 mse = 1.288 mae = 0.826\n",
            "epoch115 train time: 1.245s test time: 0.195  loss = 0.880 val_mse = 1.201 mse = 1.282 mae = 0.827\n",
            "epoch116 train time: 1.221s test time: 0.191  loss = 0.875 val_mse = 1.221 mse = 1.288 mae = 0.827\n",
            "epoch117 train time: 1.226s test time: 0.189  loss = 0.879 val_mse = 1.201 mse = 1.282 mae = 0.828\n",
            "epoch118 train time: 1.203s test time: 0.200  loss = 0.874 val_mse = 1.219 mse = 1.287 mae = 0.826\n",
            "epoch119 train time: 1.211s test time: 0.210  loss = 0.879 val_mse = 1.199 mse = 1.282 mae = 0.827\n",
            "epoch120 train time: 1.205s test time: 0.192  loss = 0.874 val_mse = 1.220 mse = 1.287 mae = 0.827\n",
            "epoch121 train time: 1.195s test time: 0.204  loss = 0.878 val_mse = 1.197 mse = 1.281 mae = 0.829\n",
            "epoch122 train time: 1.221s test time: 0.188  loss = 0.873 val_mse = 1.221 mse = 1.289 mae = 0.826\n",
            "epoch123 train time: 1.203s test time: 0.191  loss = 0.876 val_mse = 1.199 mse = 1.283 mae = 0.828\n",
            "epoch124 train time: 1.227s test time: 0.191  loss = 0.874 val_mse = 1.219 mse = 1.287 mae = 0.827\n",
            "epoch125 train time: 1.191s test time: 0.193  loss = 0.876 val_mse = 1.198 mse = 1.280 mae = 0.827\n",
            "epoch126 train time: 1.222s test time: 0.189  loss = 0.872 val_mse = 1.216 mse = 1.287 mae = 0.826\n",
            "epoch127 train time: 1.209s test time: 0.197  loss = 0.876 val_mse = 1.197 mse = 1.282 mae = 0.827\n",
            "epoch128 train time: 1.207s test time: 0.200  loss = 0.871 val_mse = 1.222 mse = 1.289 mae = 0.828\n",
            "epoch129 train time: 1.220s test time: 0.209  loss = 0.875 val_mse = 1.199 mse = 1.282 mae = 0.828\n",
            "epoch130 train time: 1.216s test time: 0.201  loss = 0.871 val_mse = 1.219 mse = 1.288 mae = 0.826\n",
            "epoch131 train time: 1.217s test time: 0.193  loss = 0.874 val_mse = 1.197 mse = 1.282 mae = 0.828\n",
            "epoch132 train time: 1.205s test time: 0.205  loss = 0.871 val_mse = 1.222 mse = 1.288 mae = 0.829\n",
            "epoch133 train time: 1.227s test time: 0.198  loss = 0.875 val_mse = 1.200 mse = 1.282 mae = 0.829\n",
            "epoch134 train time: 1.232s test time: 0.204  loss = 0.872 val_mse = 1.219 mse = 1.288 mae = 0.827\n",
            "epoch135 train time: 1.207s test time: 0.191  loss = 0.875 val_mse = 1.199 mse = 1.281 mae = 0.828\n",
            "epoch136 train time: 1.219s test time: 0.210  loss = 0.872 val_mse = 1.219 mse = 1.288 mae = 0.828\n",
            "epoch137 train time: 1.214s test time: 0.194  loss = 0.874 val_mse = 1.201 mse = 1.283 mae = 0.830\n",
            "epoch138 train time: 1.205s test time: 0.191  loss = 0.871 val_mse = 1.222 mse = 1.290 mae = 0.828\n",
            "epoch139 train time: 1.240s test time: 0.198  loss = 0.873 val_mse = 1.198 mse = 1.282 mae = 0.828\n",
            "epoch140 train time: 1.224s test time: 0.197  loss = 0.872 val_mse = 1.220 mse = 1.288 mae = 0.828\n",
            "epoch141 train time: 1.212s test time: 0.195  loss = 0.873 val_mse = 1.198 mse = 1.281 mae = 0.828\n",
            "epoch142 train time: 1.222s test time: 0.189  loss = 0.871 val_mse = 1.222 mse = 1.289 mae = 0.827\n",
            "epoch143 train time: 1.214s test time: 0.217  loss = 0.871 val_mse = 1.202 mse = 1.283 mae = 0.830\n",
            "epoch144 train time: 1.213s test time: 0.191  loss = 0.870 val_mse = 1.219 mse = 1.288 mae = 0.828\n",
            "epoch145 train time: 1.210s test time: 0.207  loss = 0.874 val_mse = 1.202 mse = 1.282 mae = 0.828\n",
            "epoch146 train time: 1.212s test time: 0.188  loss = 0.870 val_mse = 1.221 mse = 1.290 mae = 0.829\n",
            "epoch147 train time: 1.232s test time: 0.209  loss = 0.871 val_mse = 1.203 mse = 1.282 mae = 0.828\n",
            "epoch148 train time: 1.216s test time: 0.192  loss = 0.871 val_mse = 1.219 mse = 1.289 mae = 0.828\n",
            "epoch149 train time: 1.210s test time: 0.199  loss = 0.870 val_mse = 1.204 mse = 1.283 mae = 0.830\n",
            "epoch150 train time: 1.206s test time: 0.209  loss = 0.869 val_mse = 1.219 mse = 1.290 mae = 0.828\n",
            "epoch151 train time: 1.230s test time: 0.192  loss = 0.870 val_mse = 1.202 mse = 1.282 mae = 0.828\n",
            "epoch152 train time: 1.227s test time: 0.195  loss = 0.867 val_mse = 1.219 mse = 1.289 mae = 0.829\n",
            "epoch153 train time: 1.196s test time: 0.195  loss = 0.870 val_mse = 1.202 mse = 1.283 mae = 0.828\n",
            "epoch154 train time: 1.211s test time: 0.195  loss = 0.868 val_mse = 1.220 mse = 1.289 mae = 0.827\n",
            "epoch155 train time: 1.199s test time: 0.203  loss = 0.868 val_mse = 1.201 mse = 1.283 mae = 0.829\n",
            "epoch156 train time: 1.200s test time: 0.193  loss = 0.868 val_mse = 1.219 mse = 1.288 mae = 0.828\n",
            "epoch157 train time: 1.215s test time: 0.212  loss = 0.867 val_mse = 1.205 mse = 1.284 mae = 0.829\n",
            "epoch158 train time: 1.206s test time: 0.204  loss = 0.865 val_mse = 1.221 mse = 1.291 mae = 0.829\n",
            "epoch159 train time: 1.206s test time: 0.198  loss = 0.865 val_mse = 1.203 mse = 1.283 mae = 0.828\n",
            "epoch160 train time: 1.220s test time: 0.211  loss = 0.865 val_mse = 1.222 mse = 1.290 mae = 0.830\n",
            "epoch161 train time: 1.220s test time: 0.194  loss = 0.868 val_mse = 1.202 mse = 1.283 mae = 0.829\n",
            "epoch162 train time: 1.211s test time: 0.200  loss = 0.865 val_mse = 1.219 mse = 1.289 mae = 0.827\n",
            "epoch163 train time: 1.210s test time: 0.190  loss = 0.865 val_mse = 1.201 mse = 1.282 mae = 0.827\n",
            "epoch164 train time: 1.232s test time: 0.209  loss = 0.863 val_mse = 1.222 mse = 1.290 mae = 0.830\n",
            "epoch165 train time: 1.219s test time: 0.190  loss = 0.863 val_mse = 1.201 mse = 1.283 mae = 0.828\n",
            "epoch166 train time: 1.199s test time: 0.197  loss = 0.864 val_mse = 1.220 mse = 1.290 mae = 0.828\n",
            "epoch167 train time: 1.219s test time: 0.192  loss = 0.863 val_mse = 1.202 mse = 1.284 mae = 0.829\n",
            "epoch168 train time: 1.215s test time: 0.195  loss = 0.862 val_mse = 1.217 mse = 1.288 mae = 0.828\n",
            "epoch169 train time: 1.222s test time: 0.189  loss = 0.862 val_mse = 1.203 mse = 1.284 mae = 0.828\n",
            "epoch170 train time: 1.198s test time: 0.195  loss = 0.861 val_mse = 1.221 mse = 1.291 mae = 0.828\n",
            "epoch171 train time: 1.202s test time: 0.203  loss = 0.864 val_mse = 1.201 mse = 1.283 mae = 0.827\n",
            "epoch172 train time: 1.237s test time: 0.193  loss = 0.862 val_mse = 1.217 mse = 1.288 mae = 0.828\n",
            "epoch173 train time: 1.232s test time: 0.202  loss = 0.861 val_mse = 1.203 mse = 1.285 mae = 0.830\n",
            "epoch174 train time: 1.209s test time: 0.192  loss = 0.862 val_mse = 1.220 mse = 1.291 mae = 0.827\n",
            "epoch175 train time: 1.202s test time: 0.190  loss = 0.863 val_mse = 1.201 mse = 1.284 mae = 0.828\n",
            "epoch176 train time: 1.225s test time: 0.189  loss = 0.861 val_mse = 1.217 mse = 1.289 mae = 0.829\n",
            "epoch177 train time: 1.214s test time: 0.196  loss = 0.862 val_mse = 1.200 mse = 1.284 mae = 0.827\n",
            "epoch178 train time: 1.236s test time: 0.191  loss = 0.862 val_mse = 1.220 mse = 1.291 mae = 0.828\n",
            "epoch179 train time: 1.243s test time: 0.194  loss = 0.862 val_mse = 1.200 mse = 1.285 mae = 0.829\n",
            "epoch180 train time: 1.220s test time: 0.193  loss = 0.863 val_mse = 1.217 mse = 1.288 mae = 0.828\n",
            "epoch181 train time: 1.201s test time: 0.192  loss = 0.864 val_mse = 1.199 mse = 1.283 mae = 0.828\n",
            "epoch182 train time: 1.226s test time: 0.191  loss = 0.861 val_mse = 1.220 mse = 1.293 mae = 0.829\n",
            "epoch183 train time: 1.206s test time: 0.189  loss = 0.861 val_mse = 1.200 mse = 1.282 mae = 0.828\n",
            "epoch184 train time: 1.226s test time: 0.200  loss = 0.862 val_mse = 1.220 mse = 1.290 mae = 0.829\n",
            "epoch185 train time: 1.215s test time: 0.189  loss = 0.861 val_mse = 1.199 mse = 1.284 mae = 0.828\n",
            "epoch186 train time: 1.215s test time: 0.214  loss = 0.862 val_mse = 1.220 mse = 1.292 mae = 0.827\n",
            "epoch187 train time: 1.225s test time: 0.191  loss = 0.862 val_mse = 1.204 mse = 1.286 mae = 0.829\n",
            "epoch188 train time: 1.220s test time: 0.193  loss = 0.864 val_mse = 1.221 mse = 1.292 mae = 0.830\n",
            "epoch189 train time: 1.221s test time: 0.193  loss = 0.861 val_mse = 1.200 mse = 1.285 mae = 0.828\n",
            "epoch190 train time: 1.211s test time: 0.191  loss = 0.861 val_mse = 1.219 mse = 1.292 mae = 0.828\n",
            "epoch191 train time: 1.237s test time: 0.196  loss = 0.862 val_mse = 1.200 mse = 1.285 mae = 0.828\n",
            "epoch192 train time: 1.233s test time: 0.196  loss = 0.862 val_mse = 1.215 mse = 1.289 mae = 0.827\n",
            "epoch193 train time: 1.224s test time: 0.191  loss = 0.859 val_mse = 1.200 mse = 1.286 mae = 0.829\n",
            "epoch194 train time: 1.201s test time: 0.199  loss = 0.861 val_mse = 1.218 mse = 1.292 mae = 0.828\n",
            "epoch195 train time: 1.213s test time: 0.198  loss = 0.861 val_mse = 1.199 mse = 1.283 mae = 0.828\n",
            "epoch196 train time: 1.237s test time: 0.192  loss = 0.866 val_mse = 1.217 mse = 1.290 mae = 0.829\n",
            "epoch197 train time: 1.214s test time: 0.200  loss = 0.862 val_mse = 1.196 mse = 1.283 mae = 0.827\n",
            "epoch198 train time: 1.213s test time: 0.188  loss = 0.861 val_mse = 1.218 mse = 1.292 mae = 0.827\n",
            "epoch199 train time: 1.208s test time: 0.201  loss = 0.860 val_mse = 1.198 mse = 1.282 mae = 0.828\n",
            "epoch200 train time: 1.246s test time: 0.192  loss = 0.861 val_mse = 1.215 mse = 1.289 mae = 0.828\n",
            "epoch201 train time: 1.206s test time: 0.201  loss = 0.861 val_mse = 1.198 mse = 1.282 mae = 0.827\n",
            "epoch202 train time: 1.198s test time: 0.191  loss = 0.864 val_mse = 1.219 mse = 1.293 mae = 0.828\n",
            "epoch203 train time: 1.216s test time: 0.193  loss = 0.859 val_mse = 1.200 mse = 1.286 mae = 0.828\n",
            "epoch204 train time: 1.230s test time: 0.199  loss = 0.861 val_mse = 1.216 mse = 1.290 mae = 0.829\n",
            "epoch205 train time: 1.219s test time: 0.192  loss = 0.862 val_mse = 1.196 mse = 1.282 mae = 0.828\n",
            "epoch206 train time: 1.222s test time: 0.195  loss = 0.860 val_mse = 1.218 mse = 1.293 mae = 0.829\n",
            "epoch207 train time: 1.229s test time: 0.197  loss = 0.859 val_mse = 1.199 mse = 1.283 mae = 0.828\n",
            "epoch208 train time: 1.223s test time: 0.196  loss = 0.864 val_mse = 1.217 mse = 1.291 mae = 0.829\n",
            "epoch209 train time: 1.219s test time: 0.190  loss = 0.861 val_mse = 1.199 mse = 1.285 mae = 0.828\n",
            "epoch210 train time: 1.204s test time: 0.202  loss = 0.859 val_mse = 1.217 mse = 1.293 mae = 0.828\n",
            "epoch211 train time: 1.199s test time: 0.192  loss = 0.859 val_mse = 1.196 mse = 1.283 mae = 0.829\n",
            "epoch212 train time: 1.214s test time: 0.200  loss = 0.860 val_mse = 1.217 mse = 1.291 mae = 0.829\n",
            "epoch213 train time: 1.208s test time: 0.196  loss = 0.863 val_mse = 1.197 mse = 1.283 mae = 0.828\n",
            "epoch214 train time: 1.211s test time: 0.208  loss = 0.861 val_mse = 1.220 mse = 1.294 mae = 0.828\n",
            "epoch215 train time: 1.217s test time: 0.196  loss = 0.859 val_mse = 1.194 mse = 1.281 mae = 0.827\n",
            "epoch216 train time: 1.220s test time: 0.193  loss = 0.859 val_mse = 1.215 mse = 1.291 mae = 0.829\n",
            "epoch217 train time: 1.228s test time: 0.194  loss = 0.860 val_mse = 1.205 mse = 1.286 mae = 0.830\n",
            "epoch218 train time: 1.213s test time: 0.199  loss = 0.863 val_mse = 1.222 mse = 1.296 mae = 0.828\n",
            "epoch219 train time: 1.222s test time: 0.192  loss = 0.860 val_mse = 1.195 mse = 1.282 mae = 0.828\n",
            "epoch220 train time: 1.232s test time: 0.197  loss = 0.860 val_mse = 1.215 mse = 1.291 mae = 0.828\n",
            "epoch221 train time: 1.239s test time: 0.193  loss = 0.858 val_mse = 1.195 mse = 1.283 mae = 0.829\n",
            "epoch222 train time: 1.231s test time: 0.205  loss = 0.859 val_mse = 1.219 mse = 1.294 mae = 0.828\n",
            "epoch223 train time: 1.212s test time: 0.192  loss = 0.860 val_mse = 1.201 mse = 1.284 mae = 0.829\n",
            "epoch224 train time: 1.215s test time: 0.194  loss = 0.862 val_mse = 1.216 mse = 1.291 mae = 0.829\n",
            "epoch225 train time: 2.075s test time: 0.384  loss = 0.859 val_mse = 1.195 mse = 1.282 mae = 0.827\n",
            "epoch226 train time: 2.377s test time: 0.635  loss = 0.860 val_mse = 1.219 mse = 1.293 mae = 0.829\n",
            "epoch227 train time: 2.349s test time: 0.474  loss = 0.857 val_mse = 1.197 mse = 1.283 mae = 0.828\n",
            "epoch228 train time: 2.922s test time: 0.510  loss = 0.857 val_mse = 1.215 mse = 1.291 mae = 0.829\n",
            "epoch229 train time: 2.648s test time: 0.527  loss = 0.858 val_mse = 1.200 mse = 1.285 mae = 0.829\n",
            "epoch230 train time: 2.305s test time: 0.535  loss = 0.862 val_mse = 1.220 mse = 1.295 mae = 0.829\n",
            "epoch231 train time: 2.602s test time: 0.416  loss = 0.858 val_mse = 1.194 mse = 1.281 mae = 0.827\n",
            "epoch232 train time: 2.328s test time: 0.387  loss = 0.858 val_mse = 1.218 mse = 1.291 mae = 0.830\n",
            "epoch233 train time: 2.055s test time: 0.394  loss = 0.856 val_mse = 1.195 mse = 1.284 mae = 0.829\n",
            "epoch234 train time: 2.047s test time: 0.381  loss = 0.857 val_mse = 1.218 mse = 1.293 mae = 0.828\n",
            "epoch235 train time: 1.993s test time: 0.362  loss = 0.859 val_mse = 1.196 mse = 1.283 mae = 0.828\n",
            "epoch236 train time: 1.420s test time: 0.200  loss = 0.856 val_mse = 1.215 mse = 1.290 mae = 0.830\n",
            "epoch237 train time: 1.217s test time: 0.189  loss = 0.857 val_mse = 1.198 mse = 1.284 mae = 0.828\n",
            "epoch238 train time: 1.198s test time: 0.208  loss = 0.858 val_mse = 1.220 mse = 1.294 mae = 0.829\n",
            "epoch239 train time: 1.202s test time: 0.192  loss = 0.856 val_mse = 1.200 mse = 1.285 mae = 0.831\n",
            "MAE 0.9416255216488819\n",
            "MSE 1.6866763914333192\n"
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    print(sys.path[0])\n",
        "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
        "    word_latent_dim = 300\n",
        "    latent_dim = 15\n",
        "    max_doc_length = 300\n",
        "    num_filters = 50\n",
        "    window_size = 3\n",
        "    v_dim = 50\n",
        "    learning_rate = 0.001\n",
        "    lambda_1 = 0.05 #normally we set from [0.05, 0.01, 0.005, 0.001]\n",
        "    drop_out = 0.8\n",
        "    batch_size = 200\n",
        "    epochs = 180\n",
        "    avg_mae_list = []\n",
        "    avg_mse_list = []\n",
        "    \n",
        "    # loading data\n",
        "    firTime = time()\n",
        "    dataSet = Dataset(max_doc_length, sys.path[0], \"dataPreprocessingWordDict.out\")\n",
        "    word_dict, user_reviews, item_reviews, train, valRatings, testRatings = dataSet.word_id_dict, dataSet.userReview_dict, dataSet.itemReview_dict, dataSet.trainMtrx, dataSet.valRatings, dataSet.testRatings\n",
        "    secTime = time()\n",
        "\n",
        "    num_users, num_items = train.shape\n",
        "    print(\"load data: %.3fs\" % (secTime - firTime))\n",
        "    print(num_users, num_items)\n",
        "    print(latent_dim)\n",
        "\n",
        "    #load word embeddings\n",
        "    word_embedding_mtrx = ini_word_embed(len(word_dict), word_latent_dim)\n",
        "    # word_embedding_mtrx = word2vec_word_embed(len(word_dict), word_latent_dim,\n",
        "    #                                           \"Directory of pretrained WordEmbedding.out\",\n",
        "    #                                           word_dict)\n",
        "\n",
        "    print( \"shape\", word_embedding_mtrx.shape)\n",
        "\n",
        "    # get train instances\n",
        "    user_input, item_input, rateings = get_train_instance(train)\n",
        "    print (len(user_input), len(item_input), len(rateings))\n",
        "\n",
        "    # get test/val instances\n",
        "    user_vals, item_vals, user_input_val, item_input_val, rating_input_val = get_test_list(200, valRatings, user_reviews, item_reviews)\n",
        "    user_tests, item_tests, user_input_test, item_input_test, rating_input_test = get_test_list(200, testRatings, user_reviews, item_reviews)\n",
        "\n",
        "    #train & eval model\n",
        "    train_model()"
      ],
      "id": "a924fe38"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oXgncTlIrwjP"
      },
      "source": [
        "## Just review based"
      ],
      "id": "oXgncTlIrwjP"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nZI3Juflqt2L"
      },
      "outputs": [],
      "source": [
        "#combination of interaction.py and review.py\n",
        "def train_model():\n",
        "    mae_list=[]\n",
        "    mse_list=[]\n",
        "    tf.compat.v1.disable_eager_execution()\n",
        "    #I and R-----\n",
        "    users = tf.compat.v1.placeholder(tf.int32, shape=[None])\n",
        "    items = tf.compat.v1.placeholder(tf.int32, shape=[None])\n",
        "    users_inputs = tf.compat.v1.placeholder(tf.int32, shape=[None, max_doc_length])\n",
        "    items_inputs = tf.compat.v1.placeholder(tf.int32, shape=[None, max_doc_length])\n",
        "    ratings = tf.compat.v1.placeholder(tf.float32, shape=[None, 1])\n",
        "    \n",
        "    #R-----\n",
        "    dropout_rate = tf.compat.v1.placeholder(tf.float32)\n",
        "    \n",
        "    #I-----\n",
        "    # user_entity_embedding = tf.Variable(tf.random.normal([num_users, latent_dim], mean=0, stddev=0.02), name=\"user_entity_embeddings\")\n",
        "    # item_entity_embedding = tf.Variable(tf.random.normal([num_items, latent_dim], mean=0, stddev=0.02), name=\"item_entity_embeddings\")\n",
        "    # user_entity_embeds = tf.nn.embedding_lookup(user_entity_embedding, users)\n",
        "    # item_entity_embeds = tf.nn.embedding_lookup(item_entity_embedding, items)\n",
        "    \n",
        "    #R-----\n",
        "    user_bias = tf.Variable(tf.random.normal([num_users, 1], mean=0, stddev=0.02), name=\"user_bias\")\n",
        "    item_bias = tf.Variable(tf.random.normal([num_items, 1], mean=0, stddev=0.02), name=\"item_bias\")\n",
        "    user_bs = tf.nn.embedding_lookup(user_bias, users)\n",
        "    item_bs = tf.nn.embedding_lookup(item_bias, items)\n",
        "    \n",
        "    text_embedding = tf.Variable(word_embedding_mtrx, dtype=tf.float32, name=\"review_text_embeds\")\n",
        "    padding_embedding = tf.Variable(np.zeros([1, word_latent_dim]), dtype=tf.float32)\n",
        "    text_mask = tf.constant([1.0] * text_embedding.get_shape()[0] + [0.0])\n",
        "    word_embeddings = tf.concat([text_embedding, padding_embedding], 0)\n",
        "    word_embeddings = word_embeddings * tf.expand_dims(text_mask, -1)\n",
        "    user_reviews_representation = tf.nn.embedding_lookup(word_embeddings, users_inputs)\n",
        "    user_reviews_representation_expnd = tf.expand_dims(user_reviews_representation, -1)\n",
        "    item_reviews_representation = tf.nn.embedding_lookup(word_embeddings, items_inputs)\n",
        "    item_reviews_representation_expnd = tf.expand_dims(item_reviews_representation, -1)\n",
        "    \n",
        "    # CNN layers\n",
        "    W_u = tf.Variable(tf.random.truncated_normal([window_size, word_latent_dim, 1, num_filters], stddev=0.3), name=\"review_W_u\")\n",
        "    W_i = tf.Variable(tf.random.truncated_normal([window_size, word_latent_dim, 1, num_filters], stddev=0.3), name=\"review_W_i\")\n",
        "    W_u_1 = tf.Variable(tf.random.truncated_normal([window_size, num_filters, 1, num_filters], stddev=0.3), name=\"review_W_u_1\")\n",
        "    W_i_1 = tf.Variable(tf.random.truncated_normal([window_size, num_filters, 1, num_filters], stddev=0.3), name=\"review_W_i_1\")\n",
        "    # b = tf.Variable(tf.constant(0.1, shape=[self.num_filters]))\n",
        "    rand_matrix = tf.Variable(tf.random.truncated_normal([num_filters, num_filters], stddev=0.3), name=\"review_rand_matrix\")\n",
        "\n",
        "    user_embeds, item_embeds = cnn_model_average(num_filters, user_reviews_representation_expnd, item_reviews_representation_expnd, W_u, W_i, W_u_1, W_i_1, rand_matrix)\n",
        "    \n",
        "    #shared MLP layer\n",
        "    W_mlp = tf.Variable(tf.random.normal([num_filters, latent_dim], mean=0, stddev=0.02), name=\"review_W_mlp\")\n",
        "    W_mlp = tf.nn.dropout(W_mlp, dropout_rate)\n",
        "    b_mlp = tf.Variable(tf.constant(0.1, shape=[latent_dim]), name=\"review_b_mlp\")\n",
        "\n",
        "    user_embeds = tf.nn.relu(tf.matmul(user_embeds, W_mlp) + b_mlp)\n",
        "    item_embeds = tf.nn.relu(tf.matmul(item_embeds, W_mlp) + b_mlp)\n",
        "\n",
        "    embeds_sum = tf.concat([tf.multiply(user_embeds, item_embeds), user_embeds, item_embeds], 1, name=\"concat_embed\")\n",
        "    #I\n",
        "    # entity_embeds_sum = tf.concat([tf.multiply(user_entity_embeds, item_entity_embeds), user_entity_embeds, item_entity_embeds],1)\n",
        "\n",
        "    #FM layer\n",
        "    #R---\n",
        "    w_0 = tf.Variable(tf.zeros(1), name=\"review_w_0\")\n",
        "    w_1 = tf.Variable(tf.random.truncated_normal([1, latent_dim * 3], stddev=0.3), name=\"review_w_1\")\n",
        "    v = tf.Variable(tf.random.truncated_normal([latent_dim * 3, latent_dim * 3], stddev=0.3), name=\"review_v\")\n",
        "    \n",
        "    #I---\n",
        "    # w_entity_0 = tf.Variable(tf.zeros(1), name=\"entity_w_0\")\n",
        "    # w_entity_1 = tf.Variable(tf.random.truncated_normal([1, latent_dim*3], stddev=0.3), name=\"entity_w_1\")\n",
        "    # v_entity = tf.Variable(tf.random.truncated_normal([latent_dim*3, v_dim], stddev=0.3), name=\"entity_v\")\n",
        "    \n",
        "    #I---\n",
        "    # J_e_1 = w_entity_0 + tf.matmul(entity_embeds_sum, w_entity_1, transpose_b=True)\n",
        "    #R---\n",
        "    J_1 = w_0 + tf.matmul(embeds_sum, w_1, transpose_b=True)\n",
        "    \n",
        "    #I---\n",
        "    # entity_embeds_sum_1 = tf.expand_dims(entity_embeds_sum, -1)\n",
        "    # entity_embeds_sum_2 = tf.expand_dims(entity_embeds_sum, 1)\n",
        "    #R---\n",
        "    embeds_sum_1 = tf.expand_dims(embeds_sum, -1)\n",
        "    embeds_sum_2 = tf.expand_dims(embeds_sum, 1)\n",
        "    \n",
        "    #I---\n",
        "    # J_e_2 = tf.reduce_sum(tf.reduce_sum(tf.multiply(tf.matmul(entity_embeds_sum_1, entity_embeds_sum_2), tf.matmul(v_entity, v_entity, transpose_b=True)),2), 1, keepdims=True)\n",
        "    #J_e_3 = tf.trace(tf.multiply(tf.matmul(entity_embeds_sum_1, entity_embeds_sum_2), tf.matmul(v_entity, v_entity, transpose_b=True))) \n",
        "    \n",
        "    #R---\n",
        "    J_2 = tf.reduce_sum(tf.reduce_sum(tf.multiply(tf.matmul(embeds_sum_1, embeds_sum_2), tf.matmul(v, v, transpose_b=True)),2), 1, keepdims=True)\n",
        "    #J_3 = tf.trace(tf.multiply(tf.matmul(embeds_sum_1, embeds_sum_2), tf.matmul(v, v, transpose_b=True)))\n",
        "    #R---\n",
        "    J_total = (J_1 + 0.5 * (J_2))\n",
        "    \n",
        "    #I---\n",
        "    # J_e_total = (J_e_1 + 0.5 * (J_e_2))\n",
        "    \n",
        "    \n",
        "    # numerator = J_total + J_e_total\n",
        "    numerator = J_total\n",
        "    # predict_rating = tf.divide(J_total, numerator) * J_total + tf.divide(J_e_total,numerator) * J_e_total + user_bs + item_bs \n",
        "    predict_rating = tf.divide(J_total, numerator) * J_total + user_bs + item_bs\n",
        "    loss = tf.reduce_mean(tf.math.squared_difference(predict_rating, ratings))\n",
        "    loss += lambda_1 * (tf.nn.l2_loss(W_i_1) + tf.nn.l2_loss(W_u_1) + tf.nn.l2_loss(W_u) + tf.nn.l2_loss(W_i) + tf.nn.l2_loss(v) + tf.nn.l2_loss(rand_matrix) + tf.nn.l2_loss(user_bs) + tf.nn.l2_loss(item_bs))\n",
        "    \n",
        "    train_step = tf.compat.v1.train.RMSPropOptimizer(learning_rate).minimize(loss)\n",
        "    saver = tf.compat.v1.train.Saver()\n",
        "    \n",
        "    init = tf.compat.v1.global_variables_initializer()\n",
        "    with tf.compat.v1.Session() as sess:\n",
        "        sess.run(init)\n",
        "\n",
        "        for e in range(epochs):\n",
        "            t = time()\n",
        "            loss_total = 0.0\n",
        "            count = 0.0\n",
        "            \n",
        "            for i in range(int(math.ceil(len(user_input) / float(batch_size)))):\n",
        "                user_batch, item_batch, user_input_batch, item_input_batch, rates_batch = get_train_instance_batch_change(i, batch_size,user_input, item_input, rateings, user_reviews,item_reviews)\n",
        "                _, loss_val, words = sess.run([train_step, loss, word_embeddings], feed_dict={users: user_batch, items: item_batch, users_inputs: user_input_batch, items_inputs: item_input_batch, ratings: rates_batch, dropout_rate:drop_out})\n",
        "                loss_total += loss_val\n",
        "                count += 1.0\n",
        "\n",
        "            t1 = time()            \n",
        "            #LOOK FROM HERE \n",
        "            val_mses, val_maes = [], []\n",
        "            for i in range(len(user_input_val)):\n",
        "                #eval_model(users, items, users_inputs, items_inputs, dropout_rate, predict_rating, sess, user_vals[i], item_vals[i], user_input_val[i], item_input_val[i], rating_input_val[i], val_mses, val_maes)\n",
        "                #predicts = sess.run(predict_rating, feed_dict={users: user_batch, items: item_batch, users_inputs: user_input_batch, items_inputs: item_input_batch, dropout_rate:1.0})\n",
        "                predicts = sess.run(predict_rating, feed_dict={users: user_vals[i], items: item_vals[i], users_inputs: user_input_val[i], items_inputs: item_input_val[i], ratings: rating_input_val[i], dropout_rate: drop_out})\n",
        "                #print(predicts)\n",
        "                row, col = predicts.shape\n",
        "                for r in range(row):\n",
        "                    val_mses.append(pow((predicts[r, 0] - rating_input_val[i][r][0]), 2))\n",
        "                    val_maes.append(abs((predicts[r, 0] - rating_input_val[i][r][0])))\n",
        "            val_mse = np.array(val_mses).mean()            \n",
        "            t2 = time()            \n",
        "            mses, maes = [], []\n",
        "            for i in range(len(user_input_test)):\n",
        "                #eval_model(users, items, users_inputs, items_inputs, dropout_rate, predict_rating, sess, user_tests[i], item_tests[i], user_input_test[i], item_input_test[i], rating_input_test[i], mses, maes)\n",
        "                predicts_test = sess.run(predict_rating, feed_dict={users: user_tests[i], items: item_tests[i], users_inputs: user_input_test[i], items_inputs: item_input_test[i], ratings: rating_input_test[i], dropout_rate: drop_out})\n",
        "                #print(predicts)\n",
        "                row, col = predicts_test.shape\n",
        "                for r in range(row):\n",
        "                    mses.append(pow((predicts_test[r, 0] - rating_input_test[i][r][0]), 2))\n",
        "                    maes.append(abs((predicts_test[r, 0] - rating_input_test[i][r][0])))\n",
        "                    \n",
        "            mse = np.array(mses).mean()\n",
        "            mae = np.array(maes).mean()\n",
        "            t3 = time()\n",
        "            mae_list.append(mae)\n",
        "            mse_list.append(mse)\n",
        "            print(\"epoch%d train time: %.3fs test time: %.3f  loss = %.3f val_mse = %.3f mse = %.3f mae = %.3f\"%(e, (t1 - t), (t3 - t2), loss_total/count, val_mse, mse, mae))\n",
        "        \n",
        "        avg_mae=sum(mae_list) / len(mae_list)\n",
        "        avg_mse=sum(mse_list)/len(mse_list)\n",
        "        avg_mae_list.append(avg_mae)\n",
        "        avg_mse_list.append(avg_mse)\n",
        "        print(\"MAE \"+str(avg_mae))\n",
        "        print(\"MSE \"+str(avg_mse))"
      ],
      "id": "nZI3Juflqt2L"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7nFkpGRcsAzb",
        "outputId": "c6bf0c34-3cc9-4b17-cccb-fa5aee9187e2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "wordId_dict finished\n",
            "load reviews finished\n",
            "load data: 4.912s\n",
            "506 2581\n",
            "15\n",
            "shape (19930, 300)\n",
            "18880 18880 18880\n",
            "epoch0 train time: 17.480s test time: 0.963  loss = 187.961 val_mse = 1.575 mse = 1.510 mae = 0.880\n",
            "epoch1 train time: 9.013s test time: 0.894  loss = 119.428 val_mse = 1.586 mse = 1.520 mae = 0.882\n",
            "epoch2 train time: 8.972s test time: 0.902  loss = 58.817 val_mse = 1.586 mse = 1.520 mae = 0.882\n",
            "epoch3 train time: 9.008s test time: 0.889  loss = 25.554 val_mse = 1.586 mse = 1.520 mae = 0.882\n",
            "epoch4 train time: 8.973s test time: 0.892  loss = 9.639 val_mse = 1.584 mse = 1.519 mae = 0.882\n",
            "epoch5 train time: 9.001s test time: 0.906  loss = 3.448 val_mse = 1.583 mse = 1.517 mae = 0.882\n",
            "epoch6 train time: 8.997s test time: 0.914  loss = 1.797 val_mse = 1.581 mse = 1.516 mae = 0.881\n",
            "epoch7 train time: 9.011s test time: 0.903  loss = 1.562 val_mse = 1.579 mse = 1.514 mae = 0.880\n",
            "epoch8 train time: 9.082s test time: 1.029  loss = 1.486 val_mse = 1.576 mse = 1.512 mae = 0.880\n",
            "epoch9 train time: 9.022s test time: 0.907  loss = 1.428 val_mse = 1.573 mse = 1.510 mae = 0.879\n",
            "epoch10 train time: 8.997s test time: 0.906  loss = 1.381 val_mse = 1.571 mse = 1.508 mae = 0.878\n",
            "epoch11 train time: 8.999s test time: 0.890  loss = 1.343 val_mse = 1.568 mse = 1.506 mae = 0.877\n",
            "epoch12 train time: 9.009s test time: 0.893  loss = 1.311 val_mse = 1.565 mse = 1.504 mae = 0.877\n",
            "epoch13 train time: 8.976s test time: 0.890  loss = 1.285 val_mse = 1.562 mse = 1.502 mae = 0.876\n",
            "epoch14 train time: 8.975s test time: 0.901  loss = 1.262 val_mse = 1.560 mse = 1.500 mae = 0.876\n",
            "epoch15 train time: 9.005s test time: 0.895  loss = 1.242 val_mse = 1.557 mse = 1.498 mae = 0.875\n",
            "epoch16 train time: 9.034s test time: 0.896  loss = 1.224 val_mse = 1.554 mse = 1.496 mae = 0.874\n",
            "epoch17 train time: 8.989s test time: 0.892  loss = 1.209 val_mse = 1.551 mse = 1.494 mae = 0.874\n",
            "epoch18 train time: 9.026s test time: 1.125  loss = 1.196 val_mse = 1.549 mse = 1.493 mae = 0.874\n",
            "epoch19 train time: 9.158s test time: 0.891  loss = 1.184 val_mse = 1.546 mse = 1.491 mae = 0.873\n",
            "epoch20 train time: 9.016s test time: 0.899  loss = 1.173 val_mse = 1.544 mse = 1.490 mae = 0.873\n",
            "epoch21 train time: 8.991s test time: 0.909  loss = 1.163 val_mse = 1.542 mse = 1.488 mae = 0.873\n",
            "epoch22 train time: 9.115s test time: 0.903  loss = 1.154 val_mse = 1.540 mse = 1.487 mae = 0.872\n",
            "epoch23 train time: 8.972s test time: 0.881  loss = 1.146 val_mse = 1.537 mse = 1.485 mae = 0.872\n",
            "epoch24 train time: 8.993s test time: 0.896  loss = 1.139 val_mse = 1.534 mse = 1.483 mae = 0.872\n",
            "epoch25 train time: 9.008s test time: 0.901  loss = 1.132 val_mse = 1.532 mse = 1.481 mae = 0.872\n",
            "epoch26 train time: 9.029s test time: 0.905  loss = 1.125 val_mse = 1.530 mse = 1.480 mae = 0.871\n",
            "epoch27 train time: 9.037s test time: 0.901  loss = 1.119 val_mse = 1.527 mse = 1.478 mae = 0.871\n",
            "epoch28 train time: 9.045s test time: 0.889  loss = 1.114 val_mse = 1.524 mse = 1.476 mae = 0.871\n",
            "epoch29 train time: 8.981s test time: 0.909  loss = 1.109 val_mse = 1.522 mse = 1.474 mae = 0.871\n",
            "epoch30 train time: 9.010s test time: 0.895  loss = 1.104 val_mse = 1.520 mse = 1.473 mae = 0.871\n",
            "epoch31 train time: 8.990s test time: 0.895  loss = 1.099 val_mse = 1.517 mse = 1.471 mae = 0.871\n",
            "epoch32 train time: 8.981s test time: 0.886  loss = 1.095 val_mse = 1.515 mse = 1.470 mae = 0.871\n",
            "epoch33 train time: 9.002s test time: 0.896  loss = 1.091 val_mse = 1.513 mse = 1.468 mae = 0.871\n",
            "epoch34 train time: 8.982s test time: 0.891  loss = 1.087 val_mse = 1.511 mse = 1.467 mae = 0.871\n",
            "epoch35 train time: 8.982s test time: 0.880  loss = 1.084 val_mse = 1.508 mse = 1.465 mae = 0.872\n",
            "epoch36 train time: 9.008s test time: 0.896  loss = 1.080 val_mse = 1.507 mse = 1.464 mae = 0.872\n",
            "epoch37 train time: 9.002s test time: 0.905  loss = 1.077 val_mse = 1.505 mse = 1.462 mae = 0.872\n",
            "epoch38 train time: 8.969s test time: 0.903  loss = 1.074 val_mse = 1.503 mse = 1.461 mae = 0.872\n",
            "epoch39 train time: 9.006s test time: 0.894  loss = 1.071 val_mse = 1.500 mse = 1.459 mae = 0.872\n",
            "epoch40 train time: 8.970s test time: 0.879  loss = 1.069 val_mse = 1.499 mse = 1.458 mae = 0.872\n",
            "epoch41 train time: 8.992s test time: 0.897  loss = 1.066 val_mse = 1.497 mse = 1.457 mae = 0.872\n",
            "epoch42 train time: 8.983s test time: 0.897  loss = 1.063 val_mse = 1.496 mse = 1.456 mae = 0.872\n",
            "epoch43 train time: 8.958s test time: 0.892  loss = 1.061 val_mse = 1.495 mse = 1.455 mae = 0.872\n",
            "epoch44 train time: 8.987s test time: 0.896  loss = 1.059 val_mse = 1.493 mse = 1.454 mae = 0.872\n",
            "epoch45 train time: 8.981s test time: 0.885  loss = 1.057 val_mse = 1.492 mse = 1.453 mae = 0.872\n",
            "epoch46 train time: 8.975s test time: 0.907  loss = 1.055 val_mse = 1.491 mse = 1.453 mae = 0.872\n",
            "epoch47 train time: 8.973s test time: 0.892  loss = 1.053 val_mse = 1.489 mse = 1.452 mae = 0.872\n",
            "epoch48 train time: 8.965s test time: 0.888  loss = 1.051 val_mse = 1.488 mse = 1.451 mae = 0.872\n",
            "epoch49 train time: 9.009s test time: 0.907  loss = 1.049 val_mse = 1.487 mse = 1.450 mae = 0.872\n",
            "epoch50 train time: 9.003s test time: 0.903  loss = 1.047 val_mse = 1.486 mse = 1.449 mae = 0.872\n",
            "epoch51 train time: 9.013s test time: 0.893  loss = 1.046 val_mse = 1.485 mse = 1.448 mae = 0.872\n",
            "epoch52 train time: 8.981s test time: 0.882  loss = 1.044 val_mse = 1.484 mse = 1.448 mae = 0.872\n",
            "epoch53 train time: 8.981s test time: 0.899  loss = 1.043 val_mse = 1.483 mse = 1.447 mae = 0.872\n",
            "epoch54 train time: 8.955s test time: 0.906  loss = 1.041 val_mse = 1.482 mse = 1.446 mae = 0.872\n",
            "epoch55 train time: 9.001s test time: 0.891  loss = 1.040 val_mse = 1.481 mse = 1.446 mae = 0.872\n",
            "epoch56 train time: 8.987s test time: 0.882  loss = 1.039 val_mse = 1.480 mse = 1.445 mae = 0.872\n",
            "epoch57 train time: 8.979s test time: 0.906  loss = 1.037 val_mse = 1.479 mse = 1.444 mae = 0.872\n",
            "epoch58 train time: 8.995s test time: 0.896  loss = 1.036 val_mse = 1.478 mse = 1.444 mae = 0.872\n",
            "epoch59 train time: 9.032s test time: 0.894  loss = 1.035 val_mse = 1.477 mse = 1.443 mae = 0.872\n",
            "epoch60 train time: 8.957s test time: 0.890  loss = 1.034 val_mse = 1.477 mse = 1.443 mae = 0.872\n",
            "epoch61 train time: 8.989s test time: 0.903  loss = 1.033 val_mse = 1.476 mse = 1.442 mae = 0.872\n",
            "epoch62 train time: 8.997s test time: 0.903  loss = 1.031 val_mse = 1.475 mse = 1.442 mae = 0.872\n",
            "epoch63 train time: 8.948s test time: 0.887  loss = 1.030 val_mse = 1.474 mse = 1.441 mae = 0.871\n",
            "epoch64 train time: 8.966s test time: 0.888  loss = 1.029 val_mse = 1.474 mse = 1.440 mae = 0.871\n",
            "epoch65 train time: 8.976s test time: 0.908  loss = 1.028 val_mse = 1.473 mse = 1.440 mae = 0.871\n",
            "epoch66 train time: 8.978s test time: 0.912  loss = 1.027 val_mse = 1.472 mse = 1.439 mae = 0.871\n",
            "epoch67 train time: 8.997s test time: 0.888  loss = 1.026 val_mse = 1.471 mse = 1.439 mae = 0.871\n",
            "epoch68 train time: 8.976s test time: 0.880  loss = 1.025 val_mse = 1.471 mse = 1.438 mae = 0.871\n",
            "epoch69 train time: 8.959s test time: 0.881  loss = 1.024 val_mse = 1.470 mse = 1.438 mae = 0.871\n",
            "epoch70 train time: 8.974s test time: 0.907  loss = 1.023 val_mse = 1.469 mse = 1.437 mae = 0.871\n",
            "epoch71 train time: 9.000s test time: 0.893  loss = 1.022 val_mse = 1.468 mse = 1.437 mae = 0.871\n",
            "epoch72 train time: 8.961s test time: 0.889  loss = 1.022 val_mse = 1.468 mse = 1.436 mae = 0.871\n",
            "epoch73 train time: 8.989s test time: 0.891  loss = 1.021 val_mse = 1.467 mse = 1.436 mae = 0.871\n",
            "epoch74 train time: 8.986s test time: 0.896  loss = 1.020 val_mse = 1.466 mse = 1.435 mae = 0.871\n",
            "epoch75 train time: 9.003s test time: 0.895  loss = 1.019 val_mse = 1.466 mse = 1.435 mae = 0.871\n",
            "epoch76 train time: 9.020s test time: 0.872  loss = 1.018 val_mse = 1.465 mse = 1.434 mae = 0.871\n",
            "epoch77 train time: 8.988s test time: 0.894  loss = 1.017 val_mse = 1.464 mse = 1.434 mae = 0.871\n",
            "epoch78 train time: 8.939s test time: 0.886  loss = 1.017 val_mse = 1.464 mse = 1.433 mae = 0.871\n",
            "epoch79 train time: 8.941s test time: 0.889  loss = 1.016 val_mse = 1.463 mse = 1.433 mae = 0.871\n",
            "epoch80 train time: 9.007s test time: 0.894  loss = 1.016 val_mse = 1.463 mse = 1.432 mae = 0.871\n",
            "epoch81 train time: 8.920s test time: 0.924  loss = 1.015 val_mse = 1.462 mse = 1.432 mae = 0.871\n",
            "epoch82 train time: 8.930s test time: 0.897  loss = 1.014 val_mse = 1.462 mse = 1.431 mae = 0.871\n",
            "epoch83 train time: 8.945s test time: 0.884  loss = 1.014 val_mse = 1.461 mse = 1.431 mae = 0.871\n",
            "epoch84 train time: 8.897s test time: 0.885  loss = 1.013 val_mse = 1.461 mse = 1.431 mae = 0.871\n",
            "epoch85 train time: 9.003s test time: 0.903  loss = 1.013 val_mse = 1.460 mse = 1.430 mae = 0.871\n",
            "epoch86 train time: 8.965s test time: 0.894  loss = 1.012 val_mse = 1.460 mse = 1.430 mae = 0.871\n",
            "epoch87 train time: 8.975s test time: 0.886  loss = 1.012 val_mse = 1.459 mse = 1.430 mae = 0.871\n",
            "epoch88 train time: 8.946s test time: 0.887  loss = 1.012 val_mse = 1.459 mse = 1.429 mae = 0.870\n",
            "epoch89 train time: 8.958s test time: 0.896  loss = 1.011 val_mse = 1.458 mse = 1.429 mae = 0.871\n",
            "epoch90 train time: 8.998s test time: 0.914  loss = 1.011 val_mse = 1.458 mse = 1.429 mae = 0.871\n",
            "epoch91 train time: 8.929s test time: 0.898  loss = 1.011 val_mse = 1.458 mse = 1.428 mae = 0.871\n",
            "epoch92 train time: 9.004s test time: 0.896  loss = 1.010 val_mse = 1.457 mse = 1.428 mae = 0.870\n",
            "epoch93 train time: 8.990s test time: 0.878  loss = 1.010 val_mse = 1.457 mse = 1.428 mae = 0.870\n",
            "epoch94 train time: 8.965s test time: 0.913  loss = 1.009 val_mse = 1.456 mse = 1.427 mae = 0.870\n",
            "epoch95 train time: 9.016s test time: 0.905  loss = 1.009 val_mse = 1.456 mse = 1.427 mae = 0.870\n",
            "epoch96 train time: 8.997s test time: 0.892  loss = 1.009 val_mse = 1.456 mse = 1.427 mae = 0.870\n",
            "epoch97 train time: 8.912s test time: 0.907  loss = 1.009 val_mse = 1.455 mse = 1.426 mae = 0.870\n",
            "epoch98 train time: 9.036s test time: 0.905  loss = 1.008 val_mse = 1.455 mse = 1.426 mae = 0.870\n",
            "epoch99 train time: 8.969s test time: 0.885  loss = 1.008 val_mse = 1.455 mse = 1.426 mae = 0.870\n",
            "epoch100 train time: 8.987s test time: 0.881  loss = 1.008 val_mse = 1.454 mse = 1.426 mae = 0.870\n",
            "epoch101 train time: 9.018s test time: 0.901  loss = 1.008 val_mse = 1.454 mse = 1.425 mae = 0.870\n",
            "epoch102 train time: 9.025s test time: 0.912  loss = 1.008 val_mse = 1.454 mse = 1.425 mae = 0.870\n",
            "epoch103 train time: 9.021s test time: 0.892  loss = 1.008 val_mse = 1.453 mse = 1.425 mae = 0.870\n",
            "epoch104 train time: 9.024s test time: 0.892  loss = 1.008 val_mse = 1.453 mse = 1.425 mae = 0.870\n",
            "epoch105 train time: 9.008s test time: 0.889  loss = 1.008 val_mse = 1.453 mse = 1.425 mae = 0.870\n",
            "epoch106 train time: 8.994s test time: 0.913  loss = 1.007 val_mse = 1.452 mse = 1.424 mae = 0.870\n",
            "epoch107 train time: 9.018s test time: 0.897  loss = 1.007 val_mse = 1.452 mse = 1.424 mae = 0.870\n",
            "epoch108 train time: 9.021s test time: 0.884  loss = 1.007 val_mse = 1.452 mse = 1.424 mae = 0.870\n",
            "epoch109 train time: 8.940s test time: 0.898  loss = 1.007 val_mse = 1.452 mse = 1.424 mae = 0.870\n",
            "epoch110 train time: 8.992s test time: 0.904  loss = 1.007 val_mse = 1.451 mse = 1.423 mae = 0.870\n",
            "epoch111 train time: 8.963s test time: 0.893  loss = 1.007 val_mse = 1.451 mse = 1.423 mae = 0.869\n",
            "epoch112 train time: 8.967s test time: 0.902  loss = 1.007 val_mse = 1.451 mse = 1.423 mae = 0.869\n",
            "epoch113 train time: 9.022s test time: 0.898  loss = 1.007 val_mse = 1.451 mse = 1.423 mae = 0.869\n",
            "epoch114 train time: 9.048s test time: 0.906  loss = 1.007 val_mse = 1.450 mse = 1.422 mae = 0.869\n",
            "epoch115 train time: 9.036s test time: 0.895  loss = 1.007 val_mse = 1.450 mse = 1.422 mae = 0.869\n",
            "epoch116 train time: 9.017s test time: 0.900  loss = 1.007 val_mse = 1.450 mse = 1.422 mae = 0.869\n",
            "epoch117 train time: 9.001s test time: 0.927  loss = 1.006 val_mse = 1.449 mse = 1.422 mae = 0.869\n",
            "epoch118 train time: 8.966s test time: 0.909  loss = 1.006 val_mse = 1.449 mse = 1.422 mae = 0.869\n",
            "epoch119 train time: 9.025s test time: 0.907  loss = 1.006 val_mse = 1.449 mse = 1.421 mae = 0.869\n",
            "epoch120 train time: 8.961s test time: 0.909  loss = 1.006 val_mse = 1.449 mse = 1.421 mae = 0.869\n",
            "epoch121 train time: 9.029s test time: 0.894  loss = 1.006 val_mse = 1.449 mse = 1.421 mae = 0.869\n",
            "epoch122 train time: 9.005s test time: 0.895  loss = 1.006 val_mse = 1.448 mse = 1.421 mae = 0.869\n",
            "epoch123 train time: 8.956s test time: 0.886  loss = 1.006 val_mse = 1.448 mse = 1.421 mae = 0.869\n",
            "epoch124 train time: 8.947s test time: 0.880  loss = 1.006 val_mse = 1.448 mse = 1.421 mae = 0.869\n",
            "epoch125 train time: 8.994s test time: 0.902  loss = 1.006 val_mse = 1.448 mse = 1.421 mae = 0.869\n",
            "epoch126 train time: 8.986s test time: 0.903  loss = 1.006 val_mse = 1.448 mse = 1.421 mae = 0.869\n",
            "epoch127 train time: 9.010s test time: 0.887  loss = 1.006 val_mse = 1.448 mse = 1.420 mae = 0.869\n",
            "epoch128 train time: 8.985s test time: 0.894  loss = 1.005 val_mse = 1.447 mse = 1.420 mae = 0.869\n",
            "epoch129 train time: 8.993s test time: 0.889  loss = 1.006 val_mse = 1.447 mse = 1.420 mae = 0.869\n",
            "epoch130 train time: 8.981s test time: 0.901  loss = 1.006 val_mse = 1.447 mse = 1.420 mae = 0.869\n",
            "epoch131 train time: 8.971s test time: 0.899  loss = 1.005 val_mse = 1.447 mse = 1.420 mae = 0.869\n",
            "epoch132 train time: 8.994s test time: 0.902  loss = 1.006 val_mse = 1.447 mse = 1.420 mae = 0.869\n",
            "epoch133 train time: 9.016s test time: 0.904  loss = 1.006 val_mse = 1.447 mse = 1.420 mae = 0.869\n",
            "epoch134 train time: 8.967s test time: 0.917  loss = 1.005 val_mse = 1.446 mse = 1.419 mae = 0.869\n",
            "epoch135 train time: 9.010s test time: 0.891  loss = 1.005 val_mse = 1.446 mse = 1.419 mae = 0.869\n",
            "epoch136 train time: 8.961s test time: 0.886  loss = 1.005 val_mse = 1.446 mse = 1.419 mae = 0.869\n",
            "epoch137 train time: 9.012s test time: 0.897  loss = 1.005 val_mse = 1.446 mse = 1.419 mae = 0.868\n",
            "epoch138 train time: 8.960s test time: 0.896  loss = 1.005 val_mse = 1.446 mse = 1.419 mae = 0.868\n",
            "epoch139 train time: 9.012s test time: 0.889  loss = 1.005 val_mse = 1.445 mse = 1.419 mae = 0.869\n",
            "epoch140 train time: 8.994s test time: 0.896  loss = 1.005 val_mse = 1.446 mse = 1.419 mae = 0.868\n",
            "epoch141 train time: 8.988s test time: 0.906  loss = 1.005 val_mse = 1.445 mse = 1.419 mae = 0.868\n",
            "epoch142 train time: 8.966s test time: 0.905  loss = 1.005 val_mse = 1.445 mse = 1.418 mae = 0.869\n",
            "epoch143 train time: 9.038s test time: 0.900  loss = 1.005 val_mse = 1.445 mse = 1.418 mae = 0.869\n",
            "epoch144 train time: 9.012s test time: 0.894  loss = 1.005 val_mse = 1.445 mse = 1.418 mae = 0.868\n",
            "epoch145 train time: 9.014s test time: 0.884  loss = 1.005 val_mse = 1.445 mse = 1.418 mae = 0.869\n",
            "epoch146 train time: 8.986s test time: 0.915  loss = 1.005 val_mse = 1.445 mse = 1.418 mae = 0.868\n",
            "epoch147 train time: 9.005s test time: 0.889  loss = 1.005 val_mse = 1.445 mse = 1.418 mae = 0.868\n",
            "epoch148 train time: 9.002s test time: 0.890  loss = 1.005 val_mse = 1.444 mse = 1.418 mae = 0.868\n",
            "epoch149 train time: 9.024s test time: 0.895  loss = 1.005 val_mse = 1.444 mse = 1.418 mae = 0.868\n",
            "epoch150 train time: 9.017s test time: 0.905  loss = 1.005 val_mse = 1.445 mse = 1.418 mae = 0.868\n",
            "epoch151 train time: 9.034s test time: 0.891  loss = 1.005 val_mse = 1.444 mse = 1.418 mae = 0.868\n",
            "epoch152 train time: 9.003s test time: 0.894  loss = 1.004 val_mse = 1.444 mse = 1.417 mae = 0.868\n",
            "epoch153 train time: 9.018s test time: 0.904  loss = 1.004 val_mse = 1.444 mse = 1.417 mae = 0.868\n",
            "epoch154 train time: 9.002s test time: 0.906  loss = 1.005 val_mse = 1.444 mse = 1.417 mae = 0.868\n",
            "epoch155 train time: 8.965s test time: 0.897  loss = 1.004 val_mse = 1.444 mse = 1.417 mae = 0.868\n",
            "epoch156 train time: 9.038s test time: 0.891  loss = 1.005 val_mse = 1.444 mse = 1.417 mae = 0.868\n",
            "epoch157 train time: 9.005s test time: 0.892  loss = 1.005 val_mse = 1.444 mse = 1.417 mae = 0.868\n",
            "epoch158 train time: 9.017s test time: 0.918  loss = 1.004 val_mse = 1.443 mse = 1.417 mae = 0.868\n",
            "epoch159 train time: 9.531s test time: 0.896  loss = 1.005 val_mse = 1.443 mse = 1.417 mae = 0.868\n",
            "epoch160 train time: 8.995s test time: 0.888  loss = 1.004 val_mse = 1.443 mse = 1.417 mae = 0.868\n",
            "epoch161 train time: 8.952s test time: 0.909  loss = 1.005 val_mse = 1.443 mse = 1.417 mae = 0.868\n",
            "epoch162 train time: 8.989s test time: 0.909  loss = 1.004 val_mse = 1.443 mse = 1.417 mae = 0.868\n",
            "epoch163 train time: 9.016s test time: 0.895  loss = 1.004 val_mse = 1.443 mse = 1.417 mae = 0.868\n",
            "epoch164 train time: 9.013s test time: 1.095  loss = 1.005 val_mse = 1.444 mse = 1.417 mae = 0.867\n",
            "epoch165 train time: 9.331s test time: 0.912  loss = 1.004 val_mse = 1.443 mse = 1.417 mae = 0.868\n",
            "epoch166 train time: 8.997s test time: 0.918  loss = 1.004 val_mse = 1.443 mse = 1.416 mae = 0.868\n",
            "epoch167 train time: 9.037s test time: 0.913  loss = 1.004 val_mse = 1.443 mse = 1.416 mae = 0.868\n",
            "epoch168 train time: 8.995s test time: 0.897  loss = 1.004 val_mse = 1.442 mse = 1.416 mae = 0.868\n",
            "epoch169 train time: 8.999s test time: 0.908  loss = 1.004 val_mse = 1.443 mse = 1.416 mae = 0.868\n",
            "epoch170 train time: 8.992s test time: 0.912  loss = 1.004 val_mse = 1.443 mse = 1.416 mae = 0.868\n",
            "epoch171 train time: 8.997s test time: 0.891  loss = 1.004 val_mse = 1.443 mse = 1.416 mae = 0.868\n",
            "epoch172 train time: 8.996s test time: 0.895  loss = 1.004 val_mse = 1.442 mse = 1.416 mae = 0.868\n",
            "epoch173 train time: 9.009s test time: 0.899  loss = 1.004 val_mse = 1.443 mse = 1.416 mae = 0.868\n",
            "epoch174 train time: 8.961s test time: 0.903  loss = 1.004 val_mse = 1.442 mse = 1.416 mae = 0.868\n",
            "epoch175 train time: 9.027s test time: 0.892  loss = 1.004 val_mse = 1.442 mse = 1.416 mae = 0.868\n",
            "epoch176 train time: 9.027s test time: 0.882  loss = 1.004 val_mse = 1.442 mse = 1.416 mae = 0.868\n",
            "epoch177 train time: 8.966s test time: 0.897  loss = 1.004 val_mse = 1.442 mse = 1.416 mae = 0.868\n",
            "epoch178 train time: 8.979s test time: 0.893  loss = 1.004 val_mse = 1.442 mse = 1.416 mae = 0.868\n",
            "epoch179 train time: 8.944s test time: 0.890  loss = 1.004 val_mse = 1.442 mse = 1.416 mae = 0.868\n",
            "epoch180 train time: 8.923s test time: 0.889  loss = 1.004 val_mse = 1.442 mse = 1.416 mae = 0.868\n",
            "epoch181 train time: 9.005s test time: 0.914  loss = 1.004 val_mse = 1.442 mse = 1.416 mae = 0.867\n",
            "epoch182 train time: 9.008s test time: 0.907  loss = 1.004 val_mse = 1.442 mse = 1.415 mae = 0.868\n",
            "epoch183 train time: 8.984s test time: 0.891  loss = 1.004 val_mse = 1.442 mse = 1.415 mae = 0.868\n",
            "epoch184 train time: 8.946s test time: 0.884  loss = 1.004 val_mse = 1.442 mse = 1.415 mae = 0.868\n",
            "epoch185 train time: 8.957s test time: 0.883  loss = 1.004 val_mse = 1.442 mse = 1.415 mae = 0.868\n",
            "epoch186 train time: 8.985s test time: 0.889  loss = 1.004 val_mse = 1.442 mse = 1.416 mae = 0.868\n",
            "epoch187 train time: 9.032s test time: 0.889  loss = 1.004 val_mse = 1.442 mse = 1.416 mae = 0.867\n",
            "epoch188 train time: 8.995s test time: 0.884  loss = 1.004 val_mse = 1.442 mse = 1.415 mae = 0.867\n",
            "epoch189 train time: 8.935s test time: 0.900  loss = 1.004 val_mse = 1.442 mse = 1.415 mae = 0.868\n",
            "epoch190 train time: 8.940s test time: 0.906  loss = 1.004 val_mse = 1.442 mse = 1.415 mae = 0.868\n",
            "epoch191 train time: 8.968s test time: 0.901  loss = 1.004 val_mse = 1.441 mse = 1.415 mae = 0.868\n",
            "epoch192 train time: 8.952s test time: 0.886  loss = 1.004 val_mse = 1.441 mse = 1.415 mae = 0.868\n",
            "epoch193 train time: 8.999s test time: 0.894  loss = 1.004 val_mse = 1.441 mse = 1.415 mae = 0.868\n",
            "epoch194 train time: 8.983s test time: 0.915  loss = 1.004 val_mse = 1.442 mse = 1.415 mae = 0.868\n",
            "epoch195 train time: 9.002s test time: 0.906  loss = 1.004 val_mse = 1.442 mse = 1.415 mae = 0.868\n",
            "epoch196 train time: 8.987s test time: 0.901  loss = 1.004 val_mse = 1.441 mse = 1.415 mae = 0.868\n",
            "epoch197 train time: 9.012s test time: 0.894  loss = 1.004 val_mse = 1.441 mse = 1.415 mae = 0.867\n",
            "epoch198 train time: 8.987s test time: 0.911  loss = 1.004 val_mse = 1.441 mse = 1.415 mae = 0.868\n",
            "epoch199 train time: 8.993s test time: 0.884  loss = 1.004 val_mse = 1.441 mse = 1.415 mae = 0.867\n",
            "epoch200 train time: 9.028s test time: 0.900  loss = 1.004 val_mse = 1.441 mse = 1.415 mae = 0.868\n",
            "epoch201 train time: 8.944s test time: 0.904  loss = 1.004 val_mse = 1.441 mse = 1.415 mae = 0.867\n",
            "epoch202 train time: 9.010s test time: 0.898  loss = 1.004 val_mse = 1.441 mse = 1.415 mae = 0.868\n",
            "epoch203 train time: 9.007s test time: 0.902  loss = 1.004 val_mse = 1.441 mse = 1.415 mae = 0.868\n",
            "epoch204 train time: 9.029s test time: 0.896  loss = 1.004 val_mse = 1.442 mse = 1.415 mae = 0.867\n",
            "epoch205 train time: 8.996s test time: 0.895  loss = 1.004 val_mse = 1.441 mse = 1.415 mae = 0.867\n",
            "epoch206 train time: 8.995s test time: 0.899  loss = 1.004 val_mse = 1.441 mse = 1.415 mae = 0.867\n",
            "epoch207 train time: 9.040s test time: 0.888  loss = 1.004 val_mse = 1.441 mse = 1.415 mae = 0.867\n",
            "epoch208 train time: 8.949s test time: 0.893  loss = 1.004 val_mse = 1.441 mse = 1.414 mae = 0.868\n",
            "epoch209 train time: 9.013s test time: 0.918  loss = 1.004 val_mse = 1.441 mse = 1.415 mae = 0.867\n",
            "epoch210 train time: 8.970s test time: 0.907  loss = 1.004 val_mse = 1.441 mse = 1.414 mae = 0.867\n",
            "epoch211 train time: 8.991s test time: 0.889  loss = 1.004 val_mse = 1.441 mse = 1.415 mae = 0.867\n",
            "epoch212 train time: 9.026s test time: 0.891  loss = 1.004 val_mse = 1.441 mse = 1.415 mae = 0.867\n",
            "epoch213 train time: 8.954s test time: 0.904  loss = 1.003 val_mse = 1.441 mse = 1.414 mae = 0.867\n",
            "epoch214 train time: 8.958s test time: 0.897  loss = 1.004 val_mse = 1.441 mse = 1.414 mae = 0.867\n",
            "epoch215 train time: 8.977s test time: 0.893  loss = 1.004 val_mse = 1.441 mse = 1.415 mae = 0.867\n",
            "epoch216 train time: 9.003s test time: 0.885  loss = 1.004 val_mse = 1.441 mse = 1.415 mae = 0.867\n",
            "epoch217 train time: 8.961s test time: 0.900  loss = 1.004 val_mse = 1.441 mse = 1.414 mae = 0.867\n",
            "epoch218 train time: 8.984s test time: 0.901  loss = 1.004 val_mse = 1.441 mse = 1.414 mae = 0.867\n",
            "epoch219 train time: 8.987s test time: 0.888  loss = 1.004 val_mse = 1.441 mse = 1.414 mae = 0.867\n",
            "epoch220 train time: 8.988s test time: 0.894  loss = 1.004 val_mse = 1.441 mse = 1.414 mae = 0.867\n",
            "epoch221 train time: 9.006s test time: 0.897  loss = 1.004 val_mse = 1.441 mse = 1.414 mae = 0.867\n",
            "epoch222 train time: 8.982s test time: 0.902  loss = 1.004 val_mse = 1.440 mse = 1.414 mae = 0.867\n",
            "epoch223 train time: 8.962s test time: 0.886  loss = 1.004 val_mse = 1.441 mse = 1.414 mae = 0.867\n",
            "epoch224 train time: 8.958s test time: 0.887  loss = 1.004 val_mse = 1.441 mse = 1.414 mae = 0.867\n",
            "epoch225 train time: 8.951s test time: 0.899  loss = 1.003 val_mse = 1.440 mse = 1.414 mae = 0.868\n",
            "epoch226 train time: 8.964s test time: 0.894  loss = 1.004 val_mse = 1.440 mse = 1.414 mae = 0.868\n",
            "epoch227 train time: 8.984s test time: 0.894  loss = 1.004 val_mse = 1.441 mse = 1.414 mae = 0.867\n",
            "epoch228 train time: 9.002s test time: 0.883  loss = 1.003 val_mse = 1.440 mse = 1.414 mae = 0.868\n",
            "epoch229 train time: 8.997s test time: 0.904  loss = 1.004 val_mse = 1.440 mse = 1.414 mae = 0.867\n",
            "epoch230 train time: 8.998s test time: 0.907  loss = 1.004 val_mse = 1.441 mse = 1.414 mae = 0.867\n",
            "epoch231 train time: 8.990s test time: 0.893  loss = 1.004 val_mse = 1.441 mse = 1.414 mae = 0.867\n",
            "epoch232 train time: 9.039s test time: 0.913  loss = 1.003 val_mse = 1.440 mse = 1.414 mae = 0.868\n",
            "epoch233 train time: 9.044s test time: 0.890  loss = 1.004 val_mse = 1.440 mse = 1.414 mae = 0.867\n",
            "epoch234 train time: 8.966s test time: 0.910  loss = 1.003 val_mse = 1.440 mse = 1.414 mae = 0.867\n",
            "epoch235 train time: 9.001s test time: 0.897  loss = 1.003 val_mse = 1.440 mse = 1.414 mae = 0.867\n",
            "epoch236 train time: 8.988s test time: 0.883  loss = 1.004 val_mse = 1.440 mse = 1.414 mae = 0.867\n",
            "epoch237 train time: 8.951s test time: 0.910  loss = 1.003 val_mse = 1.440 mse = 1.414 mae = 0.867\n",
            "epoch238 train time: 9.006s test time: 0.909  loss = 1.004 val_mse = 1.440 mse = 1.414 mae = 0.867\n",
            "epoch239 train time: 8.988s test time: 0.897  loss = 1.004 val_mse = 1.440 mse = 1.414 mae = 0.867\n",
            "MAE 0.8700475400953301\n",
            "MSE 1.434916430121106\n"
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    print(sys.path[0])\n",
        "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
        "    word_latent_dim = 300\n",
        "    latent_dim = 15\n",
        "    max_doc_length = 300\n",
        "    num_filters = 50\n",
        "    window_size = 3\n",
        "    v_dim = 50\n",
        "    learning_rate = 0.001\n",
        "    lambda_1 = 0.05 #normally we set from [0.05, 0.01, 0.005, 0.001]\n",
        "    drop_out = 0.8\n",
        "    batch_size = 200\n",
        "    epochs = 180\n",
        "    avg_mae_list = []\n",
        "    avg_mse_list = []\n",
        "    \n",
        "    # loading data\n",
        "    firTime = time()\n",
        "    dataSet = Dataset(max_doc_length, sys.path[0], \"dataPreprocessingWordDict.out\")\n",
        "    word_dict, user_reviews, item_reviews, train, valRatings, testRatings = dataSet.word_id_dict, dataSet.userReview_dict, dataSet.itemReview_dict, dataSet.trainMtrx, dataSet.valRatings, dataSet.testRatings\n",
        "    secTime = time()\n",
        "\n",
        "    num_users, num_items = train.shape\n",
        "    print(\"load data: %.3fs\" % (secTime - firTime))\n",
        "    print(num_users, num_items)\n",
        "    print(latent_dim)\n",
        "\n",
        "    #load word embeddings\n",
        "    word_embedding_mtrx = ini_word_embed(len(word_dict), word_latent_dim)\n",
        "    # word_embedding_mtrx = word2vec_word_embed(len(word_dict), word_latent_dim,\n",
        "    #                                           \"Directory of pretrained WordEmbedding.out\",\n",
        "    #                                           word_dict)\n",
        "\n",
        "    print( \"shape\", word_embedding_mtrx.shape)\n",
        "\n",
        "    # get train instances\n",
        "    user_input, item_input, rateings = get_train_instance(train)\n",
        "    print (len(user_input), len(item_input), len(rateings))\n",
        "\n",
        "    # get test/val instances\n",
        "    user_vals, item_vals, user_input_val, item_input_val, rating_input_val = get_test_list(200, valRatings, user_reviews, item_reviews)\n",
        "    user_tests, item_tests, user_input_test, item_input_test, rating_input_test = get_test_list(200, testRatings, user_reviews, item_reviews)\n",
        "\n",
        "    #train & eval model\n",
        "    train_model()"
      ],
      "id": "7nFkpGRcsAzb"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "R1nVse_jDcMn",
        "outputId": "788254f5-0024-4870-97a7-8e6abdd12174"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "mae_list = [0.9416255216488819, 0.8700475400953301, 0.8339613501772636]\n",
        "mse_list = [1.6866763914333192, 1.434916430121106, 1.3063634430115583]\n",
        "models = ['Interaction','Review', 'CARL']\n",
        "X_axis = np.arange(len(models))\n",
        "plt.bar(X_axis - 0.1, mae_list, 0.2, label = 'MAE', color='pink')\n",
        "plt.bar(X_axis + 0.1, mse_list, 0.2, label = 'MSE', color='grey')\n",
        "plt.title('Impact of the two individual components of CARL')\n",
        "plt.xticks(X_axis, models)\n",
        "plt.xlabel('Models')\n",
        "plt.ylabel('Error')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "id": "R1nVse_jDcMn"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RLlil69DQt4z"
      },
      "source": [
        "dynamic linear fusion vs varying alpha \n",
        "\n",
        "alpha = [0.1, 0.2, 0.3, 0.4, 0.5, 0,6, 0.7, 0.8, 0.9, 1]\n"
      ],
      "id": "RLlil69DQt4z"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M3qW6jiYQuH-"
      },
      "outputs": [],
      "source": [
        "#combination of interaction.py and review.py\n",
        "def train_model():\n",
        "    mae_list=[]\n",
        "    mse_list=[]\n",
        "    tf.compat.v1.disable_eager_execution()\n",
        "    #I and R-----\n",
        "    users = tf.compat.v1.placeholder(tf.int32, shape=[None])\n",
        "    items = tf.compat.v1.placeholder(tf.int32, shape=[None])\n",
        "    users_inputs = tf.compat.v1.placeholder(tf.int32, shape=[None, max_doc_length])\n",
        "    items_inputs = tf.compat.v1.placeholder(tf.int32, shape=[None, max_doc_length])\n",
        "    ratings = tf.compat.v1.placeholder(tf.float32, shape=[None, 1])\n",
        "    \n",
        "    #R-----\n",
        "    dropout_rate = tf.compat.v1.placeholder(tf.float32)\n",
        "    alpha = tf.compat.v1.placeholder(tf.float32)\n",
        "    \n",
        "    #I-----\n",
        "    user_entity_embedding = tf.Variable(tf.random.normal([num_users, latent_dim], mean=0, stddev=0.02), name=\"user_entity_embeddings\")\n",
        "    item_entity_embedding = tf.Variable(tf.random.normal([num_items, latent_dim], mean=0, stddev=0.02), name=\"item_entity_embeddings\")\n",
        "    user_entity_embeds = tf.nn.embedding_lookup(user_entity_embedding, users)\n",
        "    item_entity_embeds = tf.nn.embedding_lookup(item_entity_embedding, items)\n",
        "    \n",
        "    #R-----\n",
        "    user_bias = tf.Variable(tf.random.normal([num_users, 1], mean=0, stddev=0.02), name=\"user_bias\")\n",
        "    item_bias = tf.Variable(tf.random.normal([num_items, 1], mean=0, stddev=0.02), name=\"item_bias\")\n",
        "    user_bs = tf.nn.embedding_lookup(user_bias, users)\n",
        "    item_bs = tf.nn.embedding_lookup(item_bias, items)\n",
        "    \n",
        "    text_embedding = tf.Variable(word_embedding_mtrx, dtype=tf.float32, name=\"review_text_embeds\")\n",
        "    padding_embedding = tf.Variable(np.zeros([1, word_latent_dim]), dtype=tf.float32)\n",
        "    text_mask = tf.constant([1.0] * text_embedding.get_shape()[0] + [0.0])\n",
        "    word_embeddings = tf.concat([text_embedding, padding_embedding], 0)\n",
        "    word_embeddings = word_embeddings * tf.expand_dims(text_mask, -1)\n",
        "    user_reviews_representation = tf.nn.embedding_lookup(word_embeddings, users_inputs)\n",
        "    user_reviews_representation_expnd = tf.expand_dims(user_reviews_representation, -1)\n",
        "    item_reviews_representation = tf.nn.embedding_lookup(word_embeddings, items_inputs)\n",
        "    item_reviews_representation_expnd = tf.expand_dims(item_reviews_representation, -1)\n",
        "    \n",
        "    # CNN layers\n",
        "    W_u = tf.Variable(tf.random.truncated_normal([window_size, word_latent_dim, 1, num_filters], stddev=0.3), name=\"review_W_u\")\n",
        "    W_i = tf.Variable(tf.random.truncated_normal([window_size, word_latent_dim, 1, num_filters], stddev=0.3), name=\"review_W_i\")\n",
        "    W_u_1 = tf.Variable(tf.random.truncated_normal([window_size, num_filters, 1, num_filters], stddev=0.3), name=\"review_W_u_1\")\n",
        "    W_i_1 = tf.Variable(tf.random.truncated_normal([window_size, num_filters, 1, num_filters], stddev=0.3), name=\"review_W_i_1\")\n",
        "    # b = tf.Variable(tf.constant(0.1, shape=[self.num_filters]))\n",
        "    rand_matrix = tf.Variable(tf.random.truncated_normal([num_filters, num_filters], stddev=0.3), name=\"review_rand_matrix\")\n",
        "\n",
        "    user_embeds, item_embeds = cnn_model_average(num_filters, user_reviews_representation_expnd, item_reviews_representation_expnd, W_u, W_i, W_u_1, W_i_1, rand_matrix)\n",
        "    \n",
        "    #shared MLP layer\n",
        "    W_mlp = tf.Variable(tf.random.normal([num_filters, latent_dim], mean=0, stddev=0.02), name=\"review_W_mlp\")\n",
        "    W_mlp = tf.nn.dropout(W_mlp, dropout_rate)\n",
        "    b_mlp = tf.Variable(tf.constant(0.1, shape=[latent_dim]), name=\"review_b_mlp\")\n",
        "\n",
        "    user_embeds = tf.nn.relu(tf.matmul(user_embeds, W_mlp) + b_mlp)\n",
        "    item_embeds = tf.nn.relu(tf.matmul(item_embeds, W_mlp) + b_mlp)\n",
        "\n",
        "    embeds_sum = tf.concat([tf.multiply(user_embeds, item_embeds), user_embeds, item_embeds], 1, name=\"concat_embed\")\n",
        "    #I\n",
        "    entity_embeds_sum = tf.concat([tf.multiply(user_entity_embeds, item_entity_embeds), user_entity_embeds, item_entity_embeds],1)\n",
        "\n",
        "    #FM layer\n",
        "    #R---\n",
        "    w_0 = tf.Variable(tf.zeros(1), name=\"review_w_0\")\n",
        "    w_1 = tf.Variable(tf.random.truncated_normal([1, latent_dim * 3], stddev=0.3), name=\"review_w_1\")\n",
        "    v = tf.Variable(tf.random.truncated_normal([latent_dim * 3, latent_dim * 3], stddev=0.3), name=\"review_v\")\n",
        "    \n",
        "    #I---\n",
        "    w_entity_0 = tf.Variable(tf.zeros(1), name=\"entity_w_0\")\n",
        "    w_entity_1 = tf.Variable(tf.random.truncated_normal([1, latent_dim*3], stddev=0.3), name=\"entity_w_1\")\n",
        "    v_entity = tf.Variable(tf.random.truncated_normal([latent_dim*3, v_dim], stddev=0.3), name=\"entity_v\")\n",
        "    \n",
        "    #I---\n",
        "    J_e_1 = w_entity_0 + tf.matmul(entity_embeds_sum, w_entity_1, transpose_b=True)\n",
        "    #R---\n",
        "    J_1 = w_0 + tf.matmul(embeds_sum, w_1, transpose_b=True)\n",
        "    \n",
        "    #I---\n",
        "    entity_embeds_sum_1 = tf.expand_dims(entity_embeds_sum, -1)\n",
        "    entity_embeds_sum_2 = tf.expand_dims(entity_embeds_sum, 1)\n",
        "    #R---\n",
        "    embeds_sum_1 = tf.expand_dims(embeds_sum, -1)\n",
        "    embeds_sum_2 = tf.expand_dims(embeds_sum, 1)\n",
        "    \n",
        "    #I---\n",
        "    J_e_2 = tf.reduce_sum(tf.reduce_sum(tf.multiply(tf.matmul(entity_embeds_sum_1, entity_embeds_sum_2), tf.matmul(v_entity, v_entity, transpose_b=True)),2), 1, keepdims=True)\n",
        "    #J_e_3 = tf.trace(tf.multiply(tf.matmul(entity_embeds_sum_1, entity_embeds_sum_2), tf.matmul(v_entity, v_entity, transpose_b=True))) \n",
        "    \n",
        "    #R---\n",
        "    J_2 = tf.reduce_sum(tf.reduce_sum(tf.multiply(tf.matmul(embeds_sum_1, embeds_sum_2), tf.matmul(v, v, transpose_b=True)),2), 1, keepdims=True)\n",
        "    #J_3 = tf.trace(tf.multiply(tf.matmul(embeds_sum_1, embeds_sum_2), tf.matmul(v, v, transpose_b=True)))\n",
        "    #R---\n",
        "    J_total = (J_1 + 0.5 * (J_2))\n",
        "    \n",
        "    #I---\n",
        "    J_e_total = (J_e_1 + 0.5 * (J_e_2))\n",
        "    \n",
        "    \n",
        "    # numerator = J_total + J_e_total\n",
        "    predict_rating = alpha * J_total + (1 - alpha) * J_e_total + user_bs + item_bs\n",
        "    loss = tf.reduce_mean(tf.math.squared_difference(predict_rating, ratings))\n",
        "    loss += lambda_1 * (tf.nn.l2_loss(W_i_1) + tf.nn.l2_loss(W_u_1) + tf.nn.l2_loss(user_entity_embedding) + tf.nn.l2_loss(item_entity_embedding) + tf.nn.l2_loss(W_u) + tf.nn.l2_loss(W_i) + tf.nn.l2_loss(v) + tf.nn.l2_loss(v_entity) + tf.nn.l2_loss(rand_matrix) + tf.nn.l2_loss(user_bs) + tf.nn.l2_loss(item_bs))\n",
        "    \n",
        "    train_step = tf.compat.v1.train.RMSPropOptimizer(learning_rate).minimize(loss)\n",
        "    saver = tf.compat.v1.train.Saver()\n",
        "    \n",
        "    init = tf.compat.v1.global_variables_initializer()\n",
        "    with tf.compat.v1.Session() as sess:\n",
        "        sess.run(init)\n",
        "\n",
        "        for e in range(epochs):\n",
        "            t = time()\n",
        "            loss_total = 0.0\n",
        "            count = 0.0\n",
        "            \n",
        "            for i in range(int(math.ceil(len(user_input) / float(batch_size)))):\n",
        "                user_batch, item_batch, user_input_batch, item_input_batch, rates_batch = get_train_instance_batch_change(i, batch_size,user_input, item_input, rateings, user_reviews,item_reviews)\n",
        "                _, loss_val, words = sess.run([train_step, loss, word_embeddings], feed_dict={users: user_batch, items: item_batch, users_inputs: user_input_batch, items_inputs: item_input_batch, ratings: rates_batch, dropout_rate:drop_out, alpha: a})\n",
        "                loss_total += loss_val\n",
        "                count += 1.0\n",
        "\n",
        "            t1 = time()            \n",
        "            #LOOK FROM HERE \n",
        "            val_mses, val_maes = [], []\n",
        "            for i in range(len(user_input_val)):\n",
        "                #eval_model(users, items, users_inputs, items_inputs, dropout_rate, predict_rating, sess, user_vals[i], item_vals[i], user_input_val[i], item_input_val[i], rating_input_val[i], val_mses, val_maes)\n",
        "                #predicts = sess.run(predict_rating, feed_dict={users: user_batch, items: item_batch, users_inputs: user_input_batch, items_inputs: item_input_batch, dropout_rate:1.0})\n",
        "                predicts = sess.run(predict_rating, feed_dict={users: user_vals[i], items: item_vals[i], users_inputs: user_input_val[i], items_inputs: item_input_val[i], ratings: rating_input_val[i], dropout_rate: drop_out, alpha: a})\n",
        "                #print(predicts)\n",
        "                row, col = predicts.shape\n",
        "                for r in range(row):\n",
        "                    val_mses.append(pow((predicts[r, 0] - rating_input_val[i][r][0]), 2))\n",
        "                    val_maes.append(abs((predicts[r, 0] - rating_input_val[i][r][0])))\n",
        "            val_mse = np.array(val_mses).mean()            \n",
        "            t2 = time()            \n",
        "            mses, maes = [], []\n",
        "            for i in range(len(user_input_test)):\n",
        "                #eval_model(users, items, users_inputs, items_inputs, dropout_rate, predict_rating, sess, user_tests[i], item_tests[i], user_input_test[i], item_input_test[i], rating_input_test[i], mses, maes)\n",
        "                predicts_test = sess.run(predict_rating, feed_dict={users: user_tests[i], items: item_tests[i], users_inputs: user_input_test[i], items_inputs: item_input_test[i], ratings: rating_input_test[i], dropout_rate: drop_out, alpha: a})\n",
        "                #print(predicts)\n",
        "                row, col = predicts_test.shape\n",
        "                for r in range(row):\n",
        "                    mses.append(pow((predicts_test[r, 0] - rating_input_test[i][r][0]), 2))\n",
        "                    maes.append(abs((predicts_test[r, 0] - rating_input_test[i][r][0])))\n",
        "                    \n",
        "            mse = np.array(mses).mean()\n",
        "            mae = np.array(maes).mean()\n",
        "            t3 = time()\n",
        "            mae_list.append(mae)\n",
        "            mse_list.append(mse)\n",
        "            print(\"epoch%d train time: %.3fs test time: %.3f  loss = %.3f val_mse = %.3f mse = %.3f mae = %.3f\"%(e, (t1 - t), (t3 - t2), loss_total/count, val_mse, mse, mae))\n",
        "        \n",
        "        avg_mae=sum(mae_list) / len(mae_list)\n",
        "        avg_mse=sum(mse_list)/len(mse_list)\n",
        "        avg_mae_list.append(avg_mae)\n",
        "        avg_mse_list.append(avg_mse)\n",
        "        print(\"MAE \"+str(avg_mae))\n",
        "        print(\"MSE \"+str(avg_mse))"
      ],
      "id": "M3qW6jiYQuH-"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vBDwt3w8QuWo",
        "outputId": "a417914c-9331-415e-f274-b1356e2a44b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "wordId_dict finished\n",
            "load reviews finished\n",
            "load data: 1.568s\n",
            "506 2581\n",
            "15\n",
            "shape (19930, 300)\n",
            "18880 18880 18880\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/training/rmsprop.py:188: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "epoch0 train time: 22.163s test time: 0.970  loss = 73.102 val_mse = 2.917 mse = 2.923 mae = 1.529\n",
            "epoch1 train time: 5.916s test time: 0.748  loss = 60.798 val_mse = 1.546 mse = 1.491 mae = 0.930\n",
            "epoch2 train time: 5.999s test time: 0.765  loss = 44.603 val_mse = 1.542 mse = 1.487 mae = 0.928\n",
            "epoch3 train time: 6.234s test time: 0.760  loss = 28.812 val_mse = 1.539 mse = 1.484 mae = 0.925\n",
            "epoch4 train time: 6.052s test time: 0.769  loss = 17.841 val_mse = 1.535 mse = 1.481 mae = 0.924\n",
            "epoch5 train time: 6.101s test time: 0.772  loss = 10.598 val_mse = 1.530 mse = 1.477 mae = 0.923\n",
            "epoch6 train time: 6.334s test time: 0.884  loss = 6.093 val_mse = 1.525 mse = 1.473 mae = 0.921\n",
            "epoch7 train time: 6.203s test time: 0.784  loss = 3.500 val_mse = 1.519 mse = 1.469 mae = 0.919\n",
            "epoch8 train time: 6.266s test time: 0.789  loss = 2.164 val_mse = 1.513 mse = 1.464 mae = 0.917\n",
            "epoch9 train time: 6.301s test time: 0.791  loss = 1.582 val_mse = 1.507 mse = 1.459 mae = 0.915\n",
            "epoch10 train time: 6.294s test time: 0.792  loss = 1.391 val_mse = 1.500 mse = 1.454 mae = 0.913\n",
            "epoch11 train time: 6.273s test time: 0.781  loss = 1.346 val_mse = 1.493 mse = 1.449 mae = 0.911\n",
            "epoch12 train time: 6.228s test time: 0.780  loss = 1.323 val_mse = 1.486 mse = 1.444 mae = 0.909\n",
            "epoch13 train time: 6.179s test time: 0.775  loss = 1.302 val_mse = 1.479 mse = 1.439 mae = 0.906\n",
            "epoch14 train time: 6.180s test time: 0.785  loss = 1.282 val_mse = 1.472 mse = 1.434 mae = 0.904\n",
            "epoch15 train time: 6.166s test time: 0.773  loss = 1.264 val_mse = 1.465 mse = 1.429 mae = 0.902\n",
            "epoch16 train time: 6.162s test time: 0.774  loss = 1.247 val_mse = 1.458 mse = 1.424 mae = 0.900\n",
            "epoch17 train time: 6.284s test time: 0.777  loss = 1.231 val_mse = 1.451 mse = 1.418 mae = 0.898\n",
            "epoch18 train time: 6.176s test time: 0.780  loss = 1.215 val_mse = 1.443 mse = 1.413 mae = 0.896\n",
            "epoch19 train time: 6.204s test time: 0.777  loss = 1.201 val_mse = 1.436 mse = 1.409 mae = 0.894\n",
            "epoch20 train time: 6.207s test time: 0.775  loss = 1.187 val_mse = 1.429 mse = 1.404 mae = 0.892\n",
            "epoch21 train time: 6.213s test time: 0.780  loss = 1.173 val_mse = 1.422 mse = 1.399 mae = 0.890\n",
            "epoch22 train time: 6.208s test time: 0.782  loss = 1.160 val_mse = 1.415 mse = 1.394 mae = 0.888\n",
            "epoch23 train time: 6.203s test time: 0.778  loss = 1.148 val_mse = 1.409 mse = 1.390 mae = 0.887\n",
            "epoch24 train time: 6.204s test time: 0.783  loss = 1.136 val_mse = 1.402 mse = 1.385 mae = 0.885\n",
            "epoch25 train time: 6.193s test time: 0.777  loss = 1.124 val_mse = 1.395 mse = 1.381 mae = 0.883\n",
            "epoch26 train time: 6.188s test time: 0.778  loss = 1.112 val_mse = 1.389 mse = 1.377 mae = 0.882\n",
            "epoch27 train time: 6.188s test time: 0.777  loss = 1.101 val_mse = 1.383 mse = 1.373 mae = 0.880\n",
            "epoch28 train time: 6.185s test time: 0.776  loss = 1.091 val_mse = 1.377 mse = 1.369 mae = 0.878\n",
            "epoch29 train time: 6.178s test time: 0.774  loss = 1.080 val_mse = 1.371 mse = 1.365 mae = 0.877\n",
            "epoch30 train time: 6.186s test time: 0.778  loss = 1.071 val_mse = 1.365 mse = 1.361 mae = 0.875\n",
            "epoch31 train time: 6.201s test time: 0.776  loss = 1.061 val_mse = 1.359 mse = 1.357 mae = 0.874\n",
            "epoch32 train time: 6.356s test time: 0.787  loss = 1.052 val_mse = 1.353 mse = 1.354 mae = 0.872\n",
            "epoch33 train time: 6.203s test time: 0.775  loss = 1.043 val_mse = 1.348 mse = 1.350 mae = 0.871\n",
            "epoch34 train time: 6.198s test time: 0.781  loss = 1.034 val_mse = 1.343 mse = 1.347 mae = 0.870\n",
            "epoch35 train time: 6.202s test time: 0.779  loss = 1.026 val_mse = 1.337 mse = 1.344 mae = 0.868\n",
            "epoch36 train time: 6.215s test time: 0.777  loss = 1.018 val_mse = 1.332 mse = 1.341 mae = 0.867\n",
            "epoch37 train time: 6.198s test time: 0.776  loss = 1.010 val_mse = 1.328 mse = 1.338 mae = 0.866\n",
            "epoch38 train time: 6.197s test time: 0.777  loss = 1.002 val_mse = 1.323 mse = 1.335 mae = 0.864\n",
            "epoch39 train time: 6.195s test time: 0.779  loss = 0.995 val_mse = 1.318 mse = 1.332 mae = 0.863\n",
            "epoch40 train time: 6.201s test time: 0.781  loss = 0.988 val_mse = 1.314 mse = 1.330 mae = 0.862\n",
            "epoch41 train time: 6.203s test time: 0.773  loss = 0.981 val_mse = 1.309 mse = 1.327 mae = 0.860\n",
            "epoch42 train time: 6.212s test time: 0.777  loss = 0.975 val_mse = 1.305 mse = 1.325 mae = 0.859\n",
            "epoch43 train time: 6.198s test time: 0.778  loss = 0.968 val_mse = 1.301 mse = 1.322 mae = 0.858\n",
            "epoch44 train time: 6.198s test time: 0.777  loss = 0.962 val_mse = 1.297 mse = 1.320 mae = 0.857\n",
            "epoch45 train time: 6.201s test time: 0.777  loss = 0.956 val_mse = 1.293 mse = 1.318 mae = 0.855\n",
            "epoch46 train time: 6.208s test time: 0.774  loss = 0.951 val_mse = 1.290 mse = 1.316 mae = 0.854\n",
            "epoch47 train time: 6.195s test time: 0.783  loss = 0.945 val_mse = 1.286 mse = 1.314 mae = 0.853\n",
            "epoch48 train time: 6.204s test time: 0.780  loss = 0.940 val_mse = 1.283 mse = 1.312 mae = 0.852\n",
            "epoch49 train time: 6.202s test time: 0.777  loss = 0.935 val_mse = 1.279 mse = 1.310 mae = 0.851\n",
            "epoch50 train time: 6.201s test time: 0.781  loss = 0.930 val_mse = 1.276 mse = 1.309 mae = 0.850\n",
            "epoch51 train time: 6.199s test time: 0.776  loss = 0.925 val_mse = 1.273 mse = 1.307 mae = 0.849\n",
            "epoch52 train time: 6.206s test time: 0.776  loss = 0.920 val_mse = 1.270 mse = 1.305 mae = 0.848\n",
            "epoch53 train time: 6.209s test time: 0.780  loss = 0.916 val_mse = 1.267 mse = 1.304 mae = 0.847\n",
            "epoch54 train time: 6.206s test time: 0.780  loss = 0.912 val_mse = 1.265 mse = 1.303 mae = 0.846\n",
            "epoch55 train time: 6.205s test time: 0.778  loss = 0.908 val_mse = 1.261 mse = 1.301 mae = 0.845\n",
            "epoch56 train time: 6.212s test time: 0.779  loss = 0.904 val_mse = 1.260 mse = 1.300 mae = 0.844\n",
            "epoch57 train time: 6.201s test time: 0.779  loss = 0.900 val_mse = 1.256 mse = 1.298 mae = 0.843\n",
            "epoch58 train time: 6.202s test time: 0.788  loss = 0.896 val_mse = 1.255 mse = 1.298 mae = 0.842\n",
            "epoch59 train time: 6.210s test time: 0.780  loss = 0.892 val_mse = 1.252 mse = 1.296 mae = 0.841\n",
            "epoch60 train time: 6.191s test time: 0.775  loss = 0.889 val_mse = 1.250 mse = 1.295 mae = 0.840\n",
            "epoch61 train time: 6.200s test time: 0.777  loss = 0.886 val_mse = 1.246 mse = 1.293 mae = 0.840\n",
            "epoch62 train time: 6.218s test time: 0.774  loss = 0.883 val_mse = 1.246 mse = 1.294 mae = 0.839\n",
            "epoch63 train time: 6.190s test time: 0.774  loss = 0.879 val_mse = 1.244 mse = 1.293 mae = 0.838\n",
            "epoch64 train time: 6.187s test time: 0.778  loss = 0.877 val_mse = 1.241 mse = 1.292 mae = 0.838\n",
            "epoch65 train time: 6.195s test time: 0.780  loss = 0.874 val_mse = 1.240 mse = 1.291 mae = 0.837\n",
            "epoch66 train time: 6.202s test time: 0.777  loss = 0.871 val_mse = 1.237 mse = 1.290 mae = 0.836\n",
            "epoch67 train time: 6.205s test time: 0.777  loss = 0.868 val_mse = 1.238 mse = 1.291 mae = 0.835\n",
            "epoch68 train time: 6.206s test time: 0.779  loss = 0.866 val_mse = 1.235 mse = 1.289 mae = 0.836\n",
            "epoch69 train time: 6.207s test time: 0.781  loss = 0.864 val_mse = 1.237 mse = 1.292 mae = 0.833\n",
            "epoch70 train time: 6.207s test time: 0.778  loss = 0.861 val_mse = 1.225 mse = 1.283 mae = 0.838\n",
            "epoch71 train time: 6.194s test time: 0.775  loss = 0.859 val_mse = 1.233 mse = 1.290 mae = 0.832\n",
            "epoch72 train time: 6.197s test time: 0.776  loss = 0.857 val_mse = 1.222 mse = 1.282 mae = 0.837\n",
            "epoch73 train time: 6.195s test time: 0.773  loss = 0.855 val_mse = 1.227 mse = 1.287 mae = 0.832\n",
            "epoch74 train time: 6.191s test time: 0.776  loss = 0.852 val_mse = 1.219 mse = 1.280 mae = 0.838\n",
            "epoch75 train time: 6.204s test time: 0.783  loss = 0.852 val_mse = 1.235 mse = 1.295 mae = 0.828\n",
            "epoch76 train time: 6.194s test time: 0.776  loss = 0.848 val_mse = 1.210 mse = 1.275 mae = 0.850\n",
            "epoch77 train time: 6.188s test time: 0.782  loss = 0.848 val_mse = 1.229 mse = 1.290 mae = 0.828\n",
            "epoch78 train time: 6.214s test time: 0.782  loss = 0.845 val_mse = 1.210 mse = 1.276 mae = 0.850\n",
            "epoch79 train time: 6.198s test time: 0.775  loss = 0.845 val_mse = 1.227 mse = 1.292 mae = 0.827\n",
            "epoch80 train time: 6.193s test time: 0.780  loss = 0.841 val_mse = 1.206 mse = 1.275 mae = 0.851\n",
            "epoch81 train time: 6.192s test time: 0.776  loss = 0.841 val_mse = 1.222 mse = 1.289 mae = 0.827\n",
            "epoch82 train time: 6.197s test time: 0.779  loss = 0.839 val_mse = 1.203 mse = 1.273 mae = 0.846\n",
            "epoch83 train time: 6.194s test time: 0.778  loss = 0.839 val_mse = 1.222 mse = 1.289 mae = 0.827\n",
            "epoch84 train time: 6.199s test time: 0.781  loss = 0.836 val_mse = 1.201 mse = 1.272 mae = 0.844\n",
            "epoch85 train time: 6.202s test time: 0.774  loss = 0.835 val_mse = 1.228 mse = 1.295 mae = 0.824\n",
            "epoch86 train time: 6.192s test time: 0.779  loss = 0.833 val_mse = 1.197 mse = 1.271 mae = 0.847\n",
            "epoch87 train time: 6.200s test time: 0.778  loss = 0.833 val_mse = 1.224 mse = 1.291 mae = 0.824\n",
            "epoch88 train time: 6.198s test time: 0.778  loss = 0.831 val_mse = 1.194 mse = 1.270 mae = 0.841\n",
            "epoch89 train time: 6.196s test time: 0.776  loss = 0.830 val_mse = 1.229 mse = 1.297 mae = 0.822\n",
            "epoch90 train time: 6.204s test time: 0.781  loss = 0.829 val_mse = 1.197 mse = 1.269 mae = 0.837\n",
            "epoch91 train time: 6.221s test time: 0.774  loss = 0.830 val_mse = 1.236 mse = 1.303 mae = 0.821\n",
            "epoch92 train time: 6.206s test time: 0.778  loss = 0.827 val_mse = 1.218 mse = 1.291 mae = 0.823\n",
            "epoch93 train time: 6.211s test time: 0.783  loss = 0.826 val_mse = 1.192 mse = 1.270 mae = 0.842\n",
            "epoch94 train time: 6.206s test time: 0.781  loss = 0.825 val_mse = 1.223 mse = 1.296 mae = 0.821\n",
            "epoch95 train time: 6.200s test time: 0.782  loss = 0.823 val_mse = 1.192 mse = 1.269 mae = 0.837\n",
            "epoch96 train time: 6.198s test time: 0.778  loss = 0.825 val_mse = 1.217 mse = 1.293 mae = 0.822\n",
            "epoch97 train time: 6.196s test time: 0.776  loss = 0.821 val_mse = 1.191 mse = 1.270 mae = 0.840\n",
            "epoch98 train time: 6.206s test time: 0.778  loss = 0.822 val_mse = 1.212 mse = 1.289 mae = 0.822\n",
            "epoch99 train time: 6.194s test time: 0.774  loss = 0.820 val_mse = 1.206 mse = 1.283 mae = 0.824\n",
            "epoch100 train time: 6.197s test time: 0.775  loss = 0.819 val_mse = 1.184 mse = 1.269 mae = 0.838\n",
            "epoch101 train time: 6.206s test time: 0.774  loss = 0.818 val_mse = 1.213 mse = 1.291 mae = 0.822\n",
            "epoch102 train time: 6.199s test time: 0.782  loss = 0.817 val_mse = 1.183 mse = 1.268 mae = 0.838\n",
            "epoch103 train time: 6.206s test time: 0.777  loss = 0.818 val_mse = 1.193 mse = 1.274 mae = 0.828\n",
            "epoch104 train time: 6.201s test time: 0.781  loss = 0.816 val_mse = 1.206 mse = 1.288 mae = 0.822\n",
            "epoch105 train time: 6.210s test time: 0.780  loss = 0.815 val_mse = 1.182 mse = 1.269 mae = 0.837\n",
            "epoch106 train time: 6.196s test time: 0.775  loss = 0.813 val_mse = 1.212 mse = 1.294 mae = 0.819\n",
            "epoch107 train time: 6.187s test time: 0.773  loss = 0.814 val_mse = 1.193 mse = 1.273 mae = 0.825\n",
            "epoch108 train time: 6.198s test time: 0.779  loss = 0.814 val_mse = 1.182 mse = 1.268 mae = 0.837\n",
            "epoch109 train time: 6.198s test time: 0.773  loss = 0.813 val_mse = 1.207 mse = 1.294 mae = 0.821\n",
            "epoch110 train time: 6.198s test time: 0.773  loss = 0.811 val_mse = 1.185 mse = 1.273 mae = 0.826\n",
            "epoch111 train time: 6.199s test time: 0.778  loss = 0.813 val_mse = 1.181 mse = 1.266 mae = 0.831\n",
            "epoch112 train time: 6.199s test time: 0.778  loss = 0.813 val_mse = 1.201 mse = 1.283 mae = 0.822\n",
            "epoch113 train time: 6.205s test time: 0.779  loss = 0.812 val_mse = 1.209 mse = 1.291 mae = 0.821\n",
            "epoch114 train time: 6.218s test time: 0.781  loss = 0.809 val_mse = 1.180 mse = 1.268 mae = 0.835\n",
            "epoch115 train time: 6.190s test time: 0.775  loss = 0.809 val_mse = 1.215 mse = 1.298 mae = 0.819\n",
            "epoch116 train time: 6.202s test time: 0.782  loss = 0.807 val_mse = 1.179 mse = 1.269 mae = 0.833\n",
            "epoch117 train time: 6.208s test time: 0.778  loss = 0.810 val_mse = 1.185 mse = 1.275 mae = 0.827\n",
            "epoch118 train time: 6.196s test time: 0.778  loss = 0.808 val_mse = 1.174 mse = 1.263 mae = 0.831\n",
            "epoch119 train time: 6.195s test time: 0.773  loss = 0.809 val_mse = 1.202 mse = 1.288 mae = 0.821\n",
            "MAE 0.8611507236816345\n",
            "MSE 1.34180853475892\n",
            "epoch0 train time: 6.637s test time: 0.787  loss = 70.474 val_mse = 1.806 mse = 1.776 mae = 1.145\n",
            "epoch1 train time: 6.263s test time: 0.788  loss = 60.098 val_mse = 1.552 mse = 1.494 mae = 0.921\n",
            "epoch2 train time: 6.269s test time: 0.782  loss = 44.162 val_mse = 1.549 mse = 1.491 mae = 0.921\n",
            "epoch3 train time: 6.292s test time: 0.785  loss = 28.483 val_mse = 1.547 mse = 1.489 mae = 0.920\n",
            "epoch4 train time: 6.260s test time: 0.782  loss = 17.610 val_mse = 1.544 mse = 1.487 mae = 0.919\n",
            "epoch5 train time: 6.278s test time: 0.785  loss = 10.448 val_mse = 1.540 mse = 1.484 mae = 0.918\n",
            "epoch6 train time: 6.285s test time: 0.784  loss = 6.003 val_mse = 1.537 mse = 1.482 mae = 0.917\n",
            "epoch7 train time: 6.255s test time: 0.786  loss = 3.452 val_mse = 1.533 mse = 1.478 mae = 0.916\n",
            "epoch8 train time: 6.253s test time: 0.777  loss = 2.143 val_mse = 1.529 mse = 1.475 mae = 0.915\n",
            "epoch9 train time: 6.249s test time: 0.991  loss = 1.578 val_mse = 1.524 mse = 1.472 mae = 0.914\n",
            "epoch10 train time: 7.377s test time: 1.051  loss = 1.397 val_mse = 1.519 mse = 1.468 mae = 0.912\n",
            "epoch11 train time: 6.993s test time: 0.882  loss = 1.356 val_mse = 1.514 mse = 1.465 mae = 0.911\n",
            "epoch12 train time: 6.600s test time: 0.781  loss = 1.336 val_mse = 1.510 mse = 1.461 mae = 0.910\n",
            "epoch13 train time: 6.231s test time: 0.777  loss = 1.318 val_mse = 1.504 mse = 1.457 mae = 0.908\n",
            "epoch14 train time: 6.244s test time: 0.774  loss = 1.301 val_mse = 1.499 mse = 1.454 mae = 0.907\n",
            "epoch15 train time: 6.239s test time: 0.775  loss = 1.285 val_mse = 1.494 mse = 1.450 mae = 0.905\n",
            "epoch16 train time: 6.223s test time: 0.773  loss = 1.270 val_mse = 1.489 mse = 1.446 mae = 0.904\n",
            "epoch17 train time: 6.235s test time: 0.772  loss = 1.255 val_mse = 1.484 mse = 1.442 mae = 0.903\n",
            "epoch18 train time: 6.250s test time: 0.784  loss = 1.242 val_mse = 1.478 mse = 1.439 mae = 0.902\n",
            "epoch19 train time: 6.231s test time: 0.779  loss = 1.229 val_mse = 1.473 mse = 1.435 mae = 0.900\n",
            "epoch20 train time: 6.244s test time: 0.779  loss = 1.216 val_mse = 1.468 mse = 1.431 mae = 0.899\n",
            "epoch21 train time: 6.236s test time: 0.791  loss = 1.204 val_mse = 1.463 mse = 1.428 mae = 0.898\n",
            "epoch22 train time: 6.234s test time: 0.779  loss = 1.193 val_mse = 1.457 mse = 1.424 mae = 0.896\n",
            "epoch23 train time: 6.236s test time: 0.779  loss = 1.182 val_mse = 1.452 mse = 1.421 mae = 0.895\n",
            "epoch24 train time: 6.243s test time: 0.773  loss = 1.171 val_mse = 1.447 mse = 1.417 mae = 0.894\n",
            "epoch25 train time: 6.230s test time: 0.775  loss = 1.160 val_mse = 1.442 mse = 1.414 mae = 0.893\n",
            "epoch26 train time: 6.234s test time: 0.773  loss = 1.150 val_mse = 1.437 mse = 1.410 mae = 0.892\n",
            "epoch27 train time: 6.236s test time: 0.778  loss = 1.140 val_mse = 1.432 mse = 1.407 mae = 0.891\n",
            "epoch28 train time: 6.235s test time: 0.777  loss = 1.131 val_mse = 1.427 mse = 1.403 mae = 0.890\n",
            "epoch29 train time: 6.239s test time: 0.772  loss = 1.122 val_mse = 1.422 mse = 1.400 mae = 0.888\n",
            "epoch30 train time: 6.224s test time: 0.778  loss = 1.113 val_mse = 1.417 mse = 1.397 mae = 0.887\n",
            "epoch31 train time: 6.238s test time: 0.775  loss = 1.104 val_mse = 1.412 mse = 1.394 mae = 0.886\n",
            "epoch32 train time: 6.226s test time: 0.778  loss = 1.096 val_mse = 1.408 mse = 1.391 mae = 0.885\n",
            "epoch33 train time: 6.227s test time: 0.773  loss = 1.088 val_mse = 1.403 mse = 1.388 mae = 0.884\n",
            "epoch34 train time: 6.229s test time: 0.774  loss = 1.080 val_mse = 1.398 mse = 1.385 mae = 0.883\n",
            "epoch35 train time: 6.226s test time: 0.772  loss = 1.073 val_mse = 1.394 mse = 1.382 mae = 0.882\n",
            "epoch36 train time: 6.244s test time: 0.777  loss = 1.066 val_mse = 1.389 mse = 1.379 mae = 0.881\n",
            "epoch37 train time: 6.242s test time: 0.778  loss = 1.059 val_mse = 1.385 mse = 1.376 mae = 0.880\n",
            "epoch38 train time: 6.229s test time: 0.781  loss = 1.052 val_mse = 1.381 mse = 1.373 mae = 0.879\n",
            "epoch39 train time: 6.249s test time: 0.776  loss = 1.045 val_mse = 1.377 mse = 1.370 mae = 0.878\n",
            "epoch40 train time: 6.237s test time: 0.778  loss = 1.039 val_mse = 1.372 mse = 1.368 mae = 0.877\n",
            "epoch41 train time: 6.231s test time: 0.776  loss = 1.033 val_mse = 1.368 mse = 1.365 mae = 0.877\n",
            "epoch42 train time: 6.239s test time: 0.774  loss = 1.026 val_mse = 1.364 mse = 1.363 mae = 0.876\n",
            "epoch43 train time: 6.232s test time: 0.778  loss = 1.020 val_mse = 1.360 mse = 1.360 mae = 0.875\n",
            "epoch44 train time: 6.244s test time: 0.780  loss = 1.015 val_mse = 1.356 mse = 1.358 mae = 0.874\n",
            "epoch45 train time: 6.242s test time: 0.776  loss = 1.009 val_mse = 1.353 mse = 1.355 mae = 0.873\n",
            "epoch46 train time: 6.234s test time: 0.777  loss = 1.004 val_mse = 1.349 mse = 1.353 mae = 0.872\n",
            "epoch47 train time: 6.234s test time: 0.777  loss = 0.998 val_mse = 1.345 mse = 1.351 mae = 0.871\n",
            "epoch48 train time: 6.241s test time: 0.786  loss = 0.993 val_mse = 1.342 mse = 1.348 mae = 0.870\n",
            "epoch49 train time: 6.244s test time: 0.776  loss = 0.988 val_mse = 1.338 mse = 1.346 mae = 0.869\n",
            "epoch50 train time: 6.235s test time: 0.774  loss = 0.983 val_mse = 1.335 mse = 1.344 mae = 0.869\n",
            "epoch51 train time: 6.224s test time: 0.779  loss = 0.979 val_mse = 1.331 mse = 1.342 mae = 0.868\n",
            "epoch52 train time: 6.216s test time: 0.779  loss = 0.974 val_mse = 1.328 mse = 1.340 mae = 0.867\n",
            "epoch53 train time: 6.225s test time: 0.777  loss = 0.969 val_mse = 1.325 mse = 1.338 mae = 0.866\n",
            "epoch54 train time: 6.228s test time: 0.780  loss = 0.965 val_mse = 1.321 mse = 1.336 mae = 0.865\n",
            "epoch55 train time: 6.232s test time: 0.775  loss = 0.961 val_mse = 1.318 mse = 1.334 mae = 0.864\n",
            "epoch56 train time: 6.243s test time: 0.777  loss = 0.957 val_mse = 1.315 mse = 1.332 mae = 0.864\n",
            "epoch57 train time: 6.241s test time: 0.775  loss = 0.953 val_mse = 1.312 mse = 1.331 mae = 0.863\n",
            "epoch58 train time: 6.221s test time: 0.781  loss = 0.949 val_mse = 1.309 mse = 1.329 mae = 0.862\n",
            "epoch59 train time: 6.232s test time: 0.776  loss = 0.945 val_mse = 1.307 mse = 1.327 mae = 0.861\n",
            "epoch60 train time: 6.229s test time: 0.775  loss = 0.941 val_mse = 1.304 mse = 1.325 mae = 0.860\n",
            "epoch61 train time: 6.237s test time: 0.774  loss = 0.938 val_mse = 1.301 mse = 1.324 mae = 0.860\n",
            "epoch62 train time: 6.244s test time: 0.800  loss = 0.934 val_mse = 1.298 mse = 1.322 mae = 0.859\n",
            "epoch63 train time: 6.814s test time: 0.880  loss = 0.931 val_mse = 1.296 mse = 1.321 mae = 0.858\n",
            "epoch64 train time: 6.853s test time: 0.795  loss = 0.927 val_mse = 1.293 mse = 1.319 mae = 0.857\n",
            "epoch65 train time: 6.227s test time: 0.779  loss = 0.924 val_mse = 1.291 mse = 1.318 mae = 0.856\n",
            "epoch66 train time: 6.226s test time: 0.772  loss = 0.921 val_mse = 1.288 mse = 1.316 mae = 0.856\n",
            "epoch67 train time: 6.225s test time: 0.776  loss = 0.918 val_mse = 1.286 mse = 1.315 mae = 0.855\n",
            "epoch68 train time: 6.230s test time: 0.772  loss = 0.915 val_mse = 1.283 mse = 1.314 mae = 0.854\n",
            "epoch69 train time: 6.249s test time: 0.777  loss = 0.912 val_mse = 1.281 mse = 1.312 mae = 0.854\n",
            "epoch70 train time: 6.231s test time: 0.775  loss = 0.909 val_mse = 1.279 mse = 1.311 mae = 0.853\n",
            "epoch71 train time: 6.231s test time: 0.780  loss = 0.906 val_mse = 1.277 mse = 1.310 mae = 0.852\n",
            "epoch72 train time: 6.234s test time: 0.775  loss = 0.904 val_mse = 1.275 mse = 1.309 mae = 0.851\n",
            "epoch73 train time: 6.237s test time: 0.772  loss = 0.901 val_mse = 1.272 mse = 1.308 mae = 0.851\n",
            "epoch74 train time: 6.225s test time: 0.771  loss = 0.898 val_mse = 1.271 mse = 1.307 mae = 0.850\n",
            "epoch75 train time: 6.220s test time: 0.774  loss = 0.896 val_mse = 1.269 mse = 1.306 mae = 0.849\n",
            "epoch76 train time: 6.218s test time: 0.777  loss = 0.893 val_mse = 1.267 mse = 1.305 mae = 0.849\n",
            "epoch77 train time: 6.227s test time: 0.770  loss = 0.891 val_mse = 1.265 mse = 1.304 mae = 0.848\n",
            "epoch78 train time: 6.373s test time: 0.771  loss = 0.889 val_mse = 1.263 mse = 1.303 mae = 0.847\n",
            "epoch79 train time: 6.218s test time: 0.779  loss = 0.887 val_mse = 1.261 mse = 1.302 mae = 0.846\n",
            "epoch80 train time: 6.237s test time: 0.777  loss = 0.884 val_mse = 1.260 mse = 1.301 mae = 0.846\n",
            "epoch81 train time: 6.236s test time: 0.778  loss = 0.882 val_mse = 1.258 mse = 1.300 mae = 0.845\n",
            "epoch82 train time: 6.229s test time: 0.779  loss = 0.880 val_mse = 1.256 mse = 1.299 mae = 0.845\n",
            "epoch83 train time: 6.224s test time: 0.775  loss = 0.878 val_mse = 1.255 mse = 1.298 mae = 0.844\n",
            "epoch84 train time: 6.237s test time: 0.774  loss = 0.876 val_mse = 1.253 mse = 1.298 mae = 0.843\n",
            "epoch85 train time: 6.225s test time: 0.779  loss = 0.874 val_mse = 1.251 mse = 1.297 mae = 0.843\n",
            "epoch86 train time: 6.215s test time: 0.778  loss = 0.872 val_mse = 1.250 mse = 1.296 mae = 0.842\n",
            "epoch87 train time: 6.230s test time: 0.778  loss = 0.870 val_mse = 1.248 mse = 1.295 mae = 0.842\n",
            "epoch88 train time: 6.229s test time: 0.774  loss = 0.869 val_mse = 1.247 mse = 1.295 mae = 0.841\n",
            "epoch89 train time: 6.239s test time: 0.773  loss = 0.867 val_mse = 1.245 mse = 1.294 mae = 0.841\n",
            "epoch90 train time: 6.208s test time: 0.774  loss = 0.865 val_mse = 1.244 mse = 1.293 mae = 0.840\n",
            "epoch91 train time: 6.240s test time: 0.787  loss = 0.863 val_mse = 1.243 mse = 1.293 mae = 0.840\n",
            "epoch92 train time: 6.385s test time: 0.775  loss = 0.862 val_mse = 1.242 mse = 1.292 mae = 0.839\n",
            "epoch93 train time: 6.221s test time: 0.774  loss = 0.860 val_mse = 1.239 mse = 1.291 mae = 0.839\n",
            "epoch94 train time: 6.228s test time: 0.778  loss = 0.859 val_mse = 1.239 mse = 1.291 mae = 0.838\n",
            "epoch95 train time: 6.207s test time: 0.772  loss = 0.857 val_mse = 1.239 mse = 1.292 mae = 0.838\n",
            "epoch96 train time: 6.227s test time: 0.779  loss = 0.856 val_mse = 1.235 mse = 1.289 mae = 0.838\n",
            "epoch97 train time: 6.211s test time: 0.776  loss = 0.854 val_mse = 1.237 mse = 1.291 mae = 0.836\n",
            "epoch98 train time: 6.225s test time: 0.779  loss = 0.853 val_mse = 1.232 mse = 1.287 mae = 0.837\n",
            "epoch99 train time: 6.223s test time: 0.773  loss = 0.851 val_mse = 1.235 mse = 1.291 mae = 0.835\n",
            "epoch100 train time: 6.208s test time: 0.778  loss = 0.850 val_mse = 1.231 mse = 1.287 mae = 0.836\n",
            "epoch101 train time: 6.229s test time: 0.787  loss = 0.849 val_mse = 1.231 mse = 1.289 mae = 0.835\n",
            "epoch102 train time: 6.222s test time: 0.780  loss = 0.848 val_mse = 1.227 mse = 1.285 mae = 0.836\n",
            "epoch103 train time: 6.224s test time: 0.775  loss = 0.846 val_mse = 1.231 mse = 1.290 mae = 0.834\n",
            "epoch104 train time: 6.210s test time: 0.772  loss = 0.845 val_mse = 1.227 mse = 1.286 mae = 0.835\n",
            "epoch105 train time: 6.225s test time: 0.778  loss = 0.844 val_mse = 1.228 mse = 1.288 mae = 0.833\n",
            "epoch106 train time: 6.214s test time: 0.773  loss = 0.842 val_mse = 1.225 mse = 1.285 mae = 0.834\n",
            "epoch107 train time: 6.375s test time: 0.776  loss = 0.842 val_mse = 1.225 mse = 1.286 mae = 0.833\n",
            "epoch108 train time: 6.228s test time: 0.773  loss = 0.840 val_mse = 1.224 mse = 1.285 mae = 0.833\n",
            "epoch109 train time: 6.228s test time: 0.772  loss = 0.839 val_mse = 1.227 mse = 1.288 mae = 0.831\n",
            "epoch110 train time: 6.214s test time: 0.781  loss = 0.838 val_mse = 1.220 mse = 1.283 mae = 0.833\n",
            "epoch111 train time: 6.220s test time: 0.777  loss = 0.837 val_mse = 1.223 mse = 1.287 mae = 0.832\n",
            "epoch112 train time: 6.233s test time: 0.779  loss = 0.837 val_mse = 1.220 mse = 1.283 mae = 0.832\n",
            "epoch113 train time: 6.237s test time: 0.773  loss = 0.835 val_mse = 1.222 mse = 1.286 mae = 0.830\n",
            "epoch114 train time: 6.242s test time: 0.771  loss = 0.834 val_mse = 1.218 mse = 1.282 mae = 0.831\n",
            "epoch115 train time: 6.232s test time: 0.772  loss = 0.833 val_mse = 1.223 mse = 1.287 mae = 0.830\n",
            "epoch116 train time: 6.228s test time: 0.775  loss = 0.833 val_mse = 1.216 mse = 1.282 mae = 0.831\n",
            "epoch117 train time: 6.240s test time: 0.777  loss = 0.832 val_mse = 1.216 mse = 1.284 mae = 0.830\n",
            "epoch118 train time: 6.239s test time: 0.778  loss = 0.830 val_mse = 1.216 mse = 1.282 mae = 0.830\n",
            "epoch119 train time: 6.249s test time: 0.772  loss = 0.830 val_mse = 1.216 mse = 1.285 mae = 0.829\n",
            "MAE 0.8678378841134752\n",
            "MSE 1.353348614027472\n",
            "epoch0 train time: 6.627s test time: 0.782  loss = 71.512 val_mse = 2.064 mse = 2.045 mae = 1.231\n",
            "epoch1 train time: 6.262s test time: 0.777  loss = 60.539 val_mse = 1.554 mse = 1.494 mae = 0.928\n",
            "epoch2 train time: 6.271s test time: 0.779  loss = 44.484 val_mse = 1.552 mse = 1.493 mae = 0.928\n",
            "epoch3 train time: 6.272s test time: 0.782  loss = 28.715 val_mse = 1.551 mse = 1.492 mae = 0.928\n",
            "epoch4 train time: 6.274s test time: 0.782  loss = 17.761 val_mse = 1.549 mse = 1.490 mae = 0.927\n",
            "epoch5 train time: 6.267s test time: 0.782  loss = 10.532 val_mse = 1.547 mse = 1.489 mae = 0.926\n",
            "epoch6 train time: 6.253s test time: 0.780  loss = 6.035 val_mse = 1.544 mse = 1.486 mae = 0.925\n",
            "epoch7 train time: 6.258s test time: 0.781  loss = 3.452 val_mse = 1.541 mse = 1.484 mae = 0.924\n",
            "epoch8 train time: 6.253s test time: 0.783  loss = 2.124 val_mse = 1.538 mse = 1.482 mae = 0.923\n",
            "epoch9 train time: 6.240s test time: 0.780  loss = 1.551 val_mse = 1.535 mse = 1.480 mae = 0.923\n",
            "epoch10 train time: 6.231s test time: 0.776  loss = 1.368 val_mse = 1.532 mse = 1.477 mae = 0.922\n",
            "epoch11 train time: 6.240s test time: 0.777  loss = 1.330 val_mse = 1.529 mse = 1.475 mae = 0.921\n",
            "epoch12 train time: 6.237s test time: 0.777  loss = 1.313 val_mse = 1.525 mse = 1.472 mae = 0.920\n",
            "epoch13 train time: 6.219s test time: 0.773  loss = 1.298 val_mse = 1.522 mse = 1.470 mae = 0.919\n",
            "epoch14 train time: 6.224s test time: 0.776  loss = 1.284 val_mse = 1.519 mse = 1.467 mae = 0.918\n",
            "epoch15 train time: 6.228s test time: 0.782  loss = 1.271 val_mse = 1.515 mse = 1.465 mae = 0.917\n",
            "epoch16 train time: 6.223s test time: 0.777  loss = 1.259 val_mse = 1.512 mse = 1.463 mae = 0.916\n",
            "epoch17 train time: 6.227s test time: 0.776  loss = 1.247 val_mse = 1.508 mse = 1.460 mae = 0.915\n",
            "epoch18 train time: 6.228s test time: 0.780  loss = 1.236 val_mse = 1.504 mse = 1.458 mae = 0.914\n",
            "epoch19 train time: 6.217s test time: 0.781  loss = 1.226 val_mse = 1.501 mse = 1.455 mae = 0.913\n",
            "epoch20 train time: 6.218s test time: 0.780  loss = 1.216 val_mse = 1.497 mse = 1.453 mae = 0.912\n",
            "epoch21 train time: 6.216s test time: 0.773  loss = 1.207 val_mse = 1.494 mse = 1.451 mae = 0.911\n",
            "epoch22 train time: 6.222s test time: 0.777  loss = 1.198 val_mse = 1.490 mse = 1.448 mae = 0.910\n",
            "epoch23 train time: 6.226s test time: 0.774  loss = 1.190 val_mse = 1.487 mse = 1.446 mae = 0.909\n",
            "epoch24 train time: 6.236s test time: 0.777  loss = 1.182 val_mse = 1.483 mse = 1.444 mae = 0.909\n",
            "epoch25 train time: 6.226s test time: 0.780  loss = 1.174 val_mse = 1.480 mse = 1.441 mae = 0.908\n",
            "epoch26 train time: 6.226s test time: 0.773  loss = 1.167 val_mse = 1.477 mse = 1.439 mae = 0.907\n",
            "epoch27 train time: 6.227s test time: 0.777  loss = 1.160 val_mse = 1.473 mse = 1.437 mae = 0.906\n",
            "epoch28 train time: 6.215s test time: 0.771  loss = 1.153 val_mse = 1.470 mse = 1.435 mae = 0.905\n",
            "epoch29 train time: 6.215s test time: 0.777  loss = 1.146 val_mse = 1.467 mse = 1.432 mae = 0.904\n",
            "epoch30 train time: 6.222s test time: 0.782  loss = 1.140 val_mse = 1.463 mse = 1.430 mae = 0.904\n",
            "epoch31 train time: 6.227s test time: 0.771  loss = 1.134 val_mse = 1.460 mse = 1.428 mae = 0.903\n",
            "epoch32 train time: 6.213s test time: 0.778  loss = 1.128 val_mse = 1.457 mse = 1.426 mae = 0.902\n",
            "epoch33 train time: 6.230s test time: 0.777  loss = 1.122 val_mse = 1.454 mse = 1.424 mae = 0.901\n",
            "epoch34 train time: 6.216s test time: 0.775  loss = 1.116 val_mse = 1.451 mse = 1.422 mae = 0.900\n",
            "epoch35 train time: 6.215s test time: 0.778  loss = 1.111 val_mse = 1.447 mse = 1.420 mae = 0.899\n",
            "epoch36 train time: 6.222s test time: 0.771  loss = 1.105 val_mse = 1.444 mse = 1.418 mae = 0.899\n",
            "epoch37 train time: 6.228s test time: 0.776  loss = 1.100 val_mse = 1.441 mse = 1.416 mae = 0.898\n",
            "epoch38 train time: 6.232s test time: 0.772  loss = 1.095 val_mse = 1.438 mse = 1.414 mae = 0.897\n",
            "epoch39 train time: 6.228s test time: 0.775  loss = 1.090 val_mse = 1.435 mse = 1.412 mae = 0.896\n",
            "epoch40 train time: 6.226s test time: 0.776  loss = 1.086 val_mse = 1.432 mse = 1.410 mae = 0.896\n",
            "epoch41 train time: 6.223s test time: 0.772  loss = 1.081 val_mse = 1.430 mse = 1.408 mae = 0.895\n",
            "epoch42 train time: 6.220s test time: 0.773  loss = 1.077 val_mse = 1.427 mse = 1.406 mae = 0.894\n",
            "epoch43 train time: 6.222s test time: 0.776  loss = 1.072 val_mse = 1.424 mse = 1.405 mae = 0.894\n",
            "epoch44 train time: 6.235s test time: 0.782  loss = 1.068 val_mse = 1.421 mse = 1.403 mae = 0.893\n",
            "epoch45 train time: 6.227s test time: 0.777  loss = 1.064 val_mse = 1.418 mse = 1.401 mae = 0.892\n",
            "epoch46 train time: 6.219s test time: 0.773  loss = 1.060 val_mse = 1.415 mse = 1.399 mae = 0.891\n",
            "epoch47 train time: 6.207s test time: 0.776  loss = 1.056 val_mse = 1.413 mse = 1.398 mae = 0.891\n",
            "epoch48 train time: 6.229s test time: 0.772  loss = 1.052 val_mse = 1.410 mse = 1.396 mae = 0.890\n",
            "epoch49 train time: 6.226s test time: 0.773  loss = 1.048 val_mse = 1.407 mse = 1.394 mae = 0.890\n",
            "epoch50 train time: 6.223s test time: 0.775  loss = 1.045 val_mse = 1.405 mse = 1.392 mae = 0.889\n",
            "epoch51 train time: 6.212s test time: 0.772  loss = 1.041 val_mse = 1.402 mse = 1.391 mae = 0.888\n",
            "epoch52 train time: 6.217s test time: 0.774  loss = 1.037 val_mse = 1.399 mse = 1.389 mae = 0.888\n",
            "epoch53 train time: 6.229s test time: 0.773  loss = 1.034 val_mse = 1.397 mse = 1.388 mae = 0.887\n",
            "epoch54 train time: 6.225s test time: 0.777  loss = 1.030 val_mse = 1.394 mse = 1.386 mae = 0.886\n",
            "epoch55 train time: 6.215s test time: 0.777  loss = 1.027 val_mse = 1.392 mse = 1.384 mae = 0.886\n",
            "epoch56 train time: 6.226s test time: 0.777  loss = 1.024 val_mse = 1.389 mse = 1.383 mae = 0.885\n",
            "epoch57 train time: 6.238s test time: 0.773  loss = 1.020 val_mse = 1.387 mse = 1.381 mae = 0.884\n",
            "epoch58 train time: 6.245s test time: 0.783  loss = 1.017 val_mse = 1.385 mse = 1.380 mae = 0.884\n",
            "epoch59 train time: 6.251s test time: 0.778  loss = 1.014 val_mse = 1.382 mse = 1.378 mae = 0.883\n",
            "epoch60 train time: 6.249s test time: 0.778  loss = 1.011 val_mse = 1.380 mse = 1.377 mae = 0.883\n",
            "epoch61 train time: 6.236s test time: 0.777  loss = 1.008 val_mse = 1.378 mse = 1.375 mae = 0.882\n",
            "epoch62 train time: 6.217s test time: 0.779  loss = 1.005 val_mse = 1.375 mse = 1.374 mae = 0.881\n",
            "epoch63 train time: 6.229s test time: 0.776  loss = 1.002 val_mse = 1.373 mse = 1.372 mae = 0.881\n",
            "epoch64 train time: 6.237s test time: 0.776  loss = 1.000 val_mse = 1.371 mse = 1.371 mae = 0.880\n",
            "epoch65 train time: 6.223s test time: 0.775  loss = 0.997 val_mse = 1.368 mse = 1.370 mae = 0.880\n",
            "epoch66 train time: 6.225s test time: 0.771  loss = 0.994 val_mse = 1.366 mse = 1.368 mae = 0.879\n",
            "epoch67 train time: 6.217s test time: 0.773  loss = 0.992 val_mse = 1.364 mse = 1.367 mae = 0.879\n",
            "epoch68 train time: 6.207s test time: 0.779  loss = 0.989 val_mse = 1.362 mse = 1.366 mae = 0.878\n",
            "epoch69 train time: 6.212s test time: 0.771  loss = 0.986 val_mse = 1.360 mse = 1.364 mae = 0.878\n",
            "epoch70 train time: 6.217s test time: 0.774  loss = 0.984 val_mse = 1.358 mse = 1.363 mae = 0.877\n",
            "epoch71 train time: 6.219s test time: 0.775  loss = 0.981 val_mse = 1.356 mse = 1.362 mae = 0.876\n",
            "epoch72 train time: 6.215s test time: 0.781  loss = 0.979 val_mse = 1.353 mse = 1.360 mae = 0.876\n",
            "epoch73 train time: 6.243s test time: 0.775  loss = 0.977 val_mse = 1.351 mse = 1.359 mae = 0.875\n",
            "epoch74 train time: 6.228s test time: 0.779  loss = 0.974 val_mse = 1.349 mse = 1.358 mae = 0.875\n",
            "epoch75 train time: 6.230s test time: 0.774  loss = 0.972 val_mse = 1.347 mse = 1.357 mae = 0.874\n",
            "epoch76 train time: 6.244s test time: 0.778  loss = 0.970 val_mse = 1.345 mse = 1.355 mae = 0.874\n",
            "epoch77 train time: 6.262s test time: 1.100  loss = 0.968 val_mse = 1.343 mse = 1.354 mae = 0.873\n",
            "epoch78 train time: 7.223s test time: 0.938  loss = 0.965 val_mse = 1.341 mse = 1.353 mae = 0.873\n",
            "epoch79 train time: 6.974s test time: 0.881  loss = 0.963 val_mse = 1.340 mse = 1.352 mae = 0.872\n",
            "epoch80 train time: 6.411s test time: 0.773  loss = 0.961 val_mse = 1.338 mse = 1.350 mae = 0.872\n",
            "epoch81 train time: 6.223s test time: 0.770  loss = 0.959 val_mse = 1.336 mse = 1.349 mae = 0.871\n",
            "epoch82 train time: 6.203s test time: 0.778  loss = 0.957 val_mse = 1.334 mse = 1.348 mae = 0.871\n",
            "epoch83 train time: 6.213s test time: 0.772  loss = 0.955 val_mse = 1.332 mse = 1.347 mae = 0.870\n",
            "epoch84 train time: 6.198s test time: 0.771  loss = 0.953 val_mse = 1.330 mse = 1.346 mae = 0.870\n",
            "epoch85 train time: 6.204s test time: 0.771  loss = 0.951 val_mse = 1.328 mse = 1.345 mae = 0.869\n",
            "epoch86 train time: 6.201s test time: 0.774  loss = 0.949 val_mse = 1.327 mse = 1.344 mae = 0.869\n",
            "epoch87 train time: 6.214s test time: 0.773  loss = 0.947 val_mse = 1.325 mse = 1.343 mae = 0.868\n",
            "epoch88 train time: 6.207s test time: 0.769  loss = 0.945 val_mse = 1.323 mse = 1.342 mae = 0.868\n",
            "epoch89 train time: 6.214s test time: 0.778  loss = 0.943 val_mse = 1.322 mse = 1.341 mae = 0.867\n",
            "epoch90 train time: 6.208s test time: 0.778  loss = 0.942 val_mse = 1.320 mse = 1.340 mae = 0.867\n",
            "epoch91 train time: 6.208s test time: 0.779  loss = 0.940 val_mse = 1.318 mse = 1.338 mae = 0.866\n",
            "epoch92 train time: 6.207s test time: 0.774  loss = 0.938 val_mse = 1.316 mse = 1.337 mae = 0.866\n",
            "epoch93 train time: 6.209s test time: 0.775  loss = 0.936 val_mse = 1.315 mse = 1.336 mae = 0.865\n",
            "epoch94 train time: 6.210s test time: 0.772  loss = 0.935 val_mse = 1.313 mse = 1.335 mae = 0.865\n",
            "epoch95 train time: 6.221s test time: 0.776  loss = 0.933 val_mse = 1.312 mse = 1.335 mae = 0.864\n",
            "epoch96 train time: 6.223s test time: 0.779  loss = 0.931 val_mse = 1.310 mse = 1.334 mae = 0.864\n",
            "epoch97 train time: 6.233s test time: 0.772  loss = 0.930 val_mse = 1.309 mse = 1.333 mae = 0.864\n",
            "epoch98 train time: 6.237s test time: 0.776  loss = 0.928 val_mse = 1.307 mse = 1.332 mae = 0.863\n",
            "epoch99 train time: 6.238s test time: 0.789  loss = 0.927 val_mse = 1.306 mse = 1.331 mae = 0.863\n",
            "epoch100 train time: 6.229s test time: 0.777  loss = 0.925 val_mse = 1.304 mse = 1.330 mae = 0.862\n",
            "epoch101 train time: 6.236s test time: 0.772  loss = 0.923 val_mse = 1.303 mse = 1.329 mae = 0.862\n",
            "epoch102 train time: 6.249s test time: 0.778  loss = 0.922 val_mse = 1.301 mse = 1.328 mae = 0.861\n",
            "epoch103 train time: 6.245s test time: 0.775  loss = 0.920 val_mse = 1.300 mse = 1.327 mae = 0.861\n",
            "epoch104 train time: 6.230s test time: 0.777  loss = 0.919 val_mse = 1.298 mse = 1.326 mae = 0.860\n",
            "epoch105 train time: 6.225s test time: 0.775  loss = 0.917 val_mse = 1.297 mse = 1.326 mae = 0.860\n",
            "epoch106 train time: 6.241s test time: 0.779  loss = 0.916 val_mse = 1.295 mse = 1.325 mae = 0.860\n",
            "epoch107 train time: 6.229s test time: 0.778  loss = 0.915 val_mse = 1.294 mse = 1.324 mae = 0.859\n",
            "epoch108 train time: 6.216s test time: 0.772  loss = 0.913 val_mse = 1.293 mse = 1.323 mae = 0.859\n",
            "epoch109 train time: 6.226s test time: 0.778  loss = 0.912 val_mse = 1.291 mse = 1.322 mae = 0.858\n",
            "epoch110 train time: 6.233s test time: 0.773  loss = 0.910 val_mse = 1.290 mse = 1.321 mae = 0.858\n",
            "epoch111 train time: 6.209s test time: 0.771  loss = 0.909 val_mse = 1.289 mse = 1.321 mae = 0.858\n",
            "epoch112 train time: 6.231s test time: 0.773  loss = 0.908 val_mse = 1.288 mse = 1.320 mae = 0.857\n",
            "epoch113 train time: 6.217s test time: 0.775  loss = 0.906 val_mse = 1.286 mse = 1.319 mae = 0.857\n",
            "epoch114 train time: 6.218s test time: 0.772  loss = 0.905 val_mse = 1.285 mse = 1.319 mae = 0.856\n",
            "epoch115 train time: 6.216s test time: 0.771  loss = 0.904 val_mse = 1.284 mse = 1.318 mae = 0.856\n",
            "epoch116 train time: 6.234s test time: 0.772  loss = 0.902 val_mse = 1.283 mse = 1.317 mae = 0.855\n",
            "epoch117 train time: 6.202s test time: 0.777  loss = 0.901 val_mse = 1.281 mse = 1.316 mae = 0.855\n",
            "epoch118 train time: 6.209s test time: 0.774  loss = 0.900 val_mse = 1.280 mse = 1.316 mae = 0.855\n",
            "epoch119 train time: 6.207s test time: 0.771  loss = 0.899 val_mse = 1.279 mse = 1.315 mae = 0.854\n",
            "MAE 0.8887613208692435\n",
            "MSE 1.3927576118486096\n",
            "epoch0 train time: 6.646s test time: 0.775  loss = 69.106 val_mse = 1.568 mse = 1.515 mae = 0.989\n",
            "epoch1 train time: 6.251s test time: 0.783  loss = 60.434 val_mse = 1.554 mse = 1.494 mae = 0.929\n",
            "epoch2 train time: 6.278s test time: 0.784  loss = 44.431 val_mse = 1.553 mse = 1.493 mae = 0.929\n",
            "epoch3 train time: 6.301s test time: 0.783  loss = 28.656 val_mse = 1.552 mse = 1.492 mae = 0.929\n",
            "epoch4 train time: 6.308s test time: 0.779  loss = 17.699 val_mse = 1.551 mse = 1.491 mae = 0.928\n",
            "epoch5 train time: 6.288s test time: 0.778  loss = 10.473 val_mse = 1.549 mse = 1.490 mae = 0.927\n",
            "epoch6 train time: 6.268s test time: 0.785  loss = 5.986 val_mse = 1.547 mse = 1.488 mae = 0.926\n",
            "epoch7 train time: 6.258s test time: 0.774  loss = 3.412 val_mse = 1.545 mse = 1.486 mae = 0.926\n",
            "epoch8 train time: 6.237s test time: 0.782  loss = 2.091 val_mse = 1.542 mse = 1.484 mae = 0.925\n",
            "epoch9 train time: 6.242s test time: 0.772  loss = 1.523 val_mse = 1.540 mse = 1.482 mae = 0.924\n",
            "epoch10 train time: 6.235s test time: 0.773  loss = 1.344 val_mse = 1.537 mse = 1.480 mae = 0.923\n",
            "epoch11 train time: 6.228s test time: 0.774  loss = 1.307 val_mse = 1.534 mse = 1.478 mae = 0.922\n",
            "epoch12 train time: 6.224s test time: 0.768  loss = 1.290 val_mse = 1.532 mse = 1.476 mae = 0.922\n",
            "epoch13 train time: 6.217s test time: 0.771  loss = 1.276 val_mse = 1.529 mse = 1.474 mae = 0.921\n",
            "epoch14 train time: 6.221s test time: 0.771  loss = 1.262 val_mse = 1.526 mse = 1.472 mae = 0.920\n",
            "epoch15 train time: 6.230s test time: 0.776  loss = 1.250 val_mse = 1.523 mse = 1.470 mae = 0.919\n",
            "epoch16 train time: 6.240s test time: 0.773  loss = 1.238 val_mse = 1.521 mse = 1.468 mae = 0.918\n",
            "epoch17 train time: 6.228s test time: 0.775  loss = 1.228 val_mse = 1.518 mse = 1.466 mae = 0.917\n",
            "epoch18 train time: 6.231s test time: 0.775  loss = 1.218 val_mse = 1.515 mse = 1.465 mae = 0.917\n",
            "epoch19 train time: 6.229s test time: 0.780  loss = 1.208 val_mse = 1.512 mse = 1.463 mae = 0.916\n",
            "epoch20 train time: 6.225s test time: 0.779  loss = 1.199 val_mse = 1.509 mse = 1.461 mae = 0.915\n",
            "epoch21 train time: 6.229s test time: 0.774  loss = 1.191 val_mse = 1.507 mse = 1.459 mae = 0.914\n",
            "epoch22 train time: 6.212s test time: 0.774  loss = 1.183 val_mse = 1.504 mse = 1.457 mae = 0.914\n",
            "epoch23 train time: 6.230s test time: 0.773  loss = 1.175 val_mse = 1.501 mse = 1.455 mae = 0.913\n",
            "epoch24 train time: 6.243s test time: 0.775  loss = 1.168 val_mse = 1.499 mse = 1.454 mae = 0.912\n",
            "epoch25 train time: 6.241s test time: 0.774  loss = 1.162 val_mse = 1.496 mse = 1.452 mae = 0.911\n",
            "epoch26 train time: 6.251s test time: 0.778  loss = 1.156 val_mse = 1.493 mse = 1.450 mae = 0.911\n",
            "epoch27 train time: 6.247s test time: 0.778  loss = 1.150 val_mse = 1.491 mse = 1.449 mae = 0.910\n",
            "epoch28 train time: 6.238s test time: 0.778  loss = 1.144 val_mse = 1.488 mse = 1.447 mae = 0.909\n",
            "epoch29 train time: 6.246s test time: 0.779  loss = 1.138 val_mse = 1.486 mse = 1.446 mae = 0.909\n",
            "epoch30 train time: 6.239s test time: 0.772  loss = 1.133 val_mse = 1.484 mse = 1.444 mae = 0.908\n",
            "epoch31 train time: 6.245s test time: 0.777  loss = 1.128 val_mse = 1.481 mse = 1.443 mae = 0.908\n",
            "epoch32 train time: 6.235s test time: 0.775  loss = 1.123 val_mse = 1.479 mse = 1.441 mae = 0.907\n",
            "epoch33 train time: 6.233s test time: 0.772  loss = 1.118 val_mse = 1.477 mse = 1.440 mae = 0.906\n",
            "epoch34 train time: 6.229s test time: 0.777  loss = 1.114 val_mse = 1.474 mse = 1.438 mae = 0.906\n",
            "epoch35 train time: 6.226s test time: 0.775  loss = 1.109 val_mse = 1.472 mse = 1.437 mae = 0.905\n",
            "epoch36 train time: 6.224s test time: 0.775  loss = 1.105 val_mse = 1.470 mse = 1.435 mae = 0.905\n",
            "epoch37 train time: 6.228s test time: 0.773  loss = 1.101 val_mse = 1.468 mse = 1.434 mae = 0.904\n",
            "epoch38 train time: 6.234s test time: 0.779  loss = 1.097 val_mse = 1.466 mse = 1.433 mae = 0.904\n",
            "epoch39 train time: 6.226s test time: 0.768  loss = 1.094 val_mse = 1.464 mse = 1.432 mae = 0.903\n",
            "epoch40 train time: 6.246s test time: 0.775  loss = 1.090 val_mse = 1.461 mse = 1.430 mae = 0.903\n",
            "epoch41 train time: 6.240s test time: 0.773  loss = 1.086 val_mse = 1.459 mse = 1.429 mae = 0.902\n",
            "epoch42 train time: 6.242s test time: 0.779  loss = 1.083 val_mse = 1.457 mse = 1.428 mae = 0.902\n",
            "epoch43 train time: 6.239s test time: 0.774  loss = 1.079 val_mse = 1.456 mse = 1.427 mae = 0.901\n",
            "epoch44 train time: 6.241s test time: 0.774  loss = 1.076 val_mse = 1.454 mse = 1.425 mae = 0.901\n",
            "epoch45 train time: 6.234s test time: 0.776  loss = 1.073 val_mse = 1.452 mse = 1.424 mae = 0.900\n",
            "epoch46 train time: 6.239s test time: 0.778  loss = 1.070 val_mse = 1.450 mse = 1.423 mae = 0.900\n",
            "epoch47 train time: 6.245s test time: 0.774  loss = 1.067 val_mse = 1.448 mse = 1.422 mae = 0.899\n",
            "epoch48 train time: 6.237s test time: 0.780  loss = 1.064 val_mse = 1.446 mse = 1.421 mae = 0.899\n",
            "epoch49 train time: 6.245s test time: 0.773  loss = 1.061 val_mse = 1.444 mse = 1.420 mae = 0.898\n",
            "epoch50 train time: 6.591s test time: 0.890  loss = 1.058 val_mse = 1.443 mse = 1.419 mae = 0.898\n",
            "epoch51 train time: 6.893s test time: 0.889  loss = 1.055 val_mse = 1.441 mse = 1.418 mae = 0.897\n",
            "epoch52 train time: 6.658s test time: 0.785  loss = 1.053 val_mse = 1.439 mse = 1.417 mae = 0.897\n",
            "epoch53 train time: 6.239s test time: 0.774  loss = 1.050 val_mse = 1.438 mse = 1.416 mae = 0.897\n",
            "epoch54 train time: 6.238s test time: 0.779  loss = 1.048 val_mse = 1.436 mse = 1.415 mae = 0.896\n",
            "epoch55 train time: 6.242s test time: 0.775  loss = 1.045 val_mse = 1.434 mse = 1.414 mae = 0.896\n",
            "epoch56 train time: 6.239s test time: 0.773  loss = 1.043 val_mse = 1.433 mse = 1.413 mae = 0.895\n",
            "epoch57 train time: 6.239s test time: 0.776  loss = 1.041 val_mse = 1.431 mse = 1.412 mae = 0.895\n",
            "epoch58 train time: 6.243s test time: 0.775  loss = 1.038 val_mse = 1.430 mse = 1.411 mae = 0.895\n",
            "epoch59 train time: 6.243s test time: 0.776  loss = 1.036 val_mse = 1.428 mse = 1.410 mae = 0.894\n",
            "epoch60 train time: 6.256s test time: 0.774  loss = 1.034 val_mse = 1.427 mse = 1.409 mae = 0.894\n",
            "epoch61 train time: 6.253s test time: 0.782  loss = 1.032 val_mse = 1.425 mse = 1.408 mae = 0.894\n",
            "epoch62 train time: 6.256s test time: 0.789  loss = 1.030 val_mse = 1.424 mse = 1.408 mae = 0.893\n",
            "epoch63 train time: 6.250s test time: 0.780  loss = 1.028 val_mse = 1.422 mse = 1.407 mae = 0.893\n",
            "epoch64 train time: 6.245s test time: 0.769  loss = 1.026 val_mse = 1.421 mse = 1.406 mae = 0.892\n",
            "epoch65 train time: 6.234s test time: 0.768  loss = 1.024 val_mse = 1.419 mse = 1.405 mae = 0.892\n",
            "epoch66 train time: 6.387s test time: 0.773  loss = 1.022 val_mse = 1.418 mse = 1.404 mae = 0.892\n",
            "epoch67 train time: 6.217s test time: 0.777  loss = 1.020 val_mse = 1.417 mse = 1.403 mae = 0.891\n",
            "epoch68 train time: 6.236s test time: 0.773  loss = 1.018 val_mse = 1.415 mse = 1.403 mae = 0.891\n",
            "epoch69 train time: 6.231s test time: 0.773  loss = 1.016 val_mse = 1.414 mse = 1.402 mae = 0.891\n",
            "epoch70 train time: 6.228s test time: 0.774  loss = 1.015 val_mse = 1.413 mse = 1.401 mae = 0.890\n",
            "epoch71 train time: 6.229s test time: 0.774  loss = 1.013 val_mse = 1.411 mse = 1.400 mae = 0.890\n",
            "epoch72 train time: 6.230s test time: 0.790  loss = 1.011 val_mse = 1.410 mse = 1.400 mae = 0.890\n",
            "epoch73 train time: 6.231s test time: 0.774  loss = 1.010 val_mse = 1.409 mse = 1.399 mae = 0.889\n",
            "epoch74 train time: 6.229s test time: 0.772  loss = 1.008 val_mse = 1.408 mse = 1.398 mae = 0.889\n",
            "epoch75 train time: 6.225s test time: 0.772  loss = 1.007 val_mse = 1.406 mse = 1.397 mae = 0.889\n",
            "epoch76 train time: 6.228s test time: 0.771  loss = 1.005 val_mse = 1.405 mse = 1.397 mae = 0.888\n",
            "epoch77 train time: 6.208s test time: 0.770  loss = 1.003 val_mse = 1.404 mse = 1.396 mae = 0.888\n",
            "epoch78 train time: 6.221s test time: 0.774  loss = 1.002 val_mse = 1.403 mse = 1.395 mae = 0.888\n",
            "epoch79 train time: 6.212s test time: 0.777  loss = 1.001 val_mse = 1.402 mse = 1.395 mae = 0.887\n",
            "epoch80 train time: 6.431s test time: 0.774  loss = 0.999 val_mse = 1.400 mse = 1.394 mae = 0.887\n",
            "epoch81 train time: 6.245s test time: 0.779  loss = 0.998 val_mse = 1.399 mse = 1.393 mae = 0.887\n",
            "epoch82 train time: 6.243s test time: 0.786  loss = 0.996 val_mse = 1.398 mse = 1.393 mae = 0.887\n",
            "epoch83 train time: 6.242s test time: 0.776  loss = 0.995 val_mse = 1.397 mse = 1.392 mae = 0.886\n",
            "epoch84 train time: 6.245s test time: 0.771  loss = 0.994 val_mse = 1.396 mse = 1.391 mae = 0.886\n",
            "epoch85 train time: 6.241s test time: 0.774  loss = 0.992 val_mse = 1.395 mse = 1.391 mae = 0.886\n",
            "epoch86 train time: 6.249s test time: 0.779  loss = 0.991 val_mse = 1.394 mse = 1.390 mae = 0.885\n",
            "epoch87 train time: 6.234s test time: 0.774  loss = 0.990 val_mse = 1.393 mse = 1.390 mae = 0.885\n",
            "epoch88 train time: 6.234s test time: 0.779  loss = 0.989 val_mse = 1.392 mse = 1.389 mae = 0.885\n",
            "epoch89 train time: 6.235s test time: 0.775  loss = 0.988 val_mse = 1.391 mse = 1.388 mae = 0.884\n",
            "epoch90 train time: 6.244s test time: 0.775  loss = 0.987 val_mse = 1.390 mse = 1.388 mae = 0.884\n",
            "epoch91 train time: 6.242s test time: 0.776  loss = 0.985 val_mse = 1.389 mse = 1.387 mae = 0.884\n",
            "epoch92 train time: 6.237s test time: 0.776  loss = 0.984 val_mse = 1.388 mse = 1.387 mae = 0.884\n",
            "epoch93 train time: 6.250s test time: 0.782  loss = 0.983 val_mse = 1.387 mse = 1.386 mae = 0.883\n",
            "epoch94 train time: 6.239s test time: 0.775  loss = 0.982 val_mse = 1.386 mse = 1.385 mae = 0.883\n",
            "epoch95 train time: 6.403s test time: 0.772  loss = 0.981 val_mse = 1.385 mse = 1.385 mae = 0.883\n",
            "epoch96 train time: 6.240s test time: 0.771  loss = 0.980 val_mse = 1.384 mse = 1.384 mae = 0.882\n",
            "epoch97 train time: 6.247s test time: 0.777  loss = 0.979 val_mse = 1.383 mse = 1.384 mae = 0.882\n",
            "epoch98 train time: 6.231s test time: 0.775  loss = 0.978 val_mse = 1.382 mse = 1.383 mae = 0.882\n",
            "epoch99 train time: 6.228s test time: 0.771  loss = 0.977 val_mse = 1.381 mse = 1.383 mae = 0.882\n",
            "epoch100 train time: 6.231s test time: 0.778  loss = 0.976 val_mse = 1.380 mse = 1.382 mae = 0.881\n",
            "epoch101 train time: 6.244s test time: 0.772  loss = 0.975 val_mse = 1.379 mse = 1.382 mae = 0.881\n",
            "epoch102 train time: 6.221s test time: 0.783  loss = 0.974 val_mse = 1.379 mse = 1.381 mae = 0.881\n",
            "epoch103 train time: 6.244s test time: 0.780  loss = 0.973 val_mse = 1.378 mse = 1.381 mae = 0.880\n",
            "epoch104 train time: 6.227s test time: 0.771  loss = 0.972 val_mse = 1.377 mse = 1.380 mae = 0.880\n",
            "epoch105 train time: 6.223s test time: 0.778  loss = 0.971 val_mse = 1.376 mse = 1.380 mae = 0.880\n",
            "epoch106 train time: 6.231s test time: 0.773  loss = 0.970 val_mse = 1.375 mse = 1.379 mae = 0.880\n",
            "epoch107 train time: 6.225s test time: 0.774  loss = 0.970 val_mse = 1.374 mse = 1.379 mae = 0.879\n",
            "epoch108 train time: 6.235s test time: 0.774  loss = 0.969 val_mse = 1.374 mse = 1.378 mae = 0.879\n",
            "epoch109 train time: 6.232s test time: 0.774  loss = 0.968 val_mse = 1.373 mse = 1.378 mae = 0.879\n",
            "epoch110 train time: 6.242s test time: 0.776  loss = 0.967 val_mse = 1.372 mse = 1.378 mae = 0.879\n",
            "epoch111 train time: 6.246s test time: 0.776  loss = 0.966 val_mse = 1.371 mse = 1.377 mae = 0.878\n",
            "epoch112 train time: 6.235s test time: 0.784  loss = 0.965 val_mse = 1.371 mse = 1.377 mae = 0.878\n",
            "epoch113 train time: 6.249s test time: 0.780  loss = 0.965 val_mse = 1.370 mse = 1.376 mae = 0.878\n",
            "epoch114 train time: 6.257s test time: 0.776  loss = 0.964 val_mse = 1.369 mse = 1.376 mae = 0.878\n",
            "epoch115 train time: 6.272s test time: 0.776  loss = 0.963 val_mse = 1.368 mse = 1.375 mae = 0.877\n",
            "epoch116 train time: 6.266s test time: 0.783  loss = 0.962 val_mse = 1.368 mse = 1.375 mae = 0.877\n",
            "epoch117 train time: 6.245s test time: 0.785  loss = 0.962 val_mse = 1.367 mse = 1.374 mae = 0.877\n",
            "epoch118 train time: 6.250s test time: 0.778  loss = 0.961 val_mse = 1.366 mse = 1.374 mae = 0.877\n",
            "epoch119 train time: 6.251s test time: 0.775  loss = 0.960 val_mse = 1.365 mse = 1.374 mae = 0.877\n",
            "MAE 0.8978200408157662\n",
            "MSE 1.419021158007399\n"
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    print(sys.path[0])\n",
        "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
        "    word_latent_dim = 300\n",
        "    latent_dim = 15\n",
        "    max_doc_length = 300\n",
        "    num_filters = 50\n",
        "    window_size = 5\n",
        "    v_dim = 50\n",
        "    learning_rate = 0.001\n",
        "    lambda_1 = 0.01 #normally we set from [0.05, 0.01, 0.005, 0.001]\n",
        "    drop_out = 0.6\n",
        "    batch_size = 300\n",
        "    epochs = 120\n",
        "    a_list = [0.1, 0.2, 0.3, 0.4, 0.5, 0,6, 0.7, 0.8, 0.9, 1]\n",
        "    a = 0.1\n",
        "    avg_mae_list = []\n",
        "    avg_mse_list = []\n",
        "    \n",
        "    # loading data\n",
        "    firTime = time()\n",
        "    dataSet = Dataset(max_doc_length, sys.path[0], \"dataPreprocessingWordDict.out\")\n",
        "    word_dict, user_reviews, item_reviews, train, valRatings, testRatings = dataSet.word_id_dict, dataSet.userReview_dict, dataSet.itemReview_dict, dataSet.trainMtrx, dataSet.valRatings, dataSet.testRatings\n",
        "    secTime = time()\n",
        "\n",
        "    num_users, num_items = train.shape\n",
        "    print(\"load data: %.3fs\" % (secTime - firTime))\n",
        "    print(num_users, num_items)\n",
        "    print(latent_dim)\n",
        "\n",
        "    #load word embeddings\n",
        "    word_embedding_mtrx = ini_word_embed(len(word_dict), word_latent_dim)\n",
        "    # word_embedding_mtrx = word2vec_word_embed(len(word_dict), word_latent_dim,\n",
        "    #                                           \"Directory of pretrained WordEmbedding.out\",\n",
        "    #                                           word_dict)\n",
        "\n",
        "    print( \"shape\", word_embedding_mtrx.shape)\n",
        "\n",
        "    # get train instances\n",
        "    user_input, item_input, rateings = get_train_instance(train)\n",
        "    print (len(user_input), len(item_input), len(rateings))\n",
        "\n",
        "    # get test/val instances\n",
        "    user_vals, item_vals, user_input_val, item_input_val, rating_input_val = get_test_list(200, valRatings, user_reviews, item_reviews)\n",
        "    user_tests, item_tests, user_input_test, item_input_test, rating_input_test = get_test_list(200, testRatings, user_reviews, item_reviews)\n",
        "\n",
        "    #train & eval model\n",
        "    # train_model()\n",
        "\n",
        "    # a = 0.2\n",
        "    # #train & eval model\n",
        "    # train_model()\n",
        "\n",
        "    # a = 0.3\n",
        "    # #train & eval model\n",
        "    # train_model()\n",
        "\n",
        "    # a = 0.4\n",
        "    # #train & eval model\n",
        "    # train_model()\n",
        "\n",
        "    # a = 0.5\n",
        "    # #train & eval model\n",
        "    # train_model()\n",
        "\n",
        "    # a = 0.6\n",
        "    # #train & eval model\n",
        "    # train_model()\n",
        "\n",
        "    a = 0.7\n",
        "    #train & eval model\n",
        "    train_model()\n",
        "\n",
        "    a = 0.8\n",
        "    #train & eval model\n",
        "    train_model()\n",
        "\n",
        "    a = 0.9\n",
        "    #train & eval model\n",
        "    train_model()\n",
        "\n",
        "    a = 1\n",
        "    #train & eval model\n",
        "    train_model()"
      ],
      "id": "vBDwt3w8QuWo"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6VYM0b5asBMB"
      },
      "source": [
        "## Plot"
      ],
      "id": "6VYM0b5asBMB"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P_LZiLjKBjaG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 446
        },
        "outputId": "9916893a-1006-460b-da23-49e02a465329"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "avg mae over alpha = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]: [0.8750364511057321, 0.863824053714779, 0.8592177252143982, 0.8527685427680036, 0.8538870170462247, 0.860016459557814, 0.8611507236816345, 0.8678378841134752, 0.8887613208692435, 0.8978200408157662]\n",
            "avg mse over alpha = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]: [1.4467357106160585, 1.3971177984800593, 1.376702370229998, 1.3462015648002874, 1.3396905775303876, 1.3510617804467742, 1.34180853475892, 1.353348614027472, 1.3927576118486096, 1.419021158007399]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 750x350 with 2 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAuQAAAFUCAYAAABshimNAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAAPYQAAD2EBqD+naQAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOzdeXgURfrA8e+bhISchEAg3AECKgii4q6KKCy6KqIiIqzigdd6/kRZxVvBVdFV8FxXXVfAk0u8EFBRWBEUxQMVFJYjnDEJRwiBHCSp3x/VEyaTmWQCk3SO9/M880B3V3e/PZOpqa6uQ4wxKKWUUkoppdwR5nYASimllFJKNWZaIFdKKaWUUspFWiBXSimllFLKRVogV0oppZRSykVaIFdKKaWUUspFWiBXSimllFLKRVogV0oppZRSykVaIFdKKaWUUspFWiBXSimllFLKRVogV0ohIukiMtXtOCpzODE6+84NcUhKKeUqEVksIovdjqMyhxOjs+8vIQ6pTtICeR0nIjeKiBGR5S7GMNqJwYjIKX62i4hscbb7LfSISKKIFDhpjgqQZqrXeXxfBaG+LqWUqkn1Pf8WkTgRmSAiv4jIPhHZKSI/isgzItLWK934SvJuIyIptXGtStVnEW4HoKo0CkgH/iAiacaYdS7GUgBcAnzps/40oD1QWMm+FwEG+B17TfcFSFcIXONnfUm1IlXVdQRQ6nYQSjUw9Tb/FpEmwBfAkcA04DkgDujpHOddYLvPsW4A8vycO+cwY1eB/dntAFRoaIG8DhORzsDJwDDgJWzmPsHFkOYBF4nILcaYYq/1lwDfAS0r2fdSZ/9NTvpABfJiY8wboQjWLSIiQFNjTL6LMcQYY/YHm94YU9nNlFKqmhpA/j0UOBYYZYx5y3uDiDQFIv2cY7YxZkcIY6511c07a+D8scaYfcGmN8YU1WQ8qvZok5W6bRSwG/gImO0sA7b2QkR2icgU351EJMFpHvKk17pOIvKB89gxS0SeEpEznceJA4KM522gBXCG13EjgeHAW4F2EpGOQH9guvPqLCInB3nOoIlIrIhMch6/ForIGhG53Skge9L8IiKL/OwbJiLbRGS2z7pbRWSV835mishLItLcZ990EZnrvJ8rgHzgugAxPi8ieSIS42fb2yLyu4iEO8vni8hHIrLduZ71InK/Z7vXfoud6zpeRL4Qkf3AoyIyTUR2ODVdvuf6RETW+FzDVK9lz2PufiIyWUSynb+dd0Uk2c97N96Jc7+ILBKRHr7HDMT5jJY5j8PzReQ7ERkexH6eGE91PpedIpIrIq/5fkZe+5wiIt84n+cGEbncZ3uSiDwpIj87n1OuiMwXkWOqikcpH/U9/+7q/LvUd4MxpsAYkxvkeaskIhFO3rbeyevSReRREYnySjNXRDYE2P8rJ+/1Xnepk5fkO+/1dBHp4JPGb94Z4By3O+93Jz/bJopIkSffEZH+IjJLRDY717PF+cyiffab6uQzXUVknojsBd4U20zogG9e6+zzsojkiL0pqtA+W0QGOHGOEJF7RWSr8/f0mYik+TneTU5emO/kjf19jxmIiFwpIp87f5OFIrJaRG4IYj9PjCOdz/l352/7A9/PyGufHs5vy36xv9XjfLZHishDzme+xzneEhEZWFU8dYUWyOu2UcAc5w74baCbiJwAYIw5gH1kONTJVL0NBaKwhV9EJBb4HDgdeBZ4BFtz83g140kHvgIu9lp3NtDMc64ALgb2AXONMd8A6/H6cfIlIi39vBIqC0xEBPgAuA1YAIwF1gBPAJO9ks4ATpWKbRpPAdr6XMdLzv5LgTHAFCfuj6ViIfcI7Gf0qZP2xwChzgBigXN84o8BzsXWMHma54zGPv6d7BzzO+Ah4DE/x20BzHfOeyuwCHjdWX+mz7lSgD8BwTyJeA44Bluz9y8nxud90kwEHgRWAHcA/wM+dq4zGGOAH4AHgHuAYmCWiJxT6V4HPQ8cBYwHXsN+Ru85fxPe0rAFo0+Bv2ELS1NFpKdXmi7Y789c7N/QE0Av4L/i1WZWqSDU9/x7k/Pv5X6+S4Ek+cm7E4PY7xVs3vY9Ng//L3C3T1wzsJU5J3jv6BSQT/ROKyL3YvOC/2G/x08Dg4Av/MTjL+/0Zya22eUIP9tGAJ8YY3Y7yxcBMdg88/+w+eH/OTH5inC2ZwG3A+9g8+4IYKTPtXpuoN4xxlTVr+ou4ALgSWwefSLwps/xbsDmn1uBccAS4D1sE6Zg3ID9O3kUm6duAV4QkZuC3P9e7G/h49i/7TOAhb43LkBz7O/6Suc8vwGPi8jZXmkSsM1dFwN3Yn8PkrG/132CjMddxhh91cEXcDz2y3+6syzYP/anvdL82UkzxGffj4D1XstjnXTne61rCvzqrB9QRSyjnXR9gZuAXCDa2TYT+Nz5fzq20O27/0/AG17LjwDZQIRPuqnOefy9FlQR4/lOunt91s/Cto3u6ix3d9Ld7JPun8Ber+s6xUl3iU+6M33XO9dtgDOD+FwFm/nN9lnvaWPf32tdtJ/9X8Te3ER5rVvs7HudT9ow529mus/625z3pLPPNUz185l/CojX+snYAnMzZ7k1cAB41+ccDzr7T/X3PvikjfZZbgL8DHzmsz5QjCuAJl7r73DWn+fnM/J+f5Ox7Wqf9FoXBYT5nDfVSXd/qL7f+mrYLxpA/g1EYws+xtk2BbgKaOXnHOMJnHf/VkV8xzjp/u2z/gln/UBnOcH3++qsvwObn3V0ljs5edQ9PumOdvKqe7zWLcZP3llJrMuAFT7rTnCOcZn3e+dn37u843TWTXX2nRjgXF/7rLvA9zN3rmGx1/IAJ81qINJr/S3O+qOd5UhgB/ANXr/FwBVOusX+3gOfePxd5wLvv98qYtwKxHut9/wO3uLnM/J+fyOBDLx+R4Fw7+t11iVi+639J5Tf75p6aQ153TUKyMS5Wzf2r2sG8Bc52GThc+wXquwu2nlkdoaT1uMsYBu2BhnneAXAvw8hrpnYjHqIiMQDQ6i8uUpvbA3j216r38a2VzzTzy4FTvy+r7uqiGswtuPnsz7rJ2F/DM8GMMasxdaEeL9n4dhahw/NwXbfFwF7gE+9a3uwtdR5gO9jsI3GmI+riNHzOc4CBotInNemkdjP6EuvtGVt0EUk3jn/EmzNy5E+hy7E/mB6n6sUWyNynvNZeYwClhljNlYVL/CyE7PHEmzG18lZHoStyXnBZ7/ngji2J07v62yOrbFbAhwX5CFeNrbG0eNf2B/kwT7pVhtjlnidNxv7FKWL17pC531DRMJFpAX2815TjXiUqvf5t/O9/CO2YAy2YP8fIENEnhOv5iReLqRi3n1lFTF5vqeTfdZPcv49x4knF1uTPcKnxn4ktuC62Vkehq2MmOmTd/+OrTH3zbsr5J2VmAEcLyJdvdaNdI7xvmeFT54W65x/Gfa36Fg/x/2Xn3WvAX/0Odco7I3df4OIdYop377ck/d58ru+2KcD/zbl+xS8iX16WCWf62zmXOd/gS4i0iyIQ7xmjNnrtTwbW9D2zbvz8Hqi61zXN5TPu0s81yu2GWUS9rdpBfUk79YCeR3kZNh/wWbmnUUkzWn7tRxbIzkIwPkSvQOc75U5DsPWMHpn6J2wd6zeBSuAavf4dwoxC7EdgYZhC2ezK9nlUmyN7gav6yjA1rj4a7ZSYoxZ6OcVqAmIRydgu8+XG2wtkme7xwygn4i0c5YHAK0o/551wxYMs7C1+d6vOCe9t2AKt97njwbOAzu0GDYDmuX9GYlIT7Fttvdga7WyOZgp+WZ224z/zj2vOee6wDnmEdjau9eDjHWzz7Ino/a00fa8r+X+lowxuwgyUxeRISLytdihLXdhr/MGKl5jIP/zOXceNlNP9Unney04MZa1N3cy8ttE5H/YH9kdTjy9qxGPasQaUv5tjNljjBlnjEnFfp+uxt6c3gzc72eXL/zk3V9VEVYnbM2xbx7yO3Z0Ft+8uwNwEoBTWD2einm3YPMF37z7KCrm3YHyTn88T1xHOucXbOXNfOPVpl5EOoptH74LW5jM5mAh2jcfKcbWFPuagc2DRjnHbIa9gXrTz9+CP4eadxdjf5+rJLaP0UIR2Yf9rLI52AY/mPzSN+82TjypPum2+rnmcnm3E88VIvITtoyx04nnnCBjcZ2OslI3/Qlog83U/+Jn+yjgE+f/07EdCM/Gtv0agX1EuLIG43sLWzuTgs2I/A5p5WRWF2PbEq/2k6SViMQ5BajaNAPbpu4ibNvCEdja8AVeacKwhfFAbd2zfZaDHlHFGPO1iKQ7530L2y47Gq8fFaed43+xBfEHsO3uC7B3+o9T8Wba7/mNMatF5DvsjdFrzr9F2JqyYAQabjLYNqWVEpH+2Jq/L4AbsQXpA9hatUtCcQ4vwVzLPcDfgVexBY5d2B/gp9EKDBWcBpF/+zLGbAJeFZF3gQ1UPnztoQimkPkhsB/7Pi1z/i3FFpQ9wpxjnY3/77zv70118u7tIrLEOe+j2HbZHbFtloGyG7JPgSRsXv0btlKqHbaJim8+UvZUzudcu8WOCz8K275+OLZJXbCjkNV03t0V+Ax7fWOxNfdF2Mql2whtflnltYjIpdj39z3sU50sZ7+7OdhBuU7TAnndNAr7x+SvY8Qw4AIRud55XPQFthAzUkS+xP4YPOKzzyagh4iIz11mhR7XQXoX2+HxRHw6nfg4Dds55AEO1lR7NAdexnZgCsUwh5uA00Uk3qeW/Eiv7QAYYzaKyDfY9+x57Hv6nik/9N96bCeqpaZmhi+cCYxxOquOBNKNMV97bR+AfZw4zBjzhWel2KHUqus1YLKItMEWcj8yBzsfHS7P+5qG11MCp6mH35FOfFyIvdE40/v9F5GqHnN764ZXRyzniUMb7DBv1TUcWGSMudp7pXODVK+Hc1O1pqHk3345BcX12DbZobAJW3jrhtfvhIi0xrYB9s679zmF1ItEZCw2/iXGGO/x0NdjC2obnSaKoTYD23HxCOf8+7E3Ch69sH2VrjDGlHXiFJEzqL7XgPedjqyjgB+MMasOOfLyvPNu7/wzAltD/VMV+5+LvUE4z6u5EFK9UU26eS84lXhpQZzbn+HYG8VhPk+a3RxqtFq0xqeOcXoXD8N2rpnt+8L2iI7Hae7g3FnPxn45LsPeZM3wOezH2Lvz87zO0xS49lBidGq0b8B25PmwkqSe5ipP+LmWf2MfVwUcbaWa5mEfv97ss/42bG3JfJ/1M7A/SFdh27P7vmczneNVeCwrdoiuYEYOqMwMbGZ2BbaNqG+NtadGwLsGIBJbi1xdb2Pfg2ewbe5COc77Z9hHrr5DXfl+DoGUYGMrG8pRRFKxN2rB+quUH/XmBuz3wPczDzaecjVIInIR9vujVKUaUv4tIsc4bYJ913cCemCbroSC58b5Vp/1Y51/P/JZPwM7ItY12A6hvu/XHOz3+EGftuaI1eIw433HOf7F2Kesc035ccP95d2CHU2quuZjKwLuxFZwhTLvXoFt1nGtUwj3GEVwlSn+rrMZVfcZ8Ha5T/+m4djKlEPNu33j+SNO86b6QGvI657zsBn2BwG2f41tLjGKgxnRDOyQShOAn40xvrXRL2ELSG+LyDPYGplR2JpJCO5RYTnGmGmVbXfaRF4IfGoCD8/0AbaWuJUxJstZF+E8evLnXRN4woQPsXf5jzgFupXYUQzOx45ssN4n/UzscFBPYpslLPTeaIz5r4i8BNwtdsikT7BNKbphM+ExVN52vlLGmO9FZB22NiyKij8qy7Bt5KaJyLM4vcw5hMeNxphsEVngxJ1DxR+4Q2aMyXT+pv4mIh9gm/0cg31cvIOq/7Y+wv7wLhCRt7DtO2/CtiPsHWQYkcBnIjITO/zkjdjOsYG+Q5WZCzwgdnzoZdjarlHYmhelqtIg8m/HGcAE53v9NbapRxdsJUYUtkDva7iI+GuC+KkxJjNALCtFZBr2xtrTVO8P2MqK94wxi3x2mYcdEetJbCHsHZ/jrReR+7DNElNF5D0nfWdsX5qXnX0PiTEmS+xcFmOxn7Vv3v0btpb+SaefUi72tzCYQq7vuQ6IyHTs519C+cERDosxpkhExmM74H/u5J+p2M6766n67+oTbBOVD53fyjjsTWIWtlAdjF3Al05+2xp7U7aOQ+uwPBd7M/yuiHyE/byvxzaXjatsxzrD1IGhXvR18IXNyPOBmErSTMF+EVo4y4LtwGHwGfbPa5/O2D/Y/dgvzJPYP14D/LGKmEY76fpWkS4dZ9gsr2NfVUn60/Aa4ojKhz00QGoV54/D9tTf5rw/a7HjukqA9F/iZ7gtnzTXYmsS9mMz1p+w7QLb+Lvuan7WDzvn/1+A7Sdjxw3e71zT4xwcKm2AV7rFwC9VnMsznNRLlXx2U6v6zDk4XJX3+cOxbRwznFg/wzYV2gH8K4j34SrnsyrAPrIejTOMWpAxnoottOzC/vC+ASQF+tv0Wb+Y8sNxRWG/G9uda/kS+ySlXDp96cvfiwaSf3udc4KTB2ViKySynDgG+uw7nsrz7gFVnDsC27Rxg/PebMa20Y4KkP4N57ifVnLMYdiRRfKc16/YJxTdvdIspoq8M8Cxr3HOn4udldl3+1HYduR7sTdgL2MrGAww2ivdVCCvinN5hlX8OMB23zxsgJN+uE+6VN/zO+v/z/nsC7Adj0/G/ubND+J9OBdb+ZWPbbI4DltDXu73upIY/+J8zpnO3/ZcvIaFrOwzct67dK9lwbYX91zL99gOneXS1eWXOBeiGiERuRV4CmhvjNnmdjyq5ojI+djOLqcar6H/avB8idga/vuMMb5tYkN1jtHYws0JxpgVVSRXqkHR/LtxEDtL8I/A5caYYEfHOpzzhWFvIuYYYw6pWVQQ5xiAfaJ9kbFNuRTahrzRkIpT9jbF9u7/n2bmjcK12NqnL6tKWF2+f1sOT3vQxaE+n1KNjebfjdq12Br+OaE+sIg09W1nD1yOHSFmcajPpyqnbcgbjzkishl7p90M2+HySELXqVLVQSLyF+yj0nOAMaZmHomNdGqr52F/OE7Bdnj6xBiztAbOp1Rjo/l3IyMi52I7z/4VeN4E7j91OE4EnhKRWdgOnsdhx5r/hfJDSapaoAXyxuNjbLu3Udg2v6uBvxhjfDukqIblbWwh+T9UnE0zVH7CjrQyDju9dSZ2RJdQjlGsVGOm+Xfj8xy2o+M84MEaOkc6dvzwW7C14ruwQy3eZYKfLEmFiLYhV0oppZRSykV1og25iNwkIukiUiAiy0XkD5WkbSIiD4jIeif9ShE563COqZRSSimllFtcL5CLyEjsUHUTsO2XVgIfi0irALs8jO3M8n/Y9lUvYsedPPYwjqmUUkoppZQrXG+yIiLLgW+NMTc7y2HYNk3PGWMe85N+O/CIMeafXuveAfKNMZceyjH9nEOwM4HtrSqtUkrVknhgew11zG0wNP9WStUxQeXdrnbqdKYCPx47oxZgpxIWkYUEnu40ioMzlHnkY0d2OKRjOrNKRnmtaoOdbUsppeqS9thJolRgbYGtbgehlFJeqsy73R5lpSW2x7jvlLqZ2CGd/PkYGCsiX2Cndx2EnZEr/DCOeTd+ejFv2bKFhISEKi5BKaVqVm5uLh06dACt9Q3GXtD8Wynlvurk3W4XyA/FGODf2Bpsgy2UT8FOv32oJmLbnHvEA1sTEhI0Q1dKqXpI82+lVH3idqfOHUAJdqxNb62B3/3tYIzJNsYMBWKBTtha7zzsLISHesxCY0yu54XWQimllFJKqVriaoHcGXj+O2yzE6CsA+Yg4Ksq9i1wpgyOAC4E3j/cYyqllFJKKVXb6kKTlcnANBFZAXwD3Iqt/Z4CICKvAduMMXc7y38E2mGnEG4HjMfeWPwj2GMqpZRSSilVV7heIDfGzBCRZOAhIAVb0D7LGOPplNkRKPXapSl2LPIu2KYq84DLjDE51TimUkoppYJUWlpKUZHOpq6UtyZNmhAeHl51wiC4Pg55XSQiCcCePXv2aKcgpVRImBJDzpIcijKKiGwTSWL/RCRcgto3NzeXZs2aATRz+rmoAKqbfx/O59JYFBUVsXHjRkpLS6tOrFQjk5iYSEpKCnYKhPKqk3e7XkOulFINXfacbNaNWUfh1sKydVHto0h7Jo3kYckuRta46edSNWMMGRkZhIeH06FDB8LC3B4LQqm6wRjD/v37ycrKAqBNmzaHdTwtkCulVA3KnpPNquGr7CCtXgq3FbJq+Cp6zu6phT8X6OcSnOLiYvbv30/btm2JiYlxOxyl6pTo6GgAsrKyaNWq1WE1X9FbXaWUqiGmxLBuzLoKhT670f6z7tZ1mBJtOlib9HMJXklJCQCRkZEuR6JU3eS5UT1w4MBhHUcL5EopVUNyluSUaw5RgYHCLYXkLMkJnEaFnH4u1eevfaxSKnTfDS2QK6VUDSnKCG5UimDTqdDQz0UpVddogVwppWpIZJvgHvMHm06Fhn4uKpREhPfee6/Gjj9+/Hj69OlTrX0GDBjArbfeWkMRqZqgBXKllKohif0TiWofFTiBQFSHKBL7J9ZeUOrg5xLoSbN+LiFnSgy7F+8m8+1Mdi/e7Wr7/NGjRzN06NBq7xeoYJyRkcHZZ58ditBUI6ajrCilVA2RcCHtmTQ7mgeU70ToFAbTnk7Tca9rWbnPRfDbuVM/l9Bp6MNLpqSkuB2CagC0hlwppWpQ8rBkes7uSVS78jXlUe2jdGg9FwX6XMLjwvVzCSHP8JK+nWg9w0tmz8mukfPOnj2bXr16ER0dTYsWLTj99NPZt28f48ePZ9q0abz//vuICCLC4sWLAbjzzjvp3r07MTExdOnShfvvv79s5IypU6cyYcIEVq5cWbbf1KlTgYpNVrZu3crFF19MUlISsbGx9O3bl+XLlweMtbLz+uOp4Z8wYQLJyckkJCRw/fXXV5hJtbS0lHHjxpGUlERKSgrjx48vt33y5Mn06tWL2NhYOnTowI033kheXl413mUVSlpDrpRSNaC0qJSfz/uZtte3JXlYMi3Pb6kzQtYx3p/Lrnm72PLEFsLiwmh5fku3Q6vzSvaVBN4YDuFNw6seXlJg3Zh1tDy/Zdl3IdBxw2ODH985IyODiy++mH/84x9ccMEF7N27lyVLlmCM4fbbb+fXX38lNzeXKVOmAJCUlARAfHw8U6dOpW3btvz8889ce+21xMfHM27cOEaOHMkvv/zCggULWLhwIYBnBsZy8vLyOO2002jXrh0ffPABKSkpfP/995XOclrZeQP57LPPaNq0KYsXLyY9PZ0rr7ySFi1a8Mgjj5SlmTZtGmPHjmX58uV89dVXjB49mn79+nHGGWcAEBYWxrPPPkvnzp3ZsGEDN954I+PGjeOFF14I+r1WoaMFcqWUqgGbHt7E7o93s3fFXpoPbE5EswiaD2judljKh4QLzQc0p9nJzdj+8nYO/H6A3G9zaXZixcKWOmhJ3JKA25IGJ9H7o97BDS+51Q4v6flufJ36NQd2VKwdHmAGBB1bRkYGxcXFDBs2jE6dOgHQq1evsu3R0dEUFhZWaGpy3333lf0/NTWV22+/nenTpzNu3Diio6OJi4sjIiKi0iYqb731FtnZ2Xz77bdlBf20tLRK463svIFERkby6quvEhMTQ8+ePXnooYe44447+Pvf/142m2rv3r158MEHAejWrRvPP/88n332WVmB3LvTZ2pqKg8//DDXX3+9Fshdok1WlFIqxHJX5LLp0U0AdH+hOxHNGl7dh4icKiIfish2ETEiEnQvORHpJyLFIvJjJWnuco77dGgirlxYZBhHTjmSE345QQvjIeLW8JLHHHMMgwYNolevXlx00UX8+9//Zvfu3VXuN2PGDPr160dKSgpxcXHcd999bN68uVrn/vHHHzn22GPLCuPBOJTzHnPMMeVmTj3ppJPIy8tjy5YtZet69+5dbp82bdqUTfMOsHDhQgYNGkS7du2Ij4/nsssuY+fOnezfvz/o2FXoNLxfCaWUclFpYSm/XfEblEDyiGRajWjldkg1JRZYCbwKzAl2JxFJBF4DPgNaB0hzAnAd8NPhhxm85Au03Xiw+uf1D7zRaV1yKMNLnph+4uGEZU8fHs6nn37KsmXL+OSTT3juuee49957Wb58OZ07d/a7z1dffcWoUaOYMGECZ555Js2aNWP69OlMmjSpWuf2TKUerFCd158mTZqUWxaRsqYz6enpDBkyhBtuuIFHHnmEpKQkvvzyS66++mqKiorKFfZV7dACuVJKhdDGBzeyf/V+mrRqQrd/dnM7nBpjjJkPzIdqz1T3IvAWUAJUqFUXkTjgTeBa4D7f7apuCKZNt2d4ycJthf7bkYvt3Ow9vGR12opXRkTo168f/fr144EHHqBTp068++67jB07lsjISEpKyrdVX7ZsGZ06deLee+8tW7dp06Zyafzt56t379688sor7Nq1K6ha8mDO68/KlSvJz88vuwH4+uuviYuLo0OHDlXuC/Ddd99RWlrKpEmTypq4zJw5M6h9Vc3QJitKKRUie77ew5Yn7CPjI14+gsiWOrGMNxG5EugCTKgk2T+Bj4wxC2snqvJyv81l9cWr2XDPBjdO36B4hpe0C74b7T81Mbzk8uXLefTRR1mxYgWbN29mzpw5ZGdnc9RRRwG2vfRPP/3EmjVr2LFjBwcOHKBbt25s3ryZ6dOns379ep599lnefffdcsdNTU1l48aN/Pjjj+zYsYPCwort4y+++GJSUlIYOnQoS5cuZcOGDbzzzjt89dVXfmMN5rz+FBUVcfXVV7N69WrmzZvHgw8+yM0331xWuK5KWloaBw4c4LnnnmPDhg28/vrrvPjii0Htq2qGFsiVUipEds3fBaXQ+rLWOlKHDxHpBjwGXGqMKQ6Q5i/AccDd1ThulIgkeF5A/OHEWZRZRNb0LH6f+jum1L3JaxoKN4b9TEhI4IsvvmDw4MF0796d++67j0mTJpVN3nPttddyxBFH0LdvX5KTk1m6dCnnnXcet912GzfffDN9+vRh2bJl3H///eWOe+GFF3LWWWcxcOBAkpOTefvttyucOzIykk8++YRWrVoxePBgevXqxWOPPUZ4uP+a/0htf6kAACAASURBVGDO68+gQYPo1q0bp556KiNHjuS8886rMKxhZY455hgmT57M448/ztFHH82bb77JxIkTg95fhZ4YoxmOLydT37Nnzx4SEhLcDkcpVY/snLeThJMSaNK8SdWJg5Sbm+sZYq2ZMSY3ZAcOERExwAXGGL/zh4tIOPA18B9jzIvOuvHAUGNMH2e5A7ACOMMY85OzbjHwozEm4BzgznEe9F1/qPl3aWEpS1svpWRPCX2+6NPoZ+ssKChg48aNdO7cmaZNmx7ycUyJ0WE/Q2T06NHk5OSUG/tcuaey70h18m5tQ66UUiHUYnALt0Ooi+KBvsCxIvK8sy4MEBEpBv4MJACtgO+92qSHA6eKyM1AlDHGXwPeicBkn3NtPdRAw6LsOOSZr2WSPSu70RfIQ8UzvKRSyj9tsqKUUoehOK+YNX9dYzuuqUBygV5AH6/Xi8Aa5//LsaOu+KZZge3g2SdAYRxjTKExJtfzAvYebrDJF9lmFNmzs7XZilKqVmgNuVJKHYYNd24g498Z5H6TS98f+lZ3xJF6yxkNxXvGk84i0gfYZYzZLCITgXbGmMuNMaXALz77ZwEFxhjv9b5p9gE7fdLUuKQzkghvFk5RRhF7lu7RWnJVp0ydOtXtEFQN0BpypZQ6RLs/2832F7YDkDYprdEUxh19gR+cF9hmIz8ADznLbYCOLsR12DzNVgCyZ2W7HI1SqjHQGnKllDoExbnF/HbVbwC0vbEtzQc1rvaxxpjFVBzMznv76Cr2Hw+MryLNgGoHFiLJFyWz76d9xPTQCVKUUjVPC+RKKXUI1v9tPYWbC2napSldHu/idjgqxFqc04KWQ3ToSqVU7dAmK0opVU075+8k45UMEDhyypFExGndRkPTyJofKaVcpgVypZSqpi3/sLNxth/TnsRTtcNfQ1a8t5isWVk62opSqkZptY5SSlVTr496sWXSFjr8rYPboagaZEoN33T/hqLfi+izpA+Jp+jNl1KqZmgNuVJKVVN4TDip96cSHuN/OmzVMEiY0PzPtrNu9kwdbUXVvtTUVJ5++ulD3l9Eymb0TE9PR0T48ccfQxVeyC1dupRevXrRpEkThg4detjHmzp1KomJ9eNGWgvkSikVhKIdRWz75zZtutDIlE0S9I5OEnQ4SkpLWJy+mLd/fpvF6YspKfU7z1PIjB49GhFBRGjSpAmtW7fmjDPO4NVXX6W0tLRGzx1K3377LX/9619DcqwOHTqQkZHB0UcfHZLj1YSxY8fSp08fNm7cGJLx1keOHMnatWsPP7BaoAVypZQKwv9u+h//u/l/rL2ufmTuKjSSzkgiPCGcou1F7Fm2x+1w6qU5v84h9ZlUBk4byCVzLmHgtIGkPpPKnF/n1Oh5zzrrLDIyMkhPT2f+/PkMHDiQMWPGMGTIEIqLi2v03KGSnJxMTExoht4MDw8nJSWFiAh3WyuXlJQEvClav349f/rTn2jfvn1Iarajo6Np1arVYR+nNmiBXCmlqpA1M8s2WQiHtte3dTscVYt0kqDDM+fXOQyfOZytuVvLrd+Wu43hM4fXaKE8KiqKlJQU2rVrx3HHHcc999zD+++/z/z588tqX6+66iqGDBlSbr8DBw7QqlUr/vOf/wAwYMAAbrnlFsaNG0dSUhIpKSmMHz++3D6TJ0+mV69exMbG0qFDB2688Uby8vLKtnuaTsydO5cjjjiCmJgYhg8fzv79+5k2bRqpqak0b96cW265hZKSg08PfJus5OTkcN1119G6dWuaNm3K0Ucfzdy5c4N6P3ybrCxevBgR4bPPPqNv377ExMRw8skns2bNmnL7vf/++xx33HE0bdqULl26MGHChHI3NMFe+wcffECPHj2Iiopi8+bNfmPbuXMnV111FSLC1KlT/TY5ee+998qNgrRy5UoGDhxIfHw8CQkJHH/88axYsaLcub3961//omvXrkRGRnLEEUfw+uuvl9suIrzyyitccMEFxMTE0K1bNz744IOg3uPDoQVypZSqRFFmEWtvtLXine7pRPzx8S5HpGpbWbOV2dpsxWNf0b6Ar4LiAsA2UxmzYAyGiu+ZZ92YBWPKNV8JdMxQ+dOf/sQxxxzDnDn2RuCaa65hwYIFZGRklKWZO3cu+/fvZ+TIkWXrpk2bRmxsLMuXL+cf//gHDz30EJ9++mnZ9rCwMJ599llWrVrFtGnT+Pzzzxk3bly5c+/fv59nn32W6dOns2DBAhYvXswFF1zAvHnzmDdvHq+//jovvfQSs2fP9ht7aWkpZ599NkuXLuWNN95g9erVPPbYY4SHH15flnvvvZdJkyaxYsUKIiIiuOqqq8q2LVmyhMsvv5wxY8awevVqXnrpJaZOncojjzxS7Wt//PHHeeWVV1i1alWFWmtPc5qEhASefvppMjIyyr3/lRk1ahTt27fn22+/5bvvvuOuu+6iSZMmftO+++67jBkzhr/97W/88ssvXHfddVx55ZUsWrSoXLoJEyYwYsQIfvrpJwYPHsyoUaPYtWtXUPEcMmOMvnxeQAJg9uzZY5RSjVdpaan56fyfzCIWmW/7fGtKCktciWPPnj0GMECCqQN5ZF1+1UT+XVJQYr5I+MIsYpHJ/T43ZMetD/Lz883q1atNfn5+ufWMJ+Br8JuDjTHGLNq4qNJ0nteijYvKjtvyHy39pqmuK664wpx//vl+t40cOdIcddRRZcs9evQwjz/+eNnyueeea0aPHl22fNppp5lTTjml3DFOOOEEc+eddwY8/6xZs0yLFi3KlqdMmWIAs27durJ11113nYmJiTF79+4tW3fmmWea6667rmy5U6dO5qmnnjLGGPPxxx+bsLAws2bNmoDn9QWYd9991xhjzMaNGw1gfvjhB2OMMYsWLTKAWbhwYVn6jz76yABln/egQYPMo48+Wu6Yr7/+umnTpk21r/3HH3+sMt5mzZqZKVOmlNu3WbNm5dK8++67xhZfrfj4eDN16lS/x/Pd/+STTzbXXnttuTQXXXSRGTx4cNkyYO67776y5by8PAOY+fPn+z1HoO+IMdXLu7WGXCmlAsh8M5Od7+9EmghHTjuSsEjNMhujsKgwekzvwYmbTiT+WH1CEqyMvRlVJ6pGulAxxpRr8nDNNdcwZcoUADIzM5k/f365WmKA3r17l1tu06YNWVlZZcsLFy5k0KBBtGvXjvj4eC677DJ27tzJ/v37y9LExMTQtWvXsuXWrVuTmppKXFxcuXXex/X2448/0r59e7p3734IVx2Y97W1adMGoCyGlStX8tBDDxEXF1f2uvbaa8nIyCi7tmCuPTIyssJ7GCpjx47lmmuu4fTTT+exxx5j/fr1AdP++uuv9OvXr9y6fv368euvv5Zb5x1rbGwsCQkJAT+XUNFxyJVSyo+S/BLW/81m7KkPphLXO66KPVRD1uLsFm6HUKfk3Z0XcFt4mG1C0Sa+TVDH8k6XPib9sOIKxq+//krnzp3Lli+//HLuuusuvvrqK5YtW0bnzp3p379/uX18m0CISFnHxPT0dIYMGcINN9zAI488QlJSEl9++SVXX301RUVFZZ0y/R2jsuP6io6OPrQLroJ3DJ4bFU8MeXl5TJgwgWHDhlXYr2nTpkFfe3R09CHNfhsWFuZ58lXmwIED5ZbHjx/PJZdcwkcffcT8+fN58MEHmT59OhdccEG1z+dRnc8lVLRArpRSfoRHh9P7495sfXorHe7UCYDUQb41rI1RbGRslWn6d+xP+4T2bMvd5rcduSC0T2hP/44HC7/BHPdwfP755/z888/cdtttZetatGjB0KFDmTJlCl999RVXXnlltY753XffUVpayqRJkwgLs0/RZs6cGdK4wdbabt26lbVr14a8ljyQ4447jjVr1pCWluZ3e01fe3JyMnv37mXfvn3Extq/DX/jqHfv3p3u3btz2223cfHFFzNlyhS/BfKjjjqKpUuXcsUVV5StW7p0KT169AhZzIdKC+RKKRVAfJ94jpp6lNthqDoiZ0kOmyduJrZnLF2f6Fr1Do1ceFg4z5z1DMNnDkeQcoVywd7QPH3W02U16qFWWFjI77//TklJCZmZmSxYsICJEycyZMgQLr/88nJpr7nmGoYMGUJJSUm5wlow0tLSOHDgAM899xznnnsuS5cu5cUXXwzlpQBw2mmnceqpp3LhhRcyefJk0tLS+O233xARzjrrrJCfD+CBBx5gyJAhdOzYkeHDhxMWFsbKlSv55ZdfePjhh2v82v/4xz8SExPDPffcwy233MLy5cvLjU+en5/PHXfcwfDhw+ncuTNbt27l22+/5cILL/R7vDvuuIMRI0Zw7LHHcvrpp/Phhx8yZ84cFi5cGLKYD5U2iFRKKS8FWwrY+8Net8NQdVDxnmJ2zd9F5luZOtpKkIYdNYzZI2bTLqFdufXtE9oze8Rshh1VsSlEqCxYsIA2bdqQmprKWWedxaJFi3j22Wd5//33K4xMcvrpp9OmTRvOPPNM2rat3tCmxxxzDJMnT+bxxx/n6KOP5s0332TixImhvJQy77zzDieccAIXX3wxPXr0YNy4ceWGSQy1M888k7lz5/LJJ59wwgkncOKJJ/LUU0/RqVMnoOavPSkpiTfeeIN58+bRq1cv3n777XJDToaHh7Nz504uv/xyunfvzogRIzj77LOZMGGC3+MNHTqUZ555hieffJKePXvy0ksvMWXKFAYMGBCymA+V+LbNqfUARG4C7gBSgJXA/xljvqkk/a3ADUBHYAcwG7jbGFPgbI8H/g5cALQCfgDGGGO+rUZMCcCePXv2kJCQcEjXpZSqf4wx/PTnn8hZnMMRrxxByhUpbocEQG5uLs2aNQNoZozJdTueuqwm8+/SwlKWtlpKSW4JfZb0IfGU+jEl9+EoKChg48aNdO7cmaZNmx7ycUpKS1iyeQkZezNoE9+G/h3711jN+KHIy8ujXbt2TJkyxW97aaUCqew7Up2829UmKyIyEpgMXA8sB24FPhaRI4wxFbqzisglwGPAVcAyoDswFTukzFgn2SvA0cBlwHbgUmChiPQwxmyr0QtSStVr21/azu6FuwmLDiPhJL0ZV+V5JgnKfD2T7FnZjaJAHirhYeEMSB3gdhgVlJaWsmPHDiZNmkRiYiLnnXee2yGpRsrtJitjgX8bY6YYY1ZjC+b7sQVuf04Glhpj3jLGpBtjPgHeBv4AICLRwIXAOGPMF8aYdcaY8cA6bK26Ukr5lb8hn/W321FVukzsQkz30ExXrRqW5BE6SVBDsnnzZlq3bs1bb73Fq6++6vq08qrxcu0vT0QigeOBssZGxphSEVkInBRgt2XApSLyB2PMNyLSBRgMeOY9jQDCgQKf/fKBU0IZv1Kq4TClht+u+o3SfaU0O60Z7f6vXdU7qUYp6YwkwhPCKdpexJ5le7SWvJ5LTU2tMKyeUm5ws4a8JbbwnOmzPhPbnrwCY8xbwAPAlyJyAFgPLDbGPOps3wt8BdwvIm1FJFxELsUW8AMOiCoiUSKS4HkBOvODUo3Itue2see/ewiLDePIV49Ewhr3kHYqME+zFYDsWdkuR6OUaijcbrJSLSIyALgHuBE4DhgGnCMi93sluwwQYBtQCNyCbdZS2YjudwN7vF5bQx27Uqpuyk/PZ8PdGwDo+kRXorvUzOQbquFo9ZdWNDu1GfEnaN2NUio03GwstQMoAVr7rG8N/B5gn78DrxtjXnGWfxaRWOBlEXnEGFNqjFkPnOasTzDGZIjIDGBDJbFMxHYu9YhHC+VKNVimxJCzJIeijCIiW0fS+ZHO5CzOoe311RvuTDVOLQa3oMXgxjVzpzbrUMq/UM3g6VqB3BhTJCLfAYOA9wBEJMxZfj7AbjFUrOn2DMBZ7hmzMWYfsE9EmgNnAuMqiaUQW5uOE0fwF6KUqley52Szbsw6CreWfeWJah9F16e76ndfKR9NmjRBRMjOziY5OVm/I0o5jDEUFRWRnZ1NWFgYkZGRh3U8t7sTTwamicgK4BvssIexwBQAEXkN2GaMudtJ/yEwVkR+wA6TmIatNf/QGFPi7HMmtnC+xtn+BPCb55hKqcYre042q4avwncW78Jthay+aDUyW0geluxOcKreKcoqYufcnaSMTmmw/Q7Cw8Np3749W7duJT093e1wlKpzYmJi6NixI2Fhh9cK3NUCuTFmhogkAw9hO3L+CJxljPF09OxI+Rrxh7E/pQ8D7YBsbCH9Xq80zbBNUNoDu4B3gHuNMQdq8FKUUnWcKTGsG7OuQmHcbgQE1t26jpbnt0TCG2bhSoVOaXEp3xz1DcW7iok5IoZm/Zq5HVKNiYuLo1u3bhw4oD+jSnkLDw8nIiIiJE+O3K4hxxjzPAGaqBhjBvgsFwMTnFeg480EZoYwRKVUA5CzJKdcM5UKDBRuKSRnSQ7NBzSvvcDqKRE5FTvL8vHYUawuMMa8F+S+/YD/Ar8YY/p4rb8b21n/SOxwtcuAO40xa0Ic/mELiwijxTktyHw9k6xZWQ26QA624OE73bxSKnTq1SgrSil1qIoyikKaThELrARuqs5OIpIIvAZ85mfzacA/gROBM4AmwCdOJ/06J/kiZ5KgWTpJkFINmSkx7F68m8y3M9m9eDemJPTfd9dryJVSqjZEtgmuw02w6Ro7Y8x8YD5UuyP8i8Bb2A75Q32OeZb3soiMBrKwtfBfHHq0NSPpzwcnCcr9KrfB15Ir1RgFGggg7Zm0kPY50hpypVSjkNg/kaj2UYETCER1iCKxv868WFNE5EqgC5U0O/ThKeHuqpmIDo/3JEFZs7JcjkYpFWqegQB8mzsWbitk1fBVZM8J3eRgWiBXSjUKEi6kPZMWYKP9J+3pNO3QWUNEpBvwGHCp0x+oqvRhwNPAUmPML5Wkc3WmZW22olTDVOVAANiBAELVfEUL5EqpRiN5WDKRHSo2SYlqH0XP2T11yMMaIiLh2GYqDxpj1ga52z+Bo4G/VJHO1ZmWy5qtZBaxb/W+2jy1UqoGVWcggFDQNuRKqUal59s92TF3B81OakbJvhIi20SS2D9Ra8ZrVjzQFzhWRDyjaoUBIiLFwJ+NMZ97EjtphgCnGmOqKmC7OtNyWFQYR79/NLFHxxLZUvsfKNVQ1PZAAFogV0o1Ks36NdPOd7UvF+jls+5G4E/AcGAj2NI58BxwATDAGLOxqgPXhZmWdZhMpRqe2h4IQAvkSimlqk1E4rCzIXt0FpE+wC5jzGYRmQi0M8ZcbowpBX7x2T8LKPBpH/5P4BLgfGCviKQ46/cYY/Jr7GJCyJSaBjtrp1KNSWL/RCLbRVK0LUANuNjmjqEaCEDbkCulGoWS/BLW3rSWHR/u0M53odEX+MF5gW028gN25mWwkwV1rOYxb8COrLIYyPB6jTzMWGvcrk928f0p37P+b+vdDkUpFQISLiQOCFDYroGBALSGXCnVKOQsymH7C9vZ+cFOWmxu4XY49Z4xZjFlP0t+t4+uYv/xwHifdfW2arm0sJTcpbkUbCyg66SuWkuuVD2Xvz6fHe/sACCieQTFuw8ODhXVPoq0p0M7DrkWyJVSjcLOj3YCkHROkivtjFXDppMEKdVwGGNYe8NaSgtKSRyUSO8Fvdnz5R6KMopqbCAALZArpRo8Y0xZgbzFOVo7rkIvLCqMlue1JPONTLJmZWmBXKl6rGBTAXk/5CFRQvd/dScsIqzGO29rG3KlVIO3f/V+CjcVIlFC8z/piBiqZiSP0EmClGoIolOjOeHXE+g5uycx3WJq5ZxaIFdKNXg759na8eYDmxMeG+5yNKqh8m22opSqvyJbRtJySMtaO58WyJVSDZ53+3Glaoqn2QpA1qwsl6NRSlVXzpc5ZM3Kwpjaf8KlbciVUg1a6YFSDmQeAKDFYG0/rmpWq0taUbynOPBwaUqpOqmkoIQ1V68hf20+3Z7vRrub2tXq+bVArpRq0MKahHHC6hMo2FBAdJdot8NRDVyLs1vQ4my98VOqvtk8cTP5a/OJTImk1ahWtX5+bbKilGrwRITorloYV0opVdG+X/exeeJmANKeTaNJYpNaj0EL5EqpBsuUGkqLSt0OQzVC+Rvz2frsVh1tRak6zpQa1l63FnPAkHROEsnDQzfZT3VogVwp1WDtXbGXpS2X8ttVv7kdimpESotKWdFnBevGrCP3ax1tRam6LOPVDPYs2UNYTBjd/9ndtYnjtECulGqwdn60k5K9JRTnFledWKkQCYsMo8W5th159qxsl6NRSgVSnFvMhnEbAOj898407dTUtVi0QK6UarB0dk7lllYjbKewrFlZ2mxFqToqIiGCHjN7kDw8mXa31O6oKr60QK6UapAKMwrJ+y4PgKSzdfxxVbua/7k54fHhFG0r0mYrStVhSacn0XNWT8Ii3C0Sa4FcKdUg7Zq/C4D4vvFEpUS5HI1qbMKbhtPiPG22olRdVLK/hIKtBW6HUY4WyJVSDZLOzqncps1WlKqb0iek8+1R3/L7a7+7HUoZLZArpRqc0qJSdn+6G9D248o9nmYrxbuKyf9fvtvhKKWAvJV5bJm0hZK8EiKa1535MetOJEopFSKlhaV0GNeB3GW5xB8f73Y4qpEKbxpO7096E9crjvDYcLfDUarRMyWGNX9dAyWQPDyZlue2dDukMlogV0o1OBHxEaTel+p2GErR7MRmboeglHJs+9c29n6zl/CEcNKeSXM7nHK0yYpSSilVC0oP6KyxSrmlYGsBG+/ZCECXx7oQ1bZudfbXArlSqkEp3FZI1owsDuQccDsUpQDY8f4Ovu39bdkEJEqp2rfu/9ZRsreEhJMSaHtdW7fDqUAL5EqpBiV7Tjar/7KaVcNWuR2KUpbAvp/3kT07W0dbUcoFpsQQ3T2asOgwur/cHQkTt0OqQAvkSqkGpWy4Q50MSNURntFWCrcW6iRBSrlAwoWuj3flxE0nEnd0nNvh+KUFcqVUg1Gyr4ScxTmADneo6g6dJEgp9xhz8KlUZHKki5FUTgvkSqkGY/dnuzGFhqapTYk5KsbtcJQq0+oiO0mQNltRqvbkLs/lh1N+IO+nPLdDqZIWyJVSDYb37Jwida+NoGq8mp/p1WxluTZbUaqmlR4oZc1f15C7LJctk7e4HU6VtECulGoQjDHsmrcL0OYqtUFEThWRD0Vku4gYERlajX37iUixiPzoZ9tNIpIuIgUislxE/hDayN2hzVaUql1bn9rKvp/2EdEigq5PdHU7nCppgVwp1SDsX7Ofwq2FhEWHkTgg0e1wGoNYYCVwU3V2EpFE4DXgMz/bRgKTgQnAcc7xPxaRVocdbR3Q+tLWtBrViqTB2uFYqZqUvzGf9PHpAHR9smudbjvu4XqBvLq1ISJyq4isEZF8EdkiIk+JSFOv7eEi8ncR2eikWS8i94s+v1aqQYs9MpYTN59Iz9k9CY/WacprmjFmvjHmPmPMu9Xc9UXgLeArP9vGAv82xkwxxqwGrgf2A1cdXrR1Q4uzWtDjjR4kna4FcqVqijGGtTespTS/lMSBiaRckeJ2SEGJcPPkXrUh1wPLgVuxtSFHGGOy/KS/BHgMmzkvA7oDUwGDzcgB7gRuAK4AVgF9gSnAHuDZGrwcpZTLmnZoStMOTatOqFwhIlcCXYBLgft8tkUCxwMTPeuMMaUishA4qZJjRgHeU+7FhzJmpVT9kjU9i90f70aihO4vdq83/YncriGvbm3IycBSY8xbxph0Y8wnwNvAH3zSvG+M+chJMxv4xCeNUkqpWiQi3bAVKpcaY4r9JGkJhAOZPuszgcqquO7GVrh4XlsPP9qaY4whb2Uemx7dpKOtKFUDMt+0WUin+zoR073+jLblWoHcqzZkoWedMabUWQ5UG7IMON7TrEVEugCDgXk+aQaJSHcnzTHAKcD8UF+DUqpuyJqdxU9n/0TWzAoP1lQdICLh2GYqDxpj1ob48BOBZl6v9iE+fkiVFpby/Snfs/HejaT/PZ3di3djSrRgrlSo9Hq/F0f85wg6juvodijV4maTlcpqQ470t4Mx5i0RaQl86bQJjwBeNMY86pXsMSAB+E1ESpxz3GuMeTNQIPrIU6n6bcd7O9i1YBexx8TSakSD6P/X0MRjmw8eKyLPO+vCABGRYuDPwJdACdDaZ9/WwO+BDmyMKQQKPct1/fH0rnm7oNT+f9P4TWxiE1Hto0h7Jo3kYcnuBqdUAyDhQpur2rgdRrW53WSlWkRkAHAPcCO2B/4w4BwRud8r2QhgFHCJk+YK4HYRuaKSQ9erR55KqYNMiWHXAh3usI7LBXoBfbxeLwJrnP8vN8YUAd8Bgzw7iUiYs+yvA2i9kz0nm1XDV1G6v7Tc+sJthawavorsOTocolKHorSwlM1PbqYkv8TtUA6ZmzXkO6h+bcjfgdeNMa84yz+LSCzwsog84jR5eQJ4zBgz3StNJ2yhe1qA407Edi71iEcL5UrVC7nLcyneWUxEYgQJJyW4HU6jISJxQJrXqs4i0gfYZYzZLCITgXbGmMudvPkXn/2zgAJjjPf6ycA0EVkBfIPt6B+L7Zhfr5kSw7ox6+wQBBU2AgLrbl1Hy/NbIuF1u5Zfqbpm82ObSR+fzs73d9Lniz51/kmZP67VkB9ibUgMZQ/7ynhuh6SKNAGv1RhTaIzJ9byAvUFdhFLKdZ7ZOZuf2ZywiHr10K++6wv84LzAFqZ/AB5yltsA1WrEaYyZAdzuHONHbO35WcYY36aN9U7OkhwKtxYGTmCgcEshOUtyai8opRqAfb/tY9OjmwBod3O7elkYB5eHPaSK2hAReQ3YZoy520n/ITBWRH7ADpOYhq01/9AYU+KV5l4R2Ywd9vBY7Ggur9bOJSmlapOnQK7NVWqXMWYxBytC/G0fXcX+44HxftY/Dzzvu76+K8ooCmk6pRSYUsPa69ZiigxJZyeRPKL+9sNwtUBujJkhIsnY2pAUbI2Id21IR8rXdj+Mfbj3MNAOyMYpgHul/Dw3/QAAIABJREFU+T9sIf0FoBWwHXiJg7U2SqkGomBrAftW7gOBpLN0shVVd0W2CW6mwGDTKaXg9ym/s+eLPYTFhNHthW71tnYc3K8hr7Q2xBgzwGe5GDul8oRKjrcXW9N+a+iiVErVRcW7i0n8UyKm2NSLqZFV45XYP5Go9lEUbiv0345cIKp9FIn9E2s9NqXqC1NiyFmSQ1FGEWFNw1h3+zoAUiekEp0a7W5wh8n1ArlSSh2quF5x9Pmsj06wouo8CRfSnklj1fBVtqGP95+sU6nXYVwHCjMKadpeZ5tVylf2nGzWjVlXoS9G09SmtL+1Tk8/EBTtAaWUqvckrP4+plSNR/KwZHrO7klUu6hy66PaR5E6IZX0+9NZdcGqej10m1I1wTNkqL+O0QXpBez8YKcLUYWW1pArpeqlwm2FSIQQ2Vqbqqj6I3lYMi3Pb1n22D2yTSSJ/RMp2FLA1me2snfFXtb+dS1HvnZkvW4Pq1SoVDpkKDSYIUO1hlwpVS9tfmIzy1KWkf5QutuhKFUtEi40H9Cc1he3pvmA5ki4EJ0aTc9ZPSEcMt/IZOtTOhWGUtB4hgzVArlSql7a9ZGdnTO2V6zLkSgVGs0HNiftKTvX0vo71rPrk10uR6SU+xrLkKFaIFdK1Tv71+4nf10+0kRofnpzt8NRKmTa3dyOlCtToBRW/2U1+evz3Q5JKVc1liFDtUCulKp3PJMBJZ6WSES8doVRDYeI0P1f3Yn/YzzFu4vLZiBUqrHyDBkacBoygagO9X/IUC2QK6XqHU+BPGmwTgakGp6wqDCOnnM0He7oQPcXursdjlKu8gwZGmj8foC0p9PqdYdO0AK5UqqeKd5bzJ4v9gDQ4pwWLkejVM2IahtF1390JSxKf6aVSh6WTNzxcRXWR7WPoufsniQPS3YhqtDSZ71KqXpl96e7MQcM0WnRxHSPcTscpWqcKTFsuGcDCSclkDy0/hc8lKqu/I355H2fB2CHBI2QsiFD63vNuIcWyJVS9UriaYkc+dqRgcekVaqB2f7ydrb8YwvhceHEfB1DbE8dWUg1LhkvZ4CB5n9uTsplKW6HUyO0QK6UqleatGjSYDNkpfxpc00bsmdlk7Moh5/P/5njvzmeJklN3A5LqVrT8a6ORLaJbNDD3FarcZqI/EFEwivZHiUiIw4/LKWUUqEkIuNEJNpruZ+IRHktx4vIC+5EpyoT1iSMHjN7ENUpioL1Bay+eDWlxaVuh6VUrYloFkH7W9rTfGDDHea2ur1FvgLKelGJSK6IdPHangi8HYrA6hNTYti9eDeZb2eye/FuTIk+S1eqJmS+mcnmJzaTv1HHZj4EE4F4r+X5QDuv5RjgulqNSAUtsmUkvd7vRVhMGLs/2c3Guze6HZJSNc4YgzGNo0xV3QK5b8t5fy3pG0br+iBlz8nm69SvWTlwJb9e8isrB67k69SvyZ6T7XZoSjU4257fxoZxG9i9cLfbodRHweTfqg6LOyaOI6ccCcCWJ7eQ+WamyxEpVbP2frOX7074rlH8rdfEeEqN41YGWxhfNXwVhVsLy60v3FbIquGrtFCuVAgVZReRuzwXgBaDdbhD1Ti1GtGKjnd3JCwmDInUeyrVsG375zbyvstj16e73A6lxukAp4fIlBjWjVnn//bDWbfu1nXafEWpENm1YBcYiOsTR1S7qKp3UKqB6vz3zvT9oS+tLmrldihK1ZiirCKyZmQB0O7mdlWkrv8OZZSVHiLiGeJAgCNFxDNae8vQhFX35SzJqVAzXo6Bwi2F5CzJofmAhtsJQanaUjY75zk6O+dhuEZE8pz/RwCjRWSHsxwfYB9Vx0i4lBuDvzCjkCYtmhAWqXVsquHI+E8GpsgQ/4d4EvomuB1OjTuUAvlnlG97OPf/2bvz+KjKq4HjvzOTBUhCEkjYwhIgLBEU3IC2orR2cWmrUuvWKlrrilXqa+vSurRatZuVVvR1X2q1i7Vaq9bat2CtdUM22fclJECAhJA9mTnvH3cSh5BJZpKZ3JnM+X4+8yFz57n3PHeYPDlz77ME/tXA9qS4JNxY1hjVcsaY0PzNfirecPqN2+qcXbYduCzo+S7gwnbKmARy4L0DrDprFXln5jH+ofFuV8eYqPA3+yl9qBSAgrm9/+o4RJ6Qj45JLRJQ2tC0qJYzxoRW9d8qmiubSRmYQv9pvf9KSSyoaqHbdTDR17yvmcbdjZT+bymZUzMZdsUwt6tkTLfte2UfDTsaSM1LJf+c5FidNqL7W6q6rbMHSXLbM2dmDunD00PPUyCQPiKdnJk5PVovY3qjuk11ePp6GHDKgF6zTLIx0TDw9IGM/olzrWzDNRuo/E+lyzUypvt2LtgJOItiefuEXP6mV4lKh7PAghKXi8gHwPJoHDPeiVcoml8UeNJOAYWi+4sseTAmCoZeMpTP7P8MY38x1u2qJCwR+ZSIfLnNtotEZIuI7BGRR4IXCjKJY+RNI8k/Jx9tVlZ9bRX1O+rdrpIx3TL82uHkfjGXYVcmzx2fbiXkInKiiDwNlAE3AP8CZkSjYokgf3Y+k16Y1P6MDx7ImNR7l3g1pqd5+3hJH2L5YjfcBkxqeSIiRwKPA/8E7gW+AtzsTtVMd4gIE5+YSMaUDJr2NLHyrJX46nxuV8uYLsv7ah5T3phCn1F93K5Kj4k4IReRISJyk4hsAP4EVAHpwJmqepOqfhjtSsaz/Nn5zNg6gykLp1D8XDFTFk5xZoHww+ZbNrtdPWMSnq/eEosomYozKL/FecD7qnqZqt4HXAucE+7BAhdkXhGRUhFRETmzk/IniMg7IrJPROpEZK2IfLdNGa+I3Bm4al8nIptE5FYRsVuNnfBmeJn80mRSBqZQ/VE12+7a5naVjDERiGhQp4i8ApwIvArMA/6uqj4RuTIWlUsU4pVDpjZMy0+j6p0qso7NQlWxvyXGdN2qs1dRv6WecQ+MI/ezNoVoN+QCwcvdnQS8HvT8Q2BEBMfLwOmi+ATwYhjla4AHgBWBn08AHhaRGlV9JFDmRuAqYA6wCjgOeBI4APw6grolpb6FfZn0wiRKHypl5M0j3a6OMRHb/bvd1G6oZdjlw0gfllx3RCOdZeVUnEbxIVXdEIP69AoZkzL41M5P4e2XHAMRjIkVX52Pyn9V4q/zkzow1e3qJLrdODNl7RCRNOAY4Pag17OApnAPpqqvE0jow7nooKpLgaVBm7aKyGxgJtCSkH8aeFlVXw0qcz4wLdx6JbvcWbm29oVJSKrKtru3Ubu6ltQBqQy/drjbVepRkXZZOQGn0f5IRN4XkWtEJGkWA4qEJePGdF/lQicZTx+eTsaRNiajm14D7hWRmcA9QC3wdtDrRwGbeqoyInI0TgL+VtDm/wIni8j4QJkpOH93Xj/8CK3HSReR/i0PkmSmr3CoKjt+tYPajbVuV8WYTlW+VUnt6lo8GR6GzBnS+Q69TKTTHr6nqpcBQ4GHcfoglgaO8wURsYawjf3/2M+qr6/C3+x3uyrGJJx9r32yOqd1/eq2W4FmnAT4MuByVQ1euexbwD9iXQkRKRGRBmAxsEBVHwt6+V7g98BaEWnCuaJ+v6r+roND3ozTpaXlURKbmieebT/ZxqbrN7HyjJU0H2x2uzrGdGjnA85Uh4O/OZiU7K6sW5nYujTLiqrWqOoTqnoCcCTwS+AmYI+I/DWaFUxkvhofqy9YTfkL5ex6fJfb1TEmoagq+1/dD9jqnNGgqntV9UScvuS5qtq23/fXgTt6oCozcfqGXwnMC3RJaXEO8A3gApwuNXOAG0RkTgfHuwfIDnok133uDgz91lDShqZRu7qWtRetxd/kp2JRBbuf303FogrUlxQLa5sEUF9Sz96X9gLJszJnW93+CqKq64Dvi8jNwJdxrrIYnFHvhbcVsvG6jWy5fQuDvjGIlMzk+9ZnTFfUrqmlfms9ki7kfs76xHaXiDzR5nmoojFtw1V1S+DHj0VkMM6XgOcD234O3Kuqvw8qMwrnKvjTIY7XADS0PLc7KZ9IH5bO5L9MZumJS9n70l7eyXsHX9UnsxalD0+naH4R+bOTYyVEE7/KHikDH2SfmE3mkZluV8cVkc6y8kTnpdjXxbr0SsOuHEbJ/BLqN9dTcl8JhbcVul0lYxLCvledpiRnVg7eDBuTEQUXA9twuoHES9bqwZk2t0U/oG3/Ph9RWsQuGfWf3p+hlw2ldEHpIck4QMPOBladvYpJL0yypNy4xt/op/SRUiB5r45D5FfIL6bzBt3ugQXxpHkYc/cYVp+3mu0/286wK4aRNjjN7WoZE/f6T+/PkEuGkDMrx+2q9BYPAefjzLTyJPCsqu7v6sFEJBMoCto0WkSmAvtVdbuI3AMUqOpFgfJzge3A2kD5E3EWlAuezvAV4Acish1n2sOjgetxplY0XaA+Zd/LIa6TKSCwcd5G8s7Is5WljSt81T7yvppH5cJK8s5K3nlCRDX8/FlEFuA06NuIQoMerwIj9Q8cOHCA/v37d/t46leWzFjCwQ8PMuzqYYxfML77lTTGJI2qqiqys7MBslW1qqvHEZF0YDZOt5RP46wp8TjwD43kj4FzrFnAwnZeelpVLxaRp4BCVZ0VKP8d4AqcLwTNODO6PAo8rKr+QJks4E7gLGAQzqQBzwM/bjMAtaN6RbX9TnQViypY/tnlnZabsnCKTZdoXKU+7XVfCiNpuyNKyCG6DXq8ikWD3tIoSoowbf00+o7uG5XjGmN6v2gl5MECfbMvBi7CuVs6SVWro3FsN1lCfqjdz+9mzQVrOi1X/Fwxg88f3AM1MiZ5RNJ2R9wvT1UbVPV5Vf0CcATObcUHcRZwSM6e+GHInZVLwTUFFD9fTJ/CPm5Xx5i4Vv5SOVWLq1B/r/iOH6/8tHZawDrp91JpQ8PrIhluOWOiqeypMqo+jMo1hoTX3YEy1qBHYNxvxjHo7EE2C4AxHVC/sv6K9Sw5fgmVb1W6XZ1eJbCIzvki8iawHmfa2muAkb3h6rg5XM7MHNKHp4ce9SWQPiKdnJk2VsP0rKaKJjZcvYEl05ZQtdiS8ogTcmvQo6O5uple0sPHmKg6uPggTXua8GZ5yf5MttvV6TVE5EGgDGfNiL8BI1T166r6WksfbtP7iFcomh8YexsiKS+6v6jX9d018W/Xk7vw1/nJOCqDrGNtXclIpz18EGd1zh04o97PV9W9sahYb1b6aClbbtnChMcmkHdG8o4oNqY9LdMd5n4xF0+azXYXRVfizHKyGTgJOKm9u3WqOruH62ViLH92PpNemMTG6zbSUNJwyGs5s3JsykPT49Sv7HzQWZmzYG6B9Rwg8mkPY9KgB6bD+h4wBFgOfEdVP+ig/DzgKmAksBd4AbhZVesDr28FRrWz64OqOjeSusVC/dZ6mvY2senGTQw4fQCeFEs6jGnRkpDb6pxR9ww2LW3Syp+dT94ZeVS+XUljWSPNFc1smLuByrcqqV5ZTeZkGwJmes7+f+ynflM93mwvg79hg4kh8oQ86g26iJwL3IeT7L8PzAPeEJEJqrqnnfIXAPfizPLyX2A88FSgXtcHih3PoX3aJwNvAn+KZt27auT3R1L2SBl16+rY9fguhl0xzO0qGRMXGnY1UP2R0/NtwKkDXK5N76KqF7tdB+Mu8cohUxtW/KuCvX/ey+abNnPU345ysWYm2ex8wLk6PvSSobbwW0BECXmMGvTrgUdV9UkAEbkSOB0n4b63nfKfBt5R1ecCz7eKyPPA9KB6lgfvICI34cx5+1b0qx+5lOwURt02io3XbmTrHVsZ9I1BpGRG+t3ImN5n/+vOsgZZx2WRPiS9k9LGmO4Yc/cY9r60l/2v7qfyrUpyTrKBnSb26rbUsf81p60fdrVdkGzhal8JEUkDjgX+2bItMLjon8CnQuz2X+BYEZkWOMYY4DTgtQ5ifBN4ItQ86YGBqv1bHkDMRxcMu2IYfcb2oXFXIyX3lcQ6nDEJoeL/KgAYcJpdHTcm1vqN78ewy52EaNONm2yiAdMj6rfUk16QTu4Xc+k3rp/b1YkbbndezsPpWrK7zfbdOP3JDxO4Mn4b8B8RacK58r1IVe8OEeNMIAenW0soNwMHgh4xz5A9aR7G3D0GgO0/207j7rAWoTOmV5v45ESmLprKkEva/fU3xkTZqNtG0WdMH4ZcNMSZyNiYGMv9XC7Tt0xn4tMT3a5KXHE7IY9YYLnmW4CrgWNwVg09XURuDbHLpcDrqlrawWHvAbKDHsOjVuEO5H89n6zjs/DX+tn/9/09EdKYuOZJ9ZBzUg59C20lW2N6QvqQdKavn07B1QU29aHpMZ4Uj3VLbMPtjst7AR/QdojtYGBXiH3uBH6rqo8Fnn8sIhnAIyLyk+D5dANLQ38eJ2kPSVUbgNa5oHpq+h0RYfz/jgeBrKNtDk5jjDE9zxJx0xNUlf1v7Cf35Fw8qQl3PTjmXH1HVLUR+Ag4uWWbiHgCz98NsVs/Dr+x5mvZvc32S4A9wKvdrmyMZB2TZcm4McCKU1ew/pr1NJQ1dF7YGBNVqsqeP+xhyQlLaK5udrs6pheqer+Kj0/9mA+KP8DfbP2j2oqHryj3AZeJyBwRKQYeAjKAlllXnhGRe4LKvwJcJSLnichoEfkCzlXzV1S1JTFvSewvAZ5W1YRoXWo31lKzqsbtahjT4+o217H/7/sp/d9SPH3joVkyJrlos7Llh1uoeqfKJhowMVG6wOk5nDMzx9ZfaYfr74iq/gG4AfgxsAyYCpyiqi0DPUcCQ4N2uQv4ZeDf1cDjwBvAFW0O/fnAvk/ErPJRtPv53XxY/CHrrlhnI91N0mlZDCj7hGxSc1Jdro0xyceT6mH03aMB2PHzHTTusYkGTPQ07mlkzx+dpWWGzbWpDtvjekIOoKoPqOooVU1X1emq+n7Qa7OC5z9X1WZV/ZGqFqlqX1UdqapzVbWyzTH/oaqiqut78FS6LHtmNpIqVL1Txd6X97pdHWN6hPqUikUVlD1WBthiQMa4Kf9sZ6IBX7WPrT/e6nZ1TC9S9lgZ2qhkTcui/3H93a5OXIqLhNxAn+F9GP5dZ3KXzTdttv5Vptcrf7Gc9wrfY/lnl1OzwumqVfKrEspfLO9kT2NMLIgIY37mTMdb9nAZtRtqXa6R6Q38zX5KH3K6qxRcU+BybeKXJeRxZOT3R5IyMIW6dXXsejzUJDPGJL7yF8tZdfYqGkoOHcDZtKeJVWevsqTcGJfkzsplwGkDnD7lP9jidnVML7DvlX00lDSQmpdK/tfz3a5O3LKEPI6kZKdQeFshAFvv2Goj3U2vpD5l43Ubob2hEoFtG+dtRH02lsIYN4y5ZwwIlP+pnJo1NtGA6Z7KRU6P4qHfHoq3j9fl2sQvS8jjzLArh9FnTB8adzXaSHfTK1W+XXnYlfFDKDTsaKDy7crQZYwxMZN5VCaj7xzNUW8eRUZxhtvVMQlu3PxxHPvRsRRca91VOmIJeZzxpHkYc/cYvFlevFn2TdL0Po1l4c3eEG45Y0z0jfrBKAZ83gZZm+jIOiaL9KG2MmdH3F6p07Qj/5x8ck7OIS0vze2qGBN1aUPD+1yHW84YE1uN5Y2kDki1FT1NRHy1Pvz1flIH2FS24bAr5HFIRCwZN71W5pRMJLWDP+wC6SPSyZmZ03OVMsa0a8f9O3h/zPvsfnZ354WNCbLrqV28W/CuTaEZJkvI49z+N/ez+ZbNblfDmKjw1ftYdfYqtCkwYLNtXh54XnR/kV2NMyYOaKPiq/ax5dYt+Op9ne9gDKCq7HxgJ/56Pyk51hkjHJaQx7G6LXWsOGUF2+/ZzoF3D7hdHWO6RX3Kmm+uofJflXgzvYz52RjSCw7tU5g+PJ1JL0wif7ZNjWVMPCj4TgHpw9Np2NHAzt/sdLs6JkFULqqkdk0tngwPQ+YMcbs6CcES8jjWd3RfhlzifJA3fW8TqjYNnElMqsr6uevZ++e9SJow+eXJjPzeSGZsncGUhVMofq6YKQunMGPLDEvGE4SInCgir4hIqYioiJzZSfkTROQdEdknInUislZEvttOuQIReTao3MciclzszsR0xNvXS+GdhQBsv3s7TRVN7lbIJISdC5wvb0MuHEJKtl0hD4cl5HFu9I9G4+nroeqdKvb9dZ/b1TGmS7bevpWyh8tAoPh3xeR+LhcA8Qq5s3IZfP5gcmflWjeVxJIBLAfmhlm+BngAOBEoBu4C7hKRy1sKiEgu8A7QBJwKHAH8D1ARvWqbSA25cAgZR2bQXNnM9nu2u10dE+fqS+rZ+9JeAIbNHeZybRKHJeRxLr0gneHfHQ7Aphs34W/2u1wjYyLjb/Jz4D9Ol6txD45j0NmDXK6RiQZVfV1Vf6iqfwmz/FJVfV5VV6nqVlV9FngDmBlU7EZgh6peoqofqOoWVf2Hqm6KxTmY8IhXGHPvGABKfl1C/fZ6l2tk4lnZw2Xgg+yTssmcnOl2dRKGJeQJYOT3R5Kal0rdujp2Pb7L7eoYExFPqocjXzuSI/50BAVX2sIQxiEiRwOfBt4K2vxVYLGI/ElE9ojIUhG5zJ0ammADTh1Azqwc8NH6BduYttSv7HrayVMK5lp7HwlLyBNASnYKo24dBcCW27fgq7GR7ib+1W2pax334O3jtSvjBgARKRGRBmAxsEBVHwt6eQxwFbAB+BLwEPBrEZnTwfHSRaR/ywPIimH1k5aIMO6hcUxbO43BFwx2uzomTolHOOaDYxjz0zHknZnndnUSiiXkCWLYlcPI/WIu434zDk8/+28z8a3q/So+nPyhMxjZb4ORzSFmAscBVwLzROT8oNc8wBJVvSXQxeUR4NFA2VBuBg4EPUpiU22TMTGDvmP7ul0NE+fSh6Qz8vsj8aRarhIJG/qaIDxpHqa8McXtahjTqZo1Naw4fQX+Wj81H9egzYqk2WBN41DVLYEfPxaRwcAdwPOBbWXA6ja7rAG+1sEh7wHuC3qehSXlMXdw2UHwO0uiGwPgb/bjSbEkvKvsnUtQ/iYb3GniT/2OelZ8aQXN+5rJmpbFpD9PwpNmzYwJyQMET0b/DjChTZnxwLZQB1DVBlWtankAB6NfTRNs19O7+OiYj1h/5Xqbjte02nDVBpadvIyq96vcrkpCsr+UCUZV2blgJ+8Vvkft+lq3q2NMq6Z9Taz40goadjTQb2I/jnz1SFIy7SZcbyUimSIyVUSmBjaNDjwfGXj9HhF5Jqj8XBH5ioiMCzwuBW4Ang067K+AGSJyi4gUicgFwOXAgh46LROGAacMwNPPw8EPD1L+Qrnb1TFxoKmiid2/203lvyrtgmEXWUKeYESEfa/vo7G0kU03b6JiUQW7n99NxaIK1GdXKow7fDU+Vpy+gto1taQPT+eoN44iLS/N7WqZ2DoOWBp4gNNtZCnw48DzocDIoPIenO4ly3AGdM7FmebwtpYCqvohcBZwPrASuBWYp6q/i9lZmIilDU5jxA0jANhyyxZLwAy7ntyFv85PxlEZZH8m2+3qJCSx202HC4zUP3DgwAH69+/vdnUOU72ymsVHLYY2/3Xpw9Mpml9kKx2aHrf3lb2sPGMlKbkpHP320WQckeF2lXqVqqoqsrOzAbID3TJMCPHefvcWzQebeb/ofZr2NDFuwTgKrrYp7pKV+pX3x79P/aZ6xj88nmGX22JALSJpu+0KeQKqW193WDIO0LCzgVVnr6L8RbuFaHpW3lfyOOL3R3Dkq0daMm5MEkjJSqHw9kIAtv5oK80Hm92tkHHN/jf2U7+pHm+2l8HfsCkxu8oS8gSjPmXjdRtDvOj8s3HeRuu+YmJOVQ+ZE3/QOYPInmG3Ko1JFkMvG0rfor407Wlixy93uF0d08PUp1QsqmDzLZsBGDJnCN4Mr8u1SlyWkCeYyrcraShpCF1AoWFHA5VvV/ZcpUxS2n73dj6a9hH1JbaMtjHJyJPqYfTdo0nNTyV9eHrnO5heo/zFct4rfI/ln11OzbIaZ9sfy+0OfTdYQp5gGssao1rOmK4ofaSULT/cQu3qWvb/fb/b1THGuCT/7Hymb5rOsG9bv+FkUf5iOavOXnXYxcHG3Y3WbbYbLCFPMGlDw5u5ItxyxkSq/M/lrL9qPQCjfjjK/hAbk8REhJQsm940WbR2m22vV6x1m+0WS8gTTM7MHOfWYKiFDwUkTUgbYgm5ib6KhRWsvmA1+GHo5UMp/HGh21UyxsQBVWXPn/aw8YYQY5ySREu/6t46HbF1m40d+1qbYMQrFM0vYtXZq5ykPPh3PfBcG5Ul05Yw4bEJDDpnkEs1Nb3NwSUHWXnGSrRRyZudx/gHxyMS6puhMSaZ1K2vY/W5q0Fh0NcH0X968k05Wf5iORuv23hIwtrbpiO2brOxY1fIE1D+7HwmvTCJ9IJDB9GkD09n/KPjyT4xG99BH6vPXc36uevx1ftCHMmY8Kgq669aj++gj5zP5lD8u2LEa8m4McbRb0I/hswZAsCm728i2dY4CdWvujdNR9xU2UTZE2VhlbVus5GzhYHakSgLS6hPqXy7ksayRtKGppEzMwfxCv5mP1tv28r2e7YDkHlMJpP+OIm+Y/u6XGOTyBpKG9h842bGLRhHSn+7udaTbGGg8CVK+90b1e+o54PxH+Cv93Pk345k4OkD3a5Sj1Cf8l7he6G7cohzwWzGlhkJeyGjYmEFa+espWFHB91VoFecazTZwkBJQrxC7qxcBp8/mNxZua0ffk+KhzF3j+HI144kZWAK1UuqWXzMYqpXVrtcY5Nogr+wpw9Lp/i3xZaMG2Pa1WdEHwqudVbs3HzT5l7XfzqU3t6vesutW1h+8nIadjTQZ2wfRt8z2uki2zbfDjwvur/IkvEusIS8Fxt46kCOW3Yc/T/Tn6zjs8gothUUTfh8tT6Wn7yc3c/vdrsqxpgEMfKmkaTkplCzsoZdz+xyuzo9orf3q04dlArqLAR13LLjGHXTqJDdZie9MKnX9Jer9tavAAAgAElEQVTvaXapq5frM7wPUxdOxVfta/3G6qv30bS7iT6j+rhcOxNPgrtApeanUnJ/CZULK6leVs2AUwaQmpvqdhWNMXEuNTeVkbeMZPP3NrP1tq0MvmAwnvTee+1PVdn1dHhfPNIGJUa/avUrDaUN9Bnu5AgFcwvIPDqTnBNyWsvkz84n74y8drvNmq6xhDwJeFI9eHI/aRA3fXcTe36/h4lPTSTvjDwXa2biRXuzAwBIqjD55cmWjBtjwlZwTQEH/n2AgmsKenUyDs487BmTM6h4o6KDQpBekM76a9cz6GuDGPH9EaRkxmf6Vb+tnrUXr6V+Rz3HLT2OlKwUxCOHJOMtWrrNmujo3b8p5jC+Oh/Vy6pprmxm5Zkr2Xj9RvyNfrerZVwUanYAAG1SmsqbXKiVMSZReft4OfKvRzLgiwPcrkpM+Gp9NOz8pL0cfddoRt/bcb/qvLPyqFtdx7Y7t/HBuA8oe7wsrvrYqyq7nt3Fh0d9SOWiShp3NVK9xMad9SRLyJOMt6+XqW9NZfj1wwEo+VUJS09cSv22epdrZtzQ4aprAGKrrhljusdX6+s1i+VUfVDF4mMWs/JrK/E3OxezvH28jLqx437VRfOLmPTCJPqM7UPjrkbWfXsdi49ZzP4397txGodo2t/E6nNXs/bCtfiqfPSf0Z/jlh1HzkmHXxU3sWPTHrYjWabN2vvyXtZevJbmymZSclOY+MxE8r5sXViSgfqUuo11VPxfBRvmbui0/JSFU+zWpIts2sPwJUv7nQjUp6y5cA17/rAHgm7EJuJiOf4mP9vu2sa2n2wDH6QNS2Pqoqn0G9fvkHKhpiNuPU6Dn50P7mTbj7fRXNkMwIBTBzDpz5Pw9vX26DkB7H9zP2svXktjaSN4ofD2QkbePBJPil2vjYZI2u747MRkekTeGXkcu/RYVp+7moMfHGTtnLXM2DyDlGz7WLipswY9Us3VzdR8XEP1smqql1dTvayamo9r8Nf6SR+R3vkBSNzZAYwx7il/qZw9z+85bHvLYjmJMiNHzeoa1ly4prULx6DzBzHugXGkDjh8bE1n/ao96R5GfHcEQy4awtY7t1K6oBTxiivJOMDO3+yksbSRvuP7UvxsMf2Pty+xbnE98xKRucD3gCHAcuA7qvpBB+XnAVcBI4G9wAvAzapaH1SmAPgpcCrQD9gIXKKqi2N1Homqb2Ffjn77aDZ9fxO5J+daMu6y7iy9rKo07GygYVsD2Z/Jbt2+ZPoSalfXHlbe09eDN9sLOzqvl626ZoyJhPqUTfM2hXiR1u5weWfkxe3MHOpXSuaXsPnmzWiDkpKbwviHxjPo3EHdPnbqwFTG3T+OgrkFh5x/w64Gdj+zm4JrC/D2iU2SrqqIODEnPDqBHb/YQeEdhXgz3PlSYByuZl8ici5wH3Al8D4wD3hDRCao6mFfq0XkAuBe4FvAf4HxwFM4v97XB8rkAu8AC3ES8nJgHNDBEOjk5knzMO7+cYds2/f3feCHgaclx0pr8aBlcGXb/tztXU3yN/qpXVPbesW75d/m/c14MjzMrJqJeJwGN/OoTJormsmckknm1EwypmSQOTWz9Vbre4XvOQOU2uu9Flh1LWem9SU0xoQvksVy4rU7nPqU3b/bjTYoA04dwITHJpA+LLy7iuFq2+Vl621bKXu0jJ0P7mTMPWMYdN6g1uS5u9SnbP/5duo21jHxsYkApA1OY+zPx0bl+KZ73L4cej3wqKo+CSAiVwKn4yTc97ZT/tPAO6r6XOD5VhF5HpgeVOZGYIeqXhK0bUvUa96L1e+oZ8031tC8v5kRN45g9F2jrT9ZjHU4uDKwLfhq0qpzVrHv5X2Hl/VC39F9adrb1Drn7cSnJnY49VjR/CLni4BwaHxbdc0Y00WJuliOqoLf6XriSfVQ/NtiDvz7AEMvHxq1xLgjObNy2P/6fhq2NbDmgjWU3F9C0X1Fh9z17Iq6LXWsvWgtB/5zAIAhFw9pdypD4x7XsiwRSQOOBf7Zsk1V/YHnnwqx23+BY0VkWuAYY4DTgNeCynwVWCwifxKRPSKyVEQui8U59FZpg9IYfMFgAHb8dAfLP7v8kCme1Ke9ZsR8vOj0ahKHLr2ceWQm3mwv2SdmU3BtARMen8CxHx3LzOqZHP/x8YcsQNHZPMD5s/Nt1TVjTFSF280tbWgaB947wLafbKOhtOM2MNYa9zSy8qyVbLn9k2t4GcUZDLtiWI8k4wCDLxjMtHXTGH3XaLyZXg5+cJClJyxl1ddXUbepLuLjqSplT5WxeMpiDvznAN4sLxOenNDtBN9En2uzrIjIMGAn8GlVfTdo+8+Ak1R1eoj9rgV+gXP9LgX4X1W9Kuj1lr7k9wF/Ao4H5gNXqurTIY6ZDgRnI1lASbKP0t/zwh7WXboOX5WP1LxUip8txlfj63IfZxPa7ud3s+aCNZ2WK36umMHnD8ZX78OT7onqH4loDyY10WOzrITPZlmJD+rTsLrDzdgygzUXrWHPc3vACwNPH8jQbw9lwKkDevTObPlL5ay/fD1N5U14+nqYsWUGaYPdHTvTsKvB6cLyeBn4Yfj1wyn6ZdFh5UK13Y17G1l/xXr2vrgXgOwTspn4zET6ju7b06eStHrtLCsiMgu4Bbgap895ETBfRG5V1TsDxTzAYlW9JfB8qYhMxumn3m5CDtwM3B6ziieoQWcPInNqJqu/vprqZdWsOGVFu+USbcR8PIrkahIQk8E+tuqaMSZaxCthd4cb+JWBNGxv4MB/DrDvr/vY99d9pBWkMfSSoQy5dAh9C2OXQDYfaGbDdRvY/fRuADKOzKD4t8WuJ+MA6UPSmfDIBAquLWDbj7Yx6gejWl9rKGsgdWAq+/62r92LZGPvH8v2u7dTvaQaSRUKf1zIyO+NtIsscczNK+RpQC1wtqq+FLT9aSBHVc9oZ5+3gfdU9XtB274JPAJkqqpfRLYBb6rqt4PKXAX8UFULQtTFrpB3wFfvY+O8jZQ9XBa6UNDVDvuFj5z6lHdHvUvjzhD9Ke39TWp2hTx8doU8vrQ7c9SIdIruP/yuas2aGsoeK2PX07to3ufM0d13Ql+mrZkWky4jFQsrWHvxWhq2N4AHRnxvBKN/NLrTbn5uU1WWf245tetq2++DH3irRt4ykr1/2Uvxs8VkHZ3Vs5U0QIJcIVfVRhH5CDgZeAlARDyB5w+E2K0fhywvAIAv8G/Lb+s7wIQ2ZcYD2zqoSwPQ2lr0VF+xROHt42XQeYM6TsgTYMR8PBOvMO7X49qdZcUGVxpjElX+7HzyzsgLqztcRnEGRb8sYszdY9j78l7KHi0j90u5rX+TffU+tt25jSFzhtBvfL/D9o9EU0UTK7+6El+1jz5j+lD8THHC9KtuKGmgelU1zeXN7RcITCu5+5ndTNswDW+6TWeYCNz+GngfcJmIzBGRYuAhIANomXXlGRG5J6j8K8BVInKeiIwWkS8AdwKvqGpLYv4rYIaI3CIiRYGpEi8HFvTUSfVGiTpiPhHUrK0BggZXDrfBlSb+iciJIvKKiJSKiIrImZ2UP0FE3hGRfSJSJyJrReS7HZS/KXDc+6Nfe9OTWrrDDT5/MLmzcju9sOBJ9zDonEFMeXMKI/5nROv2vS/uZfvd2/lgwgcsPWkpu57dha/O1+4xOpt8IDU3lbG/HMvQK4Zy3PLjEiYZB+gzog8Tn5zYcaHARbKqd+2GWqJwtQ+5qv5BRPKBH+MsDLQMOEVVdweKjOTQK+J34Xz3uwsowJlj/BXgB0HH/FBEzgLuAW7DmfJwnqr+Lsan06tF2sfZhKdkfgkbr9/IxCcnMuSiIRFdTTLGZRk4i7k9AbwYRvkanLufKwI/nwA8LCI1qvpIcEEROR64IlDWJLHgO9bpI9MZ+OWB7HttHwf+fYAD/z7Axu9sZPA3BzP0sqFkHpUJtN9NJq0gjezPZDPsymHkfta5izvs8mE9ezJR5Ktq/4tIW3aRLHG41oc8nlkfxMNFMmLeksfwlMwvYeO8jQCM+uEoRt852uUamXgV733IRUSBs4LHA4W534tAjapeGLQtE1iCM3j/h8AyVZ0XwTGt/e7l6kvq2fXkLsoeL6Nh2ydJ9/Qt06leUt1+17+AlLwUPrXtU3j7JXY3jopFFSz/7PJOy01ZOMW6kbookrbb7S4rJkG0jJh3nrRTQGHUraMsGQ9Tya8/ScZH3jKSwh8XulshY3qYiByNs9jbW21eWgC8qqr/PHwvY6DP8D4U3lrIjE0zOOrvR5H3tTxyP59LnxF9Qi+w1kI7X5shEeTMzHG6N4b6kyvO4FlbZTlxJP6n0vSYUAvItHR8KrmvhMY9dnusMyW/LnH+aOAk46PvGm0DiU3SEJESEWkAFgMLVPWxoNfOA47BmYo23OOli0j/lgfOLFkmCYhXGPClAUx+YTJH/f2osBZYa97X3LrAWiLr8CKZTQSQkCwhNxHJn53PjK0zmLJwCsXPFTNl4RSmr5tO+oh0atfW8vFXP0b91g0qlJLfBCXjN1sybpLSTOA4nLUh5onI+QAiMgJnEbdvqGp9B/u3dTNwIOhREt3qmkQgXkm6yQdsleXeJaEWBjLxob0FZKb8cworTlnB6DtHIx5LMEOp3+bkGSNvHsnon1gybpKPqrasS/6xiAwG7gCeB44FBgFLgn4vvMCJInINkB40m1awe3Bm7GqRhSXlSSkZJx+wiQB6D0vITVT0G9+PaWun4Umzmy4dGfvzseSenMuAUwZYMm6Mc5e25fLe/wFHtnn9SWAt8NMQybitI2FatfSr7mzygd7Wr9pWWe4dLHsyUROcjNesrWHd5evwN7Vdxyn57P3rXnz1Ti4hIgw8daAlDSbhiUimiEwVkamBTaMDz0cGXr9HRJ4JKj9XRL4iIuMCj0uBG4BnAVT1oKquDH7gTI+4L/CzMR2yftUmkVlCbqLO3+BnxSkrKHu0jDUXrjlsQYZksnPBTlaesZJVs1fZlxPT2xwHLA08wOk2shRnXQmAoThrSbTw4HQvWYYzoHMucCPOehHGRIX1qzaJyuYhb4fNY9t9+17fx8ozVqJNypBLhzDh0QlJd1V454KdbLhmAwAjbhzBmHvGJN17YKIj3uchjyfWfhtw1s6wftXGbZG03daH3MTEwFMHUvxcMavPXc2ux3eRkpXC2PvGJk1CuvPBoGT8+5aMG2NMT7J+1SbRWJcVEzODzh7ExCcmAlByfwlbb9/qboV6yM6HdrJhblAyfq8l48YYY4wJzRJyE1ND5gxh3APjANh25zbKnixzuUaxVfpoKRuuDiTj37Nk3BhjjDGdsy4rJuYK5hbQfLCZfS/vI+/MPLerE1MZkzPwZnkZduUwxvzUknFjjDHGdM4GdbbDBgXFhr/RnxTzlNdtraPPqD6WjJuosUGd4bP22xgTLyJpu3t/dmTiRnAyXjK/hPIXy12sTfSUPVHGwSUHW5/3LexrybgxxhhjwmYJuelxe1/Zy8Z5G1l93mr2v7Hf7ep0S+nDpay7dB3LT15O/fZ6t6tjjDHGmARkCbnpcQNPG0j+1/PRJmXlWSup/Hel21XqktJHSll/5XoAhnxrCOkj0jvZwxhjjDHmcJaQmx4nXqH42WIGnDYAf52fj7/8MVWLE6tbbOkjpay/wknGh393OGN/kTxzrBtjjDEmuiwhN67wpHmY9MIkcmbl4DvoY8WXVlC9strtaoWl9NE2yfgvLRk3xhhjTNdZQm5c4+3rZfJfJ5M1PYvm/c2s+MIKmvY3uV2tQ6hPqVhUwe7nd1OxqILyl8pZf3kgGZ9nybgxxhhjus/mITeuSslK4ajXj2LZZ5cx+JuDSR2Q6naVWpW/WM7G6zbSUNLQui2tII1+k/sx4PMDGHufJePGGGOM6T5LyI3rUnNTOfb9Y/Gkx88Nm/IXy1l19ipoM01/Y2kjjaWNFN5RaMm4McYYY6IifjIgk9SCk/HmA82su2yda91X1KdsvG7jYcm486Lzz6bvbkJ9tqiWMcYYY7rPEnITd9Z8cw1lj5Wx4tQVNB9s7rG4qkpDaQPbf7H9kG4qhxeEhh0NVL6dmNM1GmOMMSa+WEJu4s6Ye8eQMjCFgx8c5OOvfIyv1nfY4MruXp32N/qpXl5NY3lj67ayx8t4t+Bdtty0JaxjNJY1dl7IGGOMMaYT1ofcxJ2MSRlMeWMKyz63jANvHWDpCUtp3NNI485PEuD04ekUzS8if3Z+p8dr3NNI9fJqqpdXU7O8hurl1dSuqUWblfGPjmfYt4c5cY/IAA+kF6TTsKODK+QBaUPTun6SxhhjjDEBlpCbuJR1bBZHvnoky09eTvXSw+cnb9jZwKqzVzHphUmtSbm/yU/t2lq8WV76FvYFoGJhBcs/t7zdGCk5Kfhr/J/EnJbFzIMz8aR7eK/wPRp2NrTfj1ycLwQ5M3O6f6LGGGOMSXqWkJu4lf2pbFL6p9C0t53BnYFEee2layl/qZzaj2upWV2DNiojbhzB2HvHAs7VdgT6jutL5pRMMqdkknFUBplTMkkfkX7ITCmeFE/rb0TR/CJnlhXh0KQ8ULzo/iLEa7OsGGOMMab7LCE3cavy7cr2k/Egvkofe367p/W5N8uLNn+SQacNSmNm9Uy8/bwRxc6fnc+kFyYdNg95+vB0iu4Pr6uMMcYYY0w4LCE3cSvcQZN5X8tj8DcHkzklkz6j+iCeQ69cR5qMt8ifnU/eGXlUvl1JY1kjaUPTyJmZY1fGjTHGGBNVlpCbuBXuoMmCawrInZUbkzqIV2J2bGOMMcYYsGkPTRzLmZlD+vD01n7bhxFIH2GDK40xxhiT2CwhN3FLvELR/KLAk7YvOv/Y4EpjjDHGJDpLyE1caxlcmV6Qfsj29OHph0x5aIwxxhiTqKwPuYl7NrjSmPgjIicC3wOOBYYCZ6nqSx2UPwH4KTAR6AdsAx5W1V8FlbkZmB0oUwf8F7hRVdfF6jyMMSYeWEJuEoINrjQm7mQAy4EngBfDKF8DPACsCPx8AvCwiNSo6iOBMicBC4APcf4+3Q38Q0SOUNWaKNffGGPihiXkxhhjIqaqrwOvA4cssNVB+aXA0qBNW0VkNjATeCRQ5pTgfUTkYmAPzlX4f0ej3sYYE4+sD7kxxpgeJyJHA58G3uqgWHbg3/0dHCddRPq3PICsKFbTGGN6RFxcIReRuTh9EYfg3AL9jqp+0EH5ecBVwEhgL/ACcLOq1gdevwO4vc1u61R1YvRrDz6/j7e3v03ZwTKGZg1l5siZeD1dW4zG4sZPTLfi2rnaufZmIlIC5OP8/blDVR8LUc4D3A+8o6orOzjkzRze3octmT4LyRTXztXONdHiup6Qi8i5wH3AlcD7wDzgDRGZoKp72il/AXAv8C2cAT/jgacABa4PKroK+HzQ8+ZY1P/FNS9y3d+vo6SqpHXb8P7DmX/KfGYXz45FyKSLa+dq55rocd061zg1E8gEZgD3ishGVX2+nXILgMk4fc07cg/O35AWWUBJiLKHSKbPQjLFtXO1c03EuKKqUTtYlyog8j7woapeE3juAXYAv1HVe9sp/wBQrKonB237JTBdVU8IPL8DOFNVp3axTv2BAwcOHKB///4hy7245kXO/uPZKIe+hxKYJPuFc16IyYckmeLaudq5JnrcaMSsqqoiOzsbIFtVq6JawSgQEaWTWVZC7PdD4EJVndBm+wPAGcCJqrolwmPGbfudTJ97t+Laudq5xlPcSNpuVxNyEUkDaoGzgxtyEXkayFHVM9rZ5wLgQeCLqvqBiIwBXgV+q6p3B8rcgdMF5gBQD7yL06Vle5j16rRB9/l9FM4vPOQb0yHHQCjIKmDV1avwerx4PV76pPRpfb2mMfSEAR7x0De1b7tlfX4fRzx4BDsP7gwrbpvzol9qv9bntU21hPr/b1u2uqGaiQsmdhh3eP/hbLluC42+RvzqD3l+GWkZrT/XN9fj8/vaLefz+5j00KSw3+N+qf1aB5c1NDfQ7A99U6Sjst15j/um9sUjztCMRl8jTb6mkHXok9Kndf+6pjrG/WZcWO+vX/00+hpDHjc9JZ0Uj3Pzq8nX1GHZFE8KRb8pCus97pval1RvKgDN/mYamhtCHjfNm9Za1uf3Ud9c3/pad97fVG8qad40APzqp66pLmQd2patbqjuNO7w/sPZfO1mGnyhzy3Fk0J6ijMvvqpS21QbsizAxAUTO3x/W/5fO7r92YsT8tuAb6lqYeC5AL8BzgJmqeqGLtQlLtvvsD73/QtYddXhn/vAeXWp/fb5fYy6f1RYbYvX46WuqS5h2+/utC2QWO23z+9j0oOTKDkY3vsb3CYnWvvd5Gti9PzRYb2/HvF02Cb3dPudSAn5MGAn8GlVfTdo+8+Ak1R1eoj9rgV+gbNeYwrwv6p6VdDrp+LcEl2HMz/u7UABMFlVD7ZzvHQgeOWZLKCkowZ90dZFfPbpz4Z9rqeNO41XL3i19XnG3RkhPwgnjTqJRRcvan2e//N89tbuDTtWKEfkH8Gqq1e1Pp/04CRWl69ut+yo7FFsnbe19fmEByawft/6TmMsnLOQOxbdwVvb2h+n1S+1HzW3fPLH7PTnTue1Da+FeQYdq765uvWPxcUvXczTy58OWXbPDXvIz3AWFZr76lweXPxgVOqw8qqVTBo0CYA7Ft3Bj976UciyH3z7A44vOB6AK1+5koeXPNzp8RfOWciqPau45vVrQpb52/l/4/TxpwPw1LKnuOTlS0KWvf3E2/nRv0PXMdiTZzzJxVMvBuDV9a/y5ee/HLLsA6c+wNxpc4HIf1c6cvtJt3PHrDsAWLVnFZMfmhyy7A2fuoGff/HnAGyt3Mro+aPDivGXc/7CWX88K+Trc6bM4akznwKcZCvznsyQZU8cdSL/3tb55CAL5yxkVuGskK/HY0IuIplAYCldluJ0GVwI7FfV7SJyD1CgqhcFys8FtgNrA/ucCPwK+LWq/jBQ5kHgApyr48Fzjx9Q1dB/vQ+tV6cJeTK13+Gea8tncNZTs6z9JjHa70gkcvv9+5W/5/w/n9/p8RfOWcik/EkM+sWgkGV6uv2OpO1OuFlWRGQWcAtwNXAMziISp4vIrS1lVPV1Vf2Tqq5Q1TeA04Ac4JwQh70Z52p6y6PT/odlB8u6cxoJp6MrBcGS7X2JlqrG8HKsaL+/FfUVUT1eb7C7ZnfUjhV8ZakjCfp7cxxOIt4yleF9gZ9/HHg+FGfgfQsPTn/vZcBiYC5wI3BbUJmrcGZWWQSUBT3OjWbFE/T97pJwzzWZ3pNoc6v9ThZ7ag4bTtiuaL+/Pd1+u32FvCtdVt4G3lPV7wVt+ybOPLaZqu3faxORD4F/qurN7bwWsyvkr13wGieOOjFqXVb+ve3fnPbcaWHHDdadLitvbHyDU353Srtlgy2cs5DpBdOjcssz0nONVpeV7rzHXb3l+eamN/nis1/sNObCOQv5zIjPRK3Lynsl7/H5334+5OstXrvgNT4/5vNRueXZnfe3O11W3tj4Rlhx/3XRv5hWMC3k65Hc8nxnxzt86dkvdRozEa+Qx6toXiGPZvvdnc89dL39jvQKebS6rLjRfnf3PU6k9jvSc41WlxU32u9/bfkXJz9zcsiyLRbOWchJo06KWpeVaLTfCdNlBVoHdX6gqt8JPPfg3NZ8IMSgzo9wEusbg7adDzwOZKnqYa1D4Nbqdpwptn4dRp3C7oO4s2rnYZ39Ify+oZFKprh2rrGN6VZcO9fIY1pCHr54bb+T6XPvVlw719jGdCtuIp9ronVZuQ+4TETmiEgx8BDOksxPAojIM4G+iC1eAa4SkfNEZLSIfAG4E3ilJRkXkV+IyEkiUiginwb+AviA9qbW6hKvx8v8U+YDn4y2bdHy/P5T7o/6PJXJFNfONbYx3Ypr5xrbmKZzyfRZSKa4dq6xjelW3GQ5V9cTclX9A3ADTr/DZcBU4BRVbenIORKnL2KLu4BfBv5djXNl/A3giqAyw3GS73XAH4F9wAxVLY9m3WcXz+aFc16goH/BIduH9x8esyl4ki2unauda6LHdetcTceS6bOQTHHtXO1cEzWu611W4lG489i26M0rR8VLXDtXO9dEj9udmNZlJXyJ0H4n0+ferbh2rnau8RA3ofqQx6NIG3RjjIklS8jDZ+23MSZeJFofcmOMMcYYY5KWJeTGGGOMMca4yBJyY4wxxhhjXGQJuTHGGGOMMS5KcbsC8ayqysZOGWPcZ21R5Ow9M8a4LZJ2yGZZaYeIFAAlbtfDGGPaGK6qO92uRDyz9tsYE4c6bbstIW+HiAgwDDgYwW5ZOH8Ehke4X3clU1w7194Z1841/H1L1RrtDiVQ+51Mn3u34tq59s64iXauYbXd1mWlHYE3LaKrUM7fAAAO9uQ8wckU1861d8a1cw2b9cEIQ6K038n0uXcrrp1r74ybgOcaVlkb1GmMMcYYY4yLLCE3xhhjjDHGRZaQR08D8KPAvxa398R0K66da++M69a5mo4l02chmeLaufbOuL3yXG1QpzHGGGOMMS6yK+TGGGOMMca4yBJyY4wxxhhjXGQJuTHGGGOMMS6yhDwCIjJXRLaKSL2IvC8i0zooO0lE/hworyIyr4fiXiYib4tIReDxz47KRynmbBFZLCKVIlIjIstE5MJIY0Yat81+5wXe55diGVNELg7ECX7URxoz0riB8jkiskBEykSkQUTWi8hpsYwrIovaOV8VkVdjFTNQfp6IrBOROhHZISK/EpE+kcSMNK6IpIrIbSKyKVB+uYicEmG8E0XkFREpDbxPZ4axzywRWRL4P90oIhdHEtOEx4322422uwtxo9J+u9F2Rxo3Wu13MrXdkcYNlO92+52Ubbeq2iOMB3AuzsjaS4AjgEeACmBQiPLHAz8HzgPKgHk9FPd3wNXAVGAi8CRQCRTEMOYs4CygGBgLXAc0A1+K5bkG7VeIs3rWv4GXYvz+XlMCZAEAAAlXSURBVAwcAIYEPQb3wP9rGvAh8CrwmcA5nwRMiXHcAW3OdVLg//biGMa8AKgP/FsIfBEoBe6L8bn+FGdBmdOAMcBVQB1wdAQxTwXuCvw+KHBmJ+VHAzXALwO/P9d05XfHHlH/LHS7/e5CzG633V2MO4tutt+Rxgzar5Autt1dPNeL6Wb73YWYCdt2dzFut9vvLsTsFW13l3ZKxgfwPvBA0HNP4ANwUxj7bqXrCXmX4wbKe3FWibqop2IG9lkC3Bnrcw2c3zvApcBTRJ6QRxQTp0Gv7OnPE3AlsAlI7cm47ew/L/B5yojhuT4A/F+bbb8E/hPj97gUmNtm25+BZ7v4XofTqP8UWNlm2++Bv3f3M2aPrn8W2uzbpfbbjbY7GnED+0TUfrvRdnclbjTa72Rqu7t4vt1uv5O17bYuK2EQkTTgWOCfLdtU1R94/qk4j9sPSAX290RMcZwMTMC56hGWbsS9Ddijqo+HGysKMTNFZFvgVtzLIjKpB+J+FXgXWCAiu0VkpYjcIiLeGMdt61Lg96paE8OY/wWObblFKSJjcK58vBZmHbsaNx3nyk6wOuCEcON2wacIqmPAG8SwXUk2brTfbrTd0Yjblfbbjba7m3G73H4nU9vdjbjdar+Tue22hDw8eTjf5ne32b4b51ZQPMf9Kc63x7YfnKjGFJFsEakGGnFuzX1HVd8MM2aX4orICTiNzGURxOlWTGAd8C3gDOCbOL9D/xWR4TGOOwY4O7DfacCdwP8AP4xx3FaBBnYy8FgsY6rqczh/rP8jIk04V5cWqerdsYyL05heLyLjRMQjIl8AZgNDI4gbqSEh6thfRPrGMG4ycaP9dqPt7nLcbrbfbrTdXYpL99vvZGq7uxQ3Cu130rbdlpD3YiJyE04fyLNUtUsDDyNwEKfv4/HAD4D7RGRWrIKJSBbwW+AyVd0bqzhtqeq7qvqMqi5T1bdwfunLgStiHNoD7AEuV9WPVPUPwE9wbof2lEuBj1X1g1gGCXxubsHpT3sMznt8uojcGsu4OH1nNwBrcRKTB3D68fpjHNeYQ/Rw2w092H671XaDa+130rTd4Fr73Sva7hS3K5Ag9gI+YHCb7YOBXfEYV0RuAG4CPq+qK2IdM3BLaWPg6TIRKQZuBhbFKO5YnAEjr4hIyzYPgIg0AxNUdVOUYx5GVZtEZClQFE75bsQtA5pU1Re0bQ0wRETSVLUxRnEBEJEMnAThtjDidDfmncBvVbXlas7HgfiPiMhPAp+1qMdV1XLgzMBsAANxrk7eC2wOI15X7QpRxypVrYth3GTiRvvtRtvd5bjdbL/daLu7EvcwXWi/k6nt7mrc7rbfSdt22xXyMAR+YT4CTm7ZJiKewPN34y2uiHwfuBU4RVUX90TMdnhw+nXFKu5a4Eicqzotj78CCwM/74hBzMME+gEeidPohqWLcd8BigLlWowHysJs0Lt7vl/H+f98NpxY3YzZj8OvbLT8MRPC0J1zVdV6Vd2Jc8Hia8DL4cTsoneD6xjwBWLYriQbN9pvN9ru7sRtR9jttxttdxfjHibS9juZ2u5uxO1W+53UbXdXR4Mm2wNnGp56YA7OFDcP40zDMzjw+jPAPUHl0/iksSnFmUJrKlAU47g34kwX9DUOnfIoM4Yxbw58EMcEyv8P0AR8O5bn2s7+T9G1aQ8jOdfbcKZxGoNzO+55nMEjR8Q47gicEfK/wWnMT8fpr/aDnniPgbdxBgT1xO/OHYFzPQ9naqkv4Fy9+0OM407Hub06BpgJ/B/OFZacCGJm8snvvQLfDfw8MvD6PcAzQeVbps76Gc5Ud1dj0x5G/dGFz0K32+8uxOx2293FuN1uvyON2c7+T9H1aQ97tP3uQsyEbbu7eL530M32uwsxe0Xb3aWdkvWBM8/kNpxG831getBri4Cngp4XBv5T2z4WxTju1hBx74hhzLtw+m/V4cwI8F/g3Fi/x+3s+xRda9QjOddfBZXdhTMAKuy5Trtzrjijt9/Daag24fTT8/ZA3AmBz9AXeuh3JwW4HacRrwO2AwuIoHHtYtyTgNWB93cvTqM/LMJ4s0L8/j0V9Bld1M4+SwN13ESE8wTbIyafhcIQ/4+LYhhza4iYd8T4XKPSfkfarrTZ9ym60HZ34Vyj0n5Heq4kcNvdhfc4Ku13hDF7RdstgYMaY4wxxhhjXGB9yI0xxhhjjHGRJeTGGGOMMca4yBJyY4wxxhhjXGQJuTHGGGOMMS6yhNwYY4wxxhgXWUJujDHGGGOMiywhN8YYY4wxxkWWkBtjjDHGGOMiS8iN6QIRKRQRFZGpEexzsYhUxrJexhhjQrO228QrS8iNMcYYY4xxkSXkxhhjjDHGuMgScmNCEJFTROQ/IlIpIvtE5G8iMjZE2VmB26Cni8gKEakXkfdEZHI7Zb8kImtEpFpE/i4iQ4NeO15E3hSRvSJyQETeEpFjYnmexhjTm1jbbRKRJeTGhJYB3AccB5wM+IG/iEhHvzc/B/4HOB4oB14RkdSg1/sBNwAXAicCI4FfBL2eBTwNnADMADYAr4lIVjROyBhjkoC13SbhpLhdAWPilar+Ofi5iHwLp6E+AqgOsduPVPXNQPk5QAlwFvDHwOupwJWquilQ5gHgtqCY/2oT83KgEjgJ+Fs3T8kYY3o9a7tNIrIr5MaEICLjROR5EdksIlXA1sBLIzvY7d2WH1R1P7AOKA56vbalQQ8oAwYFxRwsIo+KyAYROQBUAZmdxDTGGBNgbbdJRHaF3JjQXgG2AZcBpThfYFcCad04ZlOb5wpI0POngYHAdYHYDTh/KLoT0xhjkom13SbhWEJuTDtEZCAwAbhMVd8ObDshjF1nANsD5XOB8cCaCEJ/BrhaVV8LHGMEkBfB/sYYk7Ss7TaJyhJyY9pXAewDLheRMpzbjveGsd9tIrIP2A38BNgLvBRB3A3AhSKyGOiPM9CoLpKKG2NMErO22yQk60NuTDtU1Q+cBxyLc6vzV8D3wtj1JmA+8BEwBPiKqjZGEPpSIBdYAvwW+DWwJ4L9jTEmaVnbbRKVqKrbdTAm4YnIrP9v7w5IAAZiIAiehfoXU2lfGduHGRFhCYFse7c95xwvlgEuYHbzFzbkAAAQEuQAABBysgIAACEbcgAACAlyAAAICXIAAAgJcgAACAlyAAAICXIAAAgJcgAACAlyAAAICXIAAAh9+TM6oVFl6AMAAAAASUVORK5CYII=\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "minimum avg mae over alphas = 0.8527685427680036 at alpha = 0.4 and mse = 1.3462015648002874\n",
            "minimum avg mse over alphas = 1.3396905775303876 at alpha = 0.5 and mae = 0.8538870170462247\n"
          ]
        }
      ],
      "source": [
        "avg_mae_list = [0.8750364511057321,0.863824053714779,0.8592177252143982,0.8527685427680036,0.8538870170462247,0.860016459557814,0.8611507236816345,0.8678378841134752,0.8887613208692435,0.8978200408157662]\n",
        "avg_mse_list = [1.4467357106160585,1.3971177984800593,1.376702370229998,1.3462015648002874,1.3396905775303876,1.3510617804467742,1.34180853475892,1.353348614027472,1.3927576118486096,1.419021158007399]\n",
        "mae_dlf = [0.8499268625805789]*10\n",
        "mse_dlf = [1.317287691183217]*10\n",
        "a_list = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]\n",
        "print(f'avg mae over alpha = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]: {avg_mae_list}')\n",
        "print(f'avg mse over alpha = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]: {avg_mse_list}')\n",
        "    \n",
        "plt.rcParams.update({'figure.figsize':(7.5,3.5), 'figure.dpi':100})\n",
        "fig, axes = plt.subplots(1, 2)\n",
        "axes[0].plot(a_list, avg_mae_list, 'm--o', label = 'static alpha')\n",
        "axes[0].plot(a_list, mae_dlf, 'g--o', label = 'Dynamic linear fusion')\n",
        "axes[0].set_title('Avg MAE over varying alpha')\n",
        "axes[0].set_xlabel('alpha')\n",
        "axes[0].set_ylabel('MAE')\n",
        "axes[0].set_xticks(a_list)\n",
        "\n",
        "axes[1].plot(a_list, avg_mse_list, 'm--o', label = 'static alpha')\n",
        "axes[1].plot(a_list, mse_dlf, 'g--o', label = 'Dynamic linear fusion')\n",
        "axes[1].set_title('Avg MSE over varying alpha')\n",
        "axes[1].set_xlabel('alpha')\n",
        "axes[1].set_ylabel('MSE')\n",
        "axes[1].set_xticks(a_list)\n",
        "plt.tight_layout()\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "index = 0\n",
        "min = 100\n",
        "min_index = 0\n",
        "for i in avg_mae_list:\n",
        "  if i<min:\n",
        "    min = i\n",
        "    min_index = index\n",
        "  index+=1\n",
        "print(f'minimum avg mae over alphas = {min} at alpha = {a_list[min_index]} and mse = {avg_mse_list[min_index]}')\n",
        "\n",
        "index = 0\n",
        "min = 100\n",
        "min_index = 0\n",
        "for i in avg_mse_list:\n",
        "  if i<min:\n",
        "    min = i\n",
        "    min_index = index\n",
        "  index+=1\n",
        "print(f'minimum avg mse over alphas = {min} at alpha = {a_list[min_index]} and mae = {avg_mae_list[min_index]}')"
      ],
      "id": "P_LZiLjKBjaG"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Without the FM"
      ],
      "metadata": {
        "id": "ISWexdGF3Rmx"
      },
      "id": "ISWexdGF3Rmx"
    },
    {
      "cell_type": "code",
      "source": [
        "#combination of interaction.py and review.py\n",
        "def train_model():\n",
        "    mae_list=[]\n",
        "    mse_list=[]\n",
        "    tf.compat.v1.disable_eager_execution()\n",
        "    #I and R-----\n",
        "    users = tf.compat.v1.placeholder(tf.int32, shape=[None])\n",
        "    items = tf.compat.v1.placeholder(tf.int32, shape=[None])\n",
        "    users_inputs = tf.compat.v1.placeholder(tf.int32, shape=[None, max_doc_length])\n",
        "    items_inputs = tf.compat.v1.placeholder(tf.int32, shape=[None, max_doc_length])\n",
        "    ratings = tf.compat.v1.placeholder(tf.float32, shape=[None, 1])\n",
        "    \n",
        "    #R-----\n",
        "    dropout_rate = tf.compat.v1.placeholder(tf.float32)\n",
        "    \n",
        "    #I-----\n",
        "    user_entity_embedding = tf.Variable(tf.random.normal([num_users, latent_dim], mean=0, stddev=0.02), name=\"user_entity_embeddings\")\n",
        "    item_entity_embedding = tf.Variable(tf.random.normal([num_items, latent_dim], mean=0, stddev=0.02), name=\"item_entity_embeddings\")\n",
        "    user_entity_embeds = tf.nn.embedding_lookup(user_entity_embedding, users)\n",
        "    item_entity_embeds = tf.nn.embedding_lookup(item_entity_embedding, items)\n",
        "    \n",
        "    #R-----\n",
        "    user_bias = tf.Variable(tf.random.normal([num_users, 1], mean=0, stddev=0.02), name=\"user_bias\")\n",
        "    item_bias = tf.Variable(tf.random.normal([num_items, 1], mean=0, stddev=0.02), name=\"item_bias\")\n",
        "    user_bs = tf.nn.embedding_lookup(user_bias, users)\n",
        "    item_bs = tf.nn.embedding_lookup(item_bias, items)\n",
        "    \n",
        "    text_embedding = tf.Variable(word_embedding_mtrx, dtype=tf.float32, name=\"review_text_embeds\")\n",
        "    padding_embedding = tf.Variable(np.zeros([1, word_latent_dim]), dtype=tf.float32)\n",
        "    text_mask = tf.constant([1.0] * text_embedding.get_shape()[0] + [0.0])\n",
        "    word_embeddings = tf.concat([text_embedding, padding_embedding], 0)\n",
        "    word_embeddings = word_embeddings * tf.expand_dims(text_mask, -1)\n",
        "    user_reviews_representation = tf.nn.embedding_lookup(word_embeddings, users_inputs)\n",
        "    user_reviews_representation_expnd = tf.expand_dims(user_reviews_representation, -1)\n",
        "    item_reviews_representation = tf.nn.embedding_lookup(word_embeddings, items_inputs)\n",
        "    item_reviews_representation_expnd = tf.expand_dims(item_reviews_representation, -1)\n",
        "    \n",
        "    # CNN layers\n",
        "    W_u = tf.Variable(tf.random.truncated_normal([window_size, word_latent_dim, 1, num_filters], stddev=0.3), name=\"review_W_u\")\n",
        "    W_i = tf.Variable(tf.random.truncated_normal([window_size, word_latent_dim, 1, num_filters], stddev=0.3), name=\"review_W_i\")\n",
        "    W_u_1 = tf.Variable(tf.random.truncated_normal([window_size, num_filters, 1, num_filters], stddev=0.3), name=\"review_W_u_1\")\n",
        "    W_i_1 = tf.Variable(tf.random.truncated_normal([window_size, num_filters, 1, num_filters], stddev=0.3), name=\"review_W_i_1\")\n",
        "    # b = tf.Variable(tf.constant(0.1, shape=[self.num_filters]))\n",
        "    rand_matrix = tf.Variable(tf.random.truncated_normal([num_filters, num_filters], stddev=0.3), name=\"review_rand_matrix\")\n",
        "\n",
        "    user_embeds, item_embeds = cnn_model_average(num_filters, user_reviews_representation_expnd, item_reviews_representation_expnd, W_u, W_i, W_u_1, W_i_1, rand_matrix)\n",
        "    \n",
        "    #shared MLP layer\n",
        "    W_mlp = tf.Variable(tf.random.normal([num_filters, latent_dim], mean=0, stddev=0.02), name=\"review_W_mlp\")\n",
        "    W_mlp = tf.nn.dropout(W_mlp, dropout_rate)\n",
        "    b_mlp = tf.Variable(tf.constant(0.1, shape=[latent_dim]), name=\"review_b_mlp\")\n",
        "\n",
        "    user_embeds = tf.nn.relu(tf.matmul(user_embeds, W_mlp) + b_mlp)\n",
        "    item_embeds = tf.nn.relu(tf.matmul(item_embeds, W_mlp) + b_mlp)\n",
        "\n",
        "    embeds_sum = tf.concat([tf.multiply(user_embeds, item_embeds), user_embeds, item_embeds], 1, name=\"concat_embed\")\n",
        "    #I\n",
        "    entity_embeds_sum = tf.concat([tf.multiply(user_entity_embeds, item_entity_embeds), user_entity_embeds, item_entity_embeds],1)\n",
        "\n",
        "    #FM layer\n",
        "    #R---\n",
        "    # w_0 = tf.Variable(tf.zeros(1), name=\"review_w_0\")\n",
        "    # w_1 = tf.Variable(tf.random.truncated_normal([1, latent_dim * 3], stddev=0.3), name=\"review_w_1\")\n",
        "    # v = tf.Variable(tf.random.truncated_normal([latent_dim * 3, latent_dim * 3], stddev=0.3), name=\"review_v\")\n",
        "    \n",
        "    # #I---\n",
        "    # w_entity_0 = tf.Variable(tf.zeros(1), name=\"entity_w_0\")\n",
        "    # w_entity_1 = tf.Variable(tf.random.truncated_normal([1, latent_dim*3], stddev=0.3), name=\"entity_w_1\")\n",
        "    # v_entity = tf.Variable(tf.random.truncated_normal([latent_dim*3, v_dim], stddev=0.3), name=\"entity_v\")\n",
        "    \n",
        "    # #I---\n",
        "    # J_e_1 = w_entity_0 + tf.matmul(entity_embeds_sum, w_entity_1, transpose_b=True)\n",
        "    # #R---\n",
        "    # J_1 = w_0 + tf.matmul(embeds_sum, w_1, transpose_b=True)\n",
        "    \n",
        "    # #I---\n",
        "    # entity_embeds_sum_1 = tf.expand_dims(entity_embeds_sum, -1)\n",
        "    # entity_embeds_sum_2 = tf.expand_dims(entity_embeds_sum, 1)\n",
        "    # #R---\n",
        "    # embeds_sum_1 = tf.expand_dims(embeds_sum, -1)\n",
        "    # embeds_sum_2 = tf.expand_dims(embeds_sum, 1)\n",
        "    \n",
        "    # #I---\n",
        "    # J_e_2 = tf.reduce_sum(tf.reduce_sum(tf.multiply(tf.matmul(entity_embeds_sum_1, entity_embeds_sum_2), tf.matmul(v_entity, v_entity, transpose_b=True)),2), 1, keepdims=True)\n",
        "    # #J_e_3 = tf.trace(tf.multiply(tf.matmul(entity_embeds_sum_1, entity_embeds_sum_2), tf.matmul(v_entity, v_entity, transpose_b=True))) \n",
        "    \n",
        "    # #R---\n",
        "    # J_2 = tf.reduce_sum(tf.reduce_sum(tf.multiply(tf.matmul(embeds_sum_1, embeds_sum_2), tf.matmul(v, v, transpose_b=True)),2), 1, keepdims=True)\n",
        "    # #J_3 = tf.trace(tf.multiply(tf.matmul(embeds_sum_1, embeds_sum_2), tf.matmul(v, v, transpose_b=True)))\n",
        "    # #R---\n",
        "    # J_total = (J_1 + 0.5 * (J_2))\n",
        "    \n",
        "    # #I---\n",
        "    # J_e_total = (J_e_1 + 0.5 * (J_e_2))\n",
        "    \n",
        "    \n",
        "    numerator = embeds_sum + entity_embeds_sum\n",
        "    predict_rating = tf.divide(embeds_sum, numerator) * embeds_sum + tf.divide(entity_embeds_sum, numerator) * entity_embeds_sum + user_bs + item_bs\n",
        "    loss = tf.reduce_mean(tf.math.squared_difference(predict_rating, ratings))\n",
        "    loss += lambda_1 * (tf.nn.l2_loss(W_i_1) + tf.nn.l2_loss(W_u_1) + tf.nn.l2_loss(user_entity_embedding) + tf.nn.l2_loss(item_entity_embedding) + tf.nn.l2_loss(W_u) + tf.nn.l2_loss(W_i) + tf.nn.l2_loss(rand_matrix) + tf.nn.l2_loss(user_bs) + tf.nn.l2_loss(item_bs))\n",
        "    \n",
        "    train_step = tf.compat.v1.train.RMSPropOptimizer(learning_rate).minimize(loss)\n",
        "    saver = tf.compat.v1.train.Saver()\n",
        "    \n",
        "    init = tf.compat.v1.global_variables_initializer()\n",
        "    with tf.compat.v1.Session() as sess:\n",
        "        sess.run(init)\n",
        "\n",
        "        for e in range(epochs):\n",
        "            t = time()\n",
        "            loss_total = 0.0\n",
        "            count = 0.0\n",
        "            \n",
        "            for i in range(int(math.ceil(len(user_input) / float(batch_size)))):\n",
        "                user_batch, item_batch, user_input_batch, item_input_batch, rates_batch = get_train_instance_batch_change(i, batch_size,user_input, item_input, rateings, user_reviews,item_reviews)\n",
        "                _, loss_val, words = sess.run([train_step, loss, word_embeddings], feed_dict={users: user_batch, items: item_batch, users_inputs: user_input_batch, items_inputs: item_input_batch, ratings: rates_batch, dropout_rate:drop_out})\n",
        "                loss_total += loss_val\n",
        "                count += 1.0\n",
        "\n",
        "            t1 = time()            \n",
        "            #LOOK FROM HERE \n",
        "            val_mses, val_maes = [], []\n",
        "            for i in range(len(user_input_val)):\n",
        "                #eval_model(users, items, users_inputs, items_inputs, dropout_rate, predict_rating, sess, user_vals[i], item_vals[i], user_input_val[i], item_input_val[i], rating_input_val[i], val_mses, val_maes)\n",
        "                #predicts = sess.run(predict_rating, feed_dict={users: user_batch, items: item_batch, users_inputs: user_input_batch, items_inputs: item_input_batch, dropout_rate:1.0})\n",
        "                predicts = sess.run(predict_rating, feed_dict={users: user_vals[i], items: item_vals[i], users_inputs: user_input_val[i], items_inputs: item_input_val[i], ratings: rating_input_val[i], dropout_rate: drop_out})\n",
        "                #print(predicts)\n",
        "                row, col = predicts.shape\n",
        "                for r in range(row):\n",
        "                    val_mses.append(pow((predicts[r, 0] - rating_input_val[i][r][0]), 2))\n",
        "                    val_maes.append(abs((predicts[r, 0] - rating_input_val[i][r][0])))\n",
        "            val_mse = np.array(val_mses).mean()            \n",
        "            t2 = time()            \n",
        "            mses, maes = [], []\n",
        "            for i in range(len(user_input_test)):\n",
        "                #eval_model(users, items, users_inputs, items_inputs, dropout_rate, predict_rating, sess, user_tests[i], item_tests[i], user_input_test[i], item_input_test[i], rating_input_test[i], mses, maes)\n",
        "                predicts_test = sess.run(predict_rating, feed_dict={users: user_tests[i], items: item_tests[i], users_inputs: user_input_test[i], items_inputs: item_input_test[i], ratings: rating_input_test[i], dropout_rate: drop_out})\n",
        "                #print(predicts)\n",
        "                row, col = predicts_test.shape\n",
        "                for r in range(row):\n",
        "                    mses.append(pow((predicts_test[r, 0] - rating_input_test[i][r][0]), 2))\n",
        "                    maes.append(abs((predicts_test[r, 0] - rating_input_test[i][r][0])))\n",
        "                    \n",
        "            mse = np.array(mses).mean()\n",
        "            mae = np.array(maes).mean()\n",
        "            t3 = time()\n",
        "            mae_list.append(mae)\n",
        "            mse_list.append(mse)\n",
        "            print(\"epoch%d train time: %.3fs test time: %.3f  loss = %.3f val_mse = %.3f mse = %.3f mae = %.3f\"%(e, (t1 - t), (t3 - t2), loss_total/count, val_mse, mse, mae))\n",
        "        \n",
        "        avg_mae=sum(mae_list) / len(mae_list)\n",
        "        avg_mse=sum(mse_list)/len(mse_list)\n",
        "        avg_mae_list.append(avg_mae)\n",
        "        avg_mse_list.append(avg_mse)\n",
        "        print(\"MAE \"+str(avg_mae))\n",
        "        print(\"MSE \"+str(avg_mse))"
      ],
      "metadata": {
        "id": "9RmAkLVU3SFV"
      },
      "id": "9RmAkLVU3SFV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    print(sys.path[0])\n",
        "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
        "    word_latent_dim = 300\n",
        "    latent_dim = 15\n",
        "    max_doc_length = 300\n",
        "    num_filters = 50\n",
        "    window_size = 3\n",
        "    v_dim = 50\n",
        "    learning_rate = 0.001\n",
        "    lambda_1 = 0.05 #normally we set from [0.05, 0.01, 0.005, 0.001]\n",
        "    drop_out = 0.6\n",
        "    batch_size = 200\n",
        "    epochs = 180\n",
        "    avg_mae_list = []\n",
        "    avg_mse_list = []\n",
        "    \n",
        "    # loading data\n",
        "    firTime = time()\n",
        "    dataSet = Dataset(max_doc_length, sys.path[0], \"dataPreprocessingWordDict.out\")\n",
        "    word_dict, user_reviews, item_reviews, train, valRatings, testRatings = dataSet.word_id_dict, dataSet.userReview_dict, dataSet.itemReview_dict, dataSet.trainMtrx, dataSet.valRatings, dataSet.testRatings\n",
        "    secTime = time()\n",
        "\n",
        "    num_users, num_items = train.shape\n",
        "    print(\"load data: %.3fs\" % (secTime - firTime))\n",
        "    print(num_users, num_items)\n",
        "    print(latent_dim)\n",
        "\n",
        "    #load word embeddings\n",
        "    word_embedding_mtrx = ini_word_embed(len(word_dict), word_latent_dim)\n",
        "    # word_embedding_mtrx = word2vec_word_embed(len(word_dict), word_latent_dim,\n",
        "    #                                           \"Directory of pretrained WordEmbedding.out\",\n",
        "    #                                           word_dict)\n",
        "\n",
        "    print( \"shape\", word_embedding_mtrx.shape)\n",
        "\n",
        "    # get train instances\n",
        "    user_input, item_input, rateings = get_train_instance(train)\n",
        "    print (len(user_input), len(item_input), len(rateings))\n",
        "\n",
        "    # get test/val instances\n",
        "    user_vals, item_vals, user_input_val, item_input_val, rating_input_val = get_test_list(200, valRatings, user_reviews, item_reviews)\n",
        "    user_tests, item_tests, user_input_test, item_input_test, rating_input_test = get_test_list(200, testRatings, user_reviews, item_reviews)\n",
        "\n",
        "    #train & eval model\n",
        "    train_model()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EB1hd9hp3SW_",
        "outputId": "de22be83-ebbc-46d3-819b-836006adf8bc"
      },
      "id": "EB1hd9hp3SW_",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "wordId_dict finished\n",
            "load reviews finished\n",
            "load data: 4.158s\n",
            "506 2581\n",
            "15\n",
            "shape (19930, 300)\n",
            "18880 18880 18880\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/training/rmsprop.py:188: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "epoch0 train time: 21.025s test time: 0.963  loss = 195.459 val_mse = 15.981 mse = 16.114 mae = 3.826\n",
            "epoch1 train time: 9.202s test time: 0.895  loss = 131.111 val_mse = 15.615 mse = 15.748 mae = 3.780\n",
            "epoch2 train time: 9.419s test time: 0.899  loss = 70.698 val_mse = 15.109 mse = 15.240 mae = 3.716\n",
            "epoch3 train time: 9.263s test time: 0.905  loss = 37.313 val_mse = 14.472 mse = 14.602 mae = 3.634\n",
            "epoch4 train time: 9.312s test time: 0.896  loss = 21.022 val_mse = 13.718 mse = 13.847 mae = 3.533\n",
            "epoch5 train time: 9.281s test time: 0.919  loss = 14.318 val_mse = 12.861 mse = 12.988 mae = 3.416\n",
            "epoch6 train time: 9.478s test time: 0.900  loss = 12.056 val_mse = 11.916 mse = 12.040 mae = 3.281\n",
            "epoch7 train time: 9.274s test time: 0.906  loss = 11.150 val_mse = 10.902 mse = 11.022 mae = 3.129\n",
            "epoch8 train time: 9.596s test time: 1.011  loss = 10.371 val_mse = 9.836 mse = 9.950 mae = 2.960\n",
            "epoch9 train time: 9.255s test time: 0.906  loss = 9.593 val_mse = 8.739 mse = 8.846 mae = 2.781\n",
            "epoch10 train time: 9.220s test time: 0.906  loss = 8.821 val_mse = 7.632 mse = 7.730 mae = 2.592\n",
            "epoch11 train time: 9.306s test time: 0.893  loss = 8.064 val_mse = 6.540 mse = 6.627 mae = 2.387\n",
            "epoch12 train time: 9.227s test time: 0.903  loss = 7.330 val_mse = 5.487 mse = 5.562 mae = 2.167\n",
            "epoch13 train time: 9.314s test time: 0.909  loss = 6.628 val_mse = 4.501 mse = 4.561 mae = 1.945\n",
            "epoch14 train time: 9.231s test time: 0.910  loss = 5.966 val_mse = 3.609 mse = 3.652 mae = 1.734\n",
            "epoch15 train time: 9.296s test time: 0.921  loss = 5.354 val_mse = 2.840 mse = 2.864 mae = 1.511\n",
            "epoch16 train time: 9.254s test time: 0.909  loss = 4.803 val_mse = 2.223 mse = 2.226 mae = 1.288\n",
            "epoch17 train time: 9.284s test time: 0.904  loss = 4.321 val_mse = 1.787 mse = 1.769 mae = 1.141\n",
            "epoch18 train time: 9.559s test time: 0.907  loss = 3.920 val_mse = 1.560 mse = 1.518 mae = 1.004\n",
            "epoch19 train time: 9.271s test time: 0.906  loss = 3.609 val_mse = 1.560 mse = 1.494 mae = 0.892\n",
            "epoch20 train time: 9.231s test time: 0.909  loss = 3.396 val_mse = 1.776 mse = 1.687 mae = 0.949\n",
            "epoch21 train time: 9.305s test time: 0.901  loss = 3.284 val_mse = 2.086 mse = 1.979 mae = 1.015\n",
            "epoch22 train time: 9.247s test time: 0.909  loss = 3.247 val_mse = 2.264 mse = 2.150 mae = 1.044\n",
            "epoch23 train time: 9.305s test time: 0.909  loss = 3.238 val_mse = 2.320 mse = 2.205 mae = 1.053\n",
            "epoch24 train time: 9.250s test time: 0.895  loss = 3.232 val_mse = 2.334 mse = 2.219 mae = 1.055\n",
            "epoch25 train time: 9.289s test time: 0.907  loss = 3.227 val_mse = 2.335 mse = 2.222 mae = 1.055\n",
            "epoch26 train time: 9.308s test time: 0.904  loss = 3.222 val_mse = 2.334 mse = 2.222 mae = 1.055\n",
            "epoch27 train time: 9.237s test time: 0.908  loss = 3.218 val_mse = 2.332 mse = 2.221 mae = 1.055\n",
            "epoch28 train time: 9.292s test time: 0.905  loss = 3.214 val_mse = 2.330 mse = 2.219 mae = 1.054\n",
            "epoch29 train time: 9.243s test time: 0.902  loss = 3.210 val_mse = 2.327 mse = 2.218 mae = 1.054\n",
            "epoch30 train time: 9.280s test time: 0.893  loss = 3.206 val_mse = 2.325 mse = 2.216 mae = 1.054\n",
            "epoch31 train time: 9.325s test time: 0.913  loss = 3.203 val_mse = 2.322 mse = 2.215 mae = 1.053\n",
            "epoch32 train time: 9.254s test time: 0.905  loss = 3.199 val_mse = 2.320 mse = 2.213 mae = 1.053\n",
            "epoch33 train time: 9.227s test time: 0.906  loss = 3.196 val_mse = 2.317 mse = 2.212 mae = 1.053\n",
            "epoch34 train time: 9.250s test time: 0.904  loss = 3.193 val_mse = 2.315 mse = 2.210 mae = 1.052\n",
            "epoch35 train time: 9.289s test time: 0.905  loss = 3.189 val_mse = 2.312 mse = 2.209 mae = 1.052\n",
            "epoch36 train time: 9.281s test time: 0.898  loss = 3.186 val_mse = 2.310 mse = 2.207 mae = 1.051\n",
            "epoch37 train time: 9.227s test time: 0.904  loss = 3.183 val_mse = 2.308 mse = 2.206 mae = 1.051\n",
            "epoch38 train time: 9.317s test time: 0.909  loss = 3.180 val_mse = 2.306 mse = 2.204 mae = 1.051\n",
            "epoch39 train time: 9.248s test time: 0.908  loss = 3.177 val_mse = 2.303 mse = 2.203 mae = 1.050\n",
            "epoch40 train time: 9.269s test time: 0.913  loss = 3.175 val_mse = 2.301 mse = 2.201 mae = 1.050\n",
            "epoch41 train time: 9.329s test time: 0.900  loss = 3.172 val_mse = 2.299 mse = 2.200 mae = 1.050\n",
            "epoch42 train time: 9.181s test time: 0.894  loss = 3.169 val_mse = 2.297 mse = 2.199 mae = 1.049\n",
            "epoch43 train time: 9.271s test time: 0.897  loss = 3.167 val_mse = 2.295 mse = 2.197 mae = 1.049\n",
            "epoch44 train time: 9.300s test time: 0.899  loss = 3.164 val_mse = 2.293 mse = 2.196 mae = 1.049\n",
            "epoch45 train time: 9.240s test time: 0.897  loss = 3.162 val_mse = 2.291 mse = 2.195 mae = 1.049\n",
            "epoch46 train time: 9.262s test time: 0.901  loss = 3.159 val_mse = 2.289 mse = 2.193 mae = 1.048\n",
            "epoch47 train time: 9.226s test time: 0.902  loss = 3.157 val_mse = 2.287 mse = 2.192 mae = 1.048\n",
            "epoch48 train time: 9.286s test time: 0.917  loss = 3.155 val_mse = 2.285 mse = 2.191 mae = 1.048\n",
            "epoch49 train time: 9.232s test time: 0.898  loss = 3.152 val_mse = 2.284 mse = 2.189 mae = 1.047\n",
            "epoch50 train time: 9.291s test time: 0.914  loss = 3.150 val_mse = 2.282 mse = 2.188 mae = 1.047\n",
            "epoch51 train time: 9.270s test time: 0.895  loss = 3.148 val_mse = 2.280 mse = 2.187 mae = 1.047\n",
            "epoch52 train time: 9.272s test time: 0.910  loss = 3.146 val_mse = 2.278 mse = 2.186 mae = 1.047\n",
            "epoch53 train time: 9.359s test time: 0.910  loss = 3.144 val_mse = 2.277 mse = 2.185 mae = 1.046\n",
            "epoch54 train time: 9.295s test time: 0.895  loss = 3.142 val_mse = 2.275 mse = 2.184 mae = 1.046\n",
            "epoch55 train time: 9.237s test time: 0.906  loss = 3.140 val_mse = 2.273 mse = 2.183 mae = 1.046\n",
            "epoch56 train time: 9.241s test time: 0.903  loss = 3.138 val_mse = 2.272 mse = 2.182 mae = 1.046\n",
            "epoch57 train time: 9.230s test time: 0.902  loss = 3.137 val_mse = 2.270 mse = 2.180 mae = 1.045\n",
            "epoch58 train time: 9.321s test time: 0.902  loss = 3.135 val_mse = 2.269 mse = 2.179 mae = 1.045\n",
            "epoch59 train time: 9.275s test time: 0.903  loss = 3.133 val_mse = 2.267 mse = 2.178 mae = 1.045\n",
            "epoch60 train time: 9.256s test time: 0.908  loss = 3.131 val_mse = 2.266 mse = 2.177 mae = 1.045\n",
            "epoch61 train time: 9.314s test time: 0.900  loss = 3.130 val_mse = 2.265 mse = 2.176 mae = 1.044\n",
            "epoch62 train time: 9.200s test time: 0.908  loss = 3.128 val_mse = 2.263 mse = 2.176 mae = 1.044\n",
            "epoch63 train time: 9.244s test time: 0.896  loss = 3.126 val_mse = 2.262 mse = 2.175 mae = 1.044\n",
            "epoch64 train time: 9.259s test time: 0.891  loss = 3.125 val_mse = 2.260 mse = 2.174 mae = 1.044\n",
            "epoch65 train time: 9.210s test time: 0.905  loss = 3.123 val_mse = 2.259 mse = 2.173 mae = 1.044\n",
            "epoch66 train time: 9.284s test time: 0.895  loss = 3.122 val_mse = 2.258 mse = 2.172 mae = 1.043\n",
            "epoch67 train time: 9.261s test time: 0.903  loss = 3.120 val_mse = 2.257 mse = 2.171 mae = 1.043\n",
            "epoch68 train time: 9.292s test time: 0.900  loss = 3.119 val_mse = 2.255 mse = 2.170 mae = 1.043\n",
            "epoch69 train time: 9.212s test time: 0.911  loss = 3.118 val_mse = 2.254 mse = 2.169 mae = 1.043\n",
            "epoch70 train time: 9.251s test time: 0.901  loss = 3.116 val_mse = 2.253 mse = 2.169 mae = 1.043\n",
            "epoch71 train time: 9.278s test time: 0.890  loss = 3.115 val_mse = 2.252 mse = 2.168 mae = 1.042\n",
            "epoch72 train time: 9.219s test time: 0.895  loss = 3.114 val_mse = 2.251 mse = 2.167 mae = 1.042\n",
            "epoch73 train time: 9.298s test time: 0.907  loss = 3.112 val_mse = 2.250 mse = 2.166 mae = 1.042\n",
            "epoch74 train time: 9.274s test time: 0.899  loss = 3.111 val_mse = 2.248 mse = 2.165 mae = 1.042\n",
            "epoch75 train time: 9.242s test time: 0.914  loss = 3.110 val_mse = 2.247 mse = 2.165 mae = 1.042\n",
            "epoch76 train time: 9.276s test time: 0.892  loss = 3.109 val_mse = 2.246 mse = 2.164 mae = 1.041\n",
            "epoch77 train time: 9.266s test time: 0.895  loss = 3.107 val_mse = 2.245 mse = 2.163 mae = 1.041\n",
            "epoch78 train time: 9.265s test time: 0.893  loss = 3.106 val_mse = 2.244 mse = 2.162 mae = 1.041\n",
            "epoch79 train time: 9.201s test time: 0.892  loss = 3.105 val_mse = 2.243 mse = 2.162 mae = 1.041\n",
            "epoch80 train time: 9.241s test time: 0.902  loss = 3.104 val_mse = 2.242 mse = 2.161 mae = 1.041\n",
            "epoch81 train time: 9.208s test time: 0.894  loss = 3.103 val_mse = 2.241 mse = 2.160 mae = 1.040\n",
            "epoch82 train time: 9.232s test time: 0.909  loss = 3.102 val_mse = 2.240 mse = 2.160 mae = 1.040\n",
            "epoch83 train time: 9.205s test time: 0.898  loss = 3.101 val_mse = 2.239 mse = 2.159 mae = 1.040\n",
            "epoch84 train time: 9.216s test time: 0.894  loss = 3.100 val_mse = 2.238 mse = 2.158 mae = 1.040\n",
            "epoch85 train time: 9.229s test time: 0.899  loss = 3.099 val_mse = 2.237 mse = 2.158 mae = 1.040\n",
            "epoch86 train time: 9.262s test time: 0.903  loss = 3.098 val_mse = 2.236 mse = 2.157 mae = 1.040\n",
            "epoch87 train time: 9.208s test time: 0.896  loss = 3.097 val_mse = 2.236 mse = 2.157 mae = 1.039\n",
            "epoch88 train time: 9.304s test time: 0.888  loss = 3.096 val_mse = 2.235 mse = 2.156 mae = 1.039\n",
            "epoch89 train time: 9.194s test time: 0.883  loss = 3.095 val_mse = 2.234 mse = 2.155 mae = 1.039\n",
            "epoch90 train time: 9.273s test time: 0.902  loss = 3.094 val_mse = 2.233 mse = 2.155 mae = 1.039\n",
            "epoch91 train time: 9.266s test time: 0.897  loss = 3.093 val_mse = 2.232 mse = 2.154 mae = 1.039\n",
            "epoch92 train time: 9.245s test time: 0.894  loss = 3.092 val_mse = 2.231 mse = 2.154 mae = 1.038\n",
            "epoch93 train time: 9.228s test time: 0.910  loss = 3.091 val_mse = 2.231 mse = 2.153 mae = 1.038\n",
            "epoch94 train time: 9.198s test time: 0.884  loss = 3.090 val_mse = 2.230 mse = 2.153 mae = 1.038\n",
            "epoch95 train time: 9.268s test time: 0.903  loss = 3.090 val_mse = 2.229 mse = 2.152 mae = 1.038\n",
            "epoch96 train time: 9.295s test time: 0.899  loss = 3.089 val_mse = 2.228 mse = 2.151 mae = 1.038\n",
            "epoch97 train time: 9.273s test time: 0.899  loss = 3.088 val_mse = 2.227 mse = 2.151 mae = 1.038\n",
            "epoch98 train time: 9.294s test time: 0.900  loss = 3.087 val_mse = 2.227 mse = 2.150 mae = 1.037\n",
            "epoch99 train time: 9.253s test time: 0.901  loss = 3.086 val_mse = 2.226 mse = 2.150 mae = 1.037\n",
            "epoch100 train time: 9.224s test time: 0.906  loss = 3.086 val_mse = 2.225 mse = 2.149 mae = 1.037\n",
            "epoch101 train time: 9.328s test time: 0.883  loss = 3.085 val_mse = 2.225 mse = 2.149 mae = 1.037\n",
            "epoch102 train time: 9.248s test time: 0.894  loss = 3.084 val_mse = 2.224 mse = 2.148 mae = 1.037\n",
            "epoch103 train time: 9.186s test time: 0.876  loss = 3.084 val_mse = 2.223 mse = 2.148 mae = 1.037\n",
            "epoch104 train time: 9.222s test time: 0.879  loss = 3.083 val_mse = 2.223 mse = 2.147 mae = 1.036\n",
            "epoch105 train time: 9.149s test time: 0.892  loss = 3.082 val_mse = 2.222 mse = 2.147 mae = 1.036\n",
            "epoch106 train time: 9.175s test time: 0.890  loss = 3.081 val_mse = 2.221 mse = 2.146 mae = 1.036\n",
            "epoch107 train time: 9.169s test time: 0.888  loss = 3.081 val_mse = 2.221 mse = 2.146 mae = 1.036\n",
            "epoch108 train time: 9.125s test time: 0.897  loss = 3.080 val_mse = 2.220 mse = 2.145 mae = 1.036\n",
            "epoch109 train time: 9.169s test time: 0.880  loss = 3.079 val_mse = 2.219 mse = 2.145 mae = 1.036\n",
            "epoch110 train time: 9.203s test time: 0.890  loss = 3.079 val_mse = 2.219 mse = 2.144 mae = 1.035\n",
            "epoch111 train time: 9.209s test time: 0.887  loss = 3.078 val_mse = 2.218 mse = 2.144 mae = 1.035\n",
            "epoch112 train time: 9.151s test time: 0.878  loss = 3.078 val_mse = 2.217 mse = 2.144 mae = 1.035\n",
            "epoch113 train time: 9.178s test time: 0.878  loss = 3.077 val_mse = 2.217 mse = 2.143 mae = 1.035\n",
            "epoch114 train time: 9.200s test time: 0.891  loss = 3.076 val_mse = 2.216 mse = 2.143 mae = 1.035\n",
            "epoch115 train time: 9.142s test time: 0.894  loss = 3.076 val_mse = 2.216 mse = 2.142 mae = 1.035\n",
            "epoch116 train time: 9.211s test time: 0.883  loss = 3.075 val_mse = 2.215 mse = 2.142 mae = 1.035\n",
            "epoch117 train time: 9.178s test time: 0.884  loss = 3.075 val_mse = 2.215 mse = 2.141 mae = 1.034\n",
            "epoch118 train time: 9.212s test time: 0.884  loss = 3.074 val_mse = 2.214 mse = 2.141 mae = 1.034\n",
            "epoch119 train time: 9.188s test time: 0.883  loss = 3.074 val_mse = 2.213 mse = 2.141 mae = 1.034\n",
            "epoch120 train time: 9.146s test time: 0.881  loss = 3.073 val_mse = 2.213 mse = 2.140 mae = 1.034\n",
            "epoch121 train time: 9.199s test time: 0.890  loss = 3.073 val_mse = 2.212 mse = 2.140 mae = 1.034\n",
            "epoch122 train time: 9.138s test time: 0.869  loss = 3.072 val_mse = 2.212 mse = 2.139 mae = 1.034\n",
            "epoch123 train time: 9.152s test time: 0.881  loss = 3.072 val_mse = 2.211 mse = 2.139 mae = 1.033\n",
            "epoch124 train time: 9.164s test time: 0.885  loss = 3.071 val_mse = 2.211 mse = 2.139 mae = 1.033\n",
            "epoch125 train time: 9.196s test time: 0.886  loss = 3.071 val_mse = 2.210 mse = 2.138 mae = 1.033\n",
            "epoch126 train time: 9.192s test time: 0.887  loss = 3.070 val_mse = 2.210 mse = 2.138 mae = 1.033\n",
            "epoch127 train time: 9.188s test time: 0.883  loss = 3.070 val_mse = 2.209 mse = 2.137 mae = 1.033\n",
            "epoch128 train time: 9.182s test time: 0.882  loss = 3.069 val_mse = 2.209 mse = 2.137 mae = 1.033\n",
            "epoch129 train time: 9.208s test time: 0.878  loss = 3.069 val_mse = 2.208 mse = 2.137 mae = 1.033\n",
            "epoch130 train time: 9.171s test time: 0.878  loss = 3.068 val_mse = 2.208 mse = 2.136 mae = 1.033\n",
            "epoch131 train time: 9.219s test time: 0.899  loss = 3.068 val_mse = 2.207 mse = 2.136 mae = 1.032\n",
            "epoch132 train time: 9.208s test time: 0.876  loss = 3.067 val_mse = 2.207 mse = 2.136 mae = 1.032\n",
            "epoch133 train time: 9.223s test time: 0.885  loss = 3.067 val_mse = 2.206 mse = 2.135 mae = 1.032\n",
            "epoch134 train time: 9.210s test time: 0.888  loss = 3.067 val_mse = 2.206 mse = 2.135 mae = 1.032\n",
            "epoch135 train time: 9.167s test time: 0.886  loss = 3.066 val_mse = 2.205 mse = 2.135 mae = 1.032\n",
            "epoch136 train time: 9.187s test time: 0.889  loss = 3.066 val_mse = 2.205 mse = 2.134 mae = 1.032\n",
            "epoch137 train time: 9.144s test time: 0.878  loss = 3.065 val_mse = 2.204 mse = 2.134 mae = 1.032\n",
            "epoch138 train time: 9.186s test time: 0.888  loss = 3.065 val_mse = 2.204 mse = 2.133 mae = 1.031\n",
            "epoch139 train time: 9.247s test time: 0.881  loss = 3.064 val_mse = 2.204 mse = 2.133 mae = 1.031\n",
            "epoch140 train time: 9.188s test time: 0.887  loss = 3.064 val_mse = 2.203 mse = 2.133 mae = 1.031\n",
            "epoch141 train time: 9.209s test time: 0.890  loss = 3.064 val_mse = 2.203 mse = 2.132 mae = 1.031\n",
            "epoch142 train time: 9.195s test time: 0.892  loss = 3.063 val_mse = 2.202 mse = 2.132 mae = 1.031\n",
            "epoch143 train time: 9.183s test time: 0.899  loss = 3.063 val_mse = 2.202 mse = 2.132 mae = 1.031\n",
            "epoch144 train time: 9.219s test time: 0.890  loss = 3.063 val_mse = 2.201 mse = 2.131 mae = 1.031\n",
            "epoch145 train time: 9.213s test time: 0.892  loss = 3.062 val_mse = 2.201 mse = 2.131 mae = 1.031\n",
            "epoch146 train time: 9.207s test time: 0.885  loss = 3.062 val_mse = 2.201 mse = 2.131 mae = 1.031\n",
            "epoch147 train time: 9.197s test time: 0.883  loss = 3.062 val_mse = 2.200 mse = 2.130 mae = 1.030\n",
            "epoch148 train time: 9.222s test time: 0.888  loss = 3.061 val_mse = 2.200 mse = 2.130 mae = 1.030\n",
            "epoch149 train time: 9.243s test time: 0.892  loss = 3.061 val_mse = 2.199 mse = 2.130 mae = 1.030\n",
            "epoch150 train time: 9.166s test time: 0.892  loss = 3.061 val_mse = 2.199 mse = 2.129 mae = 1.030\n",
            "epoch151 train time: 9.272s test time: 0.888  loss = 3.060 val_mse = 2.199 mse = 2.129 mae = 1.030\n",
            "epoch152 train time: 9.232s test time: 0.883  loss = 3.060 val_mse = 2.198 mse = 2.129 mae = 1.030\n",
            "epoch153 train time: 9.189s test time: 0.892  loss = 3.060 val_mse = 2.198 mse = 2.128 mae = 1.030\n",
            "epoch154 train time: 9.240s test time: 0.903  loss = 3.059 val_mse = 2.197 mse = 2.128 mae = 1.030\n",
            "epoch155 train time: 9.230s test time: 0.886  loss = 3.059 val_mse = 2.197 mse = 2.128 mae = 1.029\n",
            "epoch156 train time: 9.282s test time: 0.885  loss = 3.059 val_mse = 2.197 mse = 2.127 mae = 1.029\n",
            "epoch157 train time: 9.190s test time: 0.887  loss = 3.058 val_mse = 2.196 mse = 2.127 mae = 1.029\n",
            "epoch158 train time: 9.154s test time: 0.898  loss = 3.058 val_mse = 2.196 mse = 2.127 mae = 1.029\n",
            "epoch159 train time: 9.243s test time: 0.885  loss = 3.058 val_mse = 2.195 mse = 2.126 mae = 1.029\n",
            "epoch160 train time: 9.146s test time: 0.889  loss = 3.058 val_mse = 2.195 mse = 2.126 mae = 1.029\n",
            "epoch161 train time: 9.158s test time: 0.884  loss = 3.057 val_mse = 2.195 mse = 2.126 mae = 1.029\n",
            "epoch162 train time: 9.135s test time: 0.885  loss = 3.057 val_mse = 2.194 mse = 2.126 mae = 1.029\n",
            "epoch163 train time: 9.183s test time: 0.888  loss = 3.057 val_mse = 2.194 mse = 2.125 mae = 1.029\n",
            "epoch164 train time: 9.189s test time: 0.878  loss = 3.057 val_mse = 2.194 mse = 2.125 mae = 1.029\n",
            "epoch165 train time: 9.152s test time: 0.883  loss = 3.056 val_mse = 2.193 mse = 2.125 mae = 1.028\n",
            "epoch166 train time: 9.181s test time: 0.875  loss = 3.056 val_mse = 2.193 mse = 2.124 mae = 1.028\n",
            "epoch167 train time: 9.193s test time: 0.879  loss = 3.056 val_mse = 2.192 mse = 2.124 mae = 1.028\n",
            "epoch168 train time: 9.209s test time: 0.886  loss = 3.056 val_mse = 2.192 mse = 2.124 mae = 1.028\n",
            "epoch169 train time: 9.256s test time: 0.880  loss = 3.055 val_mse = 2.192 mse = 2.123 mae = 1.028\n",
            "epoch170 train time: 9.160s test time: 0.876  loss = 3.055 val_mse = 2.191 mse = 2.123 mae = 1.028\n",
            "epoch171 train time: 9.167s test time: 0.892  loss = 3.055 val_mse = 2.191 mse = 2.123 mae = 1.028\n",
            "epoch172 train time: 9.190s test time: 0.874  loss = 3.055 val_mse = 2.191 mse = 2.123 mae = 1.028\n",
            "epoch173 train time: 9.202s test time: 0.875  loss = 3.054 val_mse = 2.190 mse = 2.122 mae = 1.028\n",
            "epoch174 train time: 9.223s test time: 0.882  loss = 3.054 val_mse = 2.190 mse = 2.122 mae = 1.027\n",
            "epoch175 train time: 9.123s test time: 0.878  loss = 3.054 val_mse = 2.190 mse = 2.122 mae = 1.027\n",
            "epoch176 train time: 9.117s test time: 0.891  loss = 3.054 val_mse = 2.189 mse = 2.121 mae = 1.027\n",
            "epoch177 train time: 9.181s test time: 0.885  loss = 3.054 val_mse = 2.189 mse = 2.121 mae = 1.027\n",
            "epoch178 train time: 9.195s test time: 0.881  loss = 3.053 val_mse = 2.189 mse = 2.121 mae = 1.027\n",
            "epoch179 train time: 9.290s test time: 0.889  loss = 3.053 val_mse = 2.188 mse = 2.121 mae = 1.027\n",
            "MAE 1.2040480432080363\n",
            "MSE 2.848866964386456\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "7fGLBlBgJz0T"
      },
      "id": "7fGLBlBgJz0T"
    },
    {
      "cell_type": "code",
      "source": [
        "#combination of interaction.py and review.py\n",
        "def train_model():\n",
        "    mae_list=[]\n",
        "    mse_list=[]\n",
        "    tf.compat.v1.disable_eager_execution()\n",
        "    #I and R-----\n",
        "    users = tf.compat.v1.placeholder(tf.int32, shape=[None])\n",
        "    items = tf.compat.v1.placeholder(tf.int32, shape=[None])\n",
        "    users_inputs = tf.compat.v1.placeholder(tf.int32, shape=[None, max_doc_length])\n",
        "    items_inputs = tf.compat.v1.placeholder(tf.int32, shape=[None, max_doc_length])\n",
        "    ratings = tf.compat.v1.placeholder(tf.float32, shape=[None, 1])\n",
        "    \n",
        "    #R-----\n",
        "    dropout_rate = tf.compat.v1.placeholder(tf.float32)\n",
        "    \n",
        "    #I-----\n",
        "    user_entity_embedding = tf.Variable(tf.random.normal([num_users, latent_dim], mean=0, stddev=0.02), name=\"user_entity_embeddings\")\n",
        "    item_entity_embedding = tf.Variable(tf.random.normal([num_items, latent_dim], mean=0, stddev=0.02), name=\"item_entity_embeddings\")\n",
        "    user_entity_embeds = tf.nn.embedding_lookup(user_entity_embedding, users)\n",
        "    item_entity_embeds = tf.nn.embedding_lookup(item_entity_embedding, items)\n",
        "    \n",
        "    #R-----\n",
        "    user_bias = tf.Variable(tf.random.normal([num_users, 1], mean=0, stddev=0.02), name=\"user_bias\")\n",
        "    item_bias = tf.Variable(tf.random.normal([num_items, 1], mean=0, stddev=0.02), name=\"item_bias\")\n",
        "    user_bs = tf.nn.embedding_lookup(user_bias, users)\n",
        "    item_bs = tf.nn.embedding_lookup(item_bias, items)\n",
        "    \n",
        "    text_embedding = tf.Variable(word_embedding_mtrx, dtype=tf.float32, name=\"review_text_embeds\")\n",
        "    padding_embedding = tf.Variable(np.zeros([1, word_latent_dim]), dtype=tf.float32)\n",
        "    text_mask = tf.constant([1.0] * text_embedding.get_shape()[0] + [0.0])\n",
        "    word_embeddings = tf.concat([text_embedding, padding_embedding], 0)\n",
        "    word_embeddings = word_embeddings * tf.expand_dims(text_mask, -1)\n",
        "    user_reviews_representation = tf.nn.embedding_lookup(word_embeddings, users_inputs)\n",
        "    user_reviews_representation_expnd = tf.expand_dims(user_reviews_representation, -1)\n",
        "    item_reviews_representation = tf.nn.embedding_lookup(word_embeddings, items_inputs)\n",
        "    item_reviews_representation_expnd = tf.expand_dims(item_reviews_representation, -1)\n",
        "    \n",
        "    # CNN layers\n",
        "    W_u = tf.Variable(tf.random.truncated_normal([window_size, word_latent_dim, 1, num_filters], stddev=0.3), name=\"review_W_u\")\n",
        "    W_i = tf.Variable(tf.random.truncated_normal([window_size, word_latent_dim, 1, num_filters], stddev=0.3), name=\"review_W_i\")\n",
        "    W_u_1 = tf.Variable(tf.random.truncated_normal([window_size, num_filters, 1, num_filters], stddev=0.3), name=\"review_W_u_1\")\n",
        "    W_i_1 = tf.Variable(tf.random.truncated_normal([window_size, num_filters, 1, num_filters], stddev=0.3), name=\"review_W_i_1\")\n",
        "    # b = tf.Variable(tf.constant(0.1, shape=[self.num_filters]))\n",
        "    rand_matrix = tf.Variable(tf.random.truncated_normal([num_filters, num_filters], stddev=0.3), name=\"review_rand_matrix\")\n",
        "\n",
        "    user_embeds, item_embeds = cnn_model_average(num_filters, user_reviews_representation_expnd, item_reviews_representation_expnd, W_u, W_i, W_u_1, W_i_1, rand_matrix)\n",
        "    \n",
        "    #shared MLP layer\n",
        "    W_mlp = tf.Variable(tf.random.normal([num_filters, latent_dim], mean=0, stddev=0.02), name=\"review_W_mlp\")\n",
        "    W_mlp = tf.nn.dropout(W_mlp, dropout_rate)\n",
        "    b_mlp = tf.Variable(tf.constant(0.1, shape=[latent_dim]), name=\"review_b_mlp\")\n",
        "\n",
        "    user_embeds = tf.nn.relu(tf.matmul(user_embeds, W_mlp) + b_mlp)\n",
        "    item_embeds = tf.nn.relu(tf.matmul(item_embeds, W_mlp) + b_mlp)\n",
        "\n",
        "    embeds_sum = tf.concat([tf.multiply(user_embeds, item_embeds), user_embeds, item_embeds], 1, name=\"concat_embed\")\n",
        "    #I\n",
        "    entity_embeds_sum = tf.concat([tf.multiply(user_entity_embeds, item_entity_embeds), user_entity_embeds, item_entity_embeds],1)\n",
        "\n",
        "    #FM layer\n",
        "    #R---\n",
        "    # w_0 = tf.Variable(tf.zeros(1), name=\"review_w_0\")\n",
        "    # w_1 = tf.Variable(tf.random.truncated_normal([1, latent_dim * 3], stddev=0.3), name=\"review_w_1\")\n",
        "    # v = tf.Variable(tf.random.truncated_normal([latent_dim * 3, latent_dim * 3], stddev=0.3), name=\"review_v\")\n",
        "    \n",
        "    # #I---\n",
        "    # w_entity_0 = tf.Variable(tf.zeros(1), name=\"entity_w_0\")\n",
        "    # w_entity_1 = tf.Variable(tf.random.truncated_normal([1, latent_dim*3], stddev=0.3), name=\"entity_w_1\")\n",
        "    # v_entity = tf.Variable(tf.random.truncated_normal([latent_dim*3, v_dim], stddev=0.3), name=\"entity_v\")\n",
        "    \n",
        "    # #I---\n",
        "    # J_e_1 = w_entity_0 + tf.matmul(entity_embeds_sum, w_entity_1, transpose_b=True)\n",
        "    # #R---\n",
        "    # J_1 = w_0 + tf.matmul(embeds_sum, w_1, transpose_b=True)\n",
        "    \n",
        "    # #I---\n",
        "    # entity_embeds_sum_1 = tf.expand_dims(entity_embeds_sum, -1)\n",
        "    # entity_embeds_sum_2 = tf.expand_dims(entity_embeds_sum, 1)\n",
        "    # #R---\n",
        "    # embeds_sum_1 = tf.expand_dims(embeds_sum, -1)\n",
        "    # embeds_sum_2 = tf.expand_dims(embeds_sum, 1)\n",
        "    \n",
        "    # #I---\n",
        "    # J_e_2 = tf.reduce_sum(tf.reduce_sum(tf.multiply(tf.matmul(entity_embeds_sum_1, entity_embeds_sum_2), tf.matmul(v_entity, v_entity, transpose_b=True)),2), 1, keepdims=True)\n",
        "    # #J_e_3 = tf.trace(tf.multiply(tf.matmul(entity_embeds_sum_1, entity_embeds_sum_2), tf.matmul(v_entity, v_entity, transpose_b=True))) \n",
        "    \n",
        "    # #R---\n",
        "    # J_2 = tf.reduce_sum(tf.reduce_sum(tf.multiply(tf.matmul(embeds_sum_1, embeds_sum_2), tf.matmul(v, v, transpose_b=True)),2), 1, keepdims=True)\n",
        "    # #J_3 = tf.trace(tf.multiply(tf.matmul(embeds_sum_1, embeds_sum_2), tf.matmul(v, v, transpose_b=True)))\n",
        "    # #R---\n",
        "    # J_total = (J_1 + 0.5 * (J_2))\n",
        "    \n",
        "    # #I---\n",
        "    # J_e_total = (J_e_1 + 0.5 * (J_e_2))\n",
        "    \n",
        "    \n",
        "    numerator = embeds_sum + entity_embeds_sum\n",
        "    predict_rating = tf.divide(embeds_sum, numerator) * embeds_sum + tf.divide(entity_embeds_sum, numerator) * entity_embeds_sum + user_bs + item_bs\n",
        "    loss = tf.reduce_mean(tf.math.squared_difference(predict_rating, ratings))\n",
        "    loss += lambda_1 * (tf.nn.l2_loss(W_i_1) + tf.nn.l2_loss(W_u_1) + tf.nn.l2_loss(user_entity_embedding) + tf.nn.l2_loss(item_entity_embedding) + tf.nn.l2_loss(W_u) + tf.nn.l2_loss(W_i) + tf.nn.l2_loss(rand_matrix) + tf.nn.l2_loss(user_bs) + tf.nn.l2_loss(item_bs))\n",
        "    \n",
        "    train_step = tf.compat.v1.train.RMSPropOptimizer(learning_rate).minimize(loss)\n",
        "    saver = tf.compat.v1.train.Saver()\n",
        "    \n",
        "    init = tf.compat.v1.global_variables_initializer()\n",
        "    with tf.compat.v1.Session() as sess:\n",
        "        sess.run(init)\n",
        "\n",
        "        for e in range(epochs):\n",
        "            t = time()\n",
        "            loss_total = 0.0\n",
        "            count = 0.0\n",
        "            \n",
        "            for i in range(int(math.ceil(len(user_input) / float(batch_size)))):\n",
        "                user_batch, item_batch, user_input_batch, item_input_batch, rates_batch = get_train_instance_batch_change(i, batch_size,user_input, item_input, rateings, user_reviews,item_reviews)\n",
        "                _, loss_val, words = sess.run([train_step, loss, word_embeddings], feed_dict={users: user_batch, items: item_batch, users_inputs: user_input_batch, items_inputs: item_input_batch, ratings: rates_batch, dropout_rate:drop_out})\n",
        "                loss_total += loss_val\n",
        "                count += 1.0\n",
        "\n",
        "            t1 = time()            \n",
        "            #LOOK FROM HERE \n",
        "            val_mses, val_maes = [], []\n",
        "            for i in range(len(user_input_val)):\n",
        "                #eval_model(users, items, users_inputs, items_inputs, dropout_rate, predict_rating, sess, user_vals[i], item_vals[i], user_input_val[i], item_input_val[i], rating_input_val[i], val_mses, val_maes)\n",
        "                #predicts = sess.run(predict_rating, feed_dict={users: user_batch, items: item_batch, users_inputs: user_input_batch, items_inputs: item_input_batch, dropout_rate:1.0})\n",
        "                predicts = sess.run(predict_rating, feed_dict={users: user_vals[i], items: item_vals[i], users_inputs: user_input_val[i], items_inputs: item_input_val[i], ratings: rating_input_val[i], dropout_rate: drop_out})\n",
        "                #print(predicts)\n",
        "                row, col = predicts.shape\n",
        "                for r in range(row):\n",
        "                    val_mses.append(pow((predicts[r, 0] - rating_input_val[i][r][0]), 2))\n",
        "                    val_maes.append(abs((predicts[r, 0] - rating_input_val[i][r][0])))\n",
        "            val_mse = np.array(val_mses).mean()            \n",
        "            t2 = time()            \n",
        "            mses, maes = [], []\n",
        "            for i in range(len(user_input_test)):\n",
        "                #eval_model(users, items, users_inputs, items_inputs, dropout_rate, predict_rating, sess, user_tests[i], item_tests[i], user_input_test[i], item_input_test[i], rating_input_test[i], mses, maes)\n",
        "                predicts_test = sess.run(predict_rating, feed_dict={users: user_tests[i], items: item_tests[i], users_inputs: user_input_test[i], items_inputs: item_input_test[i], ratings: rating_input_test[i], dropout_rate: drop_out})\n",
        "                #print(predicts)\n",
        "                row, col = predicts_test.shape\n",
        "                for r in range(row):\n",
        "                    mses.append(pow((predicts_test[r, 0] - rating_input_test[i][r][0]), 2))\n",
        "                    maes.append(abs((predicts_test[r, 0] - rating_input_test[i][r][0])))\n",
        "                    \n",
        "            mse = np.array(mses).mean()\n",
        "            mae = np.array(maes).mean()\n",
        "            t3 = time()\n",
        "            mae_list.append(mae)\n",
        "            mse_list.append(mse)\n",
        "            print(\"epoch%d train time: %.3fs test time: %.3f  loss = %.3f val_mse = %.3f mse = %.3f mae = %.3f\"%(e, (t1 - t), (t3 - t2), loss_total/count, val_mse, mse, mae))\n",
        "        \n",
        "        avg_mae=sum(mae_list) / len(mae_list)\n",
        "        avg_mse=sum(mse_list)/len(mse_list)\n",
        "        avg_mae_list.append(avg_mae)\n",
        "        avg_mse_list.append(avg_mse)\n",
        "        print(\"MAE \"+str(avg_mae))\n",
        "        print(\"MSE \"+str(avg_mse))"
      ],
      "metadata": {
        "id": "B_HhPyCgJ0OO"
      },
      "id": "B_HhPyCgJ0OO",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Analysis_CARL_GoodReads",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}