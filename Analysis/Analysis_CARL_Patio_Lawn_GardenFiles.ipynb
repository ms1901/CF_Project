{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ms1901/CF_Project/blob/main/Analysis/Analysis_CARL_Patio_Lawn_GardenFiles.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4f364b6b",
        "outputId": "d86b62a8-52ca-4269-e966-5af048f99197"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.7/dist-packages (2.8.0)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (14.0.1)\n",
            "Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.14.0)\n",
            "Requirement already satisfied: keras<2.9,>=2.8.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.8.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.17.3)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.25.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.15.0)\n",
            "Requirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.5.3)\n",
            "Collecting tf-estimator-nightly==2.8.0.dev2021122109\n",
            "  Downloading tf_estimator_nightly-2.8.0.dev2021122109-py2.py3-none-any.whl (462 kB)\n",
            "\u001b[K     |████████████████████████████████| 462 kB 5.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tensorflow) (57.4.0)\n",
            "Requirement already satisfied: tensorboard<2.9,>=2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.8.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.21.6)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.44.0)\n",
            "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.0.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (4.2.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow) (0.37.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow) (1.5.2)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (3.3.6)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (0.4.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (2.23.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (0.6.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (1.35.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (4.8)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (4.2.4)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow) (4.11.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow) (3.8.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (0.4.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (1.24.3)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow) (3.2.0)\n",
            "Installing collected packages: tf-estimator-nightly\n",
            "Successfully installed tf-estimator-nightly-2.8.0.dev2021122109\n"
          ]
        }
      ],
      "source": [
        "pip install --upgrade tensorflow"
      ],
      "id": "4f364b6b"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ac9149d"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import scipy.sparse as sp\n",
        "import math\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import sys\n",
        "from time import time\n",
        "import os\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ],
      "id": "2ac9149d"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q5LbbrqP0Ilv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef93d68d-0655-492d-a0dc-e59e1b1b5970"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "id": "Q5LbbrqP0Ilv"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RVxfHDx8tGaF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d06dbdd-f044-4fbd-ff44-d682c771e6f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<module 'tensorflow._api.v2.version' from '/usr/local/lib/python3.7/dist-packages/tensorflow/_api/v2/version/__init__.py'>\n"
          ]
        }
      ],
      "source": [
        "print(tf.version)"
      ],
      "id": "RVxfHDx8tGaF"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZdenhkrjwCSg",
        "outputId": "6657a72f-bcb5-4e71-d9a5-9de2c9dafb31"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/.shortcut-targets-by-id/1en-KMQxlwEqh5Cl15reV67taZO_Ns7Pw/CF_end_proejct/Patio_Lawn_GardenFiles\n"
          ]
        }
      ],
      "source": [
        "%cd /content/drive/MyDrive/CF_end_proejct/Patio_Lawn_GardenFiles"
      ],
      "id": "ZdenhkrjwCSg"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RGS6qYUmtLs_",
        "outputId": "038cd111-37c8-497c-8944-cd0faa97534c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['/device:CPU:0', '/device:GPU:0']\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.python.client import device_lib\n",
        "\n",
        "def get_available_devices():\n",
        "    local_device_protos = device_lib.list_local_devices()\n",
        "    return [x.name for x in local_device_protos]\n",
        "\n",
        "print(get_available_devices())"
      ],
      "id": "RGS6qYUmtLs_"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "590f617f"
      },
      "source": [
        "ExtractData"
      ],
      "id": "590f617f"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4c63801c"
      },
      "outputs": [],
      "source": [
        "class Dataset(object):\n",
        "    \"'extract dataset from file'\"\n",
        "\n",
        "    def __init__(self, max_length, path, word_id_path):\n",
        "        self.word_id_dict = self.load_word_dict(path + word_id_path)\n",
        "        print(\"wordId_dict finished\")\n",
        "        \n",
        "        self.userReview_dict = self.load_reviews(max_length, len(self.word_id_dict), path + \"dataPreprocessingUserReviews.out\")\n",
        "        self.itemReview_dict = self.load_reviews(max_length, len(self.word_id_dict), path + \"dataPreprocessingItemReviews.out\")\n",
        "        print(\"load reviews finished\")\n",
        "        \n",
        "        self.num_users, self.num_items = len(self.userReview_dict), len(self.itemReview_dict)\n",
        "        \n",
        "        self.trainMtrx = self.load_ratingFile_as_mtrx(path + \"dataPreprocessingTrainInteraction.out\")\n",
        "        self.valRatings = self.load_ratingFile_as_list(path + \"dataPreprocessingValInteraction.out\")\n",
        "        self.testRatings = self.load_ratingFile_as_list(path + \"dataPreprocessingTestInteraction.out\")\n",
        "\n",
        "    def load_word_dict(self, path):\n",
        "        wordId_dict = {}\n",
        "\n",
        "        with open(path, \"r\") as f:\n",
        "            line = f.readline().replace(\"\\n\", \"\")\n",
        "            while line != None and line != \"\":\n",
        "                arr = line.split(\"\\t\")\n",
        "                wordId_dict[arr[0]] = int(arr[1])\n",
        "                line = f.readline().replace(\"\\n\", \"\")\n",
        "\n",
        "        return wordId_dict\n",
        "\n",
        "    def load_reviews(self, max_doc_length, padding_word_id, path):\n",
        "        entity_review_dict = {}\n",
        "\n",
        "        with open(path, \"r\") as f:\n",
        "            line = f.readline().replace(\"\\n\", \"\")\n",
        "            while line != None and line != \"\":\n",
        "                review = []\n",
        "                arr = line.split(\"\\t\")\n",
        "                entity = int(arr[0])\n",
        "                word_list = arr[1].split(\" \")\n",
        "\n",
        "                for i in range(len(word_list)):\n",
        "                    if (word_list[i] == \"\" or word_list[i] == None or (not self.word_id_dict[word_list[i]] in self.word_id_dict.keys())):\n",
        "                        continue\n",
        "                    review.append(self.word_id_dict.get(word_list[i]))\n",
        "                    if (len(review) >= max_doc_length):\n",
        "                        break\n",
        "                if (len(review) < max_doc_length):\n",
        "                    review = self.padding_word(max_doc_length, padding_word_id, review)\n",
        "                entity_review_dict[entity] = review\n",
        "                line = f.readline().replace(\"\\n\", \"\")\n",
        "        return entity_review_dict\n",
        "\n",
        "    def padding_word(self, max_size, max_word_idx, review):\n",
        "        review.extend([max_word_idx]*(max_size - len(review)))\n",
        "        return review\n",
        "\n",
        "    def load_ratingFile_as_mtrx(self, file_path):\n",
        "        mtrx = sp.dok_matrix((self.num_users, self.num_items), dtype=np.float32)\n",
        "        with open(file_path, \"r\") as f:\n",
        "            line = f.readline()\n",
        "            line = line.strip()\n",
        "            while line != None and line != \"\":\n",
        "                arr = line.split(\"\\t\")\n",
        "                user, item, rating = int(arr[0]), int(arr[1]), float(arr[2])\n",
        "                if (rating > 0):\n",
        "                    mtrx[user, item] = rating\n",
        "                line = f.readline()\n",
        "\n",
        "        return mtrx\n",
        "\n",
        "    def load_ratingFile_as_list(self, file_path):\n",
        "        rateList = []\n",
        "\n",
        "        with open(file_path, \"r\") as f:\n",
        "            line = f.readline()\n",
        "            while line != None and line != \"\":\n",
        "                arr = line.split(\"\\t\")\n",
        "                user, item = int(arr[0]), int(arr[1])\n",
        "                rate = float(arr[2])\n",
        "                rateList.append([user, item, rate])\n",
        "                line = f.readline()\n",
        "\n",
        "        return rateList"
      ],
      "id": "4c63801c"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "77e2f302"
      },
      "source": [
        "GetTest"
      ],
      "id": "77e2f302"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "523a402f"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "def get_test_list(batch_size, test_rating, user_reviews, item_reviews):\n",
        "    user_test_batchs, item_test_batchs, user_input_test_batchs, item_input_test_batchs, rating_input_test_batchs = [], [], [], [], []\n",
        "    for count in range(int(math.ceil(len(test_rating) / float(batch_size)))):\n",
        "        user_test, item_test, user_input_test, item_input_test, rating_input_test = [], [], [], [], []\n",
        "        for idx in range(batch_size):\n",
        "            index = (count * batch_size + idx)\n",
        "            if (index >= len(test_rating)):\n",
        "                break\n",
        "            rating = test_rating[index]\n",
        "            user_test.append(rating[0])\n",
        "            item_test.append(rating[1])\n",
        "            user_input_test.append(user_reviews.get(rating[0]))\n",
        "            item_input_test.append(item_reviews.get(rating[1]))\n",
        "            rating_input_test.append([rating[2]])\n",
        "        user_test_batchs.append(user_test)\n",
        "        item_test_batchs.append(item_test)\n",
        "        user_input_test_batchs.append(user_input_test)\n",
        "        item_input_test_batchs.append(item_input_test)\n",
        "        rating_input_test_batchs.append(rating_input_test)\n",
        "        #print count, len(item_input_test_batchs[count])\n",
        "    return user_test_batchs, item_test_batchs, user_input_test_batchs, item_input_test_batchs, rating_input_test_batchs"
      ],
      "id": "523a402f"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "74573912"
      },
      "source": [
        "CARL"
      ],
      "id": "74573912"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "70fbf310"
      },
      "outputs": [],
      "source": [
        "def ini_word_embed(num_words, latent_dim):\n",
        "    word_embeds = np.random.rand(num_words, latent_dim)\n",
        "    return word_embeds\n"
      ],
      "id": "70fbf310"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "15e0a2c1"
      },
      "outputs": [],
      "source": [
        "def word2vec_word_embed(num_words, latent_dim, path, word_id_dict):\n",
        "    word2vect_embed_mtrx = np.zeros((num_words, latent_dim))\n",
        "    with open(path, \"r\") as f:\n",
        "        line = f.readline()\n",
        "        while line != None and line != \"\":\n",
        "            arr = line.split(\"\\t\")\n",
        "            row_id = word_id_dict.get(arr[0])\n",
        "            vect = arr[1].strip().split(\" \")\n",
        "            for i in range(len(vect)):\n",
        "                word2vect_embed_mtrx[row_id, i] = float(vect[i])\n",
        "            line = f.readline()\n",
        "\n",
        "    return word2vect_embed_mtrx"
      ],
      "id": "15e0a2c1"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cabc3fef"
      },
      "outputs": [],
      "source": [
        "def get_train_instance(train):\n",
        "    user_input, item_input, rates = [], [], []\n",
        "\n",
        "    for (u, i) in train.keys():\n",
        "        # positive instance\n",
        "        user_input.append(u)\n",
        "        item_input.append(i)\n",
        "        rates.append(train[u,i])\n",
        "    return user_input, item_input, rates\n",
        "\n"
      ],
      "id": "cabc3fef"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e6abcbc9"
      },
      "outputs": [],
      "source": [
        "def get_train_instance_batch_change(count, batch_size, user_input, item_input, ratings, user_reviews, item_reviews):\n",
        "    users_batch, items_batch, user_input_batch, item_input_batch, labels_batch = [], [], [], [], []\n",
        "\n",
        "    for idx in range(batch_size):\n",
        "        index = (count*batch_size + idx) % len(user_input)\n",
        "        users_batch.append(user_input[index])\n",
        "        items_batch.append(item_input[index])\n",
        "        user_input_batch.append(user_reviews.get(user_input[index]))\n",
        "        item_input_batch.append(item_reviews.get(item_input[index]))\n",
        "        labels_batch.append([ratings[index]])\n",
        "\n",
        "    return users_batch, items_batch, user_input_batch, item_input_batch, labels_batch"
      ],
      "id": "e6abcbc9"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "38ffbde1"
      },
      "outputs": [],
      "source": [
        "#review.py\n",
        "def  cnn_model_average(filters, user_reviews_representation_expnd, item_reviews_representation_expnd, W_u, W_i, W_u_1, W_i_1, rand_matrix):\n",
        "    #convolution layer\n",
        "    convU = tf.nn.conv2d(user_reviews_representation_expnd, W_u, strides=[1, 1, word_latent_dim, 1], padding='SAME')\n",
        "    convI = tf.nn.conv2d(item_reviews_representation_expnd, W_i, strides=[1, 1, word_latent_dim, 1], padding='SAME')\n",
        "\n",
        "    hU = tf.nn.relu(tf.squeeze(convU, 2))\n",
        "    hI = tf.nn.relu(tf.squeeze(convI, 2))\n",
        "\n",
        "    # attentive layer\n",
        "    sec_dim = int(hU.get_shape()[1])\n",
        "    tmphU = tf.reshape(hU, [-1, filters])\n",
        "    hU_mul_rand = tf.reshape(tf.matmul(tmphU, rand_matrix), [-1, sec_dim, filters])\n",
        "    f = tf.matmul(hU_mul_rand, hI, transpose_b=True)\n",
        "    f = tf.expand_dims(f, -1)\n",
        "    att1 = tf.tanh(f)\n",
        "\n",
        "    pool_user = tf.reduce_mean(att1, 2)\n",
        "    pool_item = tf.reduce_mean(att1, 1)\n",
        "\n",
        "    user_flat = tf.squeeze(pool_user, -1)\n",
        "    item_flat = tf.squeeze(pool_item, -1)\n",
        "\n",
        "    weight_user = tf.nn.softmax(user_flat)\n",
        "    weight_item = tf.nn.softmax(item_flat)\n",
        "\n",
        "    weight_user_exp = tf.expand_dims(weight_user, -1)\n",
        "    weight_item_exp = tf.expand_dims(weight_item, -1)\n",
        "\n",
        "    hU = tf.expand_dims(hU * weight_user_exp, -1)\n",
        "    hI = tf.expand_dims(hI * weight_item_exp, -1)\n",
        "\n",
        "    #abstracting layer\n",
        "    hU_1 = tf.nn.relu(tf.nn.conv2d(hU, W_u_1, strides=[1, 1, 1, 1], padding='VALID'))\n",
        "    hI_1 = tf.nn.relu(tf.nn.conv2d(hI, W_i_1, strides=[1, 1, 1, 1], padding='VALID'))\n",
        "\n",
        "    sec_dim = hU_1.get_shape()[1]\n",
        "\n",
        "    oU = tf.nn.avg_pool(hU_1, ksize=[1, sec_dim, 1, 1], strides=[1, 1, 1, 1],\n",
        "        padding='VALID')\n",
        "    oI = tf.nn.avg_pool(hI_1, ksize=[1, sec_dim, 1, 1], strides=[1, 1, 1, 1],\n",
        "        padding='VALID')\n",
        "\n",
        "    att_user = tf.squeeze(oU)\n",
        "    att_item = tf.squeeze(oI)\n",
        "    #print \"attention\", att_user.get_shape(), att_item.get_shape()\n",
        "\n",
        "    return att_user, att_item"
      ],
      "id": "38ffbde1"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6967e9a6"
      },
      "outputs": [],
      "source": [
        "def eval_model(users, items, users_inputs, items_inputs, dropout_rate, predict_rating, sess, user_batch, item_batch, user_input_batch, item_input_batch, rate_tests, rmses, maes):\n",
        "    print(sess)\n",
        "    print(\"user batch. \"+str(user_batch))\n",
        "    print(\"item batch \"+str(item_batch))\n",
        "    print(\"user_input_batch. \"+str(user_input_batch))\n",
        "    print(\"item_input_batch \"+str( item_input_batch))\n",
        "\n",
        "    print(\"before session run\")\n",
        "    predicts = sess.run(predict_rating, feed_dict={users: user_batch, items: item_batch, users_inputs: user_input_batch, items_inputs: item_input_batch, dropout_rate:1.0})\n",
        "    print(\"after session run\")\n",
        "    #print(predicts)\n",
        "    row, col = predicts.shape\n",
        "    for r in range(row):\n",
        "        rmses.append(pow((predicts[r, 0] - rate_tests[r][0]), 2))\n",
        "        maes.append(abs((predicts[r, 0] - rate_tests[r][0])))\n",
        "    print(rmses)\n",
        "    return rmses, maes\n"
      ],
      "id": "6967e9a6"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Just interaction based"
      ],
      "metadata": {
        "id": "fvDJvR-tr2Ap"
      },
      "id": "fvDJvR-tr2Ap"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "55f18043"
      },
      "outputs": [],
      "source": [
        "#combination of interaction.py and review.py\n",
        "def train_model():\n",
        "    mae_list=[]\n",
        "    mse_list=[]\n",
        "    tf.compat.v1.disable_eager_execution()\n",
        "    #I and R-----\n",
        "    users = tf.compat.v1.placeholder(tf.int32, shape=[None])\n",
        "    items = tf.compat.v1.placeholder(tf.int32, shape=[None])\n",
        "    users_inputs = tf.compat.v1.placeholder(tf.int32, shape=[None, max_doc_length])\n",
        "    items_inputs = tf.compat.v1.placeholder(tf.int32, shape=[None, max_doc_length])\n",
        "    ratings = tf.compat.v1.placeholder(tf.float32, shape=[None, 1])\n",
        "    \n",
        "    #R-----\n",
        "    dropout_rate = tf.compat.v1.placeholder(tf.float32)\n",
        "    \n",
        "    #I-----\n",
        "    user_entity_embedding = tf.Variable(tf.random.normal([num_users, latent_dim], mean=0, stddev=0.02), name=\"user_entity_embeddings\")\n",
        "    item_entity_embedding = tf.Variable(tf.random.normal([num_items, latent_dim], mean=0, stddev=0.02), name=\"item_entity_embeddings\")\n",
        "    user_entity_embeds = tf.nn.embedding_lookup(user_entity_embedding, users)\n",
        "    item_entity_embeds = tf.nn.embedding_lookup(item_entity_embedding, items)\n",
        "    \n",
        "    #R-----\n",
        "    user_bias = tf.Variable(tf.random.normal([num_users, 1], mean=0, stddev=0.02), name=\"user_bias\")\n",
        "    item_bias = tf.Variable(tf.random.normal([num_items, 1], mean=0, stddev=0.02), name=\"item_bias\")\n",
        "    user_bs = tf.nn.embedding_lookup(user_bias, users)\n",
        "    item_bs = tf.nn.embedding_lookup(item_bias, items)\n",
        "    \n",
        "    text_embedding = tf.Variable(word_embedding_mtrx, dtype=tf.float32, name=\"review_text_embeds\")\n",
        "    padding_embedding = tf.Variable(np.zeros([1, word_latent_dim]), dtype=tf.float32)\n",
        "    text_mask = tf.constant([1.0] * text_embedding.get_shape()[0] + [0.0])\n",
        "    word_embeddings = tf.concat([text_embedding, padding_embedding], 0)\n",
        "    word_embeddings = word_embeddings * tf.expand_dims(text_mask, -1)\n",
        "    user_reviews_representation = tf.nn.embedding_lookup(word_embeddings, users_inputs)\n",
        "    user_reviews_representation_expnd = tf.expand_dims(user_reviews_representation, -1)\n",
        "    item_reviews_representation = tf.nn.embedding_lookup(word_embeddings, items_inputs)\n",
        "    item_reviews_representation_expnd = tf.expand_dims(item_reviews_representation, -1)\n",
        "    \n",
        "    # CNN layers\n",
        "    W_u = tf.Variable(tf.random.truncated_normal([window_size, word_latent_dim, 1, num_filters], stddev=0.3), name=\"review_W_u\")\n",
        "    W_i = tf.Variable(tf.random.truncated_normal([window_size, word_latent_dim, 1, num_filters], stddev=0.3), name=\"review_W_i\")\n",
        "    W_u_1 = tf.Variable(tf.random.truncated_normal([window_size, num_filters, 1, num_filters], stddev=0.3), name=\"review_W_u_1\")\n",
        "    W_i_1 = tf.Variable(tf.random.truncated_normal([window_size, num_filters, 1, num_filters], stddev=0.3), name=\"review_W_i_1\")\n",
        "    # b = tf.Variable(tf.constant(0.1, shape=[self.num_filters]))\n",
        "    rand_matrix = tf.Variable(tf.random.truncated_normal([num_filters, num_filters], stddev=0.3), name=\"review_rand_matrix\")\n",
        "\n",
        "    user_embeds, item_embeds = cnn_model_average(num_filters, user_reviews_representation_expnd, item_reviews_representation_expnd, W_u, W_i, W_u_1, W_i_1, rand_matrix)\n",
        "    \n",
        "    #shared MLP layer\n",
        "    W_mlp = tf.Variable(tf.random.normal([num_filters, latent_dim], mean=0, stddev=0.02), name=\"review_W_mlp\")\n",
        "    W_mlp = tf.nn.dropout(W_mlp, dropout_rate)\n",
        "    b_mlp = tf.Variable(tf.constant(0.1, shape=[latent_dim]), name=\"review_b_mlp\")\n",
        "\n",
        "    user_embeds = tf.nn.relu(tf.matmul(user_embeds, W_mlp) + b_mlp)\n",
        "    item_embeds = tf.nn.relu(tf.matmul(item_embeds, W_mlp) + b_mlp)\n",
        "\n",
        "    embeds_sum = tf.concat([tf.multiply(user_embeds, item_embeds), user_embeds, item_embeds], 1, name=\"concat_embed\")\n",
        "    #I\n",
        "    entity_embeds_sum = tf.concat([tf.multiply(user_entity_embeds, item_entity_embeds), user_entity_embeds, item_entity_embeds],1)\n",
        "\n",
        "    #FM layer\n",
        "    #R---\n",
        "    # w_0 = tf.Variable(tf.zeros(1), name=\"review_w_0\")\n",
        "    # w_1 = tf.Variable(tf.random.truncated_normal([1, latent_dim * 3], stddev=0.3), name=\"review_w_1\")\n",
        "    # v = tf.Variable(tf.random.truncated_normal([latent_dim * 3, latent_dim * 3], stddev=0.3), name=\"review_v\")\n",
        "    \n",
        "    #I---\n",
        "    w_entity_0 = tf.Variable(tf.zeros(1), name=\"entity_w_0\")\n",
        "    w_entity_1 = tf.Variable(tf.random.truncated_normal([1, latent_dim*3], stddev=0.3), name=\"entity_w_1\")\n",
        "    v_entity = tf.Variable(tf.random.truncated_normal([latent_dim*3, v_dim], stddev=0.3), name=\"entity_v\")\n",
        "    \n",
        "    #I---\n",
        "    J_e_1 = w_entity_0 + tf.matmul(entity_embeds_sum, w_entity_1, transpose_b=True)\n",
        "    #R---\n",
        "    # J_1 = w_0 + tf.matmul(embeds_sum, w_1, transpose_b=True)\n",
        "    \n",
        "    #I---\n",
        "    entity_embeds_sum_1 = tf.expand_dims(entity_embeds_sum, -1)\n",
        "    entity_embeds_sum_2 = tf.expand_dims(entity_embeds_sum, 1)\n",
        "    #R---\n",
        "    # embeds_sum_1 = tf.expand_dims(embeds_sum, -1)\n",
        "    # embeds_sum_2 = tf.expand_dims(embeds_sum, 1)\n",
        "    \n",
        "    #I---\n",
        "    J_e_2 = tf.reduce_sum(tf.reduce_sum(tf.multiply(tf.matmul(entity_embeds_sum_1, entity_embeds_sum_2), tf.matmul(v_entity, v_entity, transpose_b=True)),2), 1, keepdims=True)\n",
        "    #J_e_3 = tf.trace(tf.multiply(tf.matmul(entity_embeds_sum_1, entity_embeds_sum_2), tf.matmul(v_entity, v_entity, transpose_b=True))) \n",
        "    \n",
        "    #R---\n",
        "    # J_2 = tf.reduce_sum(tf.reduce_sum(tf.multiply(tf.matmul(embeds_sum_1, embeds_sum_2), tf.matmul(v, v, transpose_b=True)),2), 1, keepdims=True)\n",
        "    #J_3 = tf.trace(tf.multiply(tf.matmul(embeds_sum_1, embeds_sum_2), tf.matmul(v, v, transpose_b=True)))\n",
        "    #R---\n",
        "    # J_total = (J_1 + 0.5 * (J_2))\n",
        "    \n",
        "    #I---\n",
        "    J_e_total = (J_e_1 + 0.5 * (J_e_2))\n",
        "    \n",
        "    \n",
        "    # numerator = J_total + J_e_total\n",
        "    numerator = J_e_total\n",
        "    # predict_rating = tf.divide(J_total, numerator) * J_total + tf.divide(J_e_total,numerator) * J_e_total + user_bs + item_bs\n",
        "    predict_rating = tf.divide(J_e_total,numerator) * J_e_total + user_bs + item_bs \n",
        "    loss = tf.reduce_mean(tf.math.squared_difference(predict_rating, ratings))\n",
        "    loss += lambda_1 * (tf.nn.l2_loss(W_i_1) + tf.nn.l2_loss(W_u_1) + tf.nn.l2_loss(user_entity_embedding) + tf.nn.l2_loss(item_entity_embedding) + tf.nn.l2_loss(W_u) + tf.nn.l2_loss(W_i) + tf.nn.l2_loss(v_entity) + tf.nn.l2_loss(rand_matrix) + tf.nn.l2_loss(user_bs) + tf.nn.l2_loss(item_bs))\n",
        "    train_step = tf.compat.v1.train.RMSPropOptimizer(learning_rate).minimize(loss)\n",
        "    saver = tf.compat.v1.train.Saver()\n",
        "    \n",
        "    init = tf.compat.v1.global_variables_initializer()\n",
        "    with tf.compat.v1.Session() as sess:\n",
        "        sess.run(init)\n",
        "\n",
        "        for e in range(epochs):\n",
        "            t = time()\n",
        "            loss_total = 0.0\n",
        "            count = 0.0\n",
        "            \n",
        "            for i in range(int(math.ceil(len(user_input) / float(batch_size)))):\n",
        "                user_batch, item_batch, user_input_batch, item_input_batch, rates_batch = get_train_instance_batch_change(i, batch_size,user_input, item_input, rateings, user_reviews,item_reviews)\n",
        "                _, loss_val, words = sess.run([train_step, loss, word_embeddings], feed_dict={users: user_batch, items: item_batch, users_inputs: user_input_batch, items_inputs: item_input_batch, ratings: rates_batch, dropout_rate:drop_out})\n",
        "                loss_total += loss_val\n",
        "                count += 1.0\n",
        "\n",
        "            t1 = time()            \n",
        "            #LOOK FROM HERE \n",
        "            val_mses, val_maes = [], []\n",
        "            for i in range(len(user_input_val)):\n",
        "                #eval_model(users, items, users_inputs, items_inputs, dropout_rate, predict_rating, sess, user_vals[i], item_vals[i], user_input_val[i], item_input_val[i], rating_input_val[i], val_mses, val_maes)\n",
        "                #predicts = sess.run(predict_rating, feed_dict={users: user_batch, items: item_batch, users_inputs: user_input_batch, items_inputs: item_input_batch, dropout_rate:1.0})\n",
        "                predicts = sess.run(predict_rating, feed_dict={users: user_vals[i], items: item_vals[i], users_inputs: user_input_val[i], items_inputs: item_input_val[i], ratings: rating_input_val[i], dropout_rate: drop_out})\n",
        "                #print(predicts)\n",
        "                row, col = predicts.shape\n",
        "                for r in range(row):\n",
        "                    val_mses.append(pow((predicts[r, 0] - rating_input_val[i][r][0]), 2))\n",
        "                    val_maes.append(abs((predicts[r, 0] - rating_input_val[i][r][0])))\n",
        "            val_mse = np.array(val_mses).mean()            \n",
        "            t2 = time()            \n",
        "            mses, maes = [], []\n",
        "            for i in range(len(user_input_test)):\n",
        "                #eval_model(users, items, users_inputs, items_inputs, dropout_rate, predict_rating, sess, user_tests[i], item_tests[i], user_input_test[i], item_input_test[i], rating_input_test[i], mses, maes)\n",
        "                predicts_test = sess.run(predict_rating, feed_dict={users: user_tests[i], items: item_tests[i], users_inputs: user_input_test[i], items_inputs: item_input_test[i], ratings: rating_input_test[i], dropout_rate: drop_out})\n",
        "                #print(predicts)\n",
        "                row, col = predicts_test.shape\n",
        "                for r in range(row):\n",
        "                    mses.append(pow((predicts_test[r, 0] - rating_input_test[i][r][0]), 2))\n",
        "                    maes.append(abs((predicts_test[r, 0] - rating_input_test[i][r][0])))\n",
        "                    \n",
        "            mse = np.array(mses).mean()\n",
        "            mae = np.array(maes).mean()\n",
        "            t3 = time()\n",
        "            mae_list.append(mae)\n",
        "            mse_list.append(mse)\n",
        "            print(\"epoch%d train time: %.3fs test time: %.3f  loss = %.3f val_mse = %.3f mse = %.3f mae = %.3f\"%(e, (t1 - t), (t3 - t2), loss_total/count, val_mse, mse, mae))\n",
        "        \n",
        "        avg_mae=sum(mae_list) / len(mae_list)\n",
        "        avg_mse=sum(mse_list)/len(mse_list)\n",
        "        avg_mae_list.append(avg_mae)\n",
        "        avg_mse_list.append(avg_mse)\n",
        "        print(\"MAE \"+str(avg_mae))\n",
        "        print(\"MSE \"+str(avg_mse))"
      ],
      "id": "55f18043"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a924fe38",
        "outputId": "22cbb82e-af93-4933-848c-90df0da133bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "wordId_dict finished\n",
            "load reviews finished\n",
            "load data: 2.553s\n",
            "1686 962\n",
            "15\n",
            "shape (18584, 300)\n",
            "10612 10612 10612\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/training/rmsprop.py:188: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "epoch0 train time: 2.955s test time: 0.099  loss = 208.687 val_mse = 17.669 mse = 17.713 mae = 4.062\n",
            "epoch1 train time: 0.566s test time: 0.094  loss = 188.023 val_mse = 16.627 mse = 16.698 mae = 3.933\n",
            "epoch2 train time: 0.535s test time: 0.104  loss = 140.465 val_mse = 15.011 mse = 15.134 mae = 3.720\n",
            "epoch3 train time: 0.698s test time: 0.175  loss = 98.361 val_mse = 12.433 mse = 12.661 mae = 3.339\n",
            "epoch4 train time: 0.929s test time: 0.252  loss = 66.402 val_mse = 8.802 mse = 9.209 mae = 2.688\n",
            "epoch5 train time: 1.027s test time: 0.173  loss = 43.106 val_mse = 6.027 mse = 6.604 mae = 2.119\n",
            "epoch6 train time: 0.933s test time: 0.216  loss = 27.338 val_mse = 4.376 mse = 5.075 mae = 1.770\n",
            "epoch7 train time: 1.023s test time: 0.278  loss = 17.217 val_mse = 3.323 mse = 4.083 mae = 1.558\n",
            "epoch8 train time: 1.254s test time: 0.210  loss = 11.091 val_mse = 2.626 mse = 3.394 mae = 1.412\n",
            "epoch9 train time: 1.099s test time: 0.375  loss = 7.660 val_mse = 2.182 mse = 2.908 mae = 1.309\n",
            "epoch10 train time: 1.337s test time: 0.189  loss = 5.940 val_mse = 1.895 mse = 2.558 mae = 1.235\n",
            "epoch11 train time: 1.194s test time: 0.218  loss = 5.192 val_mse = 1.710 mse = 2.312 mae = 1.181\n",
            "epoch12 train time: 1.049s test time: 0.194  loss = 4.889 val_mse = 1.594 mse = 2.136 mae = 1.142\n",
            "epoch13 train time: 1.054s test time: 0.187  loss = 4.716 val_mse = 1.517 mse = 2.014 mae = 1.115\n",
            "epoch14 train time: 0.968s test time: 0.221  loss = 4.576 val_mse = 1.468 mse = 1.925 mae = 1.096\n",
            "epoch15 train time: 0.870s test time: 0.181  loss = 4.455 val_mse = 1.433 mse = 1.864 mae = 1.084\n",
            "epoch16 train time: 1.002s test time: 0.177  loss = 4.345 val_mse = 1.411 mse = 1.819 mae = 1.075\n",
            "epoch17 train time: 1.337s test time: 0.233  loss = 4.241 val_mse = 1.394 mse = 1.788 mae = 1.070\n",
            "epoch18 train time: 1.066s test time: 0.263  loss = 4.140 val_mse = 1.382 mse = 1.764 mae = 1.067\n",
            "epoch19 train time: 1.027s test time: 0.171  loss = 4.041 val_mse = 1.372 mse = 1.746 mae = 1.065\n",
            "epoch20 train time: 0.934s test time: 0.181  loss = 3.943 val_mse = 1.366 mse = 1.734 mae = 1.065\n",
            "epoch21 train time: 0.894s test time: 0.197  loss = 3.845 val_mse = 1.360 mse = 1.724 mae = 1.065\n",
            "epoch22 train time: 0.841s test time: 0.169  loss = 3.745 val_mse = 1.358 mse = 1.717 mae = 1.066\n",
            "epoch23 train time: 0.823s test time: 0.180  loss = 3.645 val_mse = 1.355 mse = 1.711 mae = 1.067\n",
            "epoch24 train time: 0.833s test time: 0.213  loss = 3.544 val_mse = 1.355 mse = 1.707 mae = 1.069\n",
            "epoch25 train time: 1.070s test time: 0.181  loss = 3.441 val_mse = 1.355 mse = 1.703 mae = 1.071\n",
            "epoch26 train time: 0.808s test time: 0.170  loss = 3.336 val_mse = 1.356 mse = 1.701 mae = 1.073\n",
            "epoch27 train time: 1.047s test time: 0.183  loss = 3.230 val_mse = 1.357 mse = 1.698 mae = 1.076\n",
            "epoch28 train time: 0.531s test time: 0.096  loss = 3.121 val_mse = 1.359 mse = 1.696 mae = 1.078\n",
            "epoch29 train time: 0.505s test time: 0.097  loss = 3.010 val_mse = 1.360 mse = 1.692 mae = 1.081\n",
            "epoch30 train time: 0.521s test time: 0.093  loss = 2.896 val_mse = 1.362 mse = 1.688 mae = 1.083\n",
            "epoch31 train time: 0.505s test time: 0.103  loss = 2.780 val_mse = 1.362 mse = 1.682 mae = 1.085\n",
            "epoch32 train time: 0.502s test time: 0.095  loss = 2.662 val_mse = 1.360 mse = 1.674 mae = 1.085\n",
            "epoch33 train time: 0.507s test time: 0.093  loss = 2.541 val_mse = 1.354 mse = 1.660 mae = 1.083\n",
            "epoch34 train time: 0.505s test time: 0.092  loss = 2.420 val_mse = 1.339 mse = 1.638 mae = 1.078\n",
            "epoch35 train time: 0.517s test time: 0.092  loss = 2.302 val_mse = 1.311 mse = 1.604 mae = 1.067\n",
            "epoch36 train time: 0.508s test time: 0.107  loss = 2.191 val_mse = 1.272 mse = 1.561 mae = 1.051\n",
            "epoch37 train time: 0.503s test time: 0.101  loss = 2.091 val_mse = 1.230 mse = 1.514 mae = 1.033\n",
            "epoch38 train time: 0.512s test time: 0.091  loss = 1.999 val_mse = 1.189 mse = 1.468 mae = 1.015\n",
            "epoch39 train time: 0.506s test time: 0.095  loss = 1.915 val_mse = 1.151 mse = 1.424 mae = 0.998\n",
            "epoch40 train time: 0.508s test time: 0.092  loss = 1.837 val_mse = 1.115 mse = 1.384 mae = 0.982\n",
            "epoch41 train time: 0.516s test time: 0.096  loss = 1.764 val_mse = 1.082 mse = 1.346 mae = 0.966\n",
            "epoch42 train time: 0.511s test time: 0.102  loss = 1.697 val_mse = 1.052 mse = 1.310 mae = 0.951\n",
            "epoch43 train time: 0.504s test time: 0.093  loss = 1.634 val_mse = 1.024 mse = 1.278 mae = 0.937\n",
            "epoch44 train time: 0.510s test time: 0.100  loss = 1.575 val_mse = 0.998 mse = 1.248 mae = 0.923\n",
            "epoch45 train time: 0.506s test time: 0.090  loss = 1.520 val_mse = 0.975 mse = 1.221 mae = 0.911\n",
            "epoch46 train time: 0.509s test time: 0.091  loss = 1.470 val_mse = 0.953 mse = 1.197 mae = 0.899\n",
            "epoch47 train time: 0.509s test time: 0.093  loss = 1.422 val_mse = 0.933 mse = 1.174 mae = 0.888\n",
            "epoch48 train time: 0.503s test time: 0.093  loss = 1.378 val_mse = 0.915 mse = 1.153 mae = 0.877\n",
            "epoch49 train time: 0.511s test time: 0.096  loss = 1.337 val_mse = 0.898 mse = 1.134 mae = 0.868\n",
            "epoch50 train time: 0.507s test time: 0.102  loss = 1.299 val_mse = 0.883 mse = 1.117 mae = 0.859\n",
            "epoch51 train time: 0.499s test time: 0.092  loss = 1.263 val_mse = 0.868 mse = 1.102 mae = 0.850\n",
            "epoch52 train time: 0.523s test time: 0.097  loss = 1.230 val_mse = 0.855 mse = 1.087 mae = 0.842\n",
            "epoch53 train time: 0.519s test time: 0.094  loss = 1.199 val_mse = 0.843 mse = 1.074 mae = 0.834\n",
            "epoch54 train time: 0.510s test time: 0.095  loss = 1.171 val_mse = 0.833 mse = 1.062 mae = 0.827\n",
            "epoch55 train time: 0.501s test time: 0.095  loss = 1.144 val_mse = 0.823 mse = 1.051 mae = 0.820\n",
            "epoch56 train time: 0.510s test time: 0.098  loss = 1.119 val_mse = 0.814 mse = 1.042 mae = 0.814\n",
            "epoch57 train time: 0.520s test time: 0.093  loss = 1.096 val_mse = 0.805 mse = 1.033 mae = 0.807\n",
            "epoch58 train time: 0.510s test time: 0.093  loss = 1.075 val_mse = 0.797 mse = 1.025 mae = 0.802\n",
            "epoch59 train time: 0.505s test time: 0.099  loss = 1.055 val_mse = 0.789 mse = 1.018 mae = 0.796\n",
            "epoch60 train time: 0.511s test time: 0.091  loss = 1.036 val_mse = 0.782 mse = 1.011 mae = 0.791\n",
            "epoch61 train time: 0.505s test time: 0.104  loss = 1.019 val_mse = 0.775 mse = 1.006 mae = 0.786\n",
            "epoch62 train time: 0.506s test time: 0.093  loss = 1.004 val_mse = 0.770 mse = 1.001 mae = 0.781\n",
            "epoch63 train time: 0.508s test time: 0.094  loss = 0.989 val_mse = 0.765 mse = 0.997 mae = 0.777\n",
            "epoch64 train time: 0.515s test time: 0.092  loss = 0.976 val_mse = 0.761 mse = 0.993 mae = 0.774\n",
            "epoch65 train time: 0.513s test time: 0.098  loss = 0.963 val_mse = 0.756 mse = 0.990 mae = 0.770\n",
            "epoch66 train time: 0.516s test time: 0.091  loss = 0.952 val_mse = 0.752 mse = 0.986 mae = 0.767\n",
            "epoch67 train time: 0.502s test time: 0.094  loss = 0.942 val_mse = 0.747 mse = 0.985 mae = 0.764\n",
            "epoch68 train time: 0.510s test time: 0.095  loss = 0.932 val_mse = 0.745 mse = 0.982 mae = 0.762\n",
            "epoch69 train time: 0.511s test time: 0.106  loss = 0.923 val_mse = 0.741 mse = 0.981 mae = 0.760\n",
            "epoch70 train time: 0.505s test time: 0.099  loss = 0.915 val_mse = 0.738 mse = 0.978 mae = 0.758\n",
            "epoch71 train time: 0.511s test time: 0.099  loss = 0.908 val_mse = 0.735 mse = 0.979 mae = 0.756\n",
            "epoch72 train time: 0.513s test time: 0.097  loss = 0.901 val_mse = 0.733 mse = 0.976 mae = 0.755\n",
            "epoch73 train time: 0.523s test time: 0.090  loss = 0.894 val_mse = 0.732 mse = 0.977 mae = 0.753\n",
            "epoch74 train time: 0.513s test time: 0.093  loss = 0.889 val_mse = 0.729 mse = 0.975 mae = 0.752\n",
            "epoch75 train time: 0.504s test time: 0.092  loss = 0.883 val_mse = 0.726 mse = 0.976 mae = 0.751\n",
            "epoch76 train time: 0.504s test time: 0.094  loss = 0.878 val_mse = 0.727 mse = 0.975 mae = 0.750\n",
            "epoch77 train time: 0.509s test time: 0.092  loss = 0.874 val_mse = 0.723 mse = 0.976 mae = 0.749\n",
            "epoch78 train time: 0.514s test time: 0.098  loss = 0.869 val_mse = 0.721 mse = 0.975 mae = 0.749\n",
            "epoch79 train time: 0.516s test time: 0.092  loss = 0.865 val_mse = 0.721 mse = 0.976 mae = 0.748\n",
            "epoch80 train time: 0.513s test time: 0.101  loss = 0.861 val_mse = 0.718 mse = 0.975 mae = 0.747\n",
            "epoch81 train time: 0.516s test time: 0.098  loss = 0.858 val_mse = 0.716 mse = 0.977 mae = 0.747\n",
            "epoch82 train time: 0.508s test time: 0.095  loss = 0.854 val_mse = 0.717 mse = 0.976 mae = 0.746\n",
            "epoch83 train time: 0.502s test time: 0.097  loss = 0.852 val_mse = 0.715 mse = 0.978 mae = 0.746\n",
            "epoch84 train time: 0.510s test time: 0.095  loss = 0.848 val_mse = 0.713 mse = 0.977 mae = 0.746\n",
            "epoch85 train time: 0.518s test time: 0.092  loss = 0.846 val_mse = 0.713 mse = 0.978 mae = 0.746\n",
            "epoch86 train time: 0.519s test time: 0.091  loss = 0.843 val_mse = 0.710 mse = 0.978 mae = 0.745\n",
            "epoch87 train time: 0.516s test time: 0.094  loss = 0.841 val_mse = 0.711 mse = 0.979 mae = 0.746\n",
            "epoch88 train time: 0.504s test time: 0.092  loss = 0.839 val_mse = 0.708 mse = 0.978 mae = 0.745\n",
            "epoch89 train time: 0.509s test time: 0.095  loss = 0.836 val_mse = 0.710 mse = 0.981 mae = 0.745\n",
            "epoch90 train time: 0.508s test time: 0.092  loss = 0.834 val_mse = 0.706 mse = 0.981 mae = 0.745\n",
            "epoch91 train time: 0.498s test time: 0.094  loss = 0.833 val_mse = 0.706 mse = 0.982 mae = 0.746\n",
            "epoch92 train time: 0.517s test time: 0.096  loss = 0.830 val_mse = 0.705 mse = 0.983 mae = 0.745\n",
            "epoch93 train time: 0.509s test time: 0.094  loss = 0.829 val_mse = 0.707 mse = 0.984 mae = 0.746\n",
            "epoch94 train time: 0.506s test time: 0.095  loss = 0.827 val_mse = 0.703 mse = 0.983 mae = 0.745\n",
            "epoch95 train time: 0.516s test time: 0.092  loss = 0.825 val_mse = 0.707 mse = 0.985 mae = 0.746\n",
            "epoch96 train time: 0.501s test time: 0.091  loss = 0.823 val_mse = 0.701 mse = 0.986 mae = 0.745\n",
            "epoch97 train time: 0.507s test time: 0.093  loss = 0.822 val_mse = 0.705 mse = 0.987 mae = 0.746\n",
            "epoch98 train time: 0.504s test time: 0.091  loss = 0.820 val_mse = 0.701 mse = 0.987 mae = 0.746\n",
            "epoch99 train time: 0.501s test time: 0.096  loss = 0.820 val_mse = 0.704 mse = 0.988 mae = 0.747\n",
            "epoch100 train time: 0.503s test time: 0.095  loss = 0.818 val_mse = 0.698 mse = 0.989 mae = 0.746\n",
            "epoch101 train time: 0.505s test time: 0.100  loss = 0.817 val_mse = 0.704 mse = 0.991 mae = 0.748\n",
            "epoch102 train time: 0.506s test time: 0.093  loss = 0.816 val_mse = 0.699 mse = 0.991 mae = 0.747\n",
            "epoch103 train time: 0.511s test time: 0.095  loss = 0.815 val_mse = 0.703 mse = 0.992 mae = 0.748\n",
            "epoch104 train time: 0.513s test time: 0.091  loss = 0.813 val_mse = 0.698 mse = 0.995 mae = 0.748\n",
            "epoch105 train time: 0.501s test time: 0.091  loss = 0.812 val_mse = 0.701 mse = 0.994 mae = 0.749\n",
            "epoch106 train time: 0.518s test time: 0.093  loss = 0.811 val_mse = 0.695 mse = 0.994 mae = 0.748\n",
            "epoch107 train time: 0.506s test time: 0.103  loss = 0.810 val_mse = 0.703 mse = 0.997 mae = 0.750\n",
            "epoch108 train time: 0.504s test time: 0.093  loss = 0.810 val_mse = 0.697 mse = 0.999 mae = 0.750\n",
            "epoch109 train time: 0.504s test time: 0.093  loss = 0.808 val_mse = 0.702 mse = 0.999 mae = 0.751\n",
            "epoch110 train time: 0.505s test time: 0.102  loss = 0.808 val_mse = 0.696 mse = 0.997 mae = 0.749\n",
            "epoch111 train time: 0.515s test time: 0.091  loss = 0.807 val_mse = 0.701 mse = 1.001 mae = 0.751\n",
            "epoch112 train time: 0.501s test time: 0.095  loss = 0.806 val_mse = 0.697 mse = 1.002 mae = 0.751\n",
            "epoch113 train time: 0.505s test time: 0.092  loss = 0.805 val_mse = 0.703 mse = 1.003 mae = 0.752\n",
            "epoch114 train time: 0.515s test time: 0.097  loss = 0.805 val_mse = 0.697 mse = 1.005 mae = 0.751\n",
            "epoch115 train time: 0.497s test time: 0.099  loss = 0.803 val_mse = 0.700 mse = 1.003 mae = 0.752\n",
            "epoch116 train time: 0.942s test time: 0.098  loss = 0.803 val_mse = 0.696 mse = 1.005 mae = 0.752\n",
            "epoch117 train time: 0.955s test time: 0.091  loss = 0.803 val_mse = 0.701 mse = 1.009 mae = 0.754\n",
            "epoch118 train time: 0.960s test time: 0.097  loss = 0.801 val_mse = 0.697 mse = 1.010 mae = 0.753\n",
            "epoch119 train time: 0.507s test time: 0.097  loss = 0.801 val_mse = 0.702 mse = 1.007 mae = 0.754\n",
            "epoch120 train time: 0.507s test time: 0.094  loss = 0.800 val_mse = 0.696 mse = 1.012 mae = 0.754\n",
            "epoch121 train time: 0.513s test time: 0.094  loss = 0.800 val_mse = 0.699 mse = 1.009 mae = 0.754\n",
            "epoch122 train time: 0.500s test time: 0.091  loss = 0.800 val_mse = 0.698 mse = 1.014 mae = 0.755\n",
            "epoch123 train time: 0.513s test time: 0.094  loss = 0.799 val_mse = 0.702 mse = 1.015 mae = 0.756\n",
            "epoch124 train time: 0.504s test time: 0.105  loss = 0.799 val_mse = 0.696 mse = 1.016 mae = 0.756\n",
            "epoch125 train time: 0.508s test time: 0.099  loss = 0.798 val_mse = 0.702 mse = 1.016 mae = 0.757\n",
            "epoch126 train time: 0.504s test time: 0.092  loss = 0.798 val_mse = 0.700 mse = 1.022 mae = 0.758\n",
            "epoch127 train time: 0.507s test time: 0.098  loss = 0.798 val_mse = 0.704 mse = 1.020 mae = 0.759\n",
            "epoch128 train time: 0.509s test time: 0.091  loss = 0.797 val_mse = 0.699 mse = 1.024 mae = 0.758\n",
            "epoch129 train time: 0.511s test time: 0.092  loss = 0.797 val_mse = 0.700 mse = 1.023 mae = 0.759\n",
            "epoch130 train time: 0.505s test time: 0.095  loss = 0.797 val_mse = 0.699 mse = 1.027 mae = 0.761\n",
            "epoch131 train time: 0.510s test time: 0.093  loss = 0.796 val_mse = 0.704 mse = 1.020 mae = 0.758\n",
            "epoch132 train time: 0.500s test time: 0.104  loss = 0.796 val_mse = 0.699 mse = 1.026 mae = 0.760\n",
            "epoch133 train time: 0.511s test time: 0.096  loss = 0.795 val_mse = 0.706 mse = 1.030 mae = 0.763\n",
            "epoch134 train time: 0.516s test time: 0.093  loss = 0.796 val_mse = 0.702 mse = 1.030 mae = 0.762\n",
            "epoch135 train time: 0.502s test time: 0.095  loss = 0.795 val_mse = 0.702 mse = 1.030 mae = 0.763\n",
            "epoch136 train time: 0.511s test time: 0.091  loss = 0.796 val_mse = 0.699 mse = 1.031 mae = 0.763\n",
            "epoch137 train time: 0.517s test time: 0.095  loss = 0.795 val_mse = 0.704 mse = 1.030 mae = 0.763\n",
            "epoch138 train time: 0.507s test time: 0.099  loss = 0.795 val_mse = 0.698 mse = 1.035 mae = 0.765\n",
            "epoch139 train time: 0.509s test time: 0.096  loss = 0.795 val_mse = 0.707 mse = 1.036 mae = 0.766\n",
            "epoch140 train time: 0.503s test time: 0.098  loss = 0.795 val_mse = 0.701 mse = 1.030 mae = 0.763\n",
            "epoch141 train time: 0.505s test time: 0.096  loss = 0.794 val_mse = 0.705 mse = 1.035 mae = 0.766\n",
            "epoch142 train time: 0.516s test time: 0.097  loss = 0.795 val_mse = 0.700 mse = 1.043 mae = 0.769\n",
            "epoch143 train time: 0.507s test time: 0.100  loss = 0.795 val_mse = 0.716 mse = 1.038 mae = 0.767\n",
            "epoch144 train time: 0.506s test time: 0.101  loss = 0.796 val_mse = 0.698 mse = 1.038 mae = 0.766\n",
            "epoch145 train time: 0.514s test time: 0.092  loss = 0.793 val_mse = 0.712 mse = 1.042 mae = 0.769\n",
            "epoch146 train time: 0.505s test time: 0.094  loss = 0.795 val_mse = 0.705 mse = 1.044 mae = 0.769\n",
            "epoch147 train time: 0.509s test time: 0.095  loss = 0.794 val_mse = 0.711 mse = 1.040 mae = 0.769\n",
            "epoch148 train time: 0.520s test time: 0.098  loss = 0.796 val_mse = 0.702 mse = 1.044 mae = 0.769\n",
            "epoch149 train time: 0.518s test time: 0.090  loss = 0.793 val_mse = 0.717 mse = 1.043 mae = 0.771\n",
            "epoch150 train time: 0.506s test time: 0.093  loss = 0.795 val_mse = 0.700 mse = 1.048 mae = 0.771\n",
            "epoch151 train time: 0.505s test time: 0.099  loss = 0.793 val_mse = 0.713 mse = 1.046 mae = 0.771\n",
            "epoch152 train time: 0.502s test time: 0.094  loss = 0.795 val_mse = 0.707 mse = 1.050 mae = 0.771\n",
            "epoch153 train time: 0.515s test time: 0.092  loss = 0.795 val_mse = 0.716 mse = 1.043 mae = 0.771\n",
            "epoch154 train time: 0.507s test time: 0.094  loss = 0.795 val_mse = 0.703 mse = 1.055 mae = 0.773\n",
            "epoch155 train time: 0.503s test time: 0.098  loss = 0.794 val_mse = 0.719 mse = 1.046 mae = 0.772\n",
            "epoch156 train time: 0.518s test time: 0.091  loss = 0.797 val_mse = 0.702 mse = 1.053 mae = 0.773\n",
            "epoch157 train time: 0.501s test time: 0.095  loss = 0.794 val_mse = 0.719 mse = 1.051 mae = 0.773\n",
            "epoch158 train time: 0.505s test time: 0.093  loss = 0.797 val_mse = 0.711 mse = 1.058 mae = 0.774\n",
            "epoch159 train time: 0.510s test time: 0.093  loss = 0.796 val_mse = 0.716 mse = 1.047 mae = 0.773\n",
            "epoch160 train time: 0.503s test time: 0.096  loss = 0.796 val_mse = 0.705 mse = 1.060 mae = 0.776\n",
            "epoch161 train time: 0.511s test time: 0.096  loss = 0.795 val_mse = 0.722 mse = 1.048 mae = 0.774\n",
            "epoch162 train time: 0.510s test time: 0.101  loss = 0.797 val_mse = 0.706 mse = 1.061 mae = 0.775\n",
            "epoch163 train time: 0.502s test time: 0.092  loss = 0.795 val_mse = 0.716 mse = 1.056 mae = 0.776\n",
            "epoch164 train time: 0.512s test time: 0.092  loss = 0.797 val_mse = 0.707 mse = 1.062 mae = 0.777\n",
            "epoch165 train time: 0.509s test time: 0.092  loss = 0.797 val_mse = 0.722 mse = 1.056 mae = 0.778\n",
            "epoch166 train time: 0.507s test time: 0.097  loss = 0.798 val_mse = 0.711 mse = 1.066 mae = 0.778\n",
            "epoch167 train time: 0.515s test time: 0.094  loss = 0.798 val_mse = 0.717 mse = 1.055 mae = 0.777\n",
            "epoch168 train time: 0.499s test time: 0.095  loss = 0.797 val_mse = 0.712 mse = 1.067 mae = 0.779\n",
            "epoch169 train time: 0.507s test time: 0.094  loss = 0.797 val_mse = 0.723 mse = 1.055 mae = 0.779\n",
            "epoch170 train time: 0.504s test time: 0.102  loss = 0.800 val_mse = 0.709 mse = 1.070 mae = 0.780\n",
            "epoch171 train time: 0.500s test time: 0.095  loss = 0.797 val_mse = 0.722 mse = 1.061 mae = 0.779\n",
            "epoch172 train time: 0.504s test time: 0.098  loss = 0.799 val_mse = 0.713 mse = 1.071 mae = 0.781\n",
            "epoch173 train time: 0.502s test time: 0.094  loss = 0.797 val_mse = 0.725 mse = 1.062 mae = 0.782\n",
            "epoch174 train time: 0.510s test time: 0.093  loss = 0.802 val_mse = 0.707 mse = 1.069 mae = 0.779\n",
            "epoch175 train time: 0.503s test time: 0.094  loss = 0.798 val_mse = 0.728 mse = 1.061 mae = 0.780\n",
            "epoch176 train time: 0.501s test time: 0.094  loss = 0.799 val_mse = 0.718 mse = 1.072 mae = 0.781\n",
            "epoch177 train time: 0.515s test time: 0.092  loss = 0.799 val_mse = 0.723 mse = 1.060 mae = 0.781\n",
            "epoch178 train time: 0.514s test time: 0.105  loss = 0.803 val_mse = 0.709 mse = 1.078 mae = 0.783\n",
            "epoch179 train time: 0.498s test time: 0.095  loss = 0.799 val_mse = 0.734 mse = 1.061 mae = 0.780\n",
            "epoch180 train time: 0.520s test time: 0.092  loss = 0.802 val_mse = 0.713 mse = 1.075 mae = 0.783\n",
            "epoch181 train time: 0.504s test time: 0.098  loss = 0.797 val_mse = 0.726 mse = 1.066 mae = 0.784\n",
            "epoch182 train time: 0.504s test time: 0.092  loss = 0.801 val_mse = 0.714 mse = 1.080 mae = 0.784\n",
            "epoch183 train time: 0.506s test time: 0.095  loss = 0.801 val_mse = 0.734 mse = 1.064 mae = 0.781\n",
            "epoch184 train time: 0.505s test time: 0.091  loss = 0.801 val_mse = 0.721 mse = 1.083 mae = 0.786\n",
            "epoch185 train time: 0.505s test time: 0.093  loss = 0.799 val_mse = 0.735 mse = 1.070 mae = 0.786\n",
            "epoch186 train time: 0.498s test time: 0.105  loss = 0.804 val_mse = 0.717 mse = 1.078 mae = 0.782\n",
            "epoch187 train time: 0.504s test time: 0.093  loss = 0.802 val_mse = 0.730 mse = 1.063 mae = 0.781\n",
            "epoch188 train time: 0.520s test time: 0.093  loss = 0.802 val_mse = 0.719 mse = 1.081 mae = 0.786\n",
            "epoch189 train time: 0.497s test time: 0.091  loss = 0.801 val_mse = 0.739 mse = 1.068 mae = 0.785\n",
            "epoch190 train time: 0.506s test time: 0.094  loss = 0.805 val_mse = 0.717 mse = 1.080 mae = 0.785\n",
            "epoch191 train time: 0.504s test time: 0.090  loss = 0.803 val_mse = 0.738 mse = 1.072 mae = 0.786\n",
            "epoch192 train time: 0.504s test time: 0.091  loss = 0.803 val_mse = 0.728 mse = 1.081 mae = 0.784\n",
            "epoch193 train time: 0.504s test time: 0.093  loss = 0.802 val_mse = 0.736 mse = 1.077 mae = 0.791\n",
            "epoch194 train time: 0.499s test time: 0.103  loss = 0.804 val_mse = 0.719 mse = 1.074 mae = 0.782\n",
            "epoch195 train time: 0.501s test time: 0.092  loss = 0.804 val_mse = 0.741 mse = 1.075 mae = 0.787\n",
            "epoch196 train time: 0.506s test time: 0.099  loss = 0.804 val_mse = 0.723 mse = 1.087 mae = 0.787\n",
            "epoch197 train time: 0.497s test time: 0.095  loss = 0.803 val_mse = 0.739 mse = 1.070 mae = 0.788\n",
            "epoch198 train time: 0.508s test time: 0.092  loss = 0.808 val_mse = 0.723 mse = 1.081 mae = 0.785\n",
            "epoch199 train time: 0.507s test time: 0.093  loss = 0.802 val_mse = 0.743 mse = 1.072 mae = 0.787\n",
            "epoch200 train time: 0.517s test time: 0.091  loss = 0.807 val_mse = 0.725 mse = 1.083 mae = 0.787\n",
            "epoch201 train time: 0.507s test time: 0.094  loss = 0.807 val_mse = 0.731 mse = 1.078 mae = 0.791\n",
            "epoch202 train time: 0.501s test time: 0.102  loss = 0.805 val_mse = 0.722 mse = 1.082 mae = 0.785\n",
            "epoch203 train time: 0.503s test time: 0.091  loss = 0.806 val_mse = 0.748 mse = 1.076 mae = 0.789\n",
            "epoch204 train time: 0.510s test time: 0.094  loss = 0.809 val_mse = 0.726 mse = 1.092 mae = 0.788\n",
            "epoch205 train time: 0.505s test time: 0.098  loss = 0.804 val_mse = 0.738 mse = 1.073 mae = 0.790\n",
            "epoch206 train time: 0.522s test time: 0.092  loss = 0.809 val_mse = 0.723 mse = 1.084 mae = 0.787\n",
            "epoch207 train time: 0.512s test time: 0.098  loss = 0.806 val_mse = 0.740 mse = 1.079 mae = 0.790\n",
            "epoch208 train time: 0.508s test time: 0.100  loss = 0.808 val_mse = 0.728 mse = 1.087 mae = 0.788\n",
            "epoch209 train time: 0.534s test time: 0.093  loss = 0.807 val_mse = 0.744 mse = 1.076 mae = 0.791\n",
            "epoch210 train time: 0.513s test time: 0.096  loss = 0.810 val_mse = 0.731 mse = 1.091 mae = 0.789\n",
            "epoch211 train time: 0.526s test time: 0.091  loss = 0.806 val_mse = 0.742 mse = 1.079 mae = 0.792\n",
            "epoch212 train time: 0.530s test time: 0.094  loss = 0.811 val_mse = 0.732 mse = 1.090 mae = 0.789\n",
            "epoch213 train time: 0.516s test time: 0.098  loss = 0.806 val_mse = 0.744 mse = 1.077 mae = 0.791\n",
            "epoch214 train time: 0.498s test time: 0.091  loss = 0.811 val_mse = 0.728 mse = 1.086 mae = 0.789\n",
            "epoch215 train time: 0.519s test time: 0.092  loss = 0.808 val_mse = 0.755 mse = 1.079 mae = 0.792\n",
            "epoch216 train time: 0.520s test time: 0.106  loss = 0.810 val_mse = 0.725 mse = 1.096 mae = 0.789\n",
            "epoch217 train time: 0.500s test time: 0.091  loss = 0.809 val_mse = 0.755 mse = 1.073 mae = 0.791\n",
            "epoch218 train time: 0.522s test time: 0.095  loss = 0.814 val_mse = 0.726 mse = 1.087 mae = 0.789\n",
            "epoch219 train time: 0.504s test time: 0.099  loss = 0.806 val_mse = 0.752 mse = 1.081 mae = 0.792\n",
            "epoch220 train time: 0.513s test time: 0.095  loss = 0.812 val_mse = 0.731 mse = 1.091 mae = 0.790\n",
            "epoch221 train time: 0.518s test time: 0.094  loss = 0.810 val_mse = 0.753 mse = 1.078 mae = 0.793\n",
            "epoch222 train time: 0.503s test time: 0.092  loss = 0.814 val_mse = 0.736 mse = 1.092 mae = 0.789\n",
            "epoch223 train time: 0.511s test time: 0.098  loss = 0.809 val_mse = 0.748 mse = 1.079 mae = 0.793\n",
            "epoch224 train time: 0.515s test time: 0.101  loss = 0.812 val_mse = 0.729 mse = 1.095 mae = 0.792\n",
            "epoch225 train time: 0.517s test time: 0.095  loss = 0.809 val_mse = 0.750 mse = 1.084 mae = 0.795\n",
            "epoch226 train time: 0.519s test time: 0.094  loss = 0.816 val_mse = 0.722 mse = 1.088 mae = 0.789\n",
            "epoch227 train time: 0.508s test time: 0.096  loss = 0.809 val_mse = 0.755 mse = 1.080 mae = 0.793\n",
            "epoch228 train time: 0.512s test time: 0.094  loss = 0.814 val_mse = 0.737 mse = 1.092 mae = 0.789\n",
            "epoch229 train time: 0.508s test time: 0.096  loss = 0.809 val_mse = 0.752 mse = 1.085 mae = 0.796\n",
            "epoch230 train time: 0.501s test time: 0.092  loss = 0.817 val_mse = 0.730 mse = 1.094 mae = 0.791\n",
            "epoch231 train time: 0.511s test time: 0.091  loss = 0.808 val_mse = 0.756 mse = 1.080 mae = 0.793\n",
            "epoch232 train time: 0.522s test time: 0.091  loss = 0.817 val_mse = 0.723 mse = 1.092 mae = 0.789\n",
            "epoch233 train time: 0.518s test time: 0.092  loss = 0.812 val_mse = 0.753 mse = 1.084 mae = 0.796\n",
            "epoch234 train time: 0.509s test time: 0.098  loss = 0.814 val_mse = 0.737 mse = 1.093 mae = 0.791\n",
            "epoch235 train time: 0.505s test time: 0.099  loss = 0.812 val_mse = 0.754 mse = 1.078 mae = 0.793\n",
            "epoch236 train time: 0.499s test time: 0.098  loss = 0.815 val_mse = 0.727 mse = 1.096 mae = 0.791\n",
            "epoch237 train time: 0.505s test time: 0.091  loss = 0.811 val_mse = 0.755 mse = 1.078 mae = 0.794\n",
            "epoch238 train time: 0.513s test time: 0.095  loss = 0.817 val_mse = 0.737 mse = 1.091 mae = 0.790\n",
            "epoch239 train time: 0.510s test time: 0.092  loss = 0.811 val_mse = 0.747 mse = 1.080 mae = 0.794\n",
            "MAE 0.893531407916269\n",
            "MSE 1.4843661836203774\n"
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    print(sys.path[0])\n",
        "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
        "    word_latent_dim = 300\n",
        "    latent_dim = 15\n",
        "    max_doc_length = 300\n",
        "    num_filters = 50\n",
        "    window_size = 3\n",
        "    v_dim = 50\n",
        "    learning_rate = 0.001\n",
        "    lambda_1 = 0.05 #normally we set from [0.05, 0.01, 0.005, 0.001]\n",
        "    drop_out = 0.8\n",
        "    batch_size = 200\n",
        "    epochs = 240\n",
        "    avg_mae_list = []\n",
        "    avg_mse_list = []\n",
        "    \n",
        "    # loading data\n",
        "    firTime = time()\n",
        "    dataSet = Dataset(max_doc_length, sys.path[0], \"dataPreprocessingWordDict.out\")\n",
        "    word_dict, user_reviews, item_reviews, train, valRatings, testRatings = dataSet.word_id_dict, dataSet.userReview_dict, dataSet.itemReview_dict, dataSet.trainMtrx, dataSet.valRatings, dataSet.testRatings\n",
        "    secTime = time()\n",
        "\n",
        "    num_users, num_items = train.shape\n",
        "    print(\"load data: %.3fs\" % (secTime - firTime))\n",
        "    print(num_users, num_items)\n",
        "    print(latent_dim)\n",
        "\n",
        "    #load word embeddings\n",
        "    word_embedding_mtrx = ini_word_embed(len(word_dict), word_latent_dim)\n",
        "    # word_embedding_mtrx = word2vec_word_embed(len(word_dict), word_latent_dim,\n",
        "    #                                           \"Directory of pretrained WordEmbedding.out\",\n",
        "    #                                           word_dict)\n",
        "\n",
        "    print( \"shape\", word_embedding_mtrx.shape)\n",
        "\n",
        "    # get train instances\n",
        "    user_input, item_input, rateings = get_train_instance(train)\n",
        "    print (len(user_input), len(item_input), len(rateings))\n",
        "\n",
        "    # get test/val instances\n",
        "    user_vals, item_vals, user_input_val, item_input_val, rating_input_val = get_test_list(200, valRatings, user_reviews, item_reviews)\n",
        "    user_tests, item_tests, user_input_test, item_input_test, rating_input_test = get_test_list(200, testRatings, user_reviews, item_reviews)\n",
        "\n",
        "    #train & eval model\n",
        "    train_model()"
      ],
      "id": "a924fe38"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Just review based"
      ],
      "metadata": {
        "id": "oXgncTlIrwjP"
      },
      "id": "oXgncTlIrwjP"
    },
    {
      "cell_type": "code",
      "source": [
        "#combination of interaction.py and review.py\n",
        "def train_model():\n",
        "    mae_list=[]\n",
        "    mse_list=[]\n",
        "    tf.compat.v1.disable_eager_execution()\n",
        "    #I and R-----\n",
        "    users = tf.compat.v1.placeholder(tf.int32, shape=[None])\n",
        "    items = tf.compat.v1.placeholder(tf.int32, shape=[None])\n",
        "    users_inputs = tf.compat.v1.placeholder(tf.int32, shape=[None, max_doc_length])\n",
        "    items_inputs = tf.compat.v1.placeholder(tf.int32, shape=[None, max_doc_length])\n",
        "    ratings = tf.compat.v1.placeholder(tf.float32, shape=[None, 1])\n",
        "    \n",
        "    #R-----\n",
        "    dropout_rate = tf.compat.v1.placeholder(tf.float32)\n",
        "    \n",
        "    #I-----\n",
        "    # user_entity_embedding = tf.Variable(tf.random.normal([num_users, latent_dim], mean=0, stddev=0.02), name=\"user_entity_embeddings\")\n",
        "    # item_entity_embedding = tf.Variable(tf.random.normal([num_items, latent_dim], mean=0, stddev=0.02), name=\"item_entity_embeddings\")\n",
        "    # user_entity_embeds = tf.nn.embedding_lookup(user_entity_embedding, users)\n",
        "    # item_entity_embeds = tf.nn.embedding_lookup(item_entity_embedding, items)\n",
        "    \n",
        "    #R-----\n",
        "    user_bias = tf.Variable(tf.random.normal([num_users, 1], mean=0, stddev=0.02), name=\"user_bias\")\n",
        "    item_bias = tf.Variable(tf.random.normal([num_items, 1], mean=0, stddev=0.02), name=\"item_bias\")\n",
        "    user_bs = tf.nn.embedding_lookup(user_bias, users)\n",
        "    item_bs = tf.nn.embedding_lookup(item_bias, items)\n",
        "    \n",
        "    text_embedding = tf.Variable(word_embedding_mtrx, dtype=tf.float32, name=\"review_text_embeds\")\n",
        "    padding_embedding = tf.Variable(np.zeros([1, word_latent_dim]), dtype=tf.float32)\n",
        "    text_mask = tf.constant([1.0] * text_embedding.get_shape()[0] + [0.0])\n",
        "    word_embeddings = tf.concat([text_embedding, padding_embedding], 0)\n",
        "    word_embeddings = word_embeddings * tf.expand_dims(text_mask, -1)\n",
        "    user_reviews_representation = tf.nn.embedding_lookup(word_embeddings, users_inputs)\n",
        "    user_reviews_representation_expnd = tf.expand_dims(user_reviews_representation, -1)\n",
        "    item_reviews_representation = tf.nn.embedding_lookup(word_embeddings, items_inputs)\n",
        "    item_reviews_representation_expnd = tf.expand_dims(item_reviews_representation, -1)\n",
        "    \n",
        "    # CNN layers\n",
        "    W_u = tf.Variable(tf.random.truncated_normal([window_size, word_latent_dim, 1, num_filters], stddev=0.3), name=\"review_W_u\")\n",
        "    W_i = tf.Variable(tf.random.truncated_normal([window_size, word_latent_dim, 1, num_filters], stddev=0.3), name=\"review_W_i\")\n",
        "    W_u_1 = tf.Variable(tf.random.truncated_normal([window_size, num_filters, 1, num_filters], stddev=0.3), name=\"review_W_u_1\")\n",
        "    W_i_1 = tf.Variable(tf.random.truncated_normal([window_size, num_filters, 1, num_filters], stddev=0.3), name=\"review_W_i_1\")\n",
        "    # b = tf.Variable(tf.constant(0.1, shape=[self.num_filters]))\n",
        "    rand_matrix = tf.Variable(tf.random.truncated_normal([num_filters, num_filters], stddev=0.3), name=\"review_rand_matrix\")\n",
        "\n",
        "    user_embeds, item_embeds = cnn_model_average(num_filters, user_reviews_representation_expnd, item_reviews_representation_expnd, W_u, W_i, W_u_1, W_i_1, rand_matrix)\n",
        "    \n",
        "    #shared MLP layer\n",
        "    W_mlp = tf.Variable(tf.random.normal([num_filters, latent_dim], mean=0, stddev=0.02), name=\"review_W_mlp\")\n",
        "    W_mlp = tf.nn.dropout(W_mlp, dropout_rate)\n",
        "    b_mlp = tf.Variable(tf.constant(0.1, shape=[latent_dim]), name=\"review_b_mlp\")\n",
        "\n",
        "    user_embeds = tf.nn.relu(tf.matmul(user_embeds, W_mlp) + b_mlp)\n",
        "    item_embeds = tf.nn.relu(tf.matmul(item_embeds, W_mlp) + b_mlp)\n",
        "\n",
        "    embeds_sum = tf.concat([tf.multiply(user_embeds, item_embeds), user_embeds, item_embeds], 1, name=\"concat_embed\")\n",
        "    #I\n",
        "    # entity_embeds_sum = tf.concat([tf.multiply(user_entity_embeds, item_entity_embeds), user_entity_embeds, item_entity_embeds],1)\n",
        "\n",
        "    #FM layer\n",
        "    #R---\n",
        "    w_0 = tf.Variable(tf.zeros(1), name=\"review_w_0\")\n",
        "    w_1 = tf.Variable(tf.random.truncated_normal([1, latent_dim * 3], stddev=0.3), name=\"review_w_1\")\n",
        "    v = tf.Variable(tf.random.truncated_normal([latent_dim * 3, latent_dim * 3], stddev=0.3), name=\"review_v\")\n",
        "    \n",
        "    #I---\n",
        "    # w_entity_0 = tf.Variable(tf.zeros(1), name=\"entity_w_0\")\n",
        "    # w_entity_1 = tf.Variable(tf.random.truncated_normal([1, latent_dim*3], stddev=0.3), name=\"entity_w_1\")\n",
        "    # v_entity = tf.Variable(tf.random.truncated_normal([latent_dim*3, v_dim], stddev=0.3), name=\"entity_v\")\n",
        "    \n",
        "    #I---\n",
        "    # J_e_1 = w_entity_0 + tf.matmul(entity_embeds_sum, w_entity_1, transpose_b=True)\n",
        "    #R---\n",
        "    J_1 = w_0 + tf.matmul(embeds_sum, w_1, transpose_b=True)\n",
        "    \n",
        "    #I---\n",
        "    # entity_embeds_sum_1 = tf.expand_dims(entity_embeds_sum, -1)\n",
        "    # entity_embeds_sum_2 = tf.expand_dims(entity_embeds_sum, 1)\n",
        "    #R---\n",
        "    embeds_sum_1 = tf.expand_dims(embeds_sum, -1)\n",
        "    embeds_sum_2 = tf.expand_dims(embeds_sum, 1)\n",
        "    \n",
        "    #I---\n",
        "    # J_e_2 = tf.reduce_sum(tf.reduce_sum(tf.multiply(tf.matmul(entity_embeds_sum_1, entity_embeds_sum_2), tf.matmul(v_entity, v_entity, transpose_b=True)),2), 1, keepdims=True)\n",
        "    #J_e_3 = tf.trace(tf.multiply(tf.matmul(entity_embeds_sum_1, entity_embeds_sum_2), tf.matmul(v_entity, v_entity, transpose_b=True))) \n",
        "    \n",
        "    #R---\n",
        "    J_2 = tf.reduce_sum(tf.reduce_sum(tf.multiply(tf.matmul(embeds_sum_1, embeds_sum_2), tf.matmul(v, v, transpose_b=True)),2), 1, keepdims=True)\n",
        "    #J_3 = tf.trace(tf.multiply(tf.matmul(embeds_sum_1, embeds_sum_2), tf.matmul(v, v, transpose_b=True)))\n",
        "    #R---\n",
        "    J_total = (J_1 + 0.5 * (J_2))\n",
        "    \n",
        "    #I---\n",
        "    # J_e_total = (J_e_1 + 0.5 * (J_e_2))\n",
        "    \n",
        "    \n",
        "    # numerator = J_total + J_e_total\n",
        "    numerator = J_total\n",
        "    # predict_rating = tf.divide(J_total, numerator) * J_total + tf.divide(J_e_total,numerator) * J_e_total + user_bs + item_bs \n",
        "    predict_rating = tf.divide(J_total, numerator) * J_total + user_bs + item_bs\n",
        "    loss = tf.reduce_mean(tf.math.squared_difference(predict_rating, ratings))\n",
        "    loss += lambda_1 * (tf.nn.l2_loss(W_i_1) + tf.nn.l2_loss(W_u_1) + tf.nn.l2_loss(W_u) + tf.nn.l2_loss(W_i) + tf.nn.l2_loss(v) + tf.nn.l2_loss(rand_matrix) + tf.nn.l2_loss(user_bs) + tf.nn.l2_loss(item_bs))\n",
        "    \n",
        "    train_step = tf.compat.v1.train.RMSPropOptimizer(learning_rate).minimize(loss)\n",
        "    saver = tf.compat.v1.train.Saver()\n",
        "    \n",
        "    init = tf.compat.v1.global_variables_initializer()\n",
        "    with tf.compat.v1.Session() as sess:\n",
        "        sess.run(init)\n",
        "\n",
        "        for e in range(epochs):\n",
        "            t = time()\n",
        "            loss_total = 0.0\n",
        "            count = 0.0\n",
        "            \n",
        "            for i in range(int(math.ceil(len(user_input) / float(batch_size)))):\n",
        "                user_batch, item_batch, user_input_batch, item_input_batch, rates_batch = get_train_instance_batch_change(i, batch_size,user_input, item_input, rateings, user_reviews,item_reviews)\n",
        "                _, loss_val, words = sess.run([train_step, loss, word_embeddings], feed_dict={users: user_batch, items: item_batch, users_inputs: user_input_batch, items_inputs: item_input_batch, ratings: rates_batch, dropout_rate:drop_out})\n",
        "                loss_total += loss_val\n",
        "                count += 1.0\n",
        "\n",
        "            t1 = time()            \n",
        "            #LOOK FROM HERE \n",
        "            val_mses, val_maes = [], []\n",
        "            for i in range(len(user_input_val)):\n",
        "                #eval_model(users, items, users_inputs, items_inputs, dropout_rate, predict_rating, sess, user_vals[i], item_vals[i], user_input_val[i], item_input_val[i], rating_input_val[i], val_mses, val_maes)\n",
        "                #predicts = sess.run(predict_rating, feed_dict={users: user_batch, items: item_batch, users_inputs: user_input_batch, items_inputs: item_input_batch, dropout_rate:1.0})\n",
        "                predicts = sess.run(predict_rating, feed_dict={users: user_vals[i], items: item_vals[i], users_inputs: user_input_val[i], items_inputs: item_input_val[i], ratings: rating_input_val[i], dropout_rate: drop_out})\n",
        "                #print(predicts)\n",
        "                row, col = predicts.shape\n",
        "                for r in range(row):\n",
        "                    val_mses.append(pow((predicts[r, 0] - rating_input_val[i][r][0]), 2))\n",
        "                    val_maes.append(abs((predicts[r, 0] - rating_input_val[i][r][0])))\n",
        "            val_mse = np.array(val_mses).mean()            \n",
        "            t2 = time()            \n",
        "            mses, maes = [], []\n",
        "            for i in range(len(user_input_test)):\n",
        "                #eval_model(users, items, users_inputs, items_inputs, dropout_rate, predict_rating, sess, user_tests[i], item_tests[i], user_input_test[i], item_input_test[i], rating_input_test[i], mses, maes)\n",
        "                predicts_test = sess.run(predict_rating, feed_dict={users: user_tests[i], items: item_tests[i], users_inputs: user_input_test[i], items_inputs: item_input_test[i], ratings: rating_input_test[i], dropout_rate: drop_out})\n",
        "                #print(predicts)\n",
        "                row, col = predicts_test.shape\n",
        "                for r in range(row):\n",
        "                    mses.append(pow((predicts_test[r, 0] - rating_input_test[i][r][0]), 2))\n",
        "                    maes.append(abs((predicts_test[r, 0] - rating_input_test[i][r][0])))\n",
        "                    \n",
        "            mse = np.array(mses).mean()\n",
        "            mae = np.array(maes).mean()\n",
        "            t3 = time()\n",
        "            mae_list.append(mae)\n",
        "            mse_list.append(mse)\n",
        "            print(\"epoch%d train time: %.3fs test time: %.3f  loss = %.3f val_mse = %.3f mse = %.3f mae = %.3f\"%(e, (t1 - t), (t3 - t2), loss_total/count, val_mse, mse, mae))\n",
        "        \n",
        "        avg_mae=sum(mae_list) / len(mae_list)\n",
        "        avg_mse=sum(mse_list)/len(mse_list)\n",
        "        avg_mae_list.append(avg_mae)\n",
        "        avg_mse_list.append(avg_mse)\n",
        "        print(\"MAE \"+str(avg_mae))\n",
        "        print(\"MSE \"+str(avg_mse))"
      ],
      "metadata": {
        "id": "nZI3Juflqt2L"
      },
      "id": "nZI3Juflqt2L",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    print(sys.path[0])\n",
        "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
        "    word_latent_dim = 300\n",
        "    latent_dim = 15\n",
        "    max_doc_length = 300\n",
        "    num_filters = 50\n",
        "    window_size = 3\n",
        "    v_dim = 50\n",
        "    learning_rate = 0.001\n",
        "    lambda_1 = 0.05 #normally we set from [0.05, 0.01, 0.005, 0.001]\n",
        "    drop_out = 0.8\n",
        "    batch_size = 200\n",
        "    epochs = 240\n",
        "    avg_mae_list = []\n",
        "    avg_mse_list = []\n",
        "    \n",
        "    # loading data\n",
        "    firTime = time()\n",
        "    dataSet = Dataset(max_doc_length, sys.path[0], \"dataPreprocessingWordDict.out\")\n",
        "    word_dict, user_reviews, item_reviews, train, valRatings, testRatings = dataSet.word_id_dict, dataSet.userReview_dict, dataSet.itemReview_dict, dataSet.trainMtrx, dataSet.valRatings, dataSet.testRatings\n",
        "    secTime = time()\n",
        "\n",
        "    num_users, num_items = train.shape\n",
        "    print(\"load data: %.3fs\" % (secTime - firTime))\n",
        "    print(num_users, num_items)\n",
        "    print(latent_dim)\n",
        "\n",
        "    #load word embeddings\n",
        "    word_embedding_mtrx = ini_word_embed(len(word_dict), word_latent_dim)\n",
        "    # word_embedding_mtrx = word2vec_word_embed(len(word_dict), word_latent_dim,\n",
        "    #                                           \"Directory of pretrained WordEmbedding.out\",\n",
        "    #                                           word_dict)\n",
        "\n",
        "    print( \"shape\", word_embedding_mtrx.shape)\n",
        "\n",
        "    # get train instances\n",
        "    user_input, item_input, rateings = get_train_instance(train)\n",
        "    print (len(user_input), len(item_input), len(rateings))\n",
        "\n",
        "    # get test/val instances\n",
        "    user_vals, item_vals, user_input_val, item_input_val, rating_input_val = get_test_list(200, valRatings, user_reviews, item_reviews)\n",
        "    user_tests, item_tests, user_input_test, item_input_test, rating_input_test = get_test_list(200, testRatings, user_reviews, item_reviews)\n",
        "\n",
        "    #train & eval model\n",
        "    train_model()"
      ],
      "metadata": {
        "id": "7nFkpGRcsAzb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "outputId": "4d80ef02-0b23-42c9-aa15-0a2d37ef0017"
      },
      "id": "7nFkpGRcsAzb",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "wordId_dict finished\n",
            "load reviews finished\n",
            "load data: 2.629s\n",
            "1686 962\n",
            "15\n",
            "shape (18584, 300)\n",
            "10612 10612 10612\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-8eb7bfff6e72>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;31m#train & eval model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m     \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'train_model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "dynamic linear fusion vs varying alpha \n",
        "\n",
        "alpha = [0.1, 0.2, 0.3, 0.4, 0.5, 0,6, 0.7, 0.8, 0.9, 1]"
      ],
      "metadata": {
        "id": "6_Ai8sOP7EPc"
      },
      "id": "6_Ai8sOP7EPc"
    },
    {
      "cell_type": "code",
      "source": [
        "#combination of interaction.py and review.py\n",
        "def train_model():\n",
        "    mae_list=[]\n",
        "    mse_list=[]\n",
        "    tf.compat.v1.disable_eager_execution()\n",
        "    #I and R-----\n",
        "    users = tf.compat.v1.placeholder(tf.int32, shape=[None])\n",
        "    items = tf.compat.v1.placeholder(tf.int32, shape=[None])\n",
        "    users_inputs = tf.compat.v1.placeholder(tf.int32, shape=[None, max_doc_length])\n",
        "    items_inputs = tf.compat.v1.placeholder(tf.int32, shape=[None, max_doc_length])\n",
        "    ratings = tf.compat.v1.placeholder(tf.float32, shape=[None, 1])\n",
        "    \n",
        "    #R-----\n",
        "    dropout_rate = tf.compat.v1.placeholder(tf.float32)\n",
        "    alpha = tf.compat.v1.placeholder(tf.float32)\n",
        "    \n",
        "    #I-----\n",
        "    user_entity_embedding = tf.Variable(tf.random.normal([num_users, latent_dim], mean=0, stddev=0.02), name=\"user_entity_embeddings\")\n",
        "    item_entity_embedding = tf.Variable(tf.random.normal([num_items, latent_dim], mean=0, stddev=0.02), name=\"item_entity_embeddings\")\n",
        "    user_entity_embeds = tf.nn.embedding_lookup(user_entity_embedding, users)\n",
        "    item_entity_embeds = tf.nn.embedding_lookup(item_entity_embedding, items)\n",
        "    \n",
        "    #R-----\n",
        "    user_bias = tf.Variable(tf.random.normal([num_users, 1], mean=0, stddev=0.02), name=\"user_bias\")\n",
        "    item_bias = tf.Variable(tf.random.normal([num_items, 1], mean=0, stddev=0.02), name=\"item_bias\")\n",
        "    user_bs = tf.nn.embedding_lookup(user_bias, users)\n",
        "    item_bs = tf.nn.embedding_lookup(item_bias, items)\n",
        "    \n",
        "    text_embedding = tf.Variable(word_embedding_mtrx, dtype=tf.float32, name=\"review_text_embeds\")\n",
        "    padding_embedding = tf.Variable(np.zeros([1, word_latent_dim]), dtype=tf.float32)\n",
        "    text_mask = tf.constant([1.0] * text_embedding.get_shape()[0] + [0.0])\n",
        "    word_embeddings = tf.concat([text_embedding, padding_embedding], 0)\n",
        "    word_embeddings = word_embeddings * tf.expand_dims(text_mask, -1)\n",
        "    user_reviews_representation = tf.nn.embedding_lookup(word_embeddings, users_inputs)\n",
        "    user_reviews_representation_expnd = tf.expand_dims(user_reviews_representation, -1)\n",
        "    item_reviews_representation = tf.nn.embedding_lookup(word_embeddings, items_inputs)\n",
        "    item_reviews_representation_expnd = tf.expand_dims(item_reviews_representation, -1)\n",
        "    \n",
        "    # CNN layers\n",
        "    W_u = tf.Variable(tf.random.truncated_normal([window_size, word_latent_dim, 1, num_filters], stddev=0.3), name=\"review_W_u\")\n",
        "    W_i = tf.Variable(tf.random.truncated_normal([window_size, word_latent_dim, 1, num_filters], stddev=0.3), name=\"review_W_i\")\n",
        "    W_u_1 = tf.Variable(tf.random.truncated_normal([window_size, num_filters, 1, num_filters], stddev=0.3), name=\"review_W_u_1\")\n",
        "    W_i_1 = tf.Variable(tf.random.truncated_normal([window_size, num_filters, 1, num_filters], stddev=0.3), name=\"review_W_i_1\")\n",
        "    # b = tf.Variable(tf.constant(0.1, shape=[self.num_filters]))\n",
        "    rand_matrix = tf.Variable(tf.random.truncated_normal([num_filters, num_filters], stddev=0.3), name=\"review_rand_matrix\")\n",
        "\n",
        "    user_embeds, item_embeds = cnn_model_average(num_filters, user_reviews_representation_expnd, item_reviews_representation_expnd, W_u, W_i, W_u_1, W_i_1, rand_matrix)\n",
        "    \n",
        "    #shared MLP layer\n",
        "    W_mlp = tf.Variable(tf.random.normal([num_filters, latent_dim], mean=0, stddev=0.02), name=\"review_W_mlp\")\n",
        "    W_mlp = tf.nn.dropout(W_mlp, dropout_rate)\n",
        "    b_mlp = tf.Variable(tf.constant(0.1, shape=[latent_dim]), name=\"review_b_mlp\")\n",
        "\n",
        "    user_embeds = tf.nn.relu(tf.matmul(user_embeds, W_mlp) + b_mlp)\n",
        "    item_embeds = tf.nn.relu(tf.matmul(item_embeds, W_mlp) + b_mlp)\n",
        "\n",
        "    embeds_sum = tf.concat([tf.multiply(user_embeds, item_embeds), user_embeds, item_embeds], 1, name=\"concat_embed\")\n",
        "    #I\n",
        "    entity_embeds_sum = tf.concat([tf.multiply(user_entity_embeds, item_entity_embeds), user_entity_embeds, item_entity_embeds],1)\n",
        "\n",
        "    #FM layer\n",
        "    #R---\n",
        "    w_0 = tf.Variable(tf.zeros(1), name=\"review_w_0\")\n",
        "    w_1 = tf.Variable(tf.random.truncated_normal([1, latent_dim * 3], stddev=0.3), name=\"review_w_1\")\n",
        "    v = tf.Variable(tf.random.truncated_normal([latent_dim * 3, latent_dim * 3], stddev=0.3), name=\"review_v\")\n",
        "    \n",
        "    #I---\n",
        "    w_entity_0 = tf.Variable(tf.zeros(1), name=\"entity_w_0\")\n",
        "    w_entity_1 = tf.Variable(tf.random.truncated_normal([1, latent_dim*3], stddev=0.3), name=\"entity_w_1\")\n",
        "    v_entity = tf.Variable(tf.random.truncated_normal([latent_dim*3, v_dim], stddev=0.3), name=\"entity_v\")\n",
        "    \n",
        "    #I---\n",
        "    J_e_1 = w_entity_0 + tf.matmul(entity_embeds_sum, w_entity_1, transpose_b=True)\n",
        "    #R---\n",
        "    J_1 = w_0 + tf.matmul(embeds_sum, w_1, transpose_b=True)\n",
        "    \n",
        "    #I---\n",
        "    entity_embeds_sum_1 = tf.expand_dims(entity_embeds_sum, -1)\n",
        "    entity_embeds_sum_2 = tf.expand_dims(entity_embeds_sum, 1)\n",
        "    #R---\n",
        "    embeds_sum_1 = tf.expand_dims(embeds_sum, -1)\n",
        "    embeds_sum_2 = tf.expand_dims(embeds_sum, 1)\n",
        "    \n",
        "    #I---\n",
        "    J_e_2 = tf.reduce_sum(tf.reduce_sum(tf.multiply(tf.matmul(entity_embeds_sum_1, entity_embeds_sum_2), tf.matmul(v_entity, v_entity, transpose_b=True)),2), 1, keepdims=True)\n",
        "    #J_e_3 = tf.trace(tf.multiply(tf.matmul(entity_embeds_sum_1, entity_embeds_sum_2), tf.matmul(v_entity, v_entity, transpose_b=True))) \n",
        "    \n",
        "    #R---\n",
        "    J_2 = tf.reduce_sum(tf.reduce_sum(tf.multiply(tf.matmul(embeds_sum_1, embeds_sum_2), tf.matmul(v, v, transpose_b=True)),2), 1, keepdims=True)\n",
        "    #J_3 = tf.trace(tf.multiply(tf.matmul(embeds_sum_1, embeds_sum_2), tf.matmul(v, v, transpose_b=True)))\n",
        "    #R---\n",
        "    J_total = (J_1 + 0.5 * (J_2))\n",
        "    \n",
        "    #I---\n",
        "    J_e_total = (J_e_1 + 0.5 * (J_e_2))\n",
        "    \n",
        "    \n",
        "    # numerator = J_total + J_e_total\n",
        "    predict_rating = alpha * J_total + (1 - alpha) * J_e_total + user_bs + item_bs\n",
        "    loss = tf.reduce_mean(tf.math.squared_difference(predict_rating, ratings))\n",
        "    loss += lambda_1 * (tf.nn.l2_loss(W_i_1) + tf.nn.l2_loss(W_u_1) + tf.nn.l2_loss(user_entity_embedding) + tf.nn.l2_loss(item_entity_embedding) + tf.nn.l2_loss(W_u) + tf.nn.l2_loss(W_i) + tf.nn.l2_loss(v) + tf.nn.l2_loss(v_entity) + tf.nn.l2_loss(rand_matrix) + tf.nn.l2_loss(user_bs) + tf.nn.l2_loss(item_bs))\n",
        "    \n",
        "    train_step = tf.compat.v1.train.RMSPropOptimizer(learning_rate).minimize(loss)\n",
        "    saver = tf.compat.v1.train.Saver()\n",
        "    \n",
        "    init = tf.compat.v1.global_variables_initializer()\n",
        "    with tf.compat.v1.Session() as sess:\n",
        "        sess.run(init)\n",
        "\n",
        "        for e in range(epochs):\n",
        "            t = time()\n",
        "            loss_total = 0.0\n",
        "            count = 0.0\n",
        "            \n",
        "            for i in range(int(math.ceil(len(user_input) / float(batch_size)))):\n",
        "                user_batch, item_batch, user_input_batch, item_input_batch, rates_batch = get_train_instance_batch_change(i, batch_size,user_input, item_input, rateings, user_reviews,item_reviews)\n",
        "                _, loss_val, words = sess.run([train_step, loss, word_embeddings], feed_dict={users: user_batch, items: item_batch, users_inputs: user_input_batch, items_inputs: item_input_batch, ratings: rates_batch, dropout_rate:drop_out, alpha: a})\n",
        "                loss_total += loss_val\n",
        "                count += 1.0\n",
        "\n",
        "            t1 = time()            \n",
        "            #LOOK FROM HERE \n",
        "            val_mses, val_maes = [], []\n",
        "            for i in range(len(user_input_val)):\n",
        "                #eval_model(users, items, users_inputs, items_inputs, dropout_rate, predict_rating, sess, user_vals[i], item_vals[i], user_input_val[i], item_input_val[i], rating_input_val[i], val_mses, val_maes)\n",
        "                #predicts = sess.run(predict_rating, feed_dict={users: user_batch, items: item_batch, users_inputs: user_input_batch, items_inputs: item_input_batch, dropout_rate:1.0})\n",
        "                predicts = sess.run(predict_rating, feed_dict={users: user_vals[i], items: item_vals[i], users_inputs: user_input_val[i], items_inputs: item_input_val[i], ratings: rating_input_val[i], dropout_rate: drop_out, alpha: a})\n",
        "                #print(predicts)\n",
        "                row, col = predicts.shape\n",
        "                for r in range(row):\n",
        "                    val_mses.append(pow((predicts[r, 0] - rating_input_val[i][r][0]), 2))\n",
        "                    val_maes.append(abs((predicts[r, 0] - rating_input_val[i][r][0])))\n",
        "            val_mse = np.array(val_mses).mean()            \n",
        "            t2 = time()            \n",
        "            mses, maes = [], []\n",
        "            for i in range(len(user_input_test)):\n",
        "                #eval_model(users, items, users_inputs, items_inputs, dropout_rate, predict_rating, sess, user_tests[i], item_tests[i], user_input_test[i], item_input_test[i], rating_input_test[i], mses, maes)\n",
        "                predicts_test = sess.run(predict_rating, feed_dict={users: user_tests[i], items: item_tests[i], users_inputs: user_input_test[i], items_inputs: item_input_test[i], ratings: rating_input_test[i], dropout_rate: drop_out, alpha: a})\n",
        "                #print(predicts)\n",
        "                row, col = predicts_test.shape\n",
        "                for r in range(row):\n",
        "                    mses.append(pow((predicts_test[r, 0] - rating_input_test[i][r][0]), 2))\n",
        "                    maes.append(abs((predicts_test[r, 0] - rating_input_test[i][r][0])))\n",
        "                    \n",
        "            mse = np.array(mses).mean()\n",
        "            mae = np.array(maes).mean()\n",
        "            t3 = time()\n",
        "            mae_list.append(mae)\n",
        "            mse_list.append(mse)\n",
        "            print(\"epoch%d train time: %.3fs test time: %.3f  loss = %.3f val_mse = %.3f mse = %.3f mae = %.3f\"%(e, (t1 - t), (t3 - t2), loss_total/count, val_mse, mse, mae))\n",
        "        \n",
        "        avg_mae=sum(mae_list) / len(mae_list)\n",
        "        avg_mse=sum(mse_list)/len(mse_list)\n",
        "        avg_mae_list.append(avg_mae)\n",
        "        avg_mse_list.append(avg_mse)\n",
        "        print(\"MAE \"+str(avg_mae))\n",
        "        print(\"MSE \"+str(avg_mse))"
      ],
      "metadata": {
        "id": "NaKbZbkI650f"
      },
      "id": "NaKbZbkI650f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    print(sys.path[0])\n",
        "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
        "    word_latent_dim = 300\n",
        "    latent_dim = 15\n",
        "    max_doc_length = 300\n",
        "    num_filters = 50\n",
        "    window_size = 3\n",
        "    v_dim = 50\n",
        "    learning_rate = 0.001\n",
        "    lambda_1 = 0.05 #normally we set from [0.05, 0.01, 0.005, 0.001]\n",
        "    drop_out = 0.8\n",
        "    batch_size = 200\n",
        "    epochs = 180\n",
        "    a_list = [0.1, 0.2, 0.3, 0.4, 0.5, 0,6, 0.7, 0.8, 0.9, 1]\n",
        "    a = 0.1\n",
        "    avg_mae_list = []\n",
        "    avg_mse_list = []\n",
        "    \n",
        "    # loading data\n",
        "    firTime = time()\n",
        "    dataSet = Dataset(max_doc_length, sys.path[0], \"dataPreprocessingWordDict.out\")\n",
        "    word_dict, user_reviews, item_reviews, train, valRatings, testRatings = dataSet.word_id_dict, dataSet.userReview_dict, dataSet.itemReview_dict, dataSet.trainMtrx, dataSet.valRatings, dataSet.testRatings\n",
        "    secTime = time()\n",
        "\n",
        "    num_users, num_items = train.shape\n",
        "    print(\"load data: %.3fs\" % (secTime - firTime))\n",
        "    print(num_users, num_items)\n",
        "    print(latent_dim)\n",
        "\n",
        "    #load word embeddings\n",
        "    word_embedding_mtrx = ini_word_embed(len(word_dict), word_latent_dim)\n",
        "    # word_embedding_mtrx = word2vec_word_embed(len(word_dict), word_latent_dim,\n",
        "    #                                           \"Directory of pretrained WordEmbedding.out\",\n",
        "    #                                           word_dict)\n",
        "\n",
        "    print( \"shape\", word_embedding_mtrx.shape)\n",
        "\n",
        "    # get train instances\n",
        "    user_input, item_input, rateings = get_train_instance(train)\n",
        "    print (len(user_input), len(item_input), len(rateings))\n",
        "\n",
        "    # get test/val instances\n",
        "    user_vals, item_vals, user_input_val, item_input_val, rating_input_val = get_test_list(200, valRatings, user_reviews, item_reviews)\n",
        "    user_tests, item_tests, user_input_test, item_input_test, rating_input_test = get_test_list(200, testRatings, user_reviews, item_reviews)\n",
        "\n",
        "    #train & eval model\n",
        "    train_model()\n",
        "\n",
        "    a = 0.2\n",
        "    #train & eval model\n",
        "    train_model()\n",
        "\n",
        "    a = 0.3\n",
        "    #train & eval model\n",
        "    train_model()\n",
        "\n",
        "    a = 0.4\n",
        "    #train & eval model\n",
        "    train_model()\n",
        "\n",
        "    a = 0.5\n",
        "    #train & eval model\n",
        "    train_model()\n",
        "\n",
        "    a = 0.6\n",
        "    #train & eval model\n",
        "    train_model()\n",
        "\n",
        "    a = 0.7\n",
        "    #train & eval model\n",
        "    train_model()\n",
        "\n",
        "    a = 0.8\n",
        "    #train & eval model\n",
        "    train_model()\n",
        "\n",
        "    a = 0.9\n",
        "    #train & eval model\n",
        "    train_model()\n",
        "\n",
        "    a = 1\n",
        "    #train & eval model\n",
        "    train_model()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "gzjO5Rsp6Wr3",
        "outputId": "06b1d4f0-ffd9-4994-fe19-dd9f770c98a1"
      },
      "id": "gzjO5Rsp6Wr3",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "wordId_dict finished\n",
            "load reviews finished\n",
            "load data: 0.981s\n",
            "1686 962\n",
            "15\n",
            "shape (18584, 300)\n",
            "10612 10612 10612\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/training/rmsprop.py:188: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "epoch0 train time: 18.232s test time: 0.661  loss = 210.373 val_mse = 16.291 mse = 16.333 mae = 3.889\n",
            "epoch1 train time: 7.143s test time: 0.635  loss = 186.761 val_mse = 9.245 mse = 9.294 mae = 2.865\n",
            "epoch2 train time: 7.113s test time: 0.636  loss = 131.576 val_mse = 1.339 mse = 1.356 mae = 0.996\n",
            "epoch3 train time: 7.142s test time: 0.923  loss = 87.474 val_mse = 1.190 mse = 1.194 mae = 0.863\n",
            "epoch4 train time: 7.375s test time: 0.636  loss = 57.314 val_mse = 1.170 mse = 1.171 mae = 0.849\n",
            "epoch5 train time: 7.098s test time: 0.625  loss = 36.063 val_mse = 1.153 mse = 1.153 mae = 0.839\n",
            "epoch6 train time: 7.129s test time: 0.631  loss = 21.668 val_mse = 1.140 mse = 1.140 mae = 0.831\n",
            "epoch7 train time: 7.126s test time: 0.635  loss = 12.400 val_mse = 1.129 mse = 1.131 mae = 0.825\n",
            "epoch8 train time: 7.138s test time: 0.634  loss = 6.819 val_mse = 1.119 mse = 1.124 mae = 0.821\n",
            "epoch9 train time: 7.080s test time: 0.632  loss = 3.754 val_mse = 1.111 mse = 1.118 mae = 0.816\n",
            "epoch10 train time: 7.085s test time: 0.630  loss = 2.286 val_mse = 1.103 mse = 1.113 mae = 0.813\n",
            "epoch11 train time: 7.085s test time: 0.636  loss = 1.719 val_mse = 1.095 mse = 1.108 mae = 0.810\n",
            "epoch12 train time: 7.093s test time: 0.627  loss = 1.553 val_mse = 1.088 mse = 1.104 mae = 0.807\n",
            "epoch13 train time: 7.034s test time: 0.632  loss = 1.488 val_mse = 1.081 mse = 1.099 mae = 0.804\n",
            "epoch14 train time: 7.100s test time: 0.626  loss = 1.439 val_mse = 1.074 mse = 1.096 mae = 0.801\n",
            "epoch15 train time: 7.058s test time: 0.627  loss = 1.398 val_mse = 1.067 mse = 1.092 mae = 0.799\n",
            "epoch16 train time: 7.082s test time: 0.627  loss = 1.364 val_mse = 1.061 mse = 1.089 mae = 0.796\n",
            "epoch17 train time: 7.041s test time: 0.634  loss = 1.335 val_mse = 1.055 mse = 1.085 mae = 0.794\n",
            "epoch18 train time: 7.103s test time: 0.625  loss = 1.310 val_mse = 1.049 mse = 1.082 mae = 0.792\n",
            "epoch19 train time: 7.092s test time: 0.627  loss = 1.288 val_mse = 1.043 mse = 1.080 mae = 0.790\n",
            "epoch20 train time: 7.092s test time: 0.625  loss = 1.269 val_mse = 1.037 mse = 1.077 mae = 0.788\n",
            "epoch21 train time: 7.084s test time: 0.628  loss = 1.251 val_mse = 1.032 mse = 1.074 mae = 0.786\n",
            "epoch22 train time: 7.034s test time: 0.633  loss = 1.235 val_mse = 1.026 mse = 1.072 mae = 0.784\n",
            "epoch23 train time: 7.060s test time: 0.630  loss = 1.221 val_mse = 1.021 mse = 1.070 mae = 0.782\n",
            "epoch24 train time: 7.099s test time: 0.634  loss = 1.207 val_mse = 1.016 mse = 1.067 mae = 0.780\n",
            "epoch25 train time: 7.079s test time: 0.631  loss = 1.195 val_mse = 1.011 mse = 1.065 mae = 0.778\n",
            "epoch26 train time: 7.107s test time: 0.630  loss = 1.184 val_mse = 1.006 mse = 1.063 mae = 0.777\n",
            "epoch27 train time: 7.124s test time: 0.624  loss = 1.173 val_mse = 1.001 mse = 1.061 mae = 0.775\n",
            "epoch28 train time: 7.096s test time: 0.627  loss = 1.163 val_mse = 0.997 mse = 1.059 mae = 0.773\n",
            "epoch29 train time: 7.077s test time: 0.630  loss = 1.153 val_mse = 0.992 mse = 1.057 mae = 0.772\n",
            "epoch30 train time: 7.099s test time: 0.622  loss = 1.144 val_mse = 0.988 mse = 1.055 mae = 0.770\n",
            "epoch31 train time: 7.042s test time: 0.628  loss = 1.136 val_mse = 0.983 mse = 1.054 mae = 0.769\n",
            "epoch32 train time: 7.066s test time: 0.633  loss = 1.128 val_mse = 0.979 mse = 1.052 mae = 0.767\n",
            "epoch33 train time: 7.058s test time: 0.633  loss = 1.120 val_mse = 0.974 mse = 1.050 mae = 0.766\n",
            "epoch34 train time: 7.091s test time: 0.630  loss = 1.113 val_mse = 0.970 mse = 1.049 mae = 0.764\n",
            "epoch35 train time: 7.129s test time: 0.629  loss = 1.105 val_mse = 0.966 mse = 1.047 mae = 0.763\n",
            "epoch36 train time: 7.108s test time: 0.629  loss = 1.098 val_mse = 0.962 mse = 1.046 mae = 0.762\n",
            "epoch37 train time: 7.090s test time: 0.627  loss = 1.092 val_mse = 0.958 mse = 1.044 mae = 0.760\n",
            "epoch38 train time: 7.088s test time: 0.625  loss = 1.085 val_mse = 0.954 mse = 1.043 mae = 0.759\n",
            "epoch39 train time: 7.074s test time: 0.629  loss = 1.079 val_mse = 0.950 mse = 1.041 mae = 0.758\n",
            "epoch40 train time: 7.103s test time: 0.628  loss = 1.073 val_mse = 0.946 mse = 1.040 mae = 0.756\n",
            "epoch41 train time: 7.070s test time: 0.641  loss = 1.067 val_mse = 0.942 mse = 1.039 mae = 0.755\n",
            "epoch42 train time: 7.049s test time: 0.624  loss = 1.061 val_mse = 0.938 mse = 1.037 mae = 0.754\n",
            "epoch43 train time: 7.067s test time: 0.630  loss = 1.056 val_mse = 0.933 mse = 1.035 mae = 0.753\n",
            "epoch44 train time: 7.063s test time: 0.637  loss = 1.050 val_mse = 0.929 mse = 1.034 mae = 0.751\n",
            "epoch45 train time: 7.061s test time: 0.620  loss = 1.045 val_mse = 0.926 mse = 1.033 mae = 0.750\n",
            "epoch46 train time: 7.144s test time: 0.633  loss = 1.040 val_mse = 0.921 mse = 1.031 mae = 0.749\n",
            "epoch47 train time: 7.157s test time: 0.627  loss = 1.035 val_mse = 0.918 mse = 1.030 mae = 0.748\n",
            "epoch48 train time: 7.158s test time: 0.626  loss = 1.030 val_mse = 0.914 mse = 1.029 mae = 0.747\n",
            "epoch49 train time: 7.050s test time: 0.628  loss = 1.025 val_mse = 0.911 mse = 1.028 mae = 0.746\n",
            "epoch50 train time: 7.101s test time: 0.637  loss = 1.020 val_mse = 0.907 mse = 1.027 mae = 0.744\n",
            "epoch51 train time: 7.064s test time: 0.635  loss = 1.015 val_mse = 0.903 mse = 1.026 mae = 0.743\n",
            "epoch52 train time: 7.090s test time: 0.626  loss = 1.010 val_mse = 0.900 mse = 1.024 mae = 0.742\n",
            "epoch53 train time: 7.067s test time: 0.633  loss = 1.005 val_mse = 0.896 mse = 1.023 mae = 0.741\n",
            "epoch54 train time: 7.174s test time: 0.630  loss = 1.001 val_mse = 0.893 mse = 1.023 mae = 0.740\n",
            "epoch55 train time: 7.164s test time: 0.636  loss = 0.996 val_mse = 0.888 mse = 1.021 mae = 0.740\n",
            "epoch56 train time: 7.130s test time: 0.627  loss = 0.992 val_mse = 0.885 mse = 1.020 mae = 0.738\n",
            "epoch57 train time: 7.101s test time: 0.638  loss = 0.987 val_mse = 0.882 mse = 1.019 mae = 0.737\n",
            "epoch58 train time: 7.107s test time: 0.639  loss = 0.983 val_mse = 0.878 mse = 1.018 mae = 0.737\n",
            "epoch59 train time: 7.132s test time: 0.641  loss = 0.979 val_mse = 0.874 mse = 1.016 mae = 0.736\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-0610000cc5cb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;31m#train & eval model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m     \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-16-da26c62a2703>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m    115\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mceil\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser_input\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0muser_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitem_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muser_input_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitem_input_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrates_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_train_instance_batch_change\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0muser_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitem_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrateings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muser_reviews\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mitem_reviews\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                 \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_embeddings\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0musers\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0muser_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitems\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mitem_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0musers_inputs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0muser_input_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitems_inputs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mitem_input_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mratings\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mrates_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout_rate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mdrop_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m                 \u001b[0mloss_total\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss_val\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0mcount\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    966\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 968\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    969\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    970\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1189\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1190\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1191\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1192\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1193\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1369\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1370\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1371\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1372\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1373\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1375\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1376\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1377\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1378\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1379\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1359\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1360\u001b[0m       return self._call_tf_sessionrun(options, feed_dict, fetch_list,\n\u001b[0;32m-> 1361\u001b[0;31m                                       target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1362\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1363\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1453\u001b[0m     return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,\n\u001b[1;32m   1454\u001b[0m                                             \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1455\u001b[0;31m                                             run_metadata)\n\u001b[0m\u001b[1;32m   1456\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1457\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Plot"
      ],
      "metadata": {
        "id": "6VYM0b5asBMB"
      },
      "id": "6VYM0b5asBMB"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R1nVse_jDcMn"
      },
      "outputs": [],
      "source": [
        "avg_mae_list = [0.6703910703211292,0.6417340249712058,0.6446848607940466,0.6521739528760172,0.6534780474929722,0.6601080517745804,0.7465802265738355]\n",
        "avg_mse_list = [0.8937859865065624,0.8435149712299163,0.8345654187641376,0.8325071872419542,0.8385010161584376,0.8580438932003519,1.1190624304580759]\n",
        "latent_dim_list = [15, 25, 50, 100, 150, 200, 300]\n",
        "print(f'avg mae over latent dimensions = [15, 25, 50, 100, 150, 200, 300]: {avg_mae_list}')\n",
        "print(f'avg mse over latent dimensions = [15, 25, 50, 100, 150, 200, 300]: {avg_mse_list}')\n",
        "    \n",
        "plt.rcParams.update({'figure.figsize':(7.5,3.5), 'figure.dpi':100})\n",
        "fig, axes = plt.subplots(1, 2)\n",
        "axes[0].plot(latent_dim_list, avg_mae_list, 'm--o')\n",
        "axes[0].set_title('Avg MAE over varying latent dimensions')\n",
        "axes[0].set_xlabel('Latent Dimension')\n",
        "axes[0].set_ylabel('MAE')\n",
        "\n",
        "axes[1].plot(latent_dim_list, avg_mse_list, 'g--o')\n",
        "axes[1].set_title('Avg MSE over varying latent dimensions')\n",
        "axes[1].set_xlabel('Latentdimension')\n",
        "axes[1].set_ylabel('MSE')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "index = 0\n",
        "min = 100\n",
        "min_index = 0\n",
        "for i in avg_mae_list:\n",
        "  if i<min:\n",
        "    min = i\n",
        "    min_index = index\n",
        "  index+=1\n",
        "print(f'minimum avg mae over latent dimensions = {min} at latent dimension = {latent_dim_list[min_index]} and mse = {avg_mse_list[min_index]}')\n",
        "\n",
        "index = 0\n",
        "min = 100\n",
        "min_index = 0\n",
        "for i in avg_mse_list:\n",
        "  if i<min:\n",
        "    min = i\n",
        "    min_index = index\n",
        "  index+=1\n",
        "print(f'minimum avg mse over latent dimensions = {min} at laten dimension = {latent_dim_list[min_index]} and mae = {avg_mae_list[min_index]}')"
      ],
      "id": "R1nVse_jDcMn"
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Analysis_CARL_Patio_Lawn_GardenFiles",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}