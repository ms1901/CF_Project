{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ms1901/CF_Project/blob/main/Analysis/Analysis_CARL_AutomotiveFiles.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4f364b6b",
        "outputId": "5ef599f2-72a7-48fe-8928-161c8e80c196"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.7/dist-packages (2.8.0)\n",
            "Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.15.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.25.0)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (14.0.1)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.14.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tensorflow) (57.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.21.6)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (4.2.0)\n",
            "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.0.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.2)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.17.3)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.0)\n",
            "Requirement already satisfied: tensorboard<2.9,>=2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.8.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.44.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.5.3)\n",
            "Requirement already satisfied: keras<2.9,>=2.8.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.8.0)\n",
            "Collecting tf-estimator-nightly==2.8.0.dev2021122109\n",
            "  Downloading tf_estimator_nightly-2.8.0.dev2021122109-py2.py3-none-any.whl (462 kB)\n",
            "\u001b[K     |████████████████████████████████| 462 kB 5.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow) (0.37.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow) (1.5.2)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (0.4.6)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (3.3.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (0.6.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (1.8.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (1.35.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (1.0.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (2.23.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (4.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (4.2.4)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow) (4.11.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow) (3.8.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (0.4.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (2021.10.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow) (3.2.0)\n",
            "Installing collected packages: tf-estimator-nightly\n",
            "Successfully installed tf-estimator-nightly-2.8.0.dev2021122109\n"
          ]
        }
      ],
      "source": [
        "pip install --upgrade tensorflow"
      ],
      "id": "4f364b6b"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ac9149d"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import scipy.sparse as sp\n",
        "import math\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import sys\n",
        "from time import time\n",
        "import os\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ],
      "id": "2ac9149d"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q5LbbrqP0Ilv",
        "outputId": "d22b1e16-a5bd-4ec2-c1dd-344d19d72f45"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "id": "Q5LbbrqP0Ilv"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RVxfHDx8tGaF",
        "outputId": "cc95d5e2-76c4-49b1-dfd3-00075618aa4a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<module 'tensorflow._api.v2.version' from '/usr/local/lib/python3.7/dist-packages/tensorflow/_api/v2/version/__init__.py'>\n"
          ]
        }
      ],
      "source": [
        "print(tf.version)"
      ],
      "id": "RVxfHDx8tGaF"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZdenhkrjwCSg",
        "outputId": "1d890031-4846-403a-f433-4c2a661a1996"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/CF_end_proejct/Automotive\n"
          ]
        }
      ],
      "source": [
        "%cd /content/drive/MyDrive/CF_end_proejct/Automotive"
      ],
      "id": "ZdenhkrjwCSg"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RGS6qYUmtLs_",
        "outputId": "78b34db6-2741-49b4-a3d1-686c52c0fd55"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['/device:CPU:0', '/device:GPU:0']\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.python.client import device_lib\n",
        "\n",
        "def get_available_devices():\n",
        "    local_device_protos = device_lib.list_local_devices()\n",
        "    return [x.name for x in local_device_protos]\n",
        "\n",
        "print(get_available_devices())"
      ],
      "id": "RGS6qYUmtLs_"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "590f617f"
      },
      "source": [
        "ExtractData"
      ],
      "id": "590f617f"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4c63801c"
      },
      "outputs": [],
      "source": [
        "class Dataset(object):\n",
        "    \"'extract dataset from file'\"\n",
        "\n",
        "    def __init__(self, max_length, path, word_id_path):\n",
        "        self.word_id_dict = self.load_word_dict(path + word_id_path)\n",
        "        print(\"wordId_dict finished\")\n",
        "        \n",
        "        self.userReview_dict = self.load_reviews(max_length, len(self.word_id_dict), path + \"dataPreprocessingUserReviews.out\")\n",
        "        self.itemReview_dict = self.load_reviews(max_length, len(self.word_id_dict), path + \"dataPreprocessingItemReviews.out\")\n",
        "        print(\"load reviews finished\")\n",
        "        \n",
        "        self.num_users, self.num_items = len(self.userReview_dict), len(self.itemReview_dict)\n",
        "        \n",
        "        self.trainMtrx = self.load_ratingFile_as_mtrx(path + \"dataPreprocessingTrainInteraction.out\")\n",
        "        self.valRatings = self.load_ratingFile_as_list(path + \"dataPreprocessingValInteraction.out\")\n",
        "        self.testRatings = self.load_ratingFile_as_list(path + \"dataPreprocessingTestInteraction.out\")\n",
        "\n",
        "    def load_word_dict(self, path):\n",
        "        wordId_dict = {}\n",
        "\n",
        "        with open(path, \"r\") as f:\n",
        "            line = f.readline().replace(\"\\n\", \"\")\n",
        "            while line != None and line != \"\":\n",
        "                arr = line.split(\"\\t\")\n",
        "                wordId_dict[arr[0]] = int(arr[1])\n",
        "                line = f.readline().replace(\"\\n\", \"\")\n",
        "\n",
        "        return wordId_dict\n",
        "\n",
        "    def load_reviews(self, max_doc_length, padding_word_id, path):\n",
        "        entity_review_dict = {}\n",
        "\n",
        "        with open(path, \"r\") as f:\n",
        "            line = f.readline().replace(\"\\n\", \"\")\n",
        "            while line != None and line != \"\":\n",
        "                review = []\n",
        "                arr = line.split(\"\\t\")\n",
        "                entity = int(arr[0])\n",
        "                word_list = arr[1].split(\" \")\n",
        "\n",
        "                for i in range(len(word_list)):\n",
        "                    if (word_list[i] == \"\" or word_list[i] == None or (not self.word_id_dict[word_list[i]] in self.word_id_dict.keys())):\n",
        "                        continue\n",
        "                    review.append(self.word_id_dict.get(word_list[i]))\n",
        "                    if (len(review) >= max_doc_length):\n",
        "                        break\n",
        "                if (len(review) < max_doc_length):\n",
        "                    review = self.padding_word(max_doc_length, padding_word_id, review)\n",
        "                entity_review_dict[entity] = review\n",
        "                line = f.readline().replace(\"\\n\", \"\")\n",
        "        return entity_review_dict\n",
        "\n",
        "    def padding_word(self, max_size, max_word_idx, review):\n",
        "        review.extend([max_word_idx]*(max_size - len(review)))\n",
        "        return review\n",
        "\n",
        "    def load_ratingFile_as_mtrx(self, file_path):\n",
        "        mtrx = sp.dok_matrix((self.num_users, self.num_items), dtype=np.float32)\n",
        "        with open(file_path, \"r\") as f:\n",
        "            line = f.readline()\n",
        "            line = line.strip()\n",
        "            while line != None and line != \"\":\n",
        "                arr = line.split(\"\\t\")\n",
        "                user, item, rating = int(arr[0]), int(arr[1]), float(arr[2])\n",
        "                if (rating > 0):\n",
        "                    mtrx[user, item] = rating\n",
        "                line = f.readline()\n",
        "\n",
        "        return mtrx\n",
        "\n",
        "    def load_ratingFile_as_list(self, file_path):\n",
        "        rateList = []\n",
        "\n",
        "        with open(file_path, \"r\") as f:\n",
        "            line = f.readline()\n",
        "            while line != None and line != \"\":\n",
        "                arr = line.split(\"\\t\")\n",
        "                user, item = int(arr[0]), int(arr[1])\n",
        "                rate = float(arr[2])\n",
        "                rateList.append([user, item, rate])\n",
        "                line = f.readline()\n",
        "\n",
        "        return rateList"
      ],
      "id": "4c63801c"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "77e2f302"
      },
      "source": [
        "GetTest"
      ],
      "id": "77e2f302"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "523a402f"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "def get_test_list(batch_size, test_rating, user_reviews, item_reviews):\n",
        "    user_test_batchs, item_test_batchs, user_input_test_batchs, item_input_test_batchs, rating_input_test_batchs = [], [], [], [], []\n",
        "    for count in range(int(math.ceil(len(test_rating) / float(batch_size)))):\n",
        "        user_test, item_test, user_input_test, item_input_test, rating_input_test = [], [], [], [], []\n",
        "        for idx in range(batch_size):\n",
        "            index = (count * batch_size + idx)\n",
        "            if (index >= len(test_rating)):\n",
        "                break\n",
        "            rating = test_rating[index]\n",
        "            user_test.append(rating[0])\n",
        "            item_test.append(rating[1])\n",
        "            user_input_test.append(user_reviews.get(rating[0]))\n",
        "            item_input_test.append(item_reviews.get(rating[1]))\n",
        "            rating_input_test.append([rating[2]])\n",
        "        user_test_batchs.append(user_test)\n",
        "        item_test_batchs.append(item_test)\n",
        "        user_input_test_batchs.append(user_input_test)\n",
        "        item_input_test_batchs.append(item_input_test)\n",
        "        rating_input_test_batchs.append(rating_input_test)\n",
        "        #print count, len(item_input_test_batchs[count])\n",
        "    return user_test_batchs, item_test_batchs, user_input_test_batchs, item_input_test_batchs, rating_input_test_batchs"
      ],
      "id": "523a402f"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "74573912"
      },
      "source": [
        "CARL"
      ],
      "id": "74573912"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "70fbf310"
      },
      "outputs": [],
      "source": [
        "def ini_word_embed(num_words, latent_dim):\n",
        "    word_embeds = np.random.rand(num_words, latent_dim)\n",
        "    return word_embeds\n"
      ],
      "id": "70fbf310"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "15e0a2c1"
      },
      "outputs": [],
      "source": [
        "def word2vec_word_embed(num_words, latent_dim, path, word_id_dict):\n",
        "    word2vect_embed_mtrx = np.zeros((num_words, latent_dim))\n",
        "    with open(path, \"r\") as f:\n",
        "        line = f.readline()\n",
        "        while line != None and line != \"\":\n",
        "            arr = line.split(\"\\t\")\n",
        "            row_id = word_id_dict.get(arr[0])\n",
        "            vect = arr[1].strip().split(\" \")\n",
        "            for i in range(len(vect)):\n",
        "                word2vect_embed_mtrx[row_id, i] = float(vect[i])\n",
        "            line = f.readline()\n",
        "\n",
        "    return word2vect_embed_mtrx"
      ],
      "id": "15e0a2c1"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cabc3fef"
      },
      "outputs": [],
      "source": [
        "def get_train_instance(train):\n",
        "    user_input, item_input, rates = [], [], []\n",
        "\n",
        "    for (u, i) in train.keys():\n",
        "        # positive instance\n",
        "        user_input.append(u)\n",
        "        item_input.append(i)\n",
        "        rates.append(train[u,i])\n",
        "    return user_input, item_input, rates\n",
        "\n"
      ],
      "id": "cabc3fef"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e6abcbc9"
      },
      "outputs": [],
      "source": [
        "def get_train_instance_batch_change(count, batch_size, user_input, item_input, ratings, user_reviews, item_reviews):\n",
        "    users_batch, items_batch, user_input_batch, item_input_batch, labels_batch = [], [], [], [], []\n",
        "\n",
        "    for idx in range(batch_size):\n",
        "        index = (count*batch_size + idx) % len(user_input)\n",
        "        users_batch.append(user_input[index])\n",
        "        items_batch.append(item_input[index])\n",
        "        user_input_batch.append(user_reviews.get(user_input[index]))\n",
        "        item_input_batch.append(item_reviews.get(item_input[index]))\n",
        "        labels_batch.append([ratings[index]])\n",
        "\n",
        "    return users_batch, items_batch, user_input_batch, item_input_batch, labels_batch"
      ],
      "id": "e6abcbc9"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "38ffbde1"
      },
      "outputs": [],
      "source": [
        "#review.py\n",
        "def  cnn_model_average(filters, user_reviews_representation_expnd, item_reviews_representation_expnd, W_u, W_i, W_u_1, W_i_1, rand_matrix):\n",
        "    #convolution layer\n",
        "    convU = tf.nn.conv2d(user_reviews_representation_expnd, W_u, strides=[1, 1, word_latent_dim, 1], padding='SAME')\n",
        "    convI = tf.nn.conv2d(item_reviews_representation_expnd, W_i, strides=[1, 1, word_latent_dim, 1], padding='SAME')\n",
        "\n",
        "    hU = tf.nn.relu(tf.squeeze(convU, 2))\n",
        "    hI = tf.nn.relu(tf.squeeze(convI, 2))\n",
        "\n",
        "    # attentive layer\n",
        "    sec_dim = int(hU.get_shape()[1])\n",
        "    tmphU = tf.reshape(hU, [-1, filters])\n",
        "    hU_mul_rand = tf.reshape(tf.matmul(tmphU, rand_matrix), [-1, sec_dim, filters])\n",
        "    f = tf.matmul(hU_mul_rand, hI, transpose_b=True)\n",
        "    f = tf.expand_dims(f, -1)\n",
        "    att1 = tf.tanh(f)\n",
        "\n",
        "    pool_user = tf.reduce_mean(att1, 2)\n",
        "    pool_item = tf.reduce_mean(att1, 1)\n",
        "\n",
        "    user_flat = tf.squeeze(pool_user, -1)\n",
        "    item_flat = tf.squeeze(pool_item, -1)\n",
        "\n",
        "    weight_user = tf.nn.softmax(user_flat)\n",
        "    weight_item = tf.nn.softmax(item_flat)\n",
        "\n",
        "    weight_user_exp = tf.expand_dims(weight_user, -1)\n",
        "    weight_item_exp = tf.expand_dims(weight_item, -1)\n",
        "\n",
        "    hU = tf.expand_dims(hU * weight_user_exp, -1)\n",
        "    hI = tf.expand_dims(hI * weight_item_exp, -1)\n",
        "\n",
        "    #abstracting layer\n",
        "    hU_1 = tf.nn.relu(tf.nn.conv2d(hU, W_u_1, strides=[1, 1, 1, 1], padding='VALID'))\n",
        "    hI_1 = tf.nn.relu(tf.nn.conv2d(hI, W_i_1, strides=[1, 1, 1, 1], padding='VALID'))\n",
        "\n",
        "    sec_dim = hU_1.get_shape()[1]\n",
        "\n",
        "    oU = tf.nn.avg_pool(hU_1, ksize=[1, sec_dim, 1, 1], strides=[1, 1, 1, 1],\n",
        "        padding='VALID')\n",
        "    oI = tf.nn.avg_pool(hI_1, ksize=[1, sec_dim, 1, 1], strides=[1, 1, 1, 1],\n",
        "        padding='VALID')\n",
        "\n",
        "    att_user = tf.squeeze(oU)\n",
        "    att_item = tf.squeeze(oI)\n",
        "    #print \"attention\", att_user.get_shape(), att_item.get_shape()\n",
        "\n",
        "    return att_user, att_item"
      ],
      "id": "38ffbde1"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6967e9a6"
      },
      "outputs": [],
      "source": [
        "def eval_model(users, items, users_inputs, items_inputs, dropout_rate, predict_rating, sess, user_batch, item_batch, user_input_batch, item_input_batch, rate_tests, rmses, maes):\n",
        "    print(sess)\n",
        "    print(\"user batch. \"+str(user_batch))\n",
        "    print(\"item batch \"+str(item_batch))\n",
        "    print(\"user_input_batch. \"+str(user_input_batch))\n",
        "    print(\"item_input_batch \"+str( item_input_batch))\n",
        "\n",
        "    print(\"before session run\")\n",
        "    predicts = sess.run(predict_rating, feed_dict={users: user_batch, items: item_batch, users_inputs: user_input_batch, items_inputs: item_input_batch, dropout_rate:1.0})\n",
        "    print(\"after session run\")\n",
        "    #print(predicts)\n",
        "    row, col = predicts.shape\n",
        "    for r in range(row):\n",
        "        rmses.append(pow((predicts[r, 0] - rate_tests[r][0]), 2))\n",
        "        maes.append(abs((predicts[r, 0] - rate_tests[r][0])))\n",
        "    print(rmses)\n",
        "    return rmses, maes\n"
      ],
      "id": "6967e9a6"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fvDJvR-tr2Ap"
      },
      "source": [
        "## Just interaction based"
      ],
      "id": "fvDJvR-tr2Ap"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "55f18043"
      },
      "outputs": [],
      "source": [
        "#combination of interaction.py and review.py\n",
        "def train_model():\n",
        "    mae_list=[]\n",
        "    mse_list=[]\n",
        "    tf.compat.v1.disable_eager_execution()\n",
        "    #I and R-----\n",
        "    users = tf.compat.v1.placeholder(tf.int32, shape=[None])\n",
        "    items = tf.compat.v1.placeholder(tf.int32, shape=[None])\n",
        "    users_inputs = tf.compat.v1.placeholder(tf.int32, shape=[None, max_doc_length])\n",
        "    items_inputs = tf.compat.v1.placeholder(tf.int32, shape=[None, max_doc_length])\n",
        "    ratings = tf.compat.v1.placeholder(tf.float32, shape=[None, 1])\n",
        "    \n",
        "    #R-----\n",
        "    dropout_rate = tf.compat.v1.placeholder(tf.float32)\n",
        "    \n",
        "    #I-----\n",
        "    user_entity_embedding = tf.Variable(tf.random.normal([num_users, latent_dim], mean=0, stddev=0.02), name=\"user_entity_embeddings\")\n",
        "    item_entity_embedding = tf.Variable(tf.random.normal([num_items, latent_dim], mean=0, stddev=0.02), name=\"item_entity_embeddings\")\n",
        "    user_entity_embeds = tf.nn.embedding_lookup(user_entity_embedding, users)\n",
        "    item_entity_embeds = tf.nn.embedding_lookup(item_entity_embedding, items)\n",
        "    \n",
        "    #R-----\n",
        "    user_bias = tf.Variable(tf.random.normal([num_users, 1], mean=0, stddev=0.02), name=\"user_bias\")\n",
        "    item_bias = tf.Variable(tf.random.normal([num_items, 1], mean=0, stddev=0.02), name=\"item_bias\")\n",
        "    user_bs = tf.nn.embedding_lookup(user_bias, users)\n",
        "    item_bs = tf.nn.embedding_lookup(item_bias, items)\n",
        "    \n",
        "    text_embedding = tf.Variable(word_embedding_mtrx, dtype=tf.float32, name=\"review_text_embeds\")\n",
        "    padding_embedding = tf.Variable(np.zeros([1, word_latent_dim]), dtype=tf.float32)\n",
        "    text_mask = tf.constant([1.0] * text_embedding.get_shape()[0] + [0.0])\n",
        "    word_embeddings = tf.concat([text_embedding, padding_embedding], 0)\n",
        "    word_embeddings = word_embeddings * tf.expand_dims(text_mask, -1)\n",
        "    user_reviews_representation = tf.nn.embedding_lookup(word_embeddings, users_inputs)\n",
        "    user_reviews_representation_expnd = tf.expand_dims(user_reviews_representation, -1)\n",
        "    item_reviews_representation = tf.nn.embedding_lookup(word_embeddings, items_inputs)\n",
        "    item_reviews_representation_expnd = tf.expand_dims(item_reviews_representation, -1)\n",
        "    \n",
        "    # CNN layers\n",
        "    W_u = tf.Variable(tf.random.truncated_normal([window_size, word_latent_dim, 1, num_filters], stddev=0.3), name=\"review_W_u\")\n",
        "    W_i = tf.Variable(tf.random.truncated_normal([window_size, word_latent_dim, 1, num_filters], stddev=0.3), name=\"review_W_i\")\n",
        "    W_u_1 = tf.Variable(tf.random.truncated_normal([window_size, num_filters, 1, num_filters], stddev=0.3), name=\"review_W_u_1\")\n",
        "    W_i_1 = tf.Variable(tf.random.truncated_normal([window_size, num_filters, 1, num_filters], stddev=0.3), name=\"review_W_i_1\")\n",
        "    # b = tf.Variable(tf.constant(0.1, shape=[self.num_filters]))\n",
        "    rand_matrix = tf.Variable(tf.random.truncated_normal([num_filters, num_filters], stddev=0.3), name=\"review_rand_matrix\")\n",
        "\n",
        "    user_embeds, item_embeds = cnn_model_average(num_filters, user_reviews_representation_expnd, item_reviews_representation_expnd, W_u, W_i, W_u_1, W_i_1, rand_matrix)\n",
        "    \n",
        "    #shared MLP layer\n",
        "    W_mlp = tf.Variable(tf.random.normal([num_filters, latent_dim], mean=0, stddev=0.02), name=\"review_W_mlp\")\n",
        "    W_mlp = tf.nn.dropout(W_mlp, dropout_rate)\n",
        "    b_mlp = tf.Variable(tf.constant(0.1, shape=[latent_dim]), name=\"review_b_mlp\")\n",
        "\n",
        "    user_embeds = tf.nn.relu(tf.matmul(user_embeds, W_mlp) + b_mlp)\n",
        "    item_embeds = tf.nn.relu(tf.matmul(item_embeds, W_mlp) + b_mlp)\n",
        "\n",
        "    embeds_sum = tf.concat([tf.multiply(user_embeds, item_embeds), user_embeds, item_embeds], 1, name=\"concat_embed\")\n",
        "    #I\n",
        "    entity_embeds_sum = tf.concat([tf.multiply(user_entity_embeds, item_entity_embeds), user_entity_embeds, item_entity_embeds],1)\n",
        "\n",
        "    #FM layer\n",
        "    #R---\n",
        "    # w_0 = tf.Variable(tf.zeros(1), name=\"review_w_0\")\n",
        "    # w_1 = tf.Variable(tf.random.truncated_normal([1, latent_dim * 3], stddev=0.3), name=\"review_w_1\")\n",
        "    # v = tf.Variable(tf.random.truncated_normal([latent_dim * 3, latent_dim * 3], stddev=0.3), name=\"review_v\")\n",
        "    \n",
        "    #I---\n",
        "    w_entity_0 = tf.Variable(tf.zeros(1), name=\"entity_w_0\")\n",
        "    w_entity_1 = tf.Variable(tf.random.truncated_normal([1, latent_dim*3], stddev=0.3), name=\"entity_w_1\")\n",
        "    v_entity = tf.Variable(tf.random.truncated_normal([latent_dim*3, v_dim], stddev=0.3), name=\"entity_v\")\n",
        "    \n",
        "    #I---\n",
        "    J_e_1 = w_entity_0 + tf.matmul(entity_embeds_sum, w_entity_1, transpose_b=True)\n",
        "    #R---\n",
        "    # J_1 = w_0 + tf.matmul(embeds_sum, w_1, transpose_b=True)\n",
        "    \n",
        "    #I---\n",
        "    entity_embeds_sum_1 = tf.expand_dims(entity_embeds_sum, -1)\n",
        "    entity_embeds_sum_2 = tf.expand_dims(entity_embeds_sum, 1)\n",
        "    #R---\n",
        "    # embeds_sum_1 = tf.expand_dims(embeds_sum, -1)\n",
        "    # embeds_sum_2 = tf.expand_dims(embeds_sum, 1)\n",
        "    \n",
        "    #I---\n",
        "    J_e_2 = tf.reduce_sum(tf.reduce_sum(tf.multiply(tf.matmul(entity_embeds_sum_1, entity_embeds_sum_2), tf.matmul(v_entity, v_entity, transpose_b=True)),2), 1, keepdims=True)\n",
        "    #J_e_3 = tf.trace(tf.multiply(tf.matmul(entity_embeds_sum_1, entity_embeds_sum_2), tf.matmul(v_entity, v_entity, transpose_b=True))) \n",
        "    \n",
        "    #R---\n",
        "    # J_2 = tf.reduce_sum(tf.reduce_sum(tf.multiply(tf.matmul(embeds_sum_1, embeds_sum_2), tf.matmul(v, v, transpose_b=True)),2), 1, keepdims=True)\n",
        "    #J_3 = tf.trace(tf.multiply(tf.matmul(embeds_sum_1, embeds_sum_2), tf.matmul(v, v, transpose_b=True)))\n",
        "    #R---\n",
        "    # J_total = (J_1 + 0.5 * (J_2))\n",
        "    \n",
        "    #I---\n",
        "    J_e_total = (J_e_1 + 0.5 * (J_e_2))\n",
        "    \n",
        "    \n",
        "    # numerator = J_total + J_e_total\n",
        "    numerator = J_e_total\n",
        "    # predict_rating = tf.divide(J_total, numerator) * J_total + tf.divide(J_e_total,numerator) * J_e_total + user_bs + item_bs\n",
        "    predict_rating = tf.divide(J_e_total,numerator) * J_e_total + user_bs + item_bs \n",
        "    loss = tf.reduce_mean(tf.math.squared_difference(predict_rating, ratings))\n",
        "    loss += lambda_1 * (tf.nn.l2_loss(W_i_1) + tf.nn.l2_loss(W_u_1) + tf.nn.l2_loss(user_entity_embedding) + tf.nn.l2_loss(item_entity_embedding) + tf.nn.l2_loss(W_u) + tf.nn.l2_loss(W_i) + tf.nn.l2_loss(v_entity) + tf.nn.l2_loss(rand_matrix) + tf.nn.l2_loss(user_bs) + tf.nn.l2_loss(item_bs))\n",
        "    train_step = tf.compat.v1.train.RMSPropOptimizer(learning_rate).minimize(loss)\n",
        "    saver = tf.compat.v1.train.Saver()\n",
        "    \n",
        "    init = tf.compat.v1.global_variables_initializer()\n",
        "    with tf.compat.v1.Session() as sess:\n",
        "        sess.run(init)\n",
        "\n",
        "        for e in range(epochs):\n",
        "            t = time()\n",
        "            loss_total = 0.0\n",
        "            count = 0.0\n",
        "            \n",
        "            for i in range(int(math.ceil(len(user_input) / float(batch_size)))):\n",
        "                user_batch, item_batch, user_input_batch, item_input_batch, rates_batch = get_train_instance_batch_change(i, batch_size,user_input, item_input, rateings, user_reviews,item_reviews)\n",
        "                _, loss_val, words = sess.run([train_step, loss, word_embeddings], feed_dict={users: user_batch, items: item_batch, users_inputs: user_input_batch, items_inputs: item_input_batch, ratings: rates_batch, dropout_rate:drop_out})\n",
        "                loss_total += loss_val\n",
        "                count += 1.0\n",
        "\n",
        "            t1 = time()            \n",
        "            #LOOK FROM HERE \n",
        "            val_mses, val_maes = [], []\n",
        "            for i in range(len(user_input_val)):\n",
        "                #eval_model(users, items, users_inputs, items_inputs, dropout_rate, predict_rating, sess, user_vals[i], item_vals[i], user_input_val[i], item_input_val[i], rating_input_val[i], val_mses, val_maes)\n",
        "                #predicts = sess.run(predict_rating, feed_dict={users: user_batch, items: item_batch, users_inputs: user_input_batch, items_inputs: item_input_batch, dropout_rate:1.0})\n",
        "                predicts = sess.run(predict_rating, feed_dict={users: user_vals[i], items: item_vals[i], users_inputs: user_input_val[i], items_inputs: item_input_val[i], ratings: rating_input_val[i], dropout_rate: drop_out})\n",
        "                #print(predicts)\n",
        "                row, col = predicts.shape\n",
        "                for r in range(row):\n",
        "                    val_mses.append(pow((predicts[r, 0] - rating_input_val[i][r][0]), 2))\n",
        "                    val_maes.append(abs((predicts[r, 0] - rating_input_val[i][r][0])))\n",
        "            val_mse = np.array(val_mses).mean()            \n",
        "            t2 = time()            \n",
        "            mses, maes = [], []\n",
        "            for i in range(len(user_input_test)):\n",
        "                #eval_model(users, items, users_inputs, items_inputs, dropout_rate, predict_rating, sess, user_tests[i], item_tests[i], user_input_test[i], item_input_test[i], rating_input_test[i], mses, maes)\n",
        "                predicts_test = sess.run(predict_rating, feed_dict={users: user_tests[i], items: item_tests[i], users_inputs: user_input_test[i], items_inputs: item_input_test[i], ratings: rating_input_test[i], dropout_rate: drop_out})\n",
        "                #print(predicts)\n",
        "                row, col = predicts_test.shape\n",
        "                for r in range(row):\n",
        "                    mses.append(pow((predicts_test[r, 0] - rating_input_test[i][r][0]), 2))\n",
        "                    maes.append(abs((predicts_test[r, 0] - rating_input_test[i][r][0])))\n",
        "                    \n",
        "            mse = np.array(mses).mean()\n",
        "            mae = np.array(maes).mean()\n",
        "            t3 = time()\n",
        "            mae_list.append(mae)\n",
        "            mse_list.append(mse)\n",
        "            print(\"epoch%d train time: %.3fs test time: %.3f  loss = %.3f val_mse = %.3f mse = %.3f mae = %.3f\"%(e, (t1 - t), (t3 - t2), loss_total/count, val_mse, mse, mae))\n",
        "        \n",
        "        avg_mae=sum(mae_list) / len(mae_list)\n",
        "        avg_mse=sum(mse_list)/len(mse_list)\n",
        "        avg_mae_list.append(avg_mae)\n",
        "        avg_mse_list.append(avg_mse)\n",
        "        print(\"MAE \"+str(avg_mae))\n",
        "        print(\"MSE \"+str(avg_mse))"
      ],
      "id": "55f18043"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a924fe38",
        "outputId": "01b919d2-783c-42c8-dda1-5a1646f9d50c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "wordId_dict finished\n",
            "load reviews finished\n",
            "load data: 4.373s\n",
            "2928 1835\n",
            "15\n",
            "shape (18131, 300)\n",
            "16326 16326 16326\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/training/rmsprop.py:188: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "epoch0 train time: 3.565s test time: 0.177  loss = 207.310 val_mse = 19.616 mse = 19.941 mae = 4.369\n",
            "epoch1 train time: 1.046s test time: 0.180  loss = 154.172 val_mse = 18.113 mse = 18.474 mae = 4.196\n",
            "epoch2 train time: 1.061s test time: 0.168  loss = 92.014 val_mse = 15.944 mse = 16.388 mae = 3.928\n",
            "epoch3 train time: 1.039s test time: 0.190  loss = 52.357 val_mse = 12.229 mse = 12.866 mae = 3.346\n",
            "epoch4 train time: 1.061s test time: 0.176  loss = 28.639 val_mse = 8.704 mse = 9.539 mae = 2.687\n",
            "epoch5 train time: 1.031s test time: 0.168  loss = 16.148 val_mse = 6.385 mse = 7.364 mae = 2.253\n",
            "epoch6 train time: 1.042s test time: 0.166  loss = 10.472 val_mse = 4.843 mse = 5.926 mae = 1.970\n",
            "epoch7 train time: 1.012s test time: 0.176  loss = 8.289 val_mse = 3.906 mse = 5.009 mae = 1.795\n",
            "epoch8 train time: 1.030s test time: 0.175  loss = 7.479 val_mse = 3.267 mse = 4.380 mae = 1.675\n",
            "epoch9 train time: 1.059s test time: 0.181  loss = 6.993 val_mse = 2.905 mse = 3.969 mae = 1.599\n",
            "epoch10 train time: 1.072s test time: 0.170  loss = 6.628 val_mse = 2.650 mse = 3.672 mae = 1.546\n",
            "epoch11 train time: 1.039s test time: 0.342  loss = 6.322 val_mse = 2.509 mse = 3.475 mae = 1.513\n",
            "epoch12 train time: 1.267s test time: 0.169  loss = 6.057 val_mse = 2.403 mse = 3.326 mae = 1.491\n",
            "epoch13 train time: 1.046s test time: 0.185  loss = 5.810 val_mse = 2.350 mse = 3.224 mae = 1.478\n",
            "epoch14 train time: 1.056s test time: 0.170  loss = 5.576 val_mse = 2.307 mse = 3.141 mae = 1.470\n",
            "epoch15 train time: 1.051s test time: 0.164  loss = 5.346 val_mse = 2.293 mse = 3.083 mae = 1.467\n",
            "epoch16 train time: 1.033s test time: 0.173  loss = 5.121 val_mse = 2.279 mse = 3.030 mae = 1.465\n",
            "epoch17 train time: 1.043s test time: 0.202  loss = 4.892 val_mse = 2.279 mse = 2.988 mae = 1.466\n",
            "epoch18 train time: 1.043s test time: 0.189  loss = 4.661 val_mse = 2.277 mse = 2.947 mae = 1.467\n",
            "epoch19 train time: 1.050s test time: 0.175  loss = 4.426 val_mse = 2.277 mse = 2.906 mae = 1.467\n",
            "epoch20 train time: 1.042s test time: 0.171  loss = 4.185 val_mse = 2.272 mse = 2.861 mae = 1.466\n",
            "epoch21 train time: 1.042s test time: 0.184  loss = 3.937 val_mse = 2.255 mse = 2.804 mae = 1.461\n",
            "epoch22 train time: 1.049s test time: 0.169  loss = 3.684 val_mse = 2.215 mse = 2.728 mae = 1.448\n",
            "epoch23 train time: 1.043s test time: 0.170  loss = 3.428 val_mse = 2.131 mse = 2.613 mae = 1.419\n",
            "epoch24 train time: 1.043s test time: 0.178  loss = 3.181 val_mse = 2.002 mse = 2.460 mae = 1.374\n",
            "epoch25 train time: 1.044s test time: 0.193  loss = 2.952 val_mse = 1.857 mse = 2.294 mae = 1.323\n",
            "epoch26 train time: 1.048s test time: 0.165  loss = 2.740 val_mse = 1.717 mse = 2.133 mae = 1.272\n",
            "epoch27 train time: 1.066s test time: 0.172  loss = 2.545 val_mse = 1.587 mse = 1.984 mae = 1.223\n",
            "epoch28 train time: 1.042s test time: 0.172  loss = 2.366 val_mse = 1.471 mse = 1.848 mae = 1.177\n",
            "epoch29 train time: 1.041s test time: 0.169  loss = 2.203 val_mse = 1.366 mse = 1.725 mae = 1.133\n",
            "epoch30 train time: 1.041s test time: 0.168  loss = 2.053 val_mse = 1.272 mse = 1.613 mae = 1.092\n",
            "epoch31 train time: 1.045s test time: 0.179  loss = 1.917 val_mse = 1.189 mse = 1.513 mae = 1.053\n",
            "epoch32 train time: 1.046s test time: 0.169  loss = 1.794 val_mse = 1.114 mse = 1.424 mae = 1.017\n",
            "epoch33 train time: 1.051s test time: 0.170  loss = 1.682 val_mse = 1.048 mse = 1.343 mae = 0.983\n",
            "epoch34 train time: 1.074s test time: 0.173  loss = 1.580 val_mse = 0.990 mse = 1.272 mae = 0.951\n",
            "epoch35 train time: 1.066s test time: 0.173  loss = 1.489 val_mse = 0.939 mse = 1.208 mae = 0.921\n",
            "epoch36 train time: 1.050s test time: 0.181  loss = 1.406 val_mse = 0.895 mse = 1.151 mae = 0.893\n",
            "epoch37 train time: 1.028s test time: 0.171  loss = 1.331 val_mse = 0.856 mse = 1.100 mae = 0.867\n",
            "epoch38 train time: 1.044s test time: 0.172  loss = 1.264 val_mse = 0.821 mse = 1.054 mae = 0.842\n",
            "epoch39 train time: 1.048s test time: 0.171  loss = 1.204 val_mse = 0.791 mse = 1.014 mae = 0.818\n",
            "epoch40 train time: 1.034s test time: 0.188  loss = 1.151 val_mse = 0.765 mse = 0.979 mae = 0.797\n",
            "epoch41 train time: 1.037s test time: 0.179  loss = 1.103 val_mse = 0.743 mse = 0.948 mae = 0.777\n",
            "epoch42 train time: 1.062s test time: 0.168  loss = 1.062 val_mse = 0.724 mse = 0.922 mae = 0.759\n",
            "epoch43 train time: 1.039s test time: 0.169  loss = 1.025 val_mse = 0.707 mse = 0.900 mae = 0.743\n",
            "epoch44 train time: 1.030s test time: 0.185  loss = 0.993 val_mse = 0.693 mse = 0.880 mae = 0.728\n",
            "epoch45 train time: 1.051s test time: 0.174  loss = 0.966 val_mse = 0.682 mse = 0.864 mae = 0.715\n",
            "epoch46 train time: 1.066s test time: 0.169  loss = 0.942 val_mse = 0.673 mse = 0.851 mae = 0.703\n",
            "epoch47 train time: 1.049s test time: 0.177  loss = 0.922 val_mse = 0.665 mse = 0.839 mae = 0.693\n",
            "epoch48 train time: 1.049s test time: 0.188  loss = 0.905 val_mse = 0.658 mse = 0.831 mae = 0.685\n",
            "epoch49 train time: 1.045s test time: 0.170  loss = 0.891 val_mse = 0.653 mse = 0.823 mae = 0.677\n",
            "epoch50 train time: 1.078s test time: 0.178  loss = 0.879 val_mse = 0.649 mse = 0.818 mae = 0.671\n",
            "epoch51 train time: 1.021s test time: 0.171  loss = 0.870 val_mse = 0.645 mse = 0.813 mae = 0.666\n",
            "epoch52 train time: 1.016s test time: 0.182  loss = 0.862 val_mse = 0.642 mse = 0.810 mae = 0.662\n",
            "epoch53 train time: 1.026s test time: 0.167  loss = 0.855 val_mse = 0.639 mse = 0.807 mae = 0.659\n",
            "epoch54 train time: 1.030s test time: 0.171  loss = 0.849 val_mse = 0.636 mse = 0.804 mae = 0.656\n",
            "epoch55 train time: 1.055s test time: 0.166  loss = 0.844 val_mse = 0.634 mse = 0.802 mae = 0.653\n",
            "epoch56 train time: 1.025s test time: 0.179  loss = 0.840 val_mse = 0.631 mse = 0.800 mae = 0.651\n",
            "epoch57 train time: 1.010s test time: 0.168  loss = 0.836 val_mse = 0.629 mse = 0.798 mae = 0.649\n",
            "epoch58 train time: 1.006s test time: 0.174  loss = 0.832 val_mse = 0.627 mse = 0.797 mae = 0.647\n",
            "epoch59 train time: 1.523s test time: 0.166  loss = 0.829 val_mse = 0.625 mse = 0.795 mae = 0.646\n",
            "epoch60 train time: 1.655s test time: 0.311  loss = 0.825 val_mse = 0.623 mse = 0.794 mae = 0.644\n",
            "epoch61 train time: 1.018s test time: 0.173  loss = 0.823 val_mse = 0.621 mse = 0.793 mae = 0.643\n",
            "epoch62 train time: 1.015s test time: 0.161  loss = 0.820 val_mse = 0.619 mse = 0.792 mae = 0.642\n",
            "epoch63 train time: 1.016s test time: 0.157  loss = 0.817 val_mse = 0.617 mse = 0.791 mae = 0.641\n",
            "epoch64 train time: 1.022s test time: 0.158  loss = 0.815 val_mse = 0.616 mse = 0.790 mae = 0.640\n",
            "epoch65 train time: 1.008s test time: 0.165  loss = 0.812 val_mse = 0.614 mse = 0.790 mae = 0.639\n",
            "epoch66 train time: 1.015s test time: 0.165  loss = 0.810 val_mse = 0.613 mse = 0.789 mae = 0.637\n",
            "epoch67 train time: 1.025s test time: 0.179  loss = 0.807 val_mse = 0.611 mse = 0.788 mae = 0.637\n",
            "epoch68 train time: 1.030s test time: 0.173  loss = 0.805 val_mse = 0.610 mse = 0.788 mae = 0.635\n",
            "epoch69 train time: 1.028s test time: 0.173  loss = 0.802 val_mse = 0.608 mse = 0.787 mae = 0.635\n",
            "epoch70 train time: 1.027s test time: 0.171  loss = 0.800 val_mse = 0.607 mse = 0.787 mae = 0.634\n",
            "epoch71 train time: 1.034s test time: 0.183  loss = 0.798 val_mse = 0.605 mse = 0.786 mae = 0.633\n",
            "epoch72 train time: 1.040s test time: 0.174  loss = 0.796 val_mse = 0.604 mse = 0.786 mae = 0.632\n",
            "epoch73 train time: 1.011s test time: 0.164  loss = 0.793 val_mse = 0.602 mse = 0.785 mae = 0.631\n",
            "epoch74 train time: 1.010s test time: 0.160  loss = 0.792 val_mse = 0.601 mse = 0.785 mae = 0.630\n",
            "epoch75 train time: 1.005s test time: 0.161  loss = 0.789 val_mse = 0.600 mse = 0.785 mae = 0.629\n",
            "epoch76 train time: 0.996s test time: 0.168  loss = 0.787 val_mse = 0.599 mse = 0.785 mae = 0.629\n",
            "epoch77 train time: 1.017s test time: 0.166  loss = 0.785 val_mse = 0.597 mse = 0.785 mae = 0.628\n",
            "epoch78 train time: 1.002s test time: 0.160  loss = 0.783 val_mse = 0.596 mse = 0.785 mae = 0.627\n",
            "epoch79 train time: 1.008s test time: 0.160  loss = 0.781 val_mse = 0.595 mse = 0.785 mae = 0.627\n",
            "epoch80 train time: 1.011s test time: 0.160  loss = 0.779 val_mse = 0.594 mse = 0.785 mae = 0.626\n",
            "epoch81 train time: 1.013s test time: 0.168  loss = 0.777 val_mse = 0.592 mse = 0.785 mae = 0.625\n",
            "epoch82 train time: 1.010s test time: 0.160  loss = 0.775 val_mse = 0.592 mse = 0.785 mae = 0.625\n",
            "epoch83 train time: 1.009s test time: 0.160  loss = 0.773 val_mse = 0.591 mse = 0.785 mae = 0.624\n",
            "epoch84 train time: 1.000s test time: 0.173  loss = 0.771 val_mse = 0.590 mse = 0.786 mae = 0.623\n",
            "epoch85 train time: 1.014s test time: 0.171  loss = 0.770 val_mse = 0.589 mse = 0.786 mae = 0.623\n",
            "epoch86 train time: 0.994s test time: 0.163  loss = 0.768 val_mse = 0.588 mse = 0.787 mae = 0.622\n",
            "epoch87 train time: 0.998s test time: 0.155  loss = 0.766 val_mse = 0.587 mse = 0.787 mae = 0.622\n",
            "epoch88 train time: 1.007s test time: 0.160  loss = 0.764 val_mse = 0.586 mse = 0.788 mae = 0.621\n",
            "epoch89 train time: 0.998s test time: 0.159  loss = 0.762 val_mse = 0.585 mse = 0.788 mae = 0.622\n",
            "epoch90 train time: 0.998s test time: 0.173  loss = 0.761 val_mse = 0.585 mse = 0.789 mae = 0.621\n",
            "epoch91 train time: 0.986s test time: 0.158  loss = 0.759 val_mse = 0.584 mse = 0.789 mae = 0.621\n",
            "epoch92 train time: 1.007s test time: 0.158  loss = 0.757 val_mse = 0.584 mse = 0.790 mae = 0.620\n",
            "epoch93 train time: 1.029s test time: 0.172  loss = 0.755 val_mse = 0.583 mse = 0.791 mae = 0.620\n",
            "epoch94 train time: 1.022s test time: 0.163  loss = 0.754 val_mse = 0.582 mse = 0.791 mae = 0.619\n",
            "epoch95 train time: 1.021s test time: 0.173  loss = 0.752 val_mse = 0.582 mse = 0.792 mae = 0.619\n",
            "epoch96 train time: 1.022s test time: 0.173  loss = 0.751 val_mse = 0.581 mse = 0.793 mae = 0.619\n",
            "epoch97 train time: 1.008s test time: 0.170  loss = 0.749 val_mse = 0.581 mse = 0.794 mae = 0.619\n",
            "epoch98 train time: 0.997s test time: 0.161  loss = 0.747 val_mse = 0.580 mse = 0.795 mae = 0.619\n",
            "epoch99 train time: 1.006s test time: 0.162  loss = 0.746 val_mse = 0.580 mse = 0.796 mae = 0.619\n",
            "epoch100 train time: 1.009s test time: 0.178  loss = 0.744 val_mse = 0.580 mse = 0.797 mae = 0.618\n",
            "epoch101 train time: 1.003s test time: 0.164  loss = 0.743 val_mse = 0.580 mse = 0.798 mae = 0.619\n",
            "epoch102 train time: 1.026s test time: 0.166  loss = 0.741 val_mse = 0.579 mse = 0.799 mae = 0.618\n",
            "epoch103 train time: 0.987s test time: 0.160  loss = 0.740 val_mse = 0.579 mse = 0.801 mae = 0.619\n",
            "epoch104 train time: 1.003s test time: 0.156  loss = 0.738 val_mse = 0.579 mse = 0.801 mae = 0.619\n",
            "epoch105 train time: 0.999s test time: 0.163  loss = 0.737 val_mse = 0.579 mse = 0.803 mae = 0.619\n",
            "epoch106 train time: 1.008s test time: 0.155  loss = 0.736 val_mse = 0.579 mse = 0.804 mae = 0.619\n",
            "epoch107 train time: 1.007s test time: 0.167  loss = 0.734 val_mse = 0.579 mse = 0.806 mae = 0.620\n",
            "epoch108 train time: 1.010s test time: 0.166  loss = 0.733 val_mse = 0.579 mse = 0.807 mae = 0.619\n",
            "epoch109 train time: 1.020s test time: 0.178  loss = 0.731 val_mse = 0.579 mse = 0.809 mae = 0.620\n",
            "epoch110 train time: 1.028s test time: 0.165  loss = 0.730 val_mse = 0.579 mse = 0.810 mae = 0.620\n",
            "epoch111 train time: 1.017s test time: 0.167  loss = 0.729 val_mse = 0.579 mse = 0.812 mae = 0.621\n",
            "epoch112 train time: 0.998s test time: 0.158  loss = 0.727 val_mse = 0.579 mse = 0.813 mae = 0.621\n",
            "epoch113 train time: 1.003s test time: 0.159  loss = 0.726 val_mse = 0.579 mse = 0.815 mae = 0.622\n",
            "epoch114 train time: 1.015s test time: 0.174  loss = 0.725 val_mse = 0.579 mse = 0.816 mae = 0.622\n",
            "epoch115 train time: 0.992s test time: 0.164  loss = 0.724 val_mse = 0.580 mse = 0.819 mae = 0.623\n",
            "epoch116 train time: 0.992s test time: 0.159  loss = 0.722 val_mse = 0.579 mse = 0.819 mae = 0.623\n",
            "epoch117 train time: 1.002s test time: 0.154  loss = 0.721 val_mse = 0.581 mse = 0.823 mae = 0.624\n",
            "epoch118 train time: 0.981s test time: 0.167  loss = 0.720 val_mse = 0.580 mse = 0.823 mae = 0.624\n",
            "epoch119 train time: 0.996s test time: 0.178  loss = 0.719 val_mse = 0.580 mse = 0.826 mae = 0.625\n",
            "epoch120 train time: 0.996s test time: 0.165  loss = 0.717 val_mse = 0.580 mse = 0.826 mae = 0.625\n",
            "epoch121 train time: 1.014s test time: 0.158  loss = 0.716 val_mse = 0.582 mse = 0.830 mae = 0.626\n",
            "epoch122 train time: 1.002s test time: 0.159  loss = 0.715 val_mse = 0.580 mse = 0.830 mae = 0.627\n",
            "epoch123 train time: 1.017s test time: 0.158  loss = 0.714 val_mse = 0.581 mse = 0.834 mae = 0.628\n",
            "epoch124 train time: 1.013s test time: 0.173  loss = 0.713 val_mse = 0.581 mse = 0.834 mae = 0.628\n",
            "epoch125 train time: 1.018s test time: 0.181  loss = 0.712 val_mse = 0.582 mse = 0.838 mae = 0.630\n",
            "epoch126 train time: 1.010s test time: 0.162  loss = 0.711 val_mse = 0.581 mse = 0.838 mae = 0.630\n",
            "epoch127 train time: 1.005s test time: 0.166  loss = 0.710 val_mse = 0.583 mse = 0.842 mae = 0.631\n",
            "epoch128 train time: 1.012s test time: 0.168  loss = 0.709 val_mse = 0.582 mse = 0.842 mae = 0.632\n",
            "epoch129 train time: 1.020s test time: 0.159  loss = 0.708 val_mse = 0.584 mse = 0.846 mae = 0.633\n",
            "epoch130 train time: 1.024s test time: 0.162  loss = 0.707 val_mse = 0.583 mse = 0.846 mae = 0.634\n",
            "epoch131 train time: 1.015s test time: 0.162  loss = 0.706 val_mse = 0.584 mse = 0.851 mae = 0.635\n",
            "epoch132 train time: 1.015s test time: 0.163  loss = 0.705 val_mse = 0.585 mse = 0.850 mae = 0.636\n",
            "epoch133 train time: 1.003s test time: 0.175  loss = 0.703 val_mse = 0.586 mse = 0.856 mae = 0.638\n",
            "epoch134 train time: 1.015s test time: 0.160  loss = 0.703 val_mse = 0.586 mse = 0.855 mae = 0.638\n",
            "epoch135 train time: 1.016s test time: 0.158  loss = 0.702 val_mse = 0.586 mse = 0.861 mae = 0.640\n",
            "epoch136 train time: 0.990s test time: 0.166  loss = 0.701 val_mse = 0.590 mse = 0.859 mae = 0.641\n",
            "epoch137 train time: 1.029s test time: 0.169  loss = 0.700 val_mse = 0.587 mse = 0.866 mae = 0.643\n",
            "epoch138 train time: 1.032s test time: 0.177  loss = 0.700 val_mse = 0.590 mse = 0.864 mae = 0.643\n",
            "epoch139 train time: 1.022s test time: 0.169  loss = 0.698 val_mse = 0.587 mse = 0.870 mae = 0.645\n",
            "epoch140 train time: 1.014s test time: 0.168  loss = 0.698 val_mse = 0.592 mse = 0.868 mae = 0.645\n",
            "epoch141 train time: 1.039s test time: 0.170  loss = 0.697 val_mse = 0.586 mse = 0.876 mae = 0.648\n",
            "epoch142 train time: 1.025s test time: 0.165  loss = 0.696 val_mse = 0.594 mse = 0.872 mae = 0.647\n",
            "epoch143 train time: 0.990s test time: 0.157  loss = 0.695 val_mse = 0.587 mse = 0.882 mae = 0.651\n",
            "epoch144 train time: 0.995s test time: 0.175  loss = 0.696 val_mse = 0.595 mse = 0.877 mae = 0.650\n",
            "epoch145 train time: 1.021s test time: 0.156  loss = 0.694 val_mse = 0.590 mse = 0.885 mae = 0.654\n",
            "epoch146 train time: 0.982s test time: 0.159  loss = 0.693 val_mse = 0.596 mse = 0.884 mae = 0.653\n",
            "epoch147 train time: 1.000s test time: 0.173  loss = 0.694 val_mse = 0.590 mse = 0.891 mae = 0.656\n",
            "epoch148 train time: 1.007s test time: 0.170  loss = 0.692 val_mse = 0.600 mse = 0.886 mae = 0.655\n",
            "epoch149 train time: 1.002s test time: 0.161  loss = 0.691 val_mse = 0.592 mse = 0.897 mae = 0.660\n",
            "epoch150 train time: 0.981s test time: 0.157  loss = 0.692 val_mse = 0.602 mse = 0.891 mae = 0.658\n",
            "epoch151 train time: 1.011s test time: 0.157  loss = 0.690 val_mse = 0.595 mse = 0.899 mae = 0.662\n",
            "epoch152 train time: 0.992s test time: 0.181  loss = 0.690 val_mse = 0.603 mse = 0.899 mae = 0.661\n",
            "epoch153 train time: 1.000s test time: 0.165  loss = 0.690 val_mse = 0.595 mse = 0.906 mae = 0.665\n",
            "epoch154 train time: 0.986s test time: 0.164  loss = 0.689 val_mse = 0.608 mse = 0.900 mae = 0.663\n",
            "epoch155 train time: 0.997s test time: 0.163  loss = 0.687 val_mse = 0.601 mse = 0.913 mae = 0.668\n",
            "epoch156 train time: 1.010s test time: 0.171  loss = 0.689 val_mse = 0.607 mse = 0.907 mae = 0.666\n",
            "epoch157 train time: 1.023s test time: 0.181  loss = 0.687 val_mse = 0.601 mse = 0.914 mae = 0.670\n",
            "epoch158 train time: 1.018s test time: 0.158  loss = 0.687 val_mse = 0.610 mse = 0.914 mae = 0.670\n",
            "epoch159 train time: 1.021s test time: 0.162  loss = 0.687 val_mse = 0.602 mse = 0.920 mae = 0.673\n",
            "epoch160 train time: 1.021s test time: 0.165  loss = 0.686 val_mse = 0.613 mse = 0.916 mae = 0.673\n",
            "epoch161 train time: 1.040s test time: 0.172  loss = 0.685 val_mse = 0.603 mse = 0.928 mae = 0.677\n",
            "epoch162 train time: 1.013s test time: 0.170  loss = 0.687 val_mse = 0.614 mse = 0.922 mae = 0.676\n",
            "epoch163 train time: 1.009s test time: 0.163  loss = 0.684 val_mse = 0.610 mse = 0.930 mae = 0.679\n",
            "epoch164 train time: 1.023s test time: 0.163  loss = 0.685 val_mse = 0.615 mse = 0.929 mae = 0.680\n",
            "epoch165 train time: 0.996s test time: 0.162  loss = 0.684 val_mse = 0.607 mse = 0.936 mae = 0.682\n",
            "epoch166 train time: 0.999s test time: 0.169  loss = 0.684 val_mse = 0.618 mse = 0.932 mae = 0.683\n",
            "epoch167 train time: 0.985s test time: 0.157  loss = 0.682 val_mse = 0.607 mse = 0.943 mae = 0.685\n",
            "epoch168 train time: 0.991s test time: 0.164  loss = 0.685 val_mse = 0.621 mse = 0.937 mae = 0.686\n",
            "epoch169 train time: 1.003s test time: 0.159  loss = 0.682 val_mse = 0.611 mse = 0.944 mae = 0.687\n",
            "epoch170 train time: 1.007s test time: 0.161  loss = 0.683 val_mse = 0.622 mse = 0.944 mae = 0.690\n",
            "epoch171 train time: 0.997s test time: 0.166  loss = 0.682 val_mse = 0.612 mse = 0.950 mae = 0.690\n",
            "epoch172 train time: 1.006s test time: 0.170  loss = 0.682 val_mse = 0.627 mse = 0.946 mae = 0.692\n",
            "epoch173 train time: 0.998s test time: 0.166  loss = 0.682 val_mse = 0.615 mse = 0.957 mae = 0.693\n",
            "epoch174 train time: 1.010s test time: 0.157  loss = 0.684 val_mse = 0.630 mse = 0.952 mae = 0.695\n",
            "epoch175 train time: 0.998s test time: 0.160  loss = 0.681 val_mse = 0.619 mse = 0.958 mae = 0.695\n",
            "epoch176 train time: 0.985s test time: 0.165  loss = 0.681 val_mse = 0.634 mse = 0.958 mae = 0.698\n",
            "epoch177 train time: 1.017s test time: 0.159  loss = 0.681 val_mse = 0.617 mse = 0.962 mae = 0.697\n",
            "epoch178 train time: 0.984s test time: 0.163  loss = 0.682 val_mse = 0.638 mse = 0.960 mae = 0.700\n",
            "epoch179 train time: 0.992s test time: 0.163  loss = 0.681 val_mse = 0.621 mse = 0.968 mae = 0.701\n",
            "epoch180 train time: 0.996s test time: 0.157  loss = 0.682 val_mse = 0.643 mse = 0.964 mae = 0.702\n",
            "epoch181 train time: 0.984s test time: 0.162  loss = 0.680 val_mse = 0.623 mse = 0.974 mae = 0.704\n",
            "epoch182 train time: 0.988s test time: 0.159  loss = 0.682 val_mse = 0.640 mse = 0.970 mae = 0.706\n",
            "epoch183 train time: 0.993s test time: 0.160  loss = 0.680 val_mse = 0.626 mse = 0.979 mae = 0.707\n",
            "epoch184 train time: 1.000s test time: 0.161  loss = 0.681 val_mse = 0.648 mse = 0.975 mae = 0.708\n",
            "epoch185 train time: 1.012s test time: 0.158  loss = 0.680 val_mse = 0.627 mse = 0.983 mae = 0.709\n",
            "epoch186 train time: 0.988s test time: 0.162  loss = 0.683 val_mse = 0.648 mse = 0.980 mae = 0.711\n",
            "epoch187 train time: 0.991s test time: 0.171  loss = 0.680 val_mse = 0.630 mse = 0.987 mae = 0.713\n",
            "epoch188 train time: 0.983s test time: 0.175  loss = 0.681 val_mse = 0.650 mse = 0.988 mae = 0.714\n",
            "epoch189 train time: 1.010s test time: 0.155  loss = 0.682 val_mse = 0.629 mse = 0.990 mae = 0.714\n",
            "epoch190 train time: 1.005s test time: 0.158  loss = 0.681 val_mse = 0.656 mse = 0.988 mae = 0.716\n",
            "epoch191 train time: 0.988s test time: 0.159  loss = 0.681 val_mse = 0.633 mse = 0.998 mae = 0.718\n",
            "epoch192 train time: 1.004s test time: 0.169  loss = 0.682 val_mse = 0.657 mse = 0.992 mae = 0.718\n",
            "epoch193 train time: 1.011s test time: 0.170  loss = 0.681 val_mse = 0.632 mse = 1.001 mae = 0.719\n",
            "epoch194 train time: 1.012s test time: 0.163  loss = 0.683 val_mse = 0.660 mse = 0.997 mae = 0.720\n",
            "epoch195 train time: 0.993s test time: 0.162  loss = 0.682 val_mse = 0.638 mse = 1.002 mae = 0.721\n",
            "epoch196 train time: 0.985s test time: 0.159  loss = 0.681 val_mse = 0.661 mse = 1.002 mae = 0.723\n",
            "epoch197 train time: 0.998s test time: 0.172  loss = 0.682 val_mse = 0.638 mse = 1.004 mae = 0.722\n",
            "epoch198 train time: 0.999s test time: 0.159  loss = 0.682 val_mse = 0.665 mse = 1.006 mae = 0.725\n",
            "epoch199 train time: 1.013s test time: 0.168  loss = 0.684 val_mse = 0.638 mse = 1.008 mae = 0.724\n",
            "epoch200 train time: 1.002s test time: 0.159  loss = 0.682 val_mse = 0.667 mse = 1.004 mae = 0.726\n",
            "epoch201 train time: 0.984s test time: 0.156  loss = 0.682 val_mse = 0.639 mse = 1.013 mae = 0.727\n",
            "epoch202 train time: 0.995s test time: 0.178  loss = 0.683 val_mse = 0.669 mse = 1.008 mae = 0.727\n",
            "epoch203 train time: 0.991s test time: 0.155  loss = 0.683 val_mse = 0.642 mse = 1.015 mae = 0.728\n",
            "epoch204 train time: 0.993s test time: 0.166  loss = 0.684 val_mse = 0.670 mse = 1.012 mae = 0.729\n",
            "epoch205 train time: 0.999s test time: 0.159  loss = 0.682 val_mse = 0.647 mse = 1.019 mae = 0.731\n",
            "epoch206 train time: 0.995s test time: 0.158  loss = 0.682 val_mse = 0.675 mse = 1.017 mae = 0.732\n",
            "epoch207 train time: 1.003s test time: 0.166  loss = 0.684 val_mse = 0.647 mse = 1.020 mae = 0.731\n",
            "epoch208 train time: 0.998s test time: 0.155  loss = 0.683 val_mse = 0.675 mse = 1.020 mae = 0.734\n",
            "epoch209 train time: 0.997s test time: 0.151  loss = 0.685 val_mse = 0.647 mse = 1.020 mae = 0.731\n",
            "epoch210 train time: 1.000s test time: 0.166  loss = 0.683 val_mse = 0.685 mse = 1.020 mae = 0.735\n",
            "epoch211 train time: 1.001s test time: 0.162  loss = 0.684 val_mse = 0.651 mse = 1.029 mae = 0.734\n",
            "epoch212 train time: 0.997s test time: 0.162  loss = 0.684 val_mse = 0.683 mse = 1.020 mae = 0.736\n",
            "epoch213 train time: 1.005s test time: 0.166  loss = 0.683 val_mse = 0.651 mse = 1.034 mae = 0.737\n",
            "epoch214 train time: 1.000s test time: 0.165  loss = 0.685 val_mse = 0.689 mse = 1.026 mae = 0.737\n",
            "epoch215 train time: 1.000s test time: 0.164  loss = 0.684 val_mse = 0.656 mse = 1.031 mae = 0.737\n",
            "epoch216 train time: 1.012s test time: 0.156  loss = 0.685 val_mse = 0.689 mse = 1.025 mae = 0.739\n",
            "epoch217 train time: 0.998s test time: 0.178  loss = 0.685 val_mse = 0.657 mse = 1.032 mae = 0.738\n",
            "epoch218 train time: 1.008s test time: 0.169  loss = 0.685 val_mse = 0.694 mse = 1.035 mae = 0.741\n",
            "epoch219 train time: 1.025s test time: 0.165  loss = 0.687 val_mse = 0.656 mse = 1.033 mae = 0.739\n",
            "epoch220 train time: 1.009s test time: 0.160  loss = 0.686 val_mse = 0.696 mse = 1.032 mae = 0.741\n",
            "epoch221 train time: 0.997s test time: 0.155  loss = 0.685 val_mse = 0.659 mse = 1.038 mae = 0.742\n",
            "epoch222 train time: 0.999s test time: 0.169  loss = 0.687 val_mse = 0.701 mse = 1.031 mae = 0.742\n",
            "epoch223 train time: 1.028s test time: 0.171  loss = 0.686 val_mse = 0.661 mse = 1.040 mae = 0.743\n",
            "epoch224 train time: 1.021s test time: 0.169  loss = 0.687 val_mse = 0.698 mse = 1.035 mae = 0.744\n",
            "epoch225 train time: 1.010s test time: 0.158  loss = 0.687 val_mse = 0.662 mse = 1.038 mae = 0.743\n",
            "epoch226 train time: 1.033s test time: 0.176  loss = 0.687 val_mse = 0.704 mse = 1.039 mae = 0.745\n",
            "epoch227 train time: 1.014s test time: 0.168  loss = 0.687 val_mse = 0.665 mse = 1.043 mae = 0.745\n",
            "epoch228 train time: 1.003s test time: 0.165  loss = 0.688 val_mse = 0.702 mse = 1.036 mae = 0.746\n",
            "epoch229 train time: 0.991s test time: 0.163  loss = 0.687 val_mse = 0.668 mse = 1.047 mae = 0.747\n",
            "epoch230 train time: 1.014s test time: 0.165  loss = 0.688 val_mse = 0.707 mse = 1.042 mae = 0.747\n",
            "epoch231 train time: 0.998s test time: 0.165  loss = 0.688 val_mse = 0.671 mse = 1.041 mae = 0.746\n",
            "epoch232 train time: 1.013s test time: 0.163  loss = 0.687 val_mse = 0.703 mse = 1.042 mae = 0.749\n",
            "epoch233 train time: 1.003s test time: 0.155  loss = 0.687 val_mse = 0.674 mse = 1.044 mae = 0.747\n",
            "epoch234 train time: 1.012s test time: 0.152  loss = 0.689 val_mse = 0.705 mse = 1.043 mae = 0.749\n",
            "epoch235 train time: 0.997s test time: 0.162  loss = 0.689 val_mse = 0.677 mse = 1.045 mae = 0.747\n",
            "epoch236 train time: 0.993s test time: 0.167  loss = 0.688 val_mse = 0.704 mse = 1.040 mae = 0.748\n",
            "epoch237 train time: 0.986s test time: 0.155  loss = 0.690 val_mse = 0.678 mse = 1.042 mae = 0.747\n",
            "epoch238 train time: 0.993s test time: 0.159  loss = 0.688 val_mse = 0.706 mse = 1.049 mae = 0.751\n",
            "epoch239 train time: 0.992s test time: 0.154  loss = 0.689 val_mse = 0.680 mse = 1.043 mae = 0.748\n",
            "MAE 0.8343969736732412\n",
            "MSE 1.4630443995149833\n"
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    print(sys.path[0])\n",
        "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
        "    word_latent_dim = 300\n",
        "    latent_dim = 15\n",
        "    max_doc_length = 300\n",
        "    num_filters = 50\n",
        "    window_size = 3\n",
        "    v_dim = 50\n",
        "    learning_rate = 0.001\n",
        "    lambda_1 = 0.05 #normally we set from [0.05, 0.01, 0.005, 0.001]\n",
        "    drop_out = 0.8\n",
        "    batch_size = 200\n",
        "    epochs = 240\n",
        "    avg_mae_list = []\n",
        "    avg_mse_list = []\n",
        "    \n",
        "    # loading data\n",
        "    firTime = time()\n",
        "    dataSet = Dataset(max_doc_length, sys.path[0], \"dataPreprocessingWordDict.out\")\n",
        "    word_dict, user_reviews, item_reviews, train, valRatings, testRatings = dataSet.word_id_dict, dataSet.userReview_dict, dataSet.itemReview_dict, dataSet.trainMtrx, dataSet.valRatings, dataSet.testRatings\n",
        "    secTime = time()\n",
        "\n",
        "    num_users, num_items = train.shape\n",
        "    print(\"load data: %.3fs\" % (secTime - firTime))\n",
        "    print(num_users, num_items)\n",
        "    print(latent_dim)\n",
        "\n",
        "    #load word embeddings\n",
        "    word_embedding_mtrx = ini_word_embed(len(word_dict), word_latent_dim)\n",
        "    # word_embedding_mtrx = word2vec_word_embed(len(word_dict), word_latent_dim,\n",
        "    #                                           \"Directory of pretrained WordEmbedding.out\",\n",
        "    #                                           word_dict)\n",
        "\n",
        "    print( \"shape\", word_embedding_mtrx.shape)\n",
        "\n",
        "    # get train instances\n",
        "    user_input, item_input, rateings = get_train_instance(train)\n",
        "    print (len(user_input), len(item_input), len(rateings))\n",
        "\n",
        "    # get test/val instances\n",
        "    user_vals, item_vals, user_input_val, item_input_val, rating_input_val = get_test_list(200, valRatings, user_reviews, item_reviews)\n",
        "    user_tests, item_tests, user_input_test, item_input_test, rating_input_test = get_test_list(200, testRatings, user_reviews, item_reviews)\n",
        "\n",
        "    #train & eval model\n",
        "    train_model()"
      ],
      "id": "a924fe38"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oXgncTlIrwjP"
      },
      "source": [
        "## Just review based"
      ],
      "id": "oXgncTlIrwjP"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nZI3Juflqt2L"
      },
      "outputs": [],
      "source": [
        "#combination of interaction.py and review.py\n",
        "def train_model():\n",
        "    mae_list=[]\n",
        "    mse_list=[]\n",
        "    tf.compat.v1.disable_eager_execution()\n",
        "    #I and R-----\n",
        "    users = tf.compat.v1.placeholder(tf.int32, shape=[None])\n",
        "    items = tf.compat.v1.placeholder(tf.int32, shape=[None])\n",
        "    users_inputs = tf.compat.v1.placeholder(tf.int32, shape=[None, max_doc_length])\n",
        "    items_inputs = tf.compat.v1.placeholder(tf.int32, shape=[None, max_doc_length])\n",
        "    ratings = tf.compat.v1.placeholder(tf.float32, shape=[None, 1])\n",
        "    \n",
        "    #R-----\n",
        "    dropout_rate = tf.compat.v1.placeholder(tf.float32)\n",
        "    \n",
        "    #I-----\n",
        "    # user_entity_embedding = tf.Variable(tf.random.normal([num_users, latent_dim], mean=0, stddev=0.02), name=\"user_entity_embeddings\")\n",
        "    # item_entity_embedding = tf.Variable(tf.random.normal([num_items, latent_dim], mean=0, stddev=0.02), name=\"item_entity_embeddings\")\n",
        "    # user_entity_embeds = tf.nn.embedding_lookup(user_entity_embedding, users)\n",
        "    # item_entity_embeds = tf.nn.embedding_lookup(item_entity_embedding, items)\n",
        "    \n",
        "    #R-----\n",
        "    user_bias = tf.Variable(tf.random.normal([num_users, 1], mean=0, stddev=0.02), name=\"user_bias\")\n",
        "    item_bias = tf.Variable(tf.random.normal([num_items, 1], mean=0, stddev=0.02), name=\"item_bias\")\n",
        "    user_bs = tf.nn.embedding_lookup(user_bias, users)\n",
        "    item_bs = tf.nn.embedding_lookup(item_bias, items)\n",
        "    \n",
        "    text_embedding = tf.Variable(word_embedding_mtrx, dtype=tf.float32, name=\"review_text_embeds\")\n",
        "    padding_embedding = tf.Variable(np.zeros([1, word_latent_dim]), dtype=tf.float32)\n",
        "    text_mask = tf.constant([1.0] * text_embedding.get_shape()[0] + [0.0])\n",
        "    word_embeddings = tf.concat([text_embedding, padding_embedding], 0)\n",
        "    word_embeddings = word_embeddings * tf.expand_dims(text_mask, -1)\n",
        "    user_reviews_representation = tf.nn.embedding_lookup(word_embeddings, users_inputs)\n",
        "    user_reviews_representation_expnd = tf.expand_dims(user_reviews_representation, -1)\n",
        "    item_reviews_representation = tf.nn.embedding_lookup(word_embeddings, items_inputs)\n",
        "    item_reviews_representation_expnd = tf.expand_dims(item_reviews_representation, -1)\n",
        "    \n",
        "    # CNN layers\n",
        "    W_u = tf.Variable(tf.random.truncated_normal([window_size, word_latent_dim, 1, num_filters], stddev=0.3), name=\"review_W_u\")\n",
        "    W_i = tf.Variable(tf.random.truncated_normal([window_size, word_latent_dim, 1, num_filters], stddev=0.3), name=\"review_W_i\")\n",
        "    W_u_1 = tf.Variable(tf.random.truncated_normal([window_size, num_filters, 1, num_filters], stddev=0.3), name=\"review_W_u_1\")\n",
        "    W_i_1 = tf.Variable(tf.random.truncated_normal([window_size, num_filters, 1, num_filters], stddev=0.3), name=\"review_W_i_1\")\n",
        "    # b = tf.Variable(tf.constant(0.1, shape=[self.num_filters]))\n",
        "    rand_matrix = tf.Variable(tf.random.truncated_normal([num_filters, num_filters], stddev=0.3), name=\"review_rand_matrix\")\n",
        "\n",
        "    user_embeds, item_embeds = cnn_model_average(num_filters, user_reviews_representation_expnd, item_reviews_representation_expnd, W_u, W_i, W_u_1, W_i_1, rand_matrix)\n",
        "    \n",
        "    #shared MLP layer\n",
        "    W_mlp = tf.Variable(tf.random.normal([num_filters, latent_dim], mean=0, stddev=0.02), name=\"review_W_mlp\")\n",
        "    W_mlp = tf.nn.dropout(W_mlp, dropout_rate)\n",
        "    b_mlp = tf.Variable(tf.constant(0.1, shape=[latent_dim]), name=\"review_b_mlp\")\n",
        "\n",
        "    user_embeds = tf.nn.relu(tf.matmul(user_embeds, W_mlp) + b_mlp)\n",
        "    item_embeds = tf.nn.relu(tf.matmul(item_embeds, W_mlp) + b_mlp)\n",
        "\n",
        "    embeds_sum = tf.concat([tf.multiply(user_embeds, item_embeds), user_embeds, item_embeds], 1, name=\"concat_embed\")\n",
        "    #I\n",
        "    # entity_embeds_sum = tf.concat([tf.multiply(user_entity_embeds, item_entity_embeds), user_entity_embeds, item_entity_embeds],1)\n",
        "\n",
        "    #FM layer\n",
        "    #R---\n",
        "    w_0 = tf.Variable(tf.zeros(1), name=\"review_w_0\")\n",
        "    w_1 = tf.Variable(tf.random.truncated_normal([1, latent_dim * 3], stddev=0.3), name=\"review_w_1\")\n",
        "    v = tf.Variable(tf.random.truncated_normal([latent_dim * 3, latent_dim * 3], stddev=0.3), name=\"review_v\")\n",
        "    \n",
        "    #I---\n",
        "    # w_entity_0 = tf.Variable(tf.zeros(1), name=\"entity_w_0\")\n",
        "    # w_entity_1 = tf.Variable(tf.random.truncated_normal([1, latent_dim*3], stddev=0.3), name=\"entity_w_1\")\n",
        "    # v_entity = tf.Variable(tf.random.truncated_normal([latent_dim*3, v_dim], stddev=0.3), name=\"entity_v\")\n",
        "    \n",
        "    #I---\n",
        "    # J_e_1 = w_entity_0 + tf.matmul(entity_embeds_sum, w_entity_1, transpose_b=True)\n",
        "    #R---\n",
        "    J_1 = w_0 + tf.matmul(embeds_sum, w_1, transpose_b=True)\n",
        "    \n",
        "    #I---\n",
        "    # entity_embeds_sum_1 = tf.expand_dims(entity_embeds_sum, -1)\n",
        "    # entity_embeds_sum_2 = tf.expand_dims(entity_embeds_sum, 1)\n",
        "    #R---\n",
        "    embeds_sum_1 = tf.expand_dims(embeds_sum, -1)\n",
        "    embeds_sum_2 = tf.expand_dims(embeds_sum, 1)\n",
        "    \n",
        "    #I---\n",
        "    # J_e_2 = tf.reduce_sum(tf.reduce_sum(tf.multiply(tf.matmul(entity_embeds_sum_1, entity_embeds_sum_2), tf.matmul(v_entity, v_entity, transpose_b=True)),2), 1, keepdims=True)\n",
        "    #J_e_3 = tf.trace(tf.multiply(tf.matmul(entity_embeds_sum_1, entity_embeds_sum_2), tf.matmul(v_entity, v_entity, transpose_b=True))) \n",
        "    \n",
        "    #R---\n",
        "    J_2 = tf.reduce_sum(tf.reduce_sum(tf.multiply(tf.matmul(embeds_sum_1, embeds_sum_2), tf.matmul(v, v, transpose_b=True)),2), 1, keepdims=True)\n",
        "    #J_3 = tf.trace(tf.multiply(tf.matmul(embeds_sum_1, embeds_sum_2), tf.matmul(v, v, transpose_b=True)))\n",
        "    #R---\n",
        "    J_total = (J_1 + 0.5 * (J_2))\n",
        "    \n",
        "    #I---\n",
        "    # J_e_total = (J_e_1 + 0.5 * (J_e_2))\n",
        "    \n",
        "    \n",
        "    # numerator = J_total + J_e_total\n",
        "    numerator = J_total\n",
        "    # predict_rating = tf.divide(J_total, numerator) * J_total + tf.divide(J_e_total,numerator) * J_e_total + user_bs + item_bs \n",
        "    predict_rating = tf.divide(J_total, numerator) * J_total + user_bs + item_bs\n",
        "    loss = tf.reduce_mean(tf.math.squared_difference(predict_rating, ratings))\n",
        "    loss += lambda_1 * (tf.nn.l2_loss(W_i_1) + tf.nn.l2_loss(W_u_1) + tf.nn.l2_loss(W_u) + tf.nn.l2_loss(W_i) + tf.nn.l2_loss(v) + tf.nn.l2_loss(rand_matrix) + tf.nn.l2_loss(user_bs) + tf.nn.l2_loss(item_bs))\n",
        "    \n",
        "    train_step = tf.compat.v1.train.RMSPropOptimizer(learning_rate).minimize(loss)\n",
        "    saver = tf.compat.v1.train.Saver()\n",
        "    \n",
        "    init = tf.compat.v1.global_variables_initializer()\n",
        "    with tf.compat.v1.Session() as sess:\n",
        "        sess.run(init)\n",
        "\n",
        "        for e in range(epochs):\n",
        "            t = time()\n",
        "            loss_total = 0.0\n",
        "            count = 0.0\n",
        "            \n",
        "            for i in range(int(math.ceil(len(user_input) / float(batch_size)))):\n",
        "                user_batch, item_batch, user_input_batch, item_input_batch, rates_batch = get_train_instance_batch_change(i, batch_size,user_input, item_input, rateings, user_reviews,item_reviews)\n",
        "                _, loss_val, words = sess.run([train_step, loss, word_embeddings], feed_dict={users: user_batch, items: item_batch, users_inputs: user_input_batch, items_inputs: item_input_batch, ratings: rates_batch, dropout_rate:drop_out})\n",
        "                loss_total += loss_val\n",
        "                count += 1.0\n",
        "\n",
        "            t1 = time()            \n",
        "            #LOOK FROM HERE \n",
        "            val_mses, val_maes = [], []\n",
        "            for i in range(len(user_input_val)):\n",
        "                #eval_model(users, items, users_inputs, items_inputs, dropout_rate, predict_rating, sess, user_vals[i], item_vals[i], user_input_val[i], item_input_val[i], rating_input_val[i], val_mses, val_maes)\n",
        "                #predicts = sess.run(predict_rating, feed_dict={users: user_batch, items: item_batch, users_inputs: user_input_batch, items_inputs: item_input_batch, dropout_rate:1.0})\n",
        "                predicts = sess.run(predict_rating, feed_dict={users: user_vals[i], items: item_vals[i], users_inputs: user_input_val[i], items_inputs: item_input_val[i], ratings: rating_input_val[i], dropout_rate: drop_out})\n",
        "                #print(predicts)\n",
        "                row, col = predicts.shape\n",
        "                for r in range(row):\n",
        "                    val_mses.append(pow((predicts[r, 0] - rating_input_val[i][r][0]), 2))\n",
        "                    val_maes.append(abs((predicts[r, 0] - rating_input_val[i][r][0])))\n",
        "            val_mse = np.array(val_mses).mean()            \n",
        "            t2 = time()            \n",
        "            mses, maes = [], []\n",
        "            for i in range(len(user_input_test)):\n",
        "                #eval_model(users, items, users_inputs, items_inputs, dropout_rate, predict_rating, sess, user_tests[i], item_tests[i], user_input_test[i], item_input_test[i], rating_input_test[i], mses, maes)\n",
        "                predicts_test = sess.run(predict_rating, feed_dict={users: user_tests[i], items: item_tests[i], users_inputs: user_input_test[i], items_inputs: item_input_test[i], ratings: rating_input_test[i], dropout_rate: drop_out})\n",
        "                #print(predicts)\n",
        "                row, col = predicts_test.shape\n",
        "                for r in range(row):\n",
        "                    mses.append(pow((predicts_test[r, 0] - rating_input_test[i][r][0]), 2))\n",
        "                    maes.append(abs((predicts_test[r, 0] - rating_input_test[i][r][0])))\n",
        "                    \n",
        "            mse = np.array(mses).mean()\n",
        "            mae = np.array(maes).mean()\n",
        "            t3 = time()\n",
        "            mae_list.append(mae)\n",
        "            mse_list.append(mse)\n",
        "            print(\"epoch%d train time: %.3fs test time: %.3f  loss = %.3f val_mse = %.3f mse = %.3f mae = %.3f\"%(e, (t1 - t), (t3 - t2), loss_total/count, val_mse, mse, mae))\n",
        "        \n",
        "        avg_mae=sum(mae_list) / len(mae_list)\n",
        "        avg_mse=sum(mse_list)/len(mse_list)\n",
        "        avg_mae_list.append(avg_mae)\n",
        "        avg_mse_list.append(avg_mse)\n",
        "        print(\"MAE \"+str(avg_mae))\n",
        "        print(\"MSE \"+str(avg_mse))"
      ],
      "id": "nZI3Juflqt2L"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7nFkpGRcsAzb",
        "outputId": "7ae2fd44-d165-4b9e-97f3-638cfae4f3ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "wordId_dict finished\n",
            "load reviews finished\n",
            "load data: 1.548s\n",
            "2928 1835\n",
            "15\n",
            "shape (18131, 300)\n",
            "16326 16326 16326\n",
            "epoch0 train time: 4.389s test time: 0.474  loss = 193.088 val_mse = 0.890 mse = 0.887 mae = 0.772\n",
            "epoch1 train time: 4.055s test time: 0.462  loss = 134.663 val_mse = 0.876 mse = 0.868 mae = 0.749\n",
            "epoch2 train time: 4.053s test time: 0.472  loss = 74.897 val_mse = 0.875 mse = 0.867 mae = 0.749\n",
            "epoch3 train time: 4.064s test time: 0.470  loss = 37.898 val_mse = 0.874 mse = 0.866 mae = 0.748\n",
            "epoch4 train time: 4.078s test time: 0.462  loss = 17.161 val_mse = 0.873 mse = 0.866 mae = 0.748\n",
            "epoch5 train time: 4.069s test time: 0.474  loss = 6.864 val_mse = 0.871 mse = 0.865 mae = 0.747\n",
            "epoch6 train time: 4.081s test time: 0.468  loss = 2.626 val_mse = 0.870 mse = 0.864 mae = 0.746\n",
            "epoch7 train time: 4.080s test time: 0.466  loss = 1.379 val_mse = 0.868 mse = 0.863 mae = 0.745\n",
            "epoch8 train time: 4.084s test time: 0.471  loss = 1.176 val_mse = 0.867 mse = 0.862 mae = 0.744\n",
            "epoch9 train time: 4.079s test time: 0.476  loss = 1.123 val_mse = 0.865 mse = 0.861 mae = 0.743\n",
            "epoch10 train time: 4.081s test time: 0.467  loss = 1.084 val_mse = 0.864 mse = 0.860 mae = 0.743\n",
            "epoch11 train time: 4.076s test time: 0.466  loss = 1.054 val_mse = 0.862 mse = 0.860 mae = 0.742\n",
            "epoch12 train time: 4.082s test time: 0.471  loss = 1.030 val_mse = 0.861 mse = 0.859 mae = 0.741\n",
            "epoch13 train time: 4.100s test time: 0.468  loss = 1.011 val_mse = 0.859 mse = 0.858 mae = 0.740\n",
            "epoch14 train time: 4.094s test time: 0.474  loss = 0.995 val_mse = 0.858 mse = 0.857 mae = 0.740\n",
            "epoch15 train time: 4.090s test time: 0.472  loss = 0.981 val_mse = 0.856 mse = 0.857 mae = 0.739\n",
            "epoch16 train time: 4.091s test time: 0.467  loss = 0.970 val_mse = 0.855 mse = 0.856 mae = 0.738\n",
            "epoch17 train time: 4.108s test time: 0.474  loss = 0.960 val_mse = 0.853 mse = 0.855 mae = 0.738\n",
            "epoch18 train time: 4.109s test time: 0.477  loss = 0.951 val_mse = 0.852 mse = 0.855 mae = 0.737\n",
            "epoch19 train time: 4.117s test time: 0.471  loss = 0.944 val_mse = 0.851 mse = 0.854 mae = 0.737\n",
            "epoch20 train time: 4.112s test time: 0.472  loss = 0.937 val_mse = 0.849 mse = 0.853 mae = 0.736\n",
            "epoch21 train time: 4.281s test time: 0.847  loss = 0.931 val_mse = 0.848 mse = 0.853 mae = 0.736\n",
            "epoch22 train time: 4.284s test time: 0.473  loss = 0.926 val_mse = 0.847 mse = 0.852 mae = 0.735\n",
            "epoch23 train time: 4.118s test time: 0.475  loss = 0.921 val_mse = 0.846 mse = 0.852 mae = 0.735\n",
            "epoch24 train time: 4.115s test time: 0.481  loss = 0.917 val_mse = 0.845 mse = 0.851 mae = 0.734\n",
            "epoch25 train time: 4.109s test time: 0.483  loss = 0.913 val_mse = 0.844 mse = 0.851 mae = 0.734\n",
            "epoch26 train time: 4.120s test time: 0.479  loss = 0.909 val_mse = 0.843 mse = 0.850 mae = 0.734\n",
            "epoch27 train time: 4.118s test time: 0.467  loss = 0.905 val_mse = 0.842 mse = 0.850 mae = 0.733\n",
            "epoch28 train time: 4.130s test time: 0.474  loss = 0.902 val_mse = 0.841 mse = 0.850 mae = 0.733\n",
            "epoch29 train time: 4.114s test time: 0.474  loss = 0.899 val_mse = 0.840 mse = 0.849 mae = 0.733\n",
            "epoch30 train time: 4.116s test time: 0.479  loss = 0.896 val_mse = 0.839 mse = 0.849 mae = 0.733\n",
            "epoch31 train time: 4.129s test time: 0.468  loss = 0.894 val_mse = 0.838 mse = 0.849 mae = 0.733\n",
            "epoch32 train time: 4.139s test time: 0.468  loss = 0.891 val_mse = 0.837 mse = 0.848 mae = 0.733\n",
            "epoch33 train time: 4.128s test time: 0.474  loss = 0.889 val_mse = 0.836 mse = 0.848 mae = 0.733\n",
            "epoch34 train time: 4.143s test time: 0.470  loss = 0.886 val_mse = 0.836 mse = 0.848 mae = 0.733\n",
            "epoch35 train time: 4.144s test time: 0.477  loss = 0.884 val_mse = 0.835 mse = 0.847 mae = 0.732\n",
            "epoch36 train time: 4.167s test time: 0.476  loss = 0.882 val_mse = 0.834 mse = 0.847 mae = 0.733\n",
            "epoch37 train time: 4.160s test time: 0.472  loss = 0.880 val_mse = 0.834 mse = 0.848 mae = 0.733\n",
            "epoch38 train time: 4.149s test time: 0.468  loss = 0.878 val_mse = 0.833 mse = 0.848 mae = 0.733\n",
            "epoch39 train time: 4.161s test time: 0.473  loss = 0.877 val_mse = 0.833 mse = 0.848 mae = 0.734\n",
            "epoch40 train time: 4.140s test time: 0.479  loss = 0.875 val_mse = 0.831 mse = 0.847 mae = 0.732\n",
            "epoch41 train time: 4.146s test time: 0.479  loss = 0.874 val_mse = 0.831 mse = 0.847 mae = 0.734\n",
            "epoch42 train time: 4.154s test time: 0.471  loss = 0.872 val_mse = 0.830 mse = 0.847 mae = 0.734\n",
            "epoch43 train time: 4.157s test time: 0.482  loss = 0.871 val_mse = 0.830 mse = 0.847 mae = 0.734\n",
            "epoch44 train time: 4.166s test time: 0.479  loss = 0.870 val_mse = 0.829 mse = 0.846 mae = 0.733\n",
            "epoch45 train time: 4.155s test time: 0.470  loss = 0.869 val_mse = 0.829 mse = 0.847 mae = 0.735\n",
            "epoch46 train time: 4.149s test time: 0.483  loss = 0.867 val_mse = 0.828 mse = 0.846 mae = 0.733\n",
            "epoch47 train time: 4.164s test time: 0.479  loss = 0.867 val_mse = 0.827 mse = 0.846 mae = 0.734\n",
            "epoch48 train time: 4.160s test time: 0.481  loss = 0.866 val_mse = 0.827 mse = 0.846 mae = 0.734\n",
            "epoch49 train time: 4.149s test time: 0.469  loss = 0.865 val_mse = 0.826 mse = 0.845 mae = 0.733\n",
            "epoch50 train time: 4.163s test time: 0.472  loss = 0.865 val_mse = 0.825 mse = 0.844 mae = 0.732\n",
            "epoch51 train time: 4.142s test time: 0.478  loss = 0.864 val_mse = 0.824 mse = 0.845 mae = 0.732\n",
            "epoch52 train time: 4.168s test time: 0.473  loss = 0.863 val_mse = 0.824 mse = 0.844 mae = 0.732\n",
            "epoch53 train time: 4.150s test time: 0.476  loss = 0.863 val_mse = 0.823 mse = 0.844 mae = 0.732\n",
            "epoch54 train time: 4.150s test time: 0.473  loss = 0.862 val_mse = 0.822 mse = 0.843 mae = 0.732\n",
            "epoch55 train time: 4.145s test time: 0.472  loss = 0.862 val_mse = 0.821 mse = 0.843 mae = 0.731\n",
            "epoch56 train time: 4.145s test time: 0.472  loss = 0.861 val_mse = 0.821 mse = 0.843 mae = 0.731\n",
            "epoch57 train time: 4.146s test time: 0.471  loss = 0.861 val_mse = 0.820 mse = 0.843 mae = 0.732\n",
            "epoch58 train time: 4.167s test time: 0.483  loss = 0.860 val_mse = 0.819 mse = 0.842 mae = 0.731\n",
            "epoch59 train time: 4.134s test time: 0.477  loss = 0.860 val_mse = 0.819 mse = 0.842 mae = 0.731\n",
            "epoch60 train time: 4.152s test time: 0.477  loss = 0.859 val_mse = 0.818 mse = 0.842 mae = 0.731\n",
            "epoch61 train time: 4.135s test time: 0.470  loss = 0.859 val_mse = 0.817 mse = 0.841 mae = 0.730\n",
            "epoch62 train time: 4.152s test time: 0.481  loss = 0.858 val_mse = 0.817 mse = 0.842 mae = 0.731\n",
            "epoch63 train time: 4.140s test time: 0.470  loss = 0.858 val_mse = 0.816 mse = 0.840 mae = 0.729\n",
            "epoch64 train time: 4.141s test time: 0.481  loss = 0.857 val_mse = 0.815 mse = 0.840 mae = 0.729\n",
            "epoch65 train time: 4.144s test time: 0.478  loss = 0.857 val_mse = 0.815 mse = 0.840 mae = 0.730\n",
            "epoch66 train time: 4.131s test time: 0.484  loss = 0.856 val_mse = 0.814 mse = 0.840 mae = 0.730\n",
            "epoch67 train time: 4.144s test time: 0.473  loss = 0.856 val_mse = 0.814 mse = 0.840 mae = 0.730\n",
            "epoch68 train time: 4.141s test time: 0.479  loss = 0.855 val_mse = 0.812 mse = 0.839 mae = 0.729\n",
            "epoch69 train time: 4.137s test time: 0.472  loss = 0.855 val_mse = 0.812 mse = 0.839 mae = 0.729\n",
            "epoch70 train time: 4.144s test time: 0.471  loss = 0.854 val_mse = 0.811 mse = 0.839 mae = 0.729\n",
            "epoch71 train time: 4.150s test time: 0.475  loss = 0.854 val_mse = 0.811 mse = 0.839 mae = 0.729\n",
            "epoch72 train time: 4.138s test time: 0.476  loss = 0.853 val_mse = 0.810 mse = 0.839 mae = 0.729\n",
            "epoch73 train time: 4.158s test time: 0.474  loss = 0.853 val_mse = 0.809 mse = 0.838 mae = 0.728\n",
            "epoch74 train time: 4.130s test time: 0.469  loss = 0.852 val_mse = 0.809 mse = 0.838 mae = 0.728\n",
            "epoch75 train time: 4.154s test time: 0.473  loss = 0.852 val_mse = 0.808 mse = 0.838 mae = 0.728\n",
            "epoch76 train time: 4.154s test time: 0.477  loss = 0.851 val_mse = 0.807 mse = 0.837 mae = 0.728\n",
            "epoch77 train time: 4.148s test time: 0.469  loss = 0.851 val_mse = 0.807 mse = 0.837 mae = 0.728\n",
            "epoch78 train time: 4.144s test time: 0.472  loss = 0.850 val_mse = 0.806 mse = 0.836 mae = 0.727\n",
            "epoch79 train time: 4.148s test time: 0.478  loss = 0.850 val_mse = 0.805 mse = 0.837 mae = 0.728\n",
            "epoch80 train time: 4.142s test time: 0.484  loss = 0.849 val_mse = 0.805 mse = 0.836 mae = 0.728\n",
            "epoch81 train time: 4.146s test time: 0.471  loss = 0.849 val_mse = 0.804 mse = 0.835 mae = 0.726\n",
            "epoch82 train time: 4.154s test time: 0.481  loss = 0.849 val_mse = 0.803 mse = 0.836 mae = 0.727\n",
            "epoch83 train time: 4.130s test time: 0.480  loss = 0.848 val_mse = 0.803 mse = 0.835 mae = 0.726\n",
            "epoch84 train time: 4.161s test time: 0.476  loss = 0.848 val_mse = 0.802 mse = 0.835 mae = 0.727\n",
            "epoch85 train time: 4.162s test time: 0.478  loss = 0.847 val_mse = 0.801 mse = 0.834 mae = 0.726\n",
            "epoch86 train time: 4.140s test time: 0.467  loss = 0.847 val_mse = 0.800 mse = 0.834 mae = 0.725\n",
            "epoch87 train time: 4.152s test time: 0.478  loss = 0.846 val_mse = 0.800 mse = 0.834 mae = 0.726\n",
            "epoch88 train time: 4.135s test time: 0.470  loss = 0.846 val_mse = 0.799 mse = 0.833 mae = 0.724\n",
            "epoch89 train time: 4.142s test time: 0.471  loss = 0.845 val_mse = 0.798 mse = 0.833 mae = 0.724\n",
            "epoch90 train time: 4.154s test time: 0.482  loss = 0.845 val_mse = 0.798 mse = 0.834 mae = 0.726\n",
            "epoch91 train time: 4.136s test time: 0.474  loss = 0.844 val_mse = 0.797 mse = 0.832 mae = 0.724\n",
            "epoch92 train time: 4.153s test time: 0.474  loss = 0.844 val_mse = 0.797 mse = 0.832 mae = 0.725\n",
            "epoch93 train time: 4.155s test time: 0.473  loss = 0.843 val_mse = 0.796 mse = 0.832 mae = 0.724\n",
            "epoch94 train time: 4.136s test time: 0.477  loss = 0.843 val_mse = 0.796 mse = 0.833 mae = 0.726\n",
            "epoch95 train time: 4.153s test time: 0.480  loss = 0.842 val_mse = 0.794 mse = 0.831 mae = 0.722\n",
            "epoch96 train time: 4.142s test time: 0.481  loss = 0.842 val_mse = 0.794 mse = 0.831 mae = 0.723\n",
            "epoch97 train time: 4.144s test time: 0.473  loss = 0.842 val_mse = 0.793 mse = 0.830 mae = 0.722\n",
            "epoch98 train time: 4.141s test time: 0.474  loss = 0.841 val_mse = 0.793 mse = 0.831 mae = 0.724\n",
            "epoch99 train time: 4.151s test time: 0.469  loss = 0.841 val_mse = 0.792 mse = 0.830 mae = 0.723\n",
            "epoch100 train time: 4.145s test time: 0.474  loss = 0.841 val_mse = 0.792 mse = 0.831 mae = 0.724\n",
            "epoch101 train time: 4.145s test time: 0.476  loss = 0.840 val_mse = 0.791 mse = 0.830 mae = 0.724\n",
            "epoch102 train time: 4.145s test time: 0.473  loss = 0.840 val_mse = 0.791 mse = 0.830 mae = 0.723\n",
            "epoch103 train time: 4.142s test time: 0.472  loss = 0.839 val_mse = 0.790 mse = 0.829 mae = 0.722\n",
            "epoch104 train time: 4.151s test time: 0.475  loss = 0.839 val_mse = 0.790 mse = 0.830 mae = 0.723\n",
            "epoch105 train time: 4.147s test time: 0.478  loss = 0.839 val_mse = 0.788 mse = 0.828 mae = 0.721\n",
            "epoch106 train time: 4.152s test time: 0.473  loss = 0.838 val_mse = 0.788 mse = 0.829 mae = 0.722\n",
            "epoch107 train time: 4.143s test time: 0.475  loss = 0.838 val_mse = 0.787 mse = 0.828 mae = 0.721\n",
            "epoch108 train time: 4.153s test time: 0.469  loss = 0.838 val_mse = 0.787 mse = 0.828 mae = 0.722\n",
            "epoch109 train time: 4.154s test time: 0.482  loss = 0.837 val_mse = 0.786 mse = 0.827 mae = 0.720\n",
            "epoch110 train time: 4.150s test time: 0.470  loss = 0.837 val_mse = 0.786 mse = 0.828 mae = 0.722\n",
            "epoch111 train time: 4.147s test time: 0.476  loss = 0.837 val_mse = 0.785 mse = 0.827 mae = 0.720\n",
            "epoch112 train time: 4.171s test time: 0.473  loss = 0.836 val_mse = 0.785 mse = 0.827 mae = 0.721\n",
            "epoch113 train time: 4.153s test time: 0.478  loss = 0.836 val_mse = 0.784 mse = 0.827 mae = 0.721\n",
            "epoch114 train time: 4.143s test time: 0.486  loss = 0.836 val_mse = 0.783 mse = 0.825 mae = 0.719\n",
            "epoch115 train time: 4.138s test time: 0.479  loss = 0.836 val_mse = 0.783 mse = 0.826 mae = 0.720\n",
            "epoch116 train time: 4.144s test time: 0.477  loss = 0.835 val_mse = 0.783 mse = 0.827 mae = 0.721\n",
            "epoch117 train time: 4.168s test time: 0.469  loss = 0.835 val_mse = 0.782 mse = 0.825 mae = 0.719\n",
            "epoch118 train time: 4.153s test time: 0.475  loss = 0.835 val_mse = 0.781 mse = 0.825 mae = 0.720\n",
            "epoch119 train time: 4.154s test time: 0.475  loss = 0.834 val_mse = 0.781 mse = 0.825 mae = 0.719\n",
            "epoch120 train time: 4.145s test time: 0.474  loss = 0.834 val_mse = 0.780 mse = 0.824 mae = 0.718\n",
            "epoch121 train time: 4.158s test time: 0.474  loss = 0.834 val_mse = 0.779 mse = 0.824 mae = 0.717\n",
            "epoch122 train time: 4.158s test time: 0.472  loss = 0.834 val_mse = 0.780 mse = 0.825 mae = 0.719\n",
            "epoch123 train time: 4.155s test time: 0.478  loss = 0.833 val_mse = 0.779 mse = 0.824 mae = 0.719\n",
            "epoch124 train time: 4.143s test time: 0.466  loss = 0.833 val_mse = 0.778 mse = 0.824 mae = 0.718\n",
            "epoch125 train time: 4.157s test time: 0.487  loss = 0.833 val_mse = 0.778 mse = 0.824 mae = 0.718\n",
            "epoch126 train time: 4.137s test time: 0.481  loss = 0.833 val_mse = 0.777 mse = 0.823 mae = 0.717\n",
            "epoch127 train time: 4.145s test time: 0.485  loss = 0.832 val_mse = 0.777 mse = 0.823 mae = 0.717\n",
            "epoch128 train time: 4.147s test time: 0.476  loss = 0.832 val_mse = 0.776 mse = 0.823 mae = 0.718\n",
            "epoch129 train time: 4.154s test time: 0.475  loss = 0.832 val_mse = 0.776 mse = 0.822 mae = 0.717\n",
            "epoch130 train time: 4.145s test time: 0.477  loss = 0.832 val_mse = 0.775 mse = 0.822 mae = 0.717\n",
            "epoch131 train time: 4.137s test time: 0.476  loss = 0.832 val_mse = 0.776 mse = 0.823 mae = 0.718\n",
            "epoch132 train time: 4.142s test time: 0.468  loss = 0.831 val_mse = 0.774 mse = 0.821 mae = 0.715\n",
            "epoch133 train time: 4.140s test time: 0.477  loss = 0.831 val_mse = 0.774 mse = 0.822 mae = 0.717\n",
            "epoch134 train time: 4.143s test time: 0.479  loss = 0.831 val_mse = 0.773 mse = 0.821 mae = 0.716\n",
            "epoch135 train time: 4.151s test time: 0.474  loss = 0.831 val_mse = 0.772 mse = 0.820 mae = 0.714\n",
            "epoch136 train time: 4.147s test time: 0.476  loss = 0.831 val_mse = 0.772 mse = 0.821 mae = 0.715\n",
            "epoch137 train time: 4.149s test time: 0.477  loss = 0.831 val_mse = 0.773 mse = 0.822 mae = 0.717\n",
            "epoch138 train time: 4.135s test time: 0.475  loss = 0.830 val_mse = 0.772 mse = 0.820 mae = 0.715\n",
            "epoch139 train time: 4.146s test time: 0.469  loss = 0.830 val_mse = 0.771 mse = 0.820 mae = 0.714\n",
            "epoch140 train time: 4.153s test time: 0.473  loss = 0.830 val_mse = 0.771 mse = 0.821 mae = 0.716\n",
            "epoch141 train time: 4.143s test time: 0.487  loss = 0.830 val_mse = 0.771 mse = 0.820 mae = 0.716\n",
            "epoch142 train time: 4.132s test time: 0.475  loss = 0.830 val_mse = 0.771 mse = 0.821 mae = 0.716\n",
            "epoch143 train time: 4.150s test time: 0.467  loss = 0.829 val_mse = 0.770 mse = 0.820 mae = 0.716\n",
            "epoch144 train time: 4.147s test time: 0.478  loss = 0.829 val_mse = 0.770 mse = 0.820 mae = 0.716\n",
            "epoch145 train time: 4.139s test time: 0.473  loss = 0.829 val_mse = 0.769 mse = 0.819 mae = 0.715\n",
            "epoch146 train time: 4.152s test time: 0.475  loss = 0.829 val_mse = 0.769 mse = 0.820 mae = 0.715\n",
            "epoch147 train time: 4.148s test time: 0.469  loss = 0.829 val_mse = 0.768 mse = 0.818 mae = 0.713\n",
            "epoch148 train time: 4.160s test time: 0.481  loss = 0.829 val_mse = 0.769 mse = 0.820 mae = 0.716\n",
            "epoch149 train time: 4.137s test time: 0.474  loss = 0.829 val_mse = 0.768 mse = 0.819 mae = 0.715\n",
            "epoch150 train time: 4.145s test time: 0.466  loss = 0.828 val_mse = 0.767 mse = 0.818 mae = 0.713\n",
            "epoch151 train time: 4.130s test time: 0.470  loss = 0.828 val_mse = 0.768 mse = 0.819 mae = 0.716\n",
            "epoch152 train time: 4.140s test time: 0.481  loss = 0.828 val_mse = 0.766 mse = 0.817 mae = 0.711\n",
            "epoch153 train time: 4.141s test time: 0.479  loss = 0.828 val_mse = 0.767 mse = 0.819 mae = 0.715\n",
            "epoch154 train time: 4.145s test time: 0.471  loss = 0.828 val_mse = 0.765 mse = 0.817 mae = 0.712\n",
            "epoch155 train time: 4.132s test time: 0.480  loss = 0.828 val_mse = 0.766 mse = 0.819 mae = 0.715\n",
            "epoch156 train time: 4.147s test time: 0.475  loss = 0.828 val_mse = 0.765 mse = 0.817 mae = 0.712\n",
            "epoch157 train time: 4.151s test time: 0.471  loss = 0.828 val_mse = 0.766 mse = 0.819 mae = 0.715\n",
            "epoch158 train time: 4.142s test time: 0.482  loss = 0.828 val_mse = 0.765 mse = 0.818 mae = 0.714\n",
            "epoch159 train time: 4.140s test time: 0.475  loss = 0.827 val_mse = 0.765 mse = 0.818 mae = 0.715\n",
            "epoch160 train time: 4.143s test time: 0.468  loss = 0.827 val_mse = 0.764 mse = 0.817 mae = 0.713\n",
            "epoch161 train time: 4.167s test time: 0.470  loss = 0.827 val_mse = 0.763 mse = 0.815 mae = 0.710\n",
            "epoch162 train time: 4.167s test time: 0.472  loss = 0.827 val_mse = 0.764 mse = 0.818 mae = 0.714\n",
            "epoch163 train time: 4.147s test time: 0.471  loss = 0.827 val_mse = 0.763 mse = 0.816 mae = 0.712\n",
            "epoch164 train time: 4.146s test time: 0.469  loss = 0.827 val_mse = 0.763 mse = 0.817 mae = 0.713\n",
            "epoch165 train time: 4.141s test time: 0.477  loss = 0.827 val_mse = 0.763 mse = 0.817 mae = 0.713\n",
            "epoch166 train time: 4.142s test time: 0.475  loss = 0.827 val_mse = 0.763 mse = 0.817 mae = 0.713\n",
            "epoch167 train time: 4.169s test time: 0.473  loss = 0.827 val_mse = 0.762 mse = 0.816 mae = 0.711\n",
            "epoch168 train time: 4.150s test time: 0.473  loss = 0.827 val_mse = 0.763 mse = 0.817 mae = 0.714\n",
            "epoch169 train time: 4.144s test time: 0.486  loss = 0.827 val_mse = 0.762 mse = 0.816 mae = 0.712\n",
            "epoch170 train time: 4.155s test time: 0.474  loss = 0.827 val_mse = 0.762 mse = 0.816 mae = 0.712\n",
            "epoch171 train time: 4.147s test time: 0.488  loss = 0.826 val_mse = 0.761 mse = 0.816 mae = 0.712\n",
            "epoch172 train time: 4.147s test time: 0.470  loss = 0.826 val_mse = 0.761 mse = 0.815 mae = 0.711\n",
            "epoch173 train time: 4.150s test time: 0.479  loss = 0.826 val_mse = 0.761 mse = 0.816 mae = 0.711\n",
            "epoch174 train time: 4.150s test time: 0.475  loss = 0.826 val_mse = 0.760 mse = 0.815 mae = 0.711\n",
            "epoch175 train time: 4.156s test time: 0.473  loss = 0.826 val_mse = 0.761 mse = 0.816 mae = 0.713\n",
            "epoch176 train time: 4.144s test time: 0.474  loss = 0.826 val_mse = 0.761 mse = 0.816 mae = 0.713\n",
            "epoch177 train time: 4.149s test time: 0.476  loss = 0.826 val_mse = 0.760 mse = 0.816 mae = 0.712\n",
            "epoch178 train time: 4.150s test time: 0.472  loss = 0.826 val_mse = 0.760 mse = 0.816 mae = 0.713\n",
            "epoch179 train time: 4.160s test time: 0.467  loss = 0.826 val_mse = 0.760 mse = 0.815 mae = 0.711\n",
            "epoch180 train time: 4.147s test time: 0.488  loss = 0.826 val_mse = 0.759 mse = 0.815 mae = 0.711\n",
            "epoch181 train time: 4.157s test time: 0.477  loss = 0.826 val_mse = 0.759 mse = 0.815 mae = 0.711\n",
            "epoch182 train time: 4.147s test time: 0.471  loss = 0.826 val_mse = 0.759 mse = 0.815 mae = 0.711\n",
            "epoch183 train time: 4.153s test time: 0.474  loss = 0.826 val_mse = 0.760 mse = 0.817 mae = 0.714\n",
            "epoch184 train time: 4.151s test time: 0.477  loss = 0.825 val_mse = 0.758 mse = 0.814 mae = 0.709\n",
            "epoch185 train time: 4.153s test time: 0.477  loss = 0.826 val_mse = 0.758 mse = 0.815 mae = 0.711\n",
            "epoch186 train time: 4.154s test time: 0.472  loss = 0.825 val_mse = 0.758 mse = 0.814 mae = 0.709\n",
            "epoch187 train time: 4.162s test time: 0.477  loss = 0.825 val_mse = 0.759 mse = 0.815 mae = 0.712\n",
            "epoch188 train time: 4.142s test time: 0.475  loss = 0.825 val_mse = 0.758 mse = 0.815 mae = 0.711\n",
            "epoch189 train time: 4.145s test time: 0.474  loss = 0.825 val_mse = 0.759 mse = 0.816 mae = 0.714\n",
            "epoch190 train time: 4.155s test time: 0.469  loss = 0.825 val_mse = 0.758 mse = 0.815 mae = 0.712\n",
            "epoch191 train time: 4.143s test time: 0.484  loss = 0.825 val_mse = 0.758 mse = 0.815 mae = 0.712\n",
            "epoch192 train time: 4.146s test time: 0.478  loss = 0.825 val_mse = 0.757 mse = 0.814 mae = 0.711\n",
            "epoch193 train time: 4.146s test time: 0.473  loss = 0.825 val_mse = 0.756 mse = 0.813 mae = 0.708\n",
            "epoch194 train time: 4.148s test time: 0.468  loss = 0.825 val_mse = 0.758 mse = 0.815 mae = 0.713\n",
            "epoch195 train time: 4.141s test time: 0.481  loss = 0.825 val_mse = 0.757 mse = 0.814 mae = 0.711\n",
            "epoch196 train time: 4.152s test time: 0.476  loss = 0.825 val_mse = 0.757 mse = 0.815 mae = 0.712\n",
            "epoch197 train time: 4.144s test time: 0.474  loss = 0.825 val_mse = 0.757 mse = 0.814 mae = 0.711\n",
            "epoch198 train time: 4.154s test time: 0.477  loss = 0.825 val_mse = 0.756 mse = 0.814 mae = 0.710\n",
            "epoch199 train time: 4.144s test time: 0.487  loss = 0.825 val_mse = 0.757 mse = 0.814 mae = 0.711\n",
            "epoch200 train time: 4.154s test time: 0.470  loss = 0.825 val_mse = 0.756 mse = 0.814 mae = 0.710\n",
            "epoch201 train time: 4.160s test time: 0.473  loss = 0.825 val_mse = 0.756 mse = 0.814 mae = 0.710\n",
            "epoch202 train time: 4.143s test time: 0.476  loss = 0.825 val_mse = 0.757 mse = 0.815 mae = 0.712\n",
            "epoch203 train time: 4.139s test time: 0.478  loss = 0.825 val_mse = 0.756 mse = 0.815 mae = 0.712\n",
            "epoch204 train time: 4.138s test time: 0.477  loss = 0.825 val_mse = 0.756 mse = 0.814 mae = 0.710\n",
            "epoch205 train time: 4.146s test time: 0.470  loss = 0.825 val_mse = 0.756 mse = 0.814 mae = 0.711\n",
            "epoch206 train time: 4.155s test time: 0.478  loss = 0.824 val_mse = 0.755 mse = 0.814 mae = 0.710\n",
            "epoch207 train time: 4.151s test time: 0.475  loss = 0.824 val_mse = 0.756 mse = 0.815 mae = 0.712\n",
            "epoch208 train time: 4.139s test time: 0.473  loss = 0.824 val_mse = 0.755 mse = 0.814 mae = 0.711\n",
            "epoch209 train time: 4.147s test time: 0.474  loss = 0.824 val_mse = 0.755 mse = 0.814 mae = 0.711\n",
            "epoch210 train time: 4.150s test time: 0.481  loss = 0.824 val_mse = 0.755 mse = 0.814 mae = 0.712\n",
            "epoch211 train time: 4.139s test time: 0.469  loss = 0.824 val_mse = 0.755 mse = 0.813 mae = 0.710\n",
            "epoch212 train time: 4.135s test time: 0.470  loss = 0.824 val_mse = 0.755 mse = 0.814 mae = 0.712\n",
            "epoch213 train time: 4.140s test time: 0.477  loss = 0.824 val_mse = 0.755 mse = 0.814 mae = 0.711\n",
            "epoch214 train time: 4.135s test time: 0.477  loss = 0.824 val_mse = 0.753 mse = 0.812 mae = 0.707\n",
            "epoch215 train time: 4.142s test time: 0.470  loss = 0.824 val_mse = 0.755 mse = 0.814 mae = 0.712\n",
            "epoch216 train time: 4.140s test time: 0.476  loss = 0.824 val_mse = 0.754 mse = 0.814 mae = 0.710\n",
            "epoch217 train time: 4.151s test time: 0.479  loss = 0.824 val_mse = 0.755 mse = 0.814 mae = 0.712\n",
            "epoch218 train time: 4.142s test time: 0.467  loss = 0.824 val_mse = 0.754 mse = 0.813 mae = 0.710\n",
            "epoch219 train time: 4.152s test time: 0.475  loss = 0.824 val_mse = 0.754 mse = 0.813 mae = 0.710\n",
            "epoch220 train time: 4.137s test time: 0.479  loss = 0.824 val_mse = 0.753 mse = 0.812 mae = 0.708\n",
            "epoch221 train time: 4.141s test time: 0.482  loss = 0.824 val_mse = 0.754 mse = 0.813 mae = 0.710\n",
            "epoch222 train time: 4.138s test time: 0.473  loss = 0.824 val_mse = 0.754 mse = 0.814 mae = 0.711\n",
            "epoch223 train time: 4.161s test time: 0.467  loss = 0.824 val_mse = 0.753 mse = 0.813 mae = 0.710\n",
            "epoch224 train time: 4.136s test time: 0.484  loss = 0.824 val_mse = 0.754 mse = 0.814 mae = 0.712\n",
            "epoch225 train time: 4.156s test time: 0.472  loss = 0.824 val_mse = 0.753 mse = 0.812 mae = 0.709\n",
            "epoch226 train time: 4.143s test time: 0.468  loss = 0.824 val_mse = 0.753 mse = 0.812 mae = 0.709\n",
            "epoch227 train time: 4.143s test time: 0.477  loss = 0.824 val_mse = 0.753 mse = 0.813 mae = 0.710\n",
            "epoch228 train time: 4.135s test time: 0.481  loss = 0.824 val_mse = 0.753 mse = 0.813 mae = 0.710\n",
            "epoch229 train time: 4.140s test time: 0.470  loss = 0.824 val_mse = 0.753 mse = 0.813 mae = 0.711\n",
            "epoch230 train time: 4.157s test time: 0.478  loss = 0.824 val_mse = 0.753 mse = 0.814 mae = 0.711\n",
            "epoch231 train time: 4.140s test time: 0.485  loss = 0.824 val_mse = 0.752 mse = 0.812 mae = 0.709\n",
            "epoch232 train time: 4.139s test time: 0.471  loss = 0.824 val_mse = 0.754 mse = 0.815 mae = 0.713\n",
            "epoch233 train time: 4.156s test time: 0.469  loss = 0.823 val_mse = 0.753 mse = 0.813 mae = 0.711\n",
            "epoch234 train time: 4.143s test time: 0.480  loss = 0.823 val_mse = 0.753 mse = 0.813 mae = 0.711\n",
            "epoch235 train time: 4.141s test time: 0.484  loss = 0.823 val_mse = 0.753 mse = 0.813 mae = 0.711\n",
            "epoch236 train time: 4.148s test time: 0.470  loss = 0.823 val_mse = 0.753 mse = 0.813 mae = 0.711\n",
            "epoch237 train time: 4.151s test time: 0.473  loss = 0.823 val_mse = 0.752 mse = 0.812 mae = 0.709\n",
            "epoch238 train time: 4.141s test time: 0.478  loss = 0.823 val_mse = 0.753 mse = 0.814 mae = 0.712\n",
            "epoch239 train time: 4.146s test time: 0.478  loss = 0.823 val_mse = 0.752 mse = 0.813 mae = 0.710\n",
            "MAE 0.7217927465114119\n",
            "MSE 0.8296060701243427\n"
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    print(sys.path[0])\n",
        "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
        "    word_latent_dim = 300\n",
        "    latent_dim = 15\n",
        "    max_doc_length = 300\n",
        "    num_filters = 50\n",
        "    window_size = 3\n",
        "    v_dim = 50\n",
        "    learning_rate = 0.001\n",
        "    lambda_1 = 0.05 #normally we set from [0.05, 0.01, 0.005, 0.001]\n",
        "    drop_out = 0.8\n",
        "    batch_size = 200\n",
        "    epochs = 240\n",
        "    avg_mae_list = []\n",
        "    avg_mse_list = []\n",
        "    \n",
        "    # loading data\n",
        "    firTime = time()\n",
        "    dataSet = Dataset(max_doc_length, sys.path[0], \"dataPreprocessingWordDict.out\")\n",
        "    word_dict, user_reviews, item_reviews, train, valRatings, testRatings = dataSet.word_id_dict, dataSet.userReview_dict, dataSet.itemReview_dict, dataSet.trainMtrx, dataSet.valRatings, dataSet.testRatings\n",
        "    secTime = time()\n",
        "\n",
        "    num_users, num_items = train.shape\n",
        "    print(\"load data: %.3fs\" % (secTime - firTime))\n",
        "    print(num_users, num_items)\n",
        "    print(latent_dim)\n",
        "\n",
        "    #load word embeddings\n",
        "    word_embedding_mtrx = ini_word_embed(len(word_dict), word_latent_dim)\n",
        "    # word_embedding_mtrx = word2vec_word_embed(len(word_dict), word_latent_dim,\n",
        "    #                                           \"Directory of pretrained WordEmbedding.out\",\n",
        "    #                                           word_dict)\n",
        "\n",
        "    print( \"shape\", word_embedding_mtrx.shape)\n",
        "\n",
        "    # get train instances\n",
        "    user_input, item_input, rateings = get_train_instance(train)\n",
        "    print (len(user_input), len(item_input), len(rateings))\n",
        "\n",
        "    # get test/val instances\n",
        "    user_vals, item_vals, user_input_val, item_input_val, rating_input_val = get_test_list(200, valRatings, user_reviews, item_reviews)\n",
        "    user_tests, item_tests, user_input_test, item_input_test, rating_input_test = get_test_list(200, testRatings, user_reviews, item_reviews)\n",
        "\n",
        "    #train & eval model\n",
        "    train_model()"
      ],
      "id": "7nFkpGRcsAzb"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wDneCspqAb98"
      },
      "source": [
        "CARL"
      ],
      "id": "wDneCspqAb98"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yf14MXltAbbm"
      },
      "outputs": [],
      "source": [
        "#combination of interaction.py and review.py\n",
        "def train_model():\n",
        "    mae_list=[]\n",
        "    mse_list=[]\n",
        "    tf.compat.v1.disable_eager_execution()\n",
        "    #I and R-----\n",
        "    users = tf.compat.v1.placeholder(tf.int32, shape=[None])\n",
        "    items = tf.compat.v1.placeholder(tf.int32, shape=[None])\n",
        "    users_inputs = tf.compat.v1.placeholder(tf.int32, shape=[None, max_doc_length])\n",
        "    items_inputs = tf.compat.v1.placeholder(tf.int32, shape=[None, max_doc_length])\n",
        "    ratings = tf.compat.v1.placeholder(tf.float32, shape=[None, 1])\n",
        "    \n",
        "    #R-----\n",
        "    dropout_rate = tf.compat.v1.placeholder(tf.float32)\n",
        "    \n",
        "    #I-----\n",
        "    user_entity_embedding = tf.Variable(tf.random.normal([num_users, latent_dim], mean=0, stddev=0.02), name=\"user_entity_embeddings\")\n",
        "    item_entity_embedding = tf.Variable(tf.random.normal([num_items, latent_dim], mean=0, stddev=0.02), name=\"item_entity_embeddings\")\n",
        "    user_entity_embeds = tf.nn.embedding_lookup(user_entity_embedding, users)\n",
        "    item_entity_embeds = tf.nn.embedding_lookup(item_entity_embedding, items)\n",
        "    \n",
        "    #R-----\n",
        "    user_bias = tf.Variable(tf.random.normal([num_users, 1], mean=0, stddev=0.02), name=\"user_bias\")\n",
        "    item_bias = tf.Variable(tf.random.normal([num_items, 1], mean=0, stddev=0.02), name=\"item_bias\")\n",
        "    user_bs = tf.nn.embedding_lookup(user_bias, users)\n",
        "    item_bs = tf.nn.embedding_lookup(item_bias, items)\n",
        "    \n",
        "    text_embedding = tf.Variable(word_embedding_mtrx, dtype=tf.float32, name=\"review_text_embeds\")\n",
        "    padding_embedding = tf.Variable(np.zeros([1, word_latent_dim]), dtype=tf.float32)\n",
        "    text_mask = tf.constant([1.0] * text_embedding.get_shape()[0] + [0.0])\n",
        "    word_embeddings = tf.concat([text_embedding, padding_embedding], 0)\n",
        "    word_embeddings = word_embeddings * tf.expand_dims(text_mask, -1)\n",
        "    user_reviews_representation = tf.nn.embedding_lookup(word_embeddings, users_inputs)\n",
        "    user_reviews_representation_expnd = tf.expand_dims(user_reviews_representation, -1)\n",
        "    item_reviews_representation = tf.nn.embedding_lookup(word_embeddings, items_inputs)\n",
        "    item_reviews_representation_expnd = tf.expand_dims(item_reviews_representation, -1)\n",
        "    \n",
        "    # CNN layers\n",
        "    W_u = tf.Variable(tf.random.truncated_normal([window_size, word_latent_dim, 1, num_filters], stddev=0.3), name=\"review_W_u\")\n",
        "    W_i = tf.Variable(tf.random.truncated_normal([window_size, word_latent_dim, 1, num_filters], stddev=0.3), name=\"review_W_i\")\n",
        "    W_u_1 = tf.Variable(tf.random.truncated_normal([window_size, num_filters, 1, num_filters], stddev=0.3), name=\"review_W_u_1\")\n",
        "    W_i_1 = tf.Variable(tf.random.truncated_normal([window_size, num_filters, 1, num_filters], stddev=0.3), name=\"review_W_i_1\")\n",
        "    # b = tf.Variable(tf.constant(0.1, shape=[self.num_filters]))\n",
        "    rand_matrix = tf.Variable(tf.random.truncated_normal([num_filters, num_filters], stddev=0.3), name=\"review_rand_matrix\")\n",
        "\n",
        "    user_embeds, item_embeds = cnn_model_average(num_filters, user_reviews_representation_expnd, item_reviews_representation_expnd, W_u, W_i, W_u_1, W_i_1, rand_matrix)\n",
        "    \n",
        "    #shared MLP layer\n",
        "    W_mlp = tf.Variable(tf.random.normal([num_filters, latent_dim], mean=0, stddev=0.02), name=\"review_W_mlp\")\n",
        "    W_mlp = tf.nn.dropout(W_mlp, dropout_rate)\n",
        "    b_mlp = tf.Variable(tf.constant(0.1, shape=[latent_dim]), name=\"review_b_mlp\")\n",
        "\n",
        "    user_embeds = tf.nn.relu(tf.matmul(user_embeds, W_mlp) + b_mlp)\n",
        "    item_embeds = tf.nn.relu(tf.matmul(item_embeds, W_mlp) + b_mlp)\n",
        "\n",
        "    embeds_sum = tf.concat([tf.multiply(user_embeds, item_embeds), user_embeds, item_embeds], 1, name=\"concat_embed\")\n",
        "    #I\n",
        "    entity_embeds_sum = tf.concat([tf.multiply(user_entity_embeds, item_entity_embeds), user_entity_embeds, item_entity_embeds],1)\n",
        "\n",
        "    #FM layer\n",
        "    #R---\n",
        "    w_0 = tf.Variable(tf.zeros(1), name=\"review_w_0\")\n",
        "    w_1 = tf.Variable(tf.random.truncated_normal([1, latent_dim * 3], stddev=0.3), name=\"review_w_1\")\n",
        "    v = tf.Variable(tf.random.truncated_normal([latent_dim * 3, latent_dim * 3], stddev=0.3), name=\"review_v\")\n",
        "    \n",
        "    #I---\n",
        "    w_entity_0 = tf.Variable(tf.zeros(1), name=\"entity_w_0\")\n",
        "    w_entity_1 = tf.Variable(tf.random.truncated_normal([1, latent_dim*3], stddev=0.3), name=\"entity_w_1\")\n",
        "    v_entity = tf.Variable(tf.random.truncated_normal([latent_dim*3, v_dim], stddev=0.3), name=\"entity_v\")\n",
        "    \n",
        "    #I---\n",
        "    J_e_1 = w_entity_0 + tf.matmul(entity_embeds_sum, w_entity_1, transpose_b=True)\n",
        "    #R---\n",
        "    J_1 = w_0 + tf.matmul(embeds_sum, w_1, transpose_b=True)\n",
        "    \n",
        "    #I---\n",
        "    entity_embeds_sum_1 = tf.expand_dims(entity_embeds_sum, -1)\n",
        "    entity_embeds_sum_2 = tf.expand_dims(entity_embeds_sum, 1)\n",
        "    #R---\n",
        "    embeds_sum_1 = tf.expand_dims(embeds_sum, -1)\n",
        "    embeds_sum_2 = tf.expand_dims(embeds_sum, 1)\n",
        "    \n",
        "    #I---\n",
        "    J_e_2 = tf.reduce_sum(tf.reduce_sum(tf.multiply(tf.matmul(entity_embeds_sum_1, entity_embeds_sum_2), tf.matmul(v_entity, v_entity, transpose_b=True)),2), 1, keepdims=True)\n",
        "    #J_e_3 = tf.trace(tf.multiply(tf.matmul(entity_embeds_sum_1, entity_embeds_sum_2), tf.matmul(v_entity, v_entity, transpose_b=True))) \n",
        "    \n",
        "    #R---\n",
        "    J_2 = tf.reduce_sum(tf.reduce_sum(tf.multiply(tf.matmul(embeds_sum_1, embeds_sum_2), tf.matmul(v, v, transpose_b=True)),2), 1, keepdims=True)\n",
        "    #J_3 = tf.trace(tf.multiply(tf.matmul(embeds_sum_1, embeds_sum_2), tf.matmul(v, v, transpose_b=True)))\n",
        "    #R---\n",
        "    J_total = (J_1 + 0.5 * (J_2))\n",
        "    \n",
        "    #I---\n",
        "    J_e_total = (J_e_1 + 0.5 * (J_e_2))\n",
        "    \n",
        "    \n",
        "    numerator = J_total + J_e_total\n",
        "    predict_rating = tf.divide(J_total, numerator) * J_total + tf.divide(J_e_total,numerator) * J_e_total + user_bs + item_bs\n",
        "    loss = tf.reduce_mean(tf.math.squared_difference(predict_rating, ratings))\n",
        "    loss += lambda_1 * (tf.nn.l2_loss(W_i_1) + tf.nn.l2_loss(W_u_1) + tf.nn.l2_loss(user_entity_embedding) + tf.nn.l2_loss(item_entity_embedding) + tf.nn.l2_loss(W_u) + tf.nn.l2_loss(W_i) + tf.nn.l2_loss(v) + tf.nn.l2_loss(v_entity) + tf.nn.l2_loss(rand_matrix) + tf.nn.l2_loss(user_bs) + tf.nn.l2_loss(item_bs))\n",
        "    \n",
        "    train_step = tf.compat.v1.train.RMSPropOptimizer(learning_rate).minimize(loss)\n",
        "    saver = tf.compat.v1.train.Saver()\n",
        "    \n",
        "    init = tf.compat.v1.global_variables_initializer()\n",
        "    with tf.compat.v1.Session() as sess:\n",
        "        sess.run(init)\n",
        "\n",
        "        for e in range(epochs):\n",
        "            t = time()\n",
        "            loss_total = 0.0\n",
        "            count = 0.0\n",
        "            \n",
        "            for i in range(int(math.ceil(len(user_input) / float(batch_size)))):\n",
        "                user_batch, item_batch, user_input_batch, item_input_batch, rates_batch = get_train_instance_batch_change(i, batch_size,user_input, item_input, rateings, user_reviews,item_reviews)\n",
        "                _, loss_val, words = sess.run([train_step, loss, word_embeddings], feed_dict={users: user_batch, items: item_batch, users_inputs: user_input_batch, items_inputs: item_input_batch, ratings: rates_batch, dropout_rate:drop_out})\n",
        "                loss_total += loss_val\n",
        "                count += 1.0\n",
        "\n",
        "            t1 = time()            \n",
        "            #LOOK FROM HERE \n",
        "            val_mses, val_maes = [], []\n",
        "            for i in range(len(user_input_val)):\n",
        "                #eval_model(users, items, users_inputs, items_inputs, dropout_rate, predict_rating, sess, user_vals[i], item_vals[i], user_input_val[i], item_input_val[i], rating_input_val[i], val_mses, val_maes)\n",
        "                #predicts = sess.run(predict_rating, feed_dict={users: user_batch, items: item_batch, users_inputs: user_input_batch, items_inputs: item_input_batch, dropout_rate:1.0})\n",
        "                predicts = sess.run(predict_rating, feed_dict={users: user_vals[i], items: item_vals[i], users_inputs: user_input_val[i], items_inputs: item_input_val[i], ratings: rating_input_val[i], dropout_rate: drop_out})\n",
        "                #print(predicts)\n",
        "                row, col = predicts.shape\n",
        "                for r in range(row):\n",
        "                    val_mses.append(pow((predicts[r, 0] - rating_input_val[i][r][0]), 2))\n",
        "                    val_maes.append(abs((predicts[r, 0] - rating_input_val[i][r][0])))\n",
        "            val_mse = np.array(val_mses).mean()            \n",
        "            t2 = time()            \n",
        "            mses, maes = [], []\n",
        "            for i in range(len(user_input_test)):\n",
        "                #eval_model(users, items, users_inputs, items_inputs, dropout_rate, predict_rating, sess, user_tests[i], item_tests[i], user_input_test[i], item_input_test[i], rating_input_test[i], mses, maes)\n",
        "                predicts_test = sess.run(predict_rating, feed_dict={users: user_tests[i], items: item_tests[i], users_inputs: user_input_test[i], items_inputs: item_input_test[i], ratings: rating_input_test[i], dropout_rate: drop_out})\n",
        "                #print(predicts)\n",
        "                row, col = predicts_test.shape\n",
        "                for r in range(row):\n",
        "                    mses.append(pow((predicts_test[r, 0] - rating_input_test[i][r][0]), 2))\n",
        "                    maes.append(abs((predicts_test[r, 0] - rating_input_test[i][r][0])))\n",
        "                    \n",
        "            mse = np.array(mses).mean()\n",
        "            mae = np.array(maes).mean()\n",
        "            t3 = time()\n",
        "            mae_list.append(mae)\n",
        "            mse_list.append(mse)\n",
        "            print(\"epoch%d train time: %.3fs test time: %.3f  loss = %.3f val_mse = %.3f mse = %.3f mae = %.3f\"%(e, (t1 - t), (t3 - t2), loss_total/count, val_mse, mse, mae))\n",
        "        \n",
        "        avg_mae=sum(mae_list) / len(mae_list)\n",
        "        avg_mse=sum(mse_list)/len(mse_list)\n",
        "        avg_mae_list.append(avg_mae)\n",
        "        avg_mse_list.append(avg_mse)\n",
        "        print(\"MAE \"+str(avg_mae))\n",
        "        print(\"MSE \"+str(avg_mse))"
      ],
      "id": "yf14MXltAbbm"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qash_diVA1zn",
        "outputId": "4cbfa87c-48cf-4b26-ab93-ab3416f0a344"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "wordId_dict finished\n",
            "load reviews finished\n",
            "load data: 4.672s\n",
            "2928 1835\n",
            "15\n",
            "shape (18131, 300)\n",
            "16326 16326 16326\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/training/rmsprop.py:188: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "epoch0 train time: 15.854s test time: 0.579  loss = 197.993 val_mse = 0.882 mse = 0.882 mae = 0.765\n",
            "epoch1 train time: 4.018s test time: 0.463  loss = 137.885 val_mse = 0.861 mse = 0.861 mae = 0.745\n",
            "epoch2 train time: 4.005s test time: 0.469  loss = 76.548 val_mse = 0.857 mse = 0.858 mae = 0.742\n",
            "epoch3 train time: 4.010s test time: 0.464  loss = 38.749 val_mse = 0.852 mse = 0.855 mae = 0.739\n",
            "epoch4 train time: 4.008s test time: 0.466  loss = 17.553 val_mse = 0.847 mse = 0.853 mae = 0.736\n",
            "epoch5 train time: 4.013s test time: 0.459  loss = 7.006 val_mse = 0.841 mse = 0.850 mae = 0.733\n",
            "epoch6 train time: 4.002s test time: 0.467  loss = 2.655 val_mse = 0.836 mse = 0.848 mae = 0.730\n",
            "epoch7 train time: 4.009s test time: 0.467  loss = 1.373 val_mse = 0.830 mse = 0.845 mae = 0.727\n",
            "epoch8 train time: 4.007s test time: 0.463  loss = 1.168 val_mse = 0.825 mse = 0.843 mae = 0.724\n",
            "epoch9 train time: 4.035s test time: 0.464  loss = 1.118 val_mse = 0.819 mse = 0.841 mae = 0.722\n",
            "epoch10 train time: 4.036s test time: 0.462  loss = 1.081 val_mse = 0.814 mse = 0.839 mae = 0.719\n",
            "epoch11 train time: 4.030s test time: 0.491  loss = 1.051 val_mse = 0.809 mse = 0.837 mae = 0.716\n",
            "epoch12 train time: 4.245s test time: 0.468  loss = 1.027 val_mse = 0.803 mse = 0.835 mae = 0.713\n",
            "epoch13 train time: 4.038s test time: 0.464  loss = 1.008 val_mse = 0.798 mse = 0.833 mae = 0.711\n",
            "epoch14 train time: 4.050s test time: 0.471  loss = 0.992 val_mse = 0.794 mse = 0.831 mae = 0.708\n",
            "epoch15 train time: 4.048s test time: 0.479  loss = 0.978 val_mse = 0.789 mse = 0.830 mae = 0.706\n",
            "epoch16 train time: 4.065s test time: 0.465  loss = 0.966 val_mse = 0.784 mse = 0.828 mae = 0.703\n",
            "epoch17 train time: 4.058s test time: 0.474  loss = 0.956 val_mse = 0.780 mse = 0.827 mae = 0.701\n",
            "epoch18 train time: 4.072s test time: 0.466  loss = 0.947 val_mse = 0.775 mse = 0.825 mae = 0.699\n",
            "epoch19 train time: 4.071s test time: 0.465  loss = 0.939 val_mse = 0.771 mse = 0.824 mae = 0.697\n",
            "epoch20 train time: 4.131s test time: 0.587  loss = 0.932 val_mse = 0.767 mse = 0.823 mae = 0.694\n",
            "epoch21 train time: 4.341s test time: 0.466  loss = 0.925 val_mse = 0.763 mse = 0.821 mae = 0.692\n",
            "epoch22 train time: 4.090s test time: 0.476  loss = 0.919 val_mse = 0.758 mse = 0.820 mae = 0.691\n",
            "epoch23 train time: 4.095s test time: 0.469  loss = 0.914 val_mse = 0.754 mse = 0.819 mae = 0.689\n",
            "epoch24 train time: 4.115s test time: 0.465  loss = 0.909 val_mse = 0.750 mse = 0.817 mae = 0.687\n",
            "epoch25 train time: 4.091s test time: 0.474  loss = 0.904 val_mse = 0.746 mse = 0.816 mae = 0.685\n",
            "epoch26 train time: 4.112s test time: 0.469  loss = 0.899 val_mse = 0.742 mse = 0.815 mae = 0.684\n",
            "epoch27 train time: 4.120s test time: 0.465  loss = 0.895 val_mse = 0.738 mse = 0.813 mae = 0.682\n",
            "epoch28 train time: 4.118s test time: 0.475  loss = 0.891 val_mse = 0.734 mse = 0.812 mae = 0.681\n",
            "epoch29 train time: 4.132s test time: 0.474  loss = 0.887 val_mse = 0.730 mse = 0.810 mae = 0.679\n",
            "epoch30 train time: 4.136s test time: 0.471  loss = 0.883 val_mse = 0.726 mse = 0.809 mae = 0.677\n",
            "epoch31 train time: 4.153s test time: 0.472  loss = 0.879 val_mse = 0.722 mse = 0.807 mae = 0.675\n",
            "epoch32 train time: 4.130s test time: 0.479  loss = 0.875 val_mse = 0.717 mse = 0.805 mae = 0.672\n",
            "epoch33 train time: 4.134s test time: 0.476  loss = 0.871 val_mse = 0.713 mse = 0.803 mae = 0.670\n",
            "epoch34 train time: 4.137s test time: 0.476  loss = 0.867 val_mse = 0.708 mse = 0.802 mae = 0.669\n",
            "epoch35 train time: 4.171s test time: 0.478  loss = 0.864 val_mse = 0.704 mse = 0.800 mae = 0.668\n",
            "epoch36 train time: 4.153s test time: 0.474  loss = 0.860 val_mse = 0.699 mse = 0.799 mae = 0.667\n",
            "epoch37 train time: 4.158s test time: 0.477  loss = 0.856 val_mse = 0.694 mse = 0.797 mae = 0.664\n",
            "epoch38 train time: 4.156s test time: 0.479  loss = 0.852 val_mse = 0.689 mse = 0.795 mae = 0.662\n",
            "epoch39 train time: 4.158s test time: 0.476  loss = 0.848 val_mse = 0.684 mse = 0.793 mae = 0.658\n",
            "epoch40 train time: 4.174s test time: 0.477  loss = 0.844 val_mse = 0.679 mse = 0.791 mae = 0.654\n",
            "epoch41 train time: 4.163s test time: 0.477  loss = 0.839 val_mse = 0.673 mse = 0.790 mae = 0.655\n",
            "epoch42 train time: 4.156s test time: 0.482  loss = 0.835 val_mse = 0.668 mse = 0.788 mae = 0.649\n",
            "epoch43 train time: 4.159s test time: 0.483  loss = 0.830 val_mse = 0.662 mse = 0.786 mae = 0.646\n",
            "epoch44 train time: 4.159s test time: 0.471  loss = 0.826 val_mse = 0.657 mse = 0.785 mae = 0.642\n",
            "epoch45 train time: 4.185s test time: 0.473  loss = 0.821 val_mse = 0.651 mse = 0.784 mae = 0.645\n",
            "epoch46 train time: 4.179s test time: 0.476  loss = 0.816 val_mse = 0.645 mse = 0.784 mae = 0.643\n",
            "epoch47 train time: 4.172s test time: 0.478  loss = 0.811 val_mse = 0.639 mse = 0.783 mae = 0.640\n",
            "epoch48 train time: 4.187s test time: 0.480  loss = 0.805 val_mse = 0.634 mse = 0.783 mae = 0.639\n",
            "epoch49 train time: 4.201s test time: 0.476  loss = 0.799 val_mse = 0.628 mse = 0.784 mae = 0.639\n",
            "epoch50 train time: 4.194s test time: 0.480  loss = 0.793 val_mse = 0.622 mse = 0.785 mae = 0.638\n",
            "epoch51 train time: 4.182s test time: 0.484  loss = 0.787 val_mse = 0.617 mse = 0.787 mae = 0.635\n",
            "epoch52 train time: 4.191s test time: 0.478  loss = 0.779 val_mse = 0.612 mse = 0.790 mae = 0.634\n",
            "epoch53 train time: 4.204s test time: 0.477  loss = 0.772 val_mse = 0.607 mse = 0.795 mae = 0.633\n",
            "epoch54 train time: 4.184s test time: 0.473  loss = 0.763 val_mse = 0.605 mse = 0.803 mae = 0.635\n",
            "epoch55 train time: 4.197s test time: 0.485  loss = 0.753 val_mse = 0.603 mse = 0.817 mae = 0.641\n",
            "epoch56 train time: 4.186s test time: 0.481  loss = 0.742 val_mse = 0.611 mse = 0.840 mae = 0.655\n",
            "epoch57 train time: 4.194s test time: 0.476  loss = 0.728 val_mse = 0.632 mse = 0.893 mae = 0.684\n",
            "epoch58 train time: 4.191s test time: 0.477  loss = 0.711 val_mse = 0.726 mse = 1.030 mae = 0.761\n",
            "epoch59 train time: 4.197s test time: 0.484  loss = 0.690 val_mse = 0.945 mse = 1.344 mae = 0.910\n",
            "epoch60 train time: 4.196s test time: 0.481  loss = 0.692 val_mse = 0.994 mse = 1.432 mae = 0.952\n",
            "epoch61 train time: 4.182s test time: 0.478  loss = 0.705 val_mse = 0.881 mse = 1.262 mae = 0.871\n",
            "epoch62 train time: 4.232s test time: 0.478  loss = 0.724 val_mse = 0.785 mse = 1.136 mae = 0.822\n",
            "epoch63 train time: 4.224s test time: 0.474  loss = 0.714 val_mse = 0.756 mse = 1.087 mae = 0.796\n",
            "epoch64 train time: 4.225s test time: 0.479  loss = 0.722 val_mse = 0.747 mse = 1.085 mae = 0.794\n",
            "epoch65 train time: 4.207s test time: 0.483  loss = 0.709 val_mse = 0.755 mse = 1.093 mae = 0.797\n",
            "epoch66 train time: 4.220s test time: 0.478  loss = 0.721 val_mse = 0.757 mse = 1.082 mae = 0.793\n",
            "epoch67 train time: 4.215s test time: 0.481  loss = 0.709 val_mse = 0.758 mse = 1.084 mae = 0.793\n",
            "epoch68 train time: 4.221s test time: 0.479  loss = 0.720 val_mse = 0.758 mse = 1.088 mae = 0.797\n",
            "epoch69 train time: 4.209s test time: 0.485  loss = 0.708 val_mse = 0.761 mse = 1.088 mae = 0.794\n",
            "epoch70 train time: 4.202s test time: 0.488  loss = 0.720 val_mse = 0.760 mse = 1.083 mae = 0.795\n",
            "epoch71 train time: 4.202s test time: 0.477  loss = 0.707 val_mse = 0.763 mse = 1.092 mae = 0.795\n",
            "epoch72 train time: 4.219s test time: 0.483  loss = 0.719 val_mse = 0.758 mse = 1.087 mae = 0.796\n",
            "epoch73 train time: 4.230s test time: 0.489  loss = 0.706 val_mse = 0.767 mse = 1.090 mae = 0.795\n",
            "epoch74 train time: 4.205s test time: 0.500  loss = 0.718 val_mse = 0.760 mse = 1.091 mae = 0.798\n",
            "epoch75 train time: 4.216s test time: 0.484  loss = 0.707 val_mse = 0.765 mse = 1.088 mae = 0.793\n",
            "epoch76 train time: 4.197s test time: 0.472  loss = 0.717 val_mse = 0.766 mse = 1.089 mae = 0.798\n",
            "epoch77 train time: 4.210s test time: 0.477  loss = 0.705 val_mse = 0.762 mse = 1.094 mae = 0.796\n",
            "epoch78 train time: 4.214s test time: 0.483  loss = 0.719 val_mse = 0.761 mse = 1.088 mae = 0.797\n",
            "epoch79 train time: 4.204s test time: 0.482  loss = 0.705 val_mse = 0.768 mse = 1.090 mae = 0.795\n",
            "epoch80 train time: 4.203s test time: 0.487  loss = 0.717 val_mse = 0.764 mse = 1.094 mae = 0.800\n",
            "epoch81 train time: 4.205s test time: 0.478  loss = 0.706 val_mse = 0.763 mse = 1.088 mae = 0.793\n",
            "epoch82 train time: 4.193s test time: 0.474  loss = 0.717 val_mse = 0.765 mse = 1.088 mae = 0.798\n",
            "epoch83 train time: 4.204s test time: 0.487  loss = 0.704 val_mse = 0.766 mse = 1.092 mae = 0.796\n",
            "epoch84 train time: 4.225s test time: 0.479  loss = 0.717 val_mse = 0.765 mse = 1.090 mae = 0.799\n",
            "epoch85 train time: 4.199s test time: 0.480  loss = 0.704 val_mse = 0.766 mse = 1.089 mae = 0.794\n",
            "epoch86 train time: 4.215s test time: 0.477  loss = 0.717 val_mse = 0.763 mse = 1.090 mae = 0.799\n",
            "epoch87 train time: 4.193s test time: 0.486  loss = 0.704 val_mse = 0.769 mse = 1.087 mae = 0.793\n",
            "epoch88 train time: 4.192s test time: 0.481  loss = 0.716 val_mse = 0.764 mse = 1.091 mae = 0.799\n",
            "epoch89 train time: 4.189s test time: 0.487  loss = 0.704 val_mse = 0.765 mse = 1.088 mae = 0.794\n",
            "epoch90 train time: 4.207s test time: 0.490  loss = 0.716 val_mse = 0.767 mse = 1.088 mae = 0.798\n",
            "epoch91 train time: 4.203s test time: 0.480  loss = 0.703 val_mse = 0.768 mse = 1.091 mae = 0.794\n",
            "epoch92 train time: 4.227s test time: 0.482  loss = 0.716 val_mse = 0.766 mse = 1.090 mae = 0.799\n",
            "epoch93 train time: 4.194s test time: 0.488  loss = 0.703 val_mse = 0.767 mse = 1.088 mae = 0.794\n",
            "epoch94 train time: 4.210s test time: 0.483  loss = 0.715 val_mse = 0.765 mse = 1.089 mae = 0.798\n",
            "epoch95 train time: 4.203s test time: 0.471  loss = 0.703 val_mse = 0.770 mse = 1.089 mae = 0.794\n",
            "epoch96 train time: 4.194s test time: 0.480  loss = 0.714 val_mse = 0.765 mse = 1.091 mae = 0.798\n",
            "epoch97 train time: 4.220s test time: 0.481  loss = 0.703 val_mse = 0.765 mse = 1.087 mae = 0.793\n",
            "epoch98 train time: 4.196s test time: 0.480  loss = 0.714 val_mse = 0.767 mse = 1.088 mae = 0.797\n",
            "epoch99 train time: 4.204s test time: 0.472  loss = 0.702 val_mse = 0.767 mse = 1.092 mae = 0.794\n",
            "epoch100 train time: 4.207s test time: 0.478  loss = 0.714 val_mse = 0.763 mse = 1.089 mae = 0.797\n",
            "epoch101 train time: 4.203s test time: 0.473  loss = 0.701 val_mse = 0.769 mse = 1.089 mae = 0.793\n",
            "epoch102 train time: 4.189s test time: 0.479  loss = 0.714 val_mse = 0.762 mse = 1.090 mae = 0.797\n",
            "epoch103 train time: 4.209s test time: 0.485  loss = 0.702 val_mse = 0.768 mse = 1.089 mae = 0.793\n",
            "epoch104 train time: 4.205s test time: 0.481  loss = 0.712 val_mse = 0.765 mse = 1.088 mae = 0.797\n",
            "epoch105 train time: 4.193s test time: 0.475  loss = 0.700 val_mse = 0.765 mse = 1.091 mae = 0.794\n",
            "epoch106 train time: 4.209s test time: 0.480  loss = 0.714 val_mse = 0.760 mse = 1.087 mae = 0.796\n",
            "epoch107 train time: 4.207s test time: 0.481  loss = 0.700 val_mse = 0.772 mse = 1.089 mae = 0.793\n",
            "epoch108 train time: 4.217s test time: 0.481  loss = 0.711 val_mse = 0.760 mse = 1.088 mae = 0.797\n",
            "epoch109 train time: 4.213s test time: 0.483  loss = 0.700 val_mse = 0.764 mse = 1.085 mae = 0.791\n",
            "epoch110 train time: 4.209s test time: 0.482  loss = 0.711 val_mse = 0.761 mse = 1.084 mae = 0.795\n",
            "epoch111 train time: 4.210s test time: 0.481  loss = 0.699 val_mse = 0.768 mse = 1.092 mae = 0.794\n",
            "epoch112 train time: 4.204s test time: 0.478  loss = 0.711 val_mse = 0.757 mse = 1.084 mae = 0.795\n",
            "epoch113 train time: 4.202s test time: 0.480  loss = 0.698 val_mse = 0.768 mse = 1.086 mae = 0.792\n",
            "epoch114 train time: 4.186s test time: 0.481  loss = 0.710 val_mse = 0.754 mse = 1.083 mae = 0.794\n",
            "epoch115 train time: 4.196s test time: 0.477  loss = 0.698 val_mse = 0.768 mse = 1.086 mae = 0.791\n",
            "epoch116 train time: 4.211s test time: 0.474  loss = 0.708 val_mse = 0.756 mse = 1.080 mae = 0.793\n",
            "epoch117 train time: 4.207s test time: 0.473  loss = 0.696 val_mse = 0.763 mse = 1.087 mae = 0.791\n",
            "epoch118 train time: 4.197s test time: 0.474  loss = 0.710 val_mse = 0.751 mse = 1.077 mae = 0.791\n",
            "epoch119 train time: 4.201s test time: 0.484  loss = 0.695 val_mse = 0.770 mse = 1.087 mae = 0.792\n",
            "epoch120 train time: 4.203s test time: 0.482  loss = 0.706 val_mse = 0.749 mse = 1.077 mae = 0.791\n",
            "epoch121 train time: 4.196s test time: 0.482  loss = 0.695 val_mse = 0.762 mse = 1.081 mae = 0.788\n",
            "epoch122 train time: 4.211s test time: 0.474  loss = 0.706 val_mse = 0.749 mse = 1.073 mae = 0.789\n",
            "epoch123 train time: 4.195s test time: 0.472  loss = 0.693 val_mse = 0.764 mse = 1.084 mae = 0.790\n",
            "epoch124 train time: 4.210s test time: 0.479  loss = 0.705 val_mse = 0.745 mse = 1.072 mae = 0.788\n",
            "epoch125 train time: 4.193s test time: 0.479  loss = 0.692 val_mse = 0.755 mse = 1.078 mae = 0.786\n",
            "epoch126 train time: 4.200s test time: 0.477  loss = 0.705 val_mse = 0.742 mse = 1.068 mae = 0.785\n",
            "epoch127 train time: 4.202s test time: 0.473  loss = 0.691 val_mse = 0.761 mse = 1.078 mae = 0.787\n",
            "epoch128 train time: 4.197s test time: 0.484  loss = 0.702 val_mse = 0.738 mse = 1.065 mae = 0.784\n",
            "epoch129 train time: 4.215s test time: 0.480  loss = 0.691 val_mse = 0.751 mse = 1.069 mae = 0.782\n",
            "epoch130 train time: 4.197s test time: 0.483  loss = 0.702 val_mse = 0.734 mse = 1.062 mae = 0.782\n",
            "epoch131 train time: 4.213s test time: 0.478  loss = 0.689 val_mse = 0.751 mse = 1.072 mae = 0.783\n",
            "epoch132 train time: 4.206s test time: 0.474  loss = 0.700 val_mse = 0.731 mse = 1.059 mae = 0.780\n",
            "epoch133 train time: 4.192s test time: 0.477  loss = 0.688 val_mse = 0.743 mse = 1.062 mae = 0.778\n",
            "epoch134 train time: 4.192s test time: 0.487  loss = 0.700 val_mse = 0.724 mse = 1.059 mae = 0.779\n",
            "epoch135 train time: 4.212s test time: 0.473  loss = 0.687 val_mse = 0.742 mse = 1.064 mae = 0.779\n",
            "epoch136 train time: 4.211s test time: 0.472  loss = 0.698 val_mse = 0.726 mse = 1.052 mae = 0.776\n",
            "epoch137 train time: 4.200s test time: 0.474  loss = 0.686 val_mse = 0.733 mse = 1.057 mae = 0.774\n",
            "epoch138 train time: 4.200s test time: 0.481  loss = 0.699 val_mse = 0.719 mse = 1.050 mae = 0.774\n",
            "epoch139 train time: 4.206s test time: 0.484  loss = 0.683 val_mse = 0.736 mse = 1.057 mae = 0.775\n",
            "epoch140 train time: 4.195s test time: 0.478  loss = 0.695 val_mse = 0.715 mse = 1.045 mae = 0.771\n",
            "epoch141 train time: 4.193s test time: 0.475  loss = 0.684 val_mse = 0.726 mse = 1.046 mae = 0.769\n",
            "epoch142 train time: 4.198s test time: 0.481  loss = 0.694 val_mse = 0.714 mse = 1.038 mae = 0.768\n",
            "epoch143 train time: 4.197s test time: 0.478  loss = 0.681 val_mse = 0.722 mse = 1.049 mae = 0.770\n",
            "epoch144 train time: 4.202s test time: 0.472  loss = 0.693 val_mse = 0.709 mse = 1.033 mae = 0.764\n",
            "epoch145 train time: 4.196s test time: 0.480  loss = 0.681 val_mse = 0.720 mse = 1.035 mae = 0.763\n",
            "epoch146 train time: 4.200s test time: 0.469  loss = 0.691 val_mse = 0.701 mse = 1.030 mae = 0.763\n",
            "epoch147 train time: 4.199s test time: 0.481  loss = 0.680 val_mse = 0.714 mse = 1.032 mae = 0.762\n",
            "epoch148 train time: 4.220s test time: 0.481  loss = 0.690 val_mse = 0.700 mse = 1.019 mae = 0.757\n",
            "epoch149 train time: 4.196s test time: 0.479  loss = 0.679 val_mse = 0.708 mse = 1.030 mae = 0.760\n",
            "epoch150 train time: 4.195s test time: 0.470  loss = 0.690 val_mse = 0.692 mse = 1.013 mae = 0.755\n",
            "epoch151 train time: 4.191s test time: 0.483  loss = 0.677 val_mse = 0.704 mse = 1.022 mae = 0.756\n",
            "epoch152 train time: 4.204s test time: 0.488  loss = 0.688 val_mse = 0.685 mse = 1.008 mae = 0.751\n",
            "epoch153 train time: 4.194s test time: 0.475  loss = 0.678 val_mse = 0.697 mse = 1.015 mae = 0.752\n",
            "epoch154 train time: 4.202s test time: 0.481  loss = 0.687 val_mse = 0.683 mse = 1.001 mae = 0.748\n",
            "epoch155 train time: 4.189s test time: 0.476  loss = 0.675 val_mse = 0.691 mse = 1.014 mae = 0.750\n",
            "epoch156 train time: 4.204s test time: 0.479  loss = 0.687 val_mse = 0.676 mse = 0.991 mae = 0.742\n",
            "epoch157 train time: 4.201s test time: 0.481  loss = 0.675 val_mse = 0.688 mse = 1.006 mae = 0.747\n",
            "epoch158 train time: 4.186s test time: 0.479  loss = 0.685 val_mse = 0.670 mse = 0.992 mae = 0.742\n",
            "epoch159 train time: 4.202s test time: 0.475  loss = 0.675 val_mse = 0.679 mse = 0.995 mae = 0.741\n",
            "epoch160 train time: 4.200s test time: 0.477  loss = 0.684 val_mse = 0.671 mse = 0.984 mae = 0.738\n",
            "epoch161 train time: 4.212s test time: 0.477  loss = 0.674 val_mse = 0.676 mse = 1.000 mae = 0.742\n",
            "epoch162 train time: 4.190s test time: 0.479  loss = 0.684 val_mse = 0.665 mse = 0.977 mae = 0.735\n",
            "epoch163 train time: 4.212s test time: 0.477  loss = 0.673 val_mse = 0.672 mse = 0.985 mae = 0.735\n",
            "epoch164 train time: 4.202s test time: 0.475  loss = 0.683 val_mse = 0.659 mse = 0.977 mae = 0.733\n",
            "epoch165 train time: 4.188s test time: 0.477  loss = 0.673 val_mse = 0.669 mse = 0.986 mae = 0.735\n",
            "epoch166 train time: 4.200s test time: 0.478  loss = 0.681 val_mse = 0.662 mse = 0.967 mae = 0.730\n",
            "epoch167 train time: 4.199s test time: 0.470  loss = 0.673 val_mse = 0.664 mse = 0.982 mae = 0.732\n",
            "epoch168 train time: 4.199s test time: 0.473  loss = 0.682 val_mse = 0.655 mse = 0.968 mae = 0.727\n",
            "epoch169 train time: 4.188s test time: 0.474  loss = 0.671 val_mse = 0.664 mse = 0.979 mae = 0.730\n",
            "epoch170 train time: 4.192s test time: 0.477  loss = 0.681 val_mse = 0.650 mse = 0.959 mae = 0.724\n",
            "epoch171 train time: 4.189s test time: 0.478  loss = 0.672 val_mse = 0.660 mse = 0.974 mae = 0.727\n",
            "epoch172 train time: 4.193s test time: 0.486  loss = 0.680 val_mse = 0.653 mse = 0.960 mae = 0.724\n",
            "epoch173 train time: 4.199s test time: 0.483  loss = 0.670 val_mse = 0.656 mse = 0.973 mae = 0.726\n",
            "epoch174 train time: 4.195s test time: 0.478  loss = 0.680 val_mse = 0.650 mse = 0.953 mae = 0.721\n",
            "epoch175 train time: 4.190s test time: 0.474  loss = 0.670 val_mse = 0.655 mse = 0.963 mae = 0.722\n",
            "epoch176 train time: 4.197s test time: 0.474  loss = 0.680 val_mse = 0.644 mse = 0.954 mae = 0.718\n",
            "epoch177 train time: 4.200s test time: 0.469  loss = 0.669 val_mse = 0.654 mse = 0.971 mae = 0.723\n",
            "epoch178 train time: 4.196s test time: 0.483  loss = 0.678 val_mse = 0.649 mse = 0.944 mae = 0.716\n",
            "epoch179 train time: 4.192s test time: 0.485  loss = 0.671 val_mse = 0.650 mse = 0.960 mae = 0.719\n",
            "epoch180 train time: 4.197s test time: 0.480  loss = 0.678 val_mse = 0.643 mse = 0.950 mae = 0.717\n",
            "epoch181 train time: 4.187s test time: 0.474  loss = 0.667 val_mse = 0.650 mse = 0.964 mae = 0.720\n",
            "epoch182 train time: 4.193s test time: 0.478  loss = 0.679 val_mse = 0.640 mse = 0.938 mae = 0.712\n",
            "epoch183 train time: 4.194s test time: 0.482  loss = 0.669 val_mse = 0.648 mse = 0.953 mae = 0.715\n",
            "epoch184 train time: 4.189s test time: 0.477  loss = 0.679 val_mse = 0.642 mse = 0.944 mae = 0.714\n",
            "epoch185 train time: 4.206s test time: 0.473  loss = 0.667 val_mse = 0.646 mse = 0.964 mae = 0.720\n",
            "epoch186 train time: 4.203s test time: 0.474  loss = 0.677 val_mse = 0.639 mse = 0.934 mae = 0.709\n",
            "epoch187 train time: 4.184s test time: 0.479  loss = 0.669 val_mse = 0.644 mse = 0.948 mae = 0.713\n",
            "epoch188 train time: 4.194s test time: 0.480  loss = 0.677 val_mse = 0.635 mse = 0.942 mae = 0.711\n",
            "epoch189 train time: 4.197s test time: 0.470  loss = 0.667 val_mse = 0.647 mse = 0.957 mae = 0.717\n",
            "epoch190 train time: 4.192s test time: 0.474  loss = 0.677 val_mse = 0.641 mse = 0.931 mae = 0.708\n",
            "epoch191 train time: 4.197s test time: 0.485  loss = 0.668 val_mse = 0.642 mse = 0.948 mae = 0.712\n",
            "epoch192 train time: 4.189s test time: 0.486  loss = 0.677 val_mse = 0.634 mse = 0.941 mae = 0.711\n",
            "epoch193 train time: 4.195s test time: 0.471  loss = 0.666 val_mse = 0.644 mse = 0.956 mae = 0.716\n",
            "epoch194 train time: 4.202s test time: 0.483  loss = 0.676 val_mse = 0.632 mse = 0.926 mae = 0.705\n",
            "epoch195 train time: 4.204s test time: 0.474  loss = 0.669 val_mse = 0.644 mse = 0.944 mae = 0.710\n",
            "epoch196 train time: 4.197s test time: 0.475  loss = 0.676 val_mse = 0.633 mse = 0.937 mae = 0.709\n",
            "epoch197 train time: 4.192s test time: 0.473  loss = 0.666 val_mse = 0.636 mse = 0.951 mae = 0.714\n",
            "epoch198 train time: 4.194s test time: 0.470  loss = 0.677 val_mse = 0.633 mse = 0.927 mae = 0.706\n",
            "epoch199 train time: 4.215s test time: 0.476  loss = 0.667 val_mse = 0.637 mse = 0.943 mae = 0.709\n",
            "epoch200 train time: 4.200s test time: 0.483  loss = 0.676 val_mse = 0.628 mse = 0.936 mae = 0.708\n",
            "epoch201 train time: 4.192s test time: 0.476  loss = 0.665 val_mse = 0.638 mse = 0.948 mae = 0.712\n",
            "epoch202 train time: 4.201s test time: 0.482  loss = 0.675 val_mse = 0.631 mse = 0.925 mae = 0.704\n",
            "epoch203 train time: 4.194s test time: 0.478  loss = 0.668 val_mse = 0.638 mse = 0.946 mae = 0.710\n",
            "epoch204 train time: 4.195s test time: 0.480  loss = 0.675 val_mse = 0.628 mse = 0.932 mae = 0.706\n",
            "epoch205 train time: 4.202s test time: 0.474  loss = 0.665 val_mse = 0.638 mse = 0.947 mae = 0.712\n",
            "epoch206 train time: 4.189s test time: 0.469  loss = 0.675 val_mse = 0.629 mse = 0.926 mae = 0.704\n",
            "epoch207 train time: 4.195s test time: 0.479  loss = 0.667 val_mse = 0.636 mse = 0.941 mae = 0.708\n",
            "epoch208 train time: 4.198s test time: 0.475  loss = 0.675 val_mse = 0.625 mse = 0.929 mae = 0.706\n",
            "epoch209 train time: 4.187s test time: 0.475  loss = 0.665 val_mse = 0.635 mse = 0.951 mae = 0.712\n",
            "epoch210 train time: 4.190s test time: 0.473  loss = 0.675 val_mse = 0.632 mse = 0.924 mae = 0.703\n",
            "epoch211 train time: 4.195s test time: 0.470  loss = 0.666 val_mse = 0.637 mse = 0.941 mae = 0.707\n",
            "epoch212 train time: 4.194s test time: 0.478  loss = 0.674 val_mse = 0.627 mse = 0.926 mae = 0.703\n",
            "epoch213 train time: 4.191s test time: 0.480  loss = 0.665 val_mse = 0.634 mse = 0.946 mae = 0.710\n",
            "epoch214 train time: 4.187s test time: 0.478  loss = 0.674 val_mse = 0.628 mse = 0.921 mae = 0.703\n",
            "epoch215 train time: 4.197s test time: 0.483  loss = 0.667 val_mse = 0.634 mse = 0.944 mae = 0.708\n",
            "epoch216 train time: 4.202s test time: 0.471  loss = 0.674 val_mse = 0.625 mse = 0.926 mae = 0.704\n",
            "epoch217 train time: 4.182s test time: 0.474  loss = 0.665 val_mse = 0.635 mse = 0.947 mae = 0.711\n",
            "epoch218 train time: 4.198s test time: 0.498  loss = 0.673 val_mse = 0.629 mse = 0.923 mae = 0.703\n",
            "epoch219 train time: 4.180s test time: 0.475  loss = 0.666 val_mse = 0.639 mse = 0.940 mae = 0.707\n",
            "epoch220 train time: 4.196s test time: 0.477  loss = 0.673 val_mse = 0.627 mse = 0.924 mae = 0.704\n",
            "epoch221 train time: 4.206s test time: 0.479  loss = 0.664 val_mse = 0.629 mse = 0.943 mae = 0.709\n",
            "epoch222 train time: 4.201s test time: 0.474  loss = 0.673 val_mse = 0.629 mse = 0.923 mae = 0.703\n",
            "epoch223 train time: 4.195s test time: 0.480  loss = 0.665 val_mse = 0.633 mse = 0.938 mae = 0.706\n",
            "epoch224 train time: 4.227s test time: 0.475  loss = 0.673 val_mse = 0.624 mse = 0.928 mae = 0.703\n",
            "epoch225 train time: 4.205s test time: 0.477  loss = 0.663 val_mse = 0.631 mse = 0.944 mae = 0.710\n",
            "epoch226 train time: 4.179s test time: 0.478  loss = 0.672 val_mse = 0.632 mse = 0.921 mae = 0.702\n",
            "epoch227 train time: 4.188s test time: 0.482  loss = 0.666 val_mse = 0.632 mse = 0.941 mae = 0.707\n",
            "epoch228 train time: 4.194s test time: 0.473  loss = 0.671 val_mse = 0.628 mse = 0.927 mae = 0.704\n",
            "epoch229 train time: 4.204s test time: 0.477  loss = 0.663 val_mse = 0.630 mse = 0.940 mae = 0.708\n",
            "epoch230 train time: 4.191s test time: 0.475  loss = 0.672 val_mse = 0.624 mse = 0.922 mae = 0.702\n",
            "epoch231 train time: 4.185s test time: 0.486  loss = 0.664 val_mse = 0.636 mse = 0.938 mae = 0.706\n",
            "epoch232 train time: 4.189s test time: 0.479  loss = 0.671 val_mse = 0.629 mse = 0.921 mae = 0.701\n",
            "epoch233 train time: 4.191s test time: 0.477  loss = 0.663 val_mse = 0.626 mse = 0.940 mae = 0.707\n",
            "epoch234 train time: 4.203s test time: 0.473  loss = 0.672 val_mse = 0.623 mse = 0.921 mae = 0.701\n",
            "epoch235 train time: 4.192s test time: 0.482  loss = 0.663 val_mse = 0.632 mse = 0.938 mae = 0.707\n",
            "epoch236 train time: 4.200s test time: 0.479  loss = 0.670 val_mse = 0.623 mse = 0.922 mae = 0.701\n",
            "epoch237 train time: 4.211s test time: 0.480  loss = 0.662 val_mse = 0.630 mse = 0.939 mae = 0.708\n",
            "epoch238 train time: 4.198s test time: 0.474  loss = 0.670 val_mse = 0.623 mse = 0.919 mae = 0.700\n",
            "epoch239 train time: 4.197s test time: 0.478  loss = 0.664 val_mse = 0.633 mse = 0.937 mae = 0.705\n",
            "MAE 0.7376690645403283\n",
            "MSE 0.9709354154188156\n"
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    print(sys.path[0])\n",
        "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
        "    word_latent_dim = 300\n",
        "    latent_dim = 15\n",
        "    max_doc_length = 300\n",
        "    num_filters = 50\n",
        "    window_size = 3\n",
        "    v_dim = 50\n",
        "    learning_rate = 0.001\n",
        "    lambda_1 = 0.05 #normally we set from [0.05, 0.01, 0.005, 0.001]\n",
        "    drop_out = 0.8\n",
        "    batch_size = 200\n",
        "    epochs = 240\n",
        "    avg_mae_list = []\n",
        "    avg_mse_list = []\n",
        "    \n",
        "    # loading data\n",
        "    firTime = time()\n",
        "    dataSet = Dataset(max_doc_length, sys.path[0], \"dataPreprocessingWordDict.out\")\n",
        "    word_dict, user_reviews, item_reviews, train, valRatings, testRatings = dataSet.word_id_dict, dataSet.userReview_dict, dataSet.itemReview_dict, dataSet.trainMtrx, dataSet.valRatings, dataSet.testRatings\n",
        "    secTime = time()\n",
        "\n",
        "    num_users, num_items = train.shape\n",
        "    print(\"load data: %.3fs\" % (secTime - firTime))\n",
        "    print(num_users, num_items)\n",
        "    print(latent_dim)\n",
        "\n",
        "    #load word embeddings\n",
        "    word_embedding_mtrx = ini_word_embed(len(word_dict), word_latent_dim)\n",
        "    # word_embedding_mtrx = word2vec_word_embed(len(word_dict), word_latent_dim,\n",
        "    #                                           \"Directory of pretrained WordEmbedding.out\",\n",
        "    #                                           word_dict)\n",
        "\n",
        "    print( \"shape\", word_embedding_mtrx.shape)\n",
        "\n",
        "    # get train instances\n",
        "    user_input, item_input, rateings = get_train_instance(train)\n",
        "    print (len(user_input), len(item_input), len(rateings))\n",
        "\n",
        "    # get test/val instances\n",
        "    user_vals, item_vals, user_input_val, item_input_val, rating_input_val = get_test_list(200, valRatings, user_reviews, item_reviews)\n",
        "    user_tests, item_tests, user_input_test, item_input_test, rating_input_test = get_test_list(200, testRatings, user_reviews, item_reviews)\n",
        "\n",
        "    #train & eval model\n",
        "    train_model()"
      ],
      "id": "Qash_diVA1zn"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6VYM0b5asBMB"
      },
      "source": [
        "## Plot"
      ],
      "id": "6VYM0b5asBMB"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "R1nVse_jDcMn",
        "outputId": "65fa7cd9-cf9d-43e0-a540-b8777ef48083"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3df7wWZZ3/8ddbwDAhWYFKBT1karKKWJiWVrDpppTaDy3ph5GYu5uWZZq2XxO0stp2c7ddXRdbltQNxNaKVdLM31qaoERgawIeA38eyd9Iiny+f1zXgTn3ue9zbuDMOcK8n4/HeZyZua6Z+czcc89n5pr7vm5FBGZmVl3b9HUAZmbWt5wIzMwqzonAzKzinAjMzCrOicDMrOKcCMzMKs6JYCsk6UOSVkh6XtL+TdQfL2llb8TWUyRdLOlrmzjvZEm3F8afl/SmJub7e0k/6KK8VdKhmxJTV/FZ+STtJWmhpOckfaGv4+ltlU8EPfXm7UmSQtKbN2MR/wicEhGDIuLeEpbfUG+dxCLibyPi6z20rEERsbyJeudHxIk9sU7beJJa8rHbv4TFfwW4KSIGR8T3G6z/fZJuzcmiTdItko6qqTM+x3hmg9ifz3+tks6qqdNn56LKJ4Kt1G7Akr4OwmwL0uV7RtIxwJXApcAI4A3AOcCRNVU/DfwJOL7BooZExCDgGOBrkg7bzLh7RkRU+g9oBQ7Nw5OBO4ALgKeB5cA78/QVwBPApwvzzgQuBq4HngNuAXYrlP9Lnu9ZYAHwrkJZP+DvgWV53gXASOBWIIAXgOeBj9WJeRvgbOChHNOlwA7Aa/I87fMvqzNvp+UD44GVwJfz8h4FPlOY5zWku4w/Ao/nbd6uzrL3BtYAr+RlPw2Myv+3yXUuAZ4ozHMZ8MU8vDMwl/RGWgp8tovXbSbwjTzcXfxD83KfBX4DfB24vVAewJuBA4HHgH6Fsg8Bi/LwNODyQtmn8muwCvh/dDyW1sdXjLEwflbhtb8P+FChbHIxvjrbfgjwq7xfVwCT8/Qd8rHQluM6u7DfJ9Ozx/Y7gbuBZ/L/dxbKbs77+I487y+AYYXygwrx/xYY38y8pOMvSMfW88A78ut2S47jSeCKLvbbUaST/dN5PXvn6TeSjtk1ebl71synvO4zujmXbJ9jPg54CRhXKGvJsfcvTPtNcZnF46fXz4N9sdJX0x+dE8Fa4DOkE/U38gFwIelk+Nf5hR5UeLM8B7w7l/8LHU8wnySdhPqTTlKPAQNz2RnA74C98oG2HzA0lwXw5i5iPoF0onwTMAi4CrisUN7d/B3KSSeptcB5wABgIrAa+ItcfgHpRLojMBj4X+BbDZY9mZqTWN6Hb8vD95NOQnsXyvbPw7cCFwEDgbGkE9pfNVjPTDomgq7inw3MyW/UfYCHqZMI8vAy4LBC2ZXAWXl4GjkRAKNJJ4321/57OYZmE8GxpMS3DSkZvwDs1GgfFubbjXTMTcrbOhQYm8suBX6WX6MW4A/AlJ4+tvNx8BQpEfbPsTzFhuP35rwf9wS2y+PfzmW7kBLnxLzth+Xx4U3M20Lnk+ksUhLehnTcHNJgv+2Z9/Fheb99hfQe2raw3hMbzPuWvN5R3ZxLPkW6COlHeo/8a6GsQ+ykZLiajhcArTgR9M0fnRPBA4WyffOL94bCtFWFN95MYHahbBDpymJkg3U9BeyXh+8Hjm5Qr7sT+Q3A5wrjewEvFw6yTUkEL9a8wZ7IB6vyG2j3Qtk7gAcbLHsynRPBZcBpwBvzdv8D8LcU7hZId0OvAIML830LmNlgPTPpmAgaxd8v75u3FMrOp3Ei+AYwIw8Pztu+Wx6fxoZEcE7Na7896SqwqURQZ3sWth8P9fZhod5XgZ/Umd4vr390YdrfADf39LFNOuH9pmb9v2bDncnNwNmFss8B1+bhMylctORp15HvRrqZt4XOieBSYDowopv3+deAOYXxbUgXBOML622UCA7O6x3YzTp+CfxzHp5EupAZUBP706RjNUh32SrM30ofJQI/I+js8cLwiwARUTttUGF8RftARDxPatbYGUDS6ZJ+L+kZSU+Tbt2H5eojSVc+m2Jn0q1/u4dIV2Zv2MTlAayKiLWF8dWk7RwOvBZYIOnpvB3X5unNuoV0Inw36ar/ZuA9+e+2iFhH2qY/RcRzhfkeIl1Bbm78/Sm8TnTcd7V+BHxY0muADwP3RES9+jvT8bV/gXQibYqk4/OnVNr36T5sODa60ui4GUa60q09Lor7r6eO7drjr966HisMt78WkO5ojm3f7rzthwA7NTFvPV8hXaz8RtISSSc0qNch5nzMraC546v9dd2pUQVJI4EJwH/nST8j3aG8v6bqMNL2fJn0nhjQxPpL50Sw+Ua2D0gaRLptfkTSu0gH6UdJTRRDSO2YytVXALtv4jofIb2h2u1Kuu1/vH71zfIk6QTxlxExJP/tEOmBVz1RZ9otwLtIB/4twO2kq6z35HFI27SjpMGF+XYlXbVtjjbSvhlZmLZro8oRcR/phHEE8HFSYqjnUTq+9q8lNdO0e4GUQNu9sVB3N9KzklNIzSlDgMVsODa60ui4eZJ051N7XGzO/qt7bNP5+NuYda0g3REMKfxtHxHfbmLeTsdWRDwWEZ+NiJ1Jd0AXNfhEXIeYJYm0fc3EfH+O+yNd1PkU6Xz6v5IeIzV/DiQ9PK6N+ZWI+B7pmcTnmlh/6ZwINt9ESYdI2pb0kOvOiFhBalZYSzoR9Zd0DvC6wnw/AL4uaQ8lYyS1n0geJ7X/NzIL+JKkUfkNej7pIdnaLuYp6m756+Urp0uACyS9HkDSLpLe18WyR+T90b6MB0jJ5JPALRHxbK73EXIiyPvsV8C3JA2UNAaYAlze5DY1iv8V0jOUaZJeK2k0dd6cNX4EnEq6g7myQZ0fAx8ovPbn0fH9tJB0bOwo6Y3AFwtl25NOam0Akj5DuiNoxn8Dh0r6qKT+koZKGpu3cw7wTUmDc7I5jc3bf42O7XnAnpI+nmP4GOmZydVNLPNy4Mj8Ucx++bUeL2lEE/O2AesoHLuSji3M+xRpv66rM+8c4P2S3itpAOmK/M+kY65LkdptTiN9yuczkl4naZu8b6bnap8GziU922r/+whpHw6tu2D4NvAVSQML0wbkfdL+V8ZHZTtxIth8PwKmkm6b30Y62UFq97yW9MDuIVL2LzZPfI90cP6C9GmW/yQ9HIPUFv3DfOv80TrrnEFqd78VeDAv+/MbEXN3y691JunB2p2SniW1he7VoO6NpE9mPCbpycL0W0jNNysK4wLuKdSZRGpLfQT4CTA1In7Z1BZ17RTS7fhjpLbv/+qm/izS3cqNEfFkvQoRsQQ4mfT6P0o6CRW/lHcZ6RMxraTX+IrCvPcB/0RqV3+c1F5/RzMbEhF/JD1o/TLpmFtI+qABpGPgBdLV6O05thnNLLeBusd2RKwCPpBjWEW68/1Ao31VE/8K4GjSJ+baSO+JM2jiXBQRq4FvAnfkY/cg4ADgLknPkz7QcGrU+U5IRNyf4/9X0t3TkcCREfFSd+vN8/+Y9FD/BNLx+TjpedLPchy7ARfmO5T2v7mk982kBou9hnTcfLYwbR7poqn9b1oz8W0u5YcUtgkkzSQ9ADy7r2Mx60k+tqvFdwRmZhXnRGBmVnFuGjIzqzjfEZiZVVyvfDSpJw0bNixaWlr6Ogwzsy3KggULnoyIul8E3eISQUtLC/Pnz+/rMMzMtiiSGn6j3k1DZmYV50RgZlZxTgRmZhW3xT0jMDPbVC+//DIrV65kzZo1fR1KaQYOHMiIESMYMKD5jk2dCMysMlauXMngwYNpaWkhdUC6dYkIVq1axcqVKxk1alTT87lpyMwqY82aNQwdOnSrTAIAkhg6dOhG3/E4EZhZpWytSaDdpmyfE4GZWcX5GYGZVdctPfzl1PeM67aKJD7xiU9w+eXpN4PWrl3LTjvtxIEHHsjVV2/4bZ8PfvCDPPbYY9x5553rp02bNo1LLrmE4cM3fEH45ptvZsiQIZsVthNBCc4999zS1zF16tTS12FmPW/77bdn8eLFvPjii2y33XZcf/317LJLx59Ofvrpp1mwYAGDBg1i+fLlvOlNG35Q8Etf+hKnn356j8bkpiEzs142ceJErrnmGgBmzZrFpEkdf8Tsqquu4sgjj+S4445j9uzZpcfjRGBm1svaT/Br1qxh0aJFHHjggR3K25PDpEmTmDVrVoeyCy64gLFjxzJ27FgmTJjQI/G4acjMrJeNGTOG1tZWZs2axcSJEzuUPf744zzwwAMccsghSGLAgAEsXryYffbZB3DTkJnZVuOoo47i9NNP79QsNGfOHJ566ilGjRpFS0vL+oRRptISgaQZkp6QtLibegdIWivpmLJiMTN7tTnhhBOYOnUq++67b4fps2bN4tprr6W1tZXW1lYWLFhQ+nOCMpuGZgL/BlzaqIKkfsB3gF+UGIeZWX1NfNyzLCNGjOALX/hCh2mtra089NBDHHTQQeunjRo1ih122IG77roLSM8I2j96CvDTn/6Uzf2xrtISQUTcKqmlm2qfB/4HOKCsOMzMXk2ef/75TtPGjx/P+PHjAXj44Yc7ld9zzz0AHHjggUybNq3HY+qzZwSSdgE+BPx7X8VgZmZ9+7D4n4EzI2JddxUlnSRpvqT5bW1tvRCamVl19OXHR8cBs3MHScOAiZLWRsRPaytGxHRgOsC4ceOiV6M0M9vK9VkiiIj1nWVLmglcXS8JmJlZuUpLBJJmAeOBYZJWAlOBAQARcXFZ6zUzs41T5qeGJnVfa33dyWXFYWZmXXMXE2ZWWT3dU3AzvQJ31w31448/zpQpU1ixYgUvv/wyLS0tzJs3j9bWVvbee2/22muv9cs67bTTOP744zc7bicCM7Ne1F031Oeccw6HHXYYp556KgCLFi1aX7b77ruzcOHCHo/JfQ2ZmfWyrrqhfvTRRxkxYsT68TFjxpQejxOBmVkv66ob6pNPPpkpU6YwYcIEvvnNb/LII4+sL1u2bNn6LqjHjh3Lbbfd1iPxuGnIzKyXddUN9fve9z6WL1/Otddey89//nP2339/Fi9OfXe6acjMbCvSqBtqgB133JGPf/zjXHbZZRxwwAHceuutpcbiRGBm1gcadUN94403snr1agCee+45li1bxq677lpqLG4aMrPKaubjnmWp1w01wIIFCzjllFPo378/69at48QTT+SAAw6gtbV1/TOCdieccELdZWwsJwIzs17UXTfUZ5xxBmeccUanOi0tLbz44oulxOSmITOzinMiMDOrOCcCM6uUiK27J/tN2T4nAjOrjIEDB7Jq1aqtNhlEBKtWrWLgwIEbNZ8fFptZZYwYMYKVK1eyNf/S4cCBAzt0UdEMJwIzq4wBAwYwatSo7itWjJuGzMwqzonAzKzinAjMzCrOicDMrOKcCMzMKq60RCBphqQnJC1uUP4JSYsk/U7SryTtV1YsZmbWWJl3BDOBw7sofxB4T0TsC3wdmF5iLGZm1kBp3yOIiFsltXRR/qvC6J3Axn0DwszMesSr5RnBFODnjQolnSRpvqT5W/M3As3M+kKfJwJJE0iJ4MxGdSJiekSMi4hxw4cP773gzMwqoE+7mJA0BvgBcERErOrLWMzMqqrP7ggk7QpcBXwqIv7QV3GYmVVdaXcEkmYB44FhklYCU4EBABFxMXAOMBS4SBLA2ogYV1Y8ZmZWX5mfGprUTfmJwIllrd/MzJrT5w+LzcysbzkRmJlVnBOBmVnFORGYmVWcE4GZWcU5EZiZVZwTgZlZxTkRmJlVnBOBmVnFORGYmVWcE4GZWcU5EZiZVZwTgZlZxTkRmJlVnBOBmVnFORGYmVWcE4GZWcU5EZiZVZwTgZlZxZWWCCTNkPSEpMUNyiXp+5KWSlok6a1lxWJmZo2VeUcwEzi8i/IjgD3y30nAv5cYi5mZNVBaIoiIW4E/dVHlaODSSO4Ehkjaqax4zMysvr58RrALsKIwvjJP60TSSZLmS5rf1tbWK8GZmVXFFvGwOCKmR8S4iBg3fPjwvg7HzGyr0peJ4GFgZGF8RJ5mZma9qC8TwVzg+PzpoYOAZyLi0T6Mx8yskvqXtWBJs4DxwDBJK4GpwACAiLgYmAdMBJYCq4HPlBWLmVmZzj333F5Zz9SpU0tZbmmJICImdVMewMllrd/MzJqzRTwsNjOz8jgRmJlVnBOBmVnFORGYmVWcE4GZWcU5EZiZVZwTgZlZxTkRmJlVnBOBmVnFORGYmVWcE4GZWcU5EZiZVZwTgZlZxTkRmJlVnBOBmVnFORGYmVWcE4GZWcV1mwgkbSPpnb0RjJmZ9b5uf6oyItZJuhDYvxfiKdct8/s6AtvK9cZv15b1u7VWXc02Dd0g6SOStDELl3S4pPslLZV0Vp3yXSXdJOleSYskTdyY5ZuZ2eZrNhH8DXAl8JKkZyU9J+nZrmaQ1A+4EDgCGA1MkjS6ptrZwJyI2B84Drhoo6I3M7PN1m3TEEBEDN6EZb8dWBoRywEkzQaOBu4rLhp4XR7eAXhkE9ZjZmaboalEACDpKODdefTmiLi6m1l2AVYUxlcCB9bUmQb8QtLnge2BQ5uNx8zMekZTTUOSvg2cSrqavw84VdK3emD9k4CZETECmAhcJqlTTJJOkjRf0vy2trYeWK2ZmbVr9hnBROCwiJgRETOAw4H3dzPPw8DIwviIPK1oCjAHICJ+DQwEhtUuKCKmR8S4iBg3fPjwJkM2M7NmbMwXyoYUhndoov7dwB6SRknalvQweG5NnT8C7wWQtDcpEfiS38ysFzX7jOB84F5JNwEiPSvo9HHQoohYK+kU4DqgHzAjIpZIOg+YHxFzgS8Dl0j6EunB8eSIiE3cFjMz2wTdJoLcZr8OOAg4IE8+MyIe627eiJgHzKuZdk5h+D7g4I0J2MzMelaz3yz+SkTMoXPTjpmZbeGafUbwS0mnSxopacf2v1IjMzOzXtHsM4KP5f8nF6YF8KaeDcfMzHpbs88IzoqIK3ohHjOznuXOJrvVbdNQRKwDzuiFWMzMrA/4GYGZWcX5GYFVg5sHzBpqtvfRUWUHYmZmfaPLpiFJXykMH1tTdn5ZQZmZWe/p7hnBcYXhr9aUHd7DsZiZWR/oLhGowXC9cTMz2wJ1lwiiwXC9cTMz2wJ197B4v/zbxAK2K/xOsUhdRpuZ2Rauy0QQEf16KxAzM+sbG/PDNGZmthVyIjAzqzgnAjOzinMiMDOrOCcCM7OKcyIwM6u4UhOBpMMl3S9pqaSzGtT5qKT7JC2R9KMy4zEzs86a7YZ6o0nqB1wIHAasBO6WNDci7ivU2YPUh9HBEfGUpNeXFY+ZmdVX5h3B24GlEbE8Il4CZgNH19T5LHBhRDwFEBFPlBiPmZnVUWYi2AVYURhfmacV7QnsKekOSXdKqtujqaSTJM2XNL+tra2kcM3MqqmvHxb3B/YAxgOTgEskDamtFBHTI2JcRIwbPnx4L4doZrZ1KzMRPAyMLIyPyNOKVgJzI+LliHgQ+AMpMZiZWS8pMxHcDewhaZSkbUk/cjO3ps5PSXcDSBpGaipaXmJMZmZWo7REEBFrgVOA64DfA3MiYomk8yQdlatdB6ySdB9wE3BGRKwqKyYzM+ustI+PAkTEPGBezbRzCsMBnJb/zMysD/T1w2IzM+tjTgRmZhXnRGBmVnFOBGZmFedEYGZWcU4EZmYV50RgZlZxTgRmZhXnRGBmVnFOBGZmFedEYGZWcU4EZmYV50RgZlZxTgRmZhXnRGBmVnFOBGZmFedEYGZWcU4EZmYV50RgZlZxpSYCSYdLul/SUklndVHvI5JC0rgy4zEzs85KSwSS+gEXAkcAo4FJkkbXqTcYOBW4q6xYzMyssTLvCN4OLI2I5RHxEjAbOLpOva8D3wHWlBiLmZk1UGYi2AVYURhfmaetJ+mtwMiIuKarBUk6SdJ8SfPb2tp6PlIzswrrs4fFkrYBvgd8ubu6ETE9IsZFxLjhw4eXH5yZWYWUmQgeBkYWxkfkae0GA/sAN0tqBQ4C5vqBsZlZ7yozEdwN7CFplKRtgeOAue2FEfFMRAyLiJaIaAHuBI6KiPklxmRmZjVKSwQRsRY4BbgO+D0wJyKWSDpP0lFlrdfMzDZO/zIXHhHzgHk1085pUHd8mbGYmVl9/maxmVnFORGYmVWcE4GZWcU5EZiZVZwTgZlZxTkRmJlVnBOBmVnFORGYmVWcE4GZWcU5EZiZVZwTgZlZxTkRmJlVnBOBmVnFORGYmVWcE4GZWcU5EZiZVZwTgZlZxTkRmJlVnBOBmVnFlZoIJB0u6X5JSyWdVaf8NEn3SVok6QZJu5UZj5mZdVZaIpDUD7gQOAIYDUySNLqm2r3AuIgYA/wY+Iey4jEzs/rKvCN4O7A0IpZHxEvAbODoYoWIuCkiVufRO4ERJcZjZmZ1lJkIdgFWFMZX5mmNTAF+Xq9A0kmS5kua39bW1oMhmpnZq+JhsaRPAuOA79Yrj4jpETEuIsYNHz68d4MzM9vK9S9x2Q8DIwvjI/K0DiQdCvw/4D0R8ecS4zEzszrKvCO4G9hD0ihJ2wLHAXOLFSTtD/wHcFREPFFiLGZm1kBpiSAi1gKnANcBvwfmRMQSSedJOipX+y4wCLhS0kJJcxsszszMSlJm0xARMQ+YVzPtnMLwoWWu38zMuveqeFhsZmZ9x4nAzKzinAjMzCrOicDMrOKcCMzMKs6JwMys4pwIzMwqzonAzKzinAjMzCrOicDMrOKcCMzMKs6JwMys4pwIzMwqzonAzKzinAjMzCrOicDMrOKcCMzMKs6JwMys4pwIzMwqzonAzKziSk0Ekg6XdL+kpZLOqlP+GklX5PK7JLWUGY+ZmXVWWiKQ1A+4EDgCGA1MkjS6ptoU4KmIeDNwAfCdsuIxM7P6yrwjeDuwNCKWR8RLwGzg6Jo6RwM/zMM/Bt4rSSXGZGZmNRQR5SxYOgY4PCJOzOOfAg6MiFMKdRbnOivz+LJc58maZZ0EnJRH9wLuLyXonjMMeLLbWrY18mtfTVvC675bRAyvV9C/tyPZFBExHZje13E0S9L8iBjX13FY7/NrX01b+uteZtPQw8DIwviIPK1uHUn9gR2AVSXGZGZmNcpMBHcDe0gaJWlb4Dhgbk2ducCn8/AxwI1RVluVmZnVVVrTUESslXQKcB3QD5gREUsknQfMj4i5wH8Cl0laCvyJlCy2BltMM5b1OL/21bRFv+6lPSw2M7Mtg79ZbGZWcU4EZmYVV6lEIOn5Jup8UdJrS45jiKTPFcZ3lvTjMtdp3ZP0iqSFkhZL+l9JQzZxOedJOrSn47PeJemNkmZLWiZpgaR5kvbMZV+UtEbSDoX64yU9k4+h/5P0j4WyyZL+rS+2oxmVSgRN+iKwUYkgd6exMYYA6xNBRDwSEcds5DKs570YEWMjYh/ShxdO3pSFRMQ5EfHLng3NelPu4eAnwM0RsXtEvA34KvCGXGUS6ZORH66Z9baIGAvsD3xA0sG9FfPmqGQiyJn7Zkk/zpn7v5V8AdgZuEnSTbnuX0v6taR7JF0paVCe3irpO5LuAY6V9FlJd0v6raT/ab+rkPQGST/J038r6Z3At4Hd85XDdyW15G9ZI2mgpP+S9DtJ90qakKdPlnSVpGslPSDpH/pg11XJr4FdACTtnvf7Akm3SXqLpB0kPSRpm1xne0krJA2QNDN/sx5Jb5N0S573Okk7SXq9pAW5fD9JIWnXPL6s7DtSa8oE4OWIuLh9QkT8NiJuk7Q7MAg4m5QQOomIF4GF5GPo1a6SiSDbn3T1Pxp4E3BwRHwfeASYEBETJA0jvdiHRsRbgfnAaYVlrIqIt0bEbOCqiDggIvYDfk/qUA/g+8AtefpbgSXAWcCyfPV5Rk1cJwMREfuSDrIfShqYy8YCHwP2BT4maSTW4/Id3nvZ8L2X6cDn81Xh6cBFEfEM6Y3+nlznA8B1EfFyYTkDgH8FjsnzzgC+GRFPAAMlvQ54F+m4epek3YAnImJ16Rtp3dkHWNCg7DhS32m3AXtJekNtBUl/AewB3FpahD1oi+hioiS/KfRxtBBoAW6vqXMQKVHcke4U2ZZ0pdjuisLwPpK+QWr2GUT6/gTAXwHHA0TEK8Az+SBp5BDSyYOI+D9JDwF75rIb8gkISfcBuwErmtxe6952+VjYhZTMr893gO8ErtSG/hBfk/9fQUrMN5FODhfVLG8v0gnl+jxvP+DRXPYr4GDg3cD5wOGASCcXe3WbBHwoItZJ+h/gWKC9/f9dkn5LSgL/HBGP9VWQG6PKieDPheFXqL8vBFwfEXVv/4AXCsMzgQ9GxG8lTQbG90CMtZqJ2TbdixExNjfNXEe6O5sJPJ3bfWvNBc6XtCPwNuDGmnIBSyLiHXXmvZV0N7Ab8DPgTCCAa3piQ2yzLSH1dtCBpH1JJ/nrCxeHD7IhEdwWER+QNAq4U9KciFjYSzFvsio3DTXyHDA4D98JHCzpzbC+HXjPBvMNBh7NzQGfKEy/Afi7PH+//CmD4jpq3dY+f17Xrrz6e1vdquSmmS8AXwZWAw9KOhbSQ0RJ++V6z5MeGP4LcHW+4yu6Hxgu6R153gGS/jKX3QZ8EnggItaRHk5PpPNdqfWNG4HXKPV8DICkMaSm3mkR0ZL/dgZ2zs1660XEg6RngWf2ZtCbyomgs+nAtZJuiog2YDIwS9IiUrPQWxrM9zXgLuAO4P8K008FJkj6HanNcXRErCI1Ny2W9N2a5VwEbJPrXwFMjog/Y70qIu4FFpGaAT4BTMm3/Evo+LsaV5BO6FfUWcZLpKvK7+R5F5KamYiIVtIdQ3sb8u2kO4+nytge2zi5z7MPAYfmB/hLgG+R7vR/UlP9J9TvHudi4N3a8MuLkyWtLPyNKCX4TeAuJszMKs53BGZmFedEYGZWcU4EZmYV50RgZlZxTgRmZhXnRGCW5T5/Li+M95fUJunqjVxOa+6eZLPqmPUWJwKzDV4gdRWyXR4/DHi4D+Mx6xVOBGYdzQPen4cnAbPaCyTtKOmnkhZJujN/0xRJQyX9QtISST8gfVGsfZ5PSvpN7mn2P1TTZXn+tvo1Sj3TLpb0sfI30awjJwKzjtHIjUIAAAFgSURBVGYDx+UeX8eQvi3e7lzg3ogYA/w9cGmePhW4PSL+kvQt0/YupfcmdUp3cO6r6BU6dj8CqbO5RyJiv/w7CNeWs1lmjbnTMrOCiFiUuwSYRLo7KDoE+Eiud2O+E3gdqQfRD+fp10hq7ybivaTO6O7OHZRtBzxRs8zfAf8k6Tuk/orc+6j1OicCs87mAv9I6ldm6GYsR8API+KrjSpExB8kvZXU4dw3JN0QEedtxjrNNpqbhsw6mwGcGxG/q5le7Bl2PPBkRDxL6jju43n6EUD7703cABwj6fW5bMfaXiol7QysjojLge+SfrzIrFf5jsCsRv7Bou/XKZoGzMg90a4GPp2nn0vqoXYJ6Qdn/piXc5+ks4FfKP2k5cuk3zh4qLDMfYHvSlqXy/+u57fIrGvufdTMrOLcNGRmVnFOBGZmFedEYGZWcU4EZmYV50RgZlZxTgRmZhXnRGBmVnH/H2m9y1bR15pzAAAAAElFTkSuQmCC\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "mae_list = [0.8343969736732412, 0.7217927465114119, 0.7376690645403283]\n",
        "mse_list = [1.4630443995149833, 0.8296060701243427, 0.9709354154188156]\n",
        "models = ['Interaction','Review', 'CARL']\n",
        "X_axis = np.arange(len(models))\n",
        "plt.bar(X_axis - 0.1, mae_list, 0.2, label = 'MAE', color='pink')\n",
        "plt.bar(X_axis + 0.1, mse_list, 0.2, label = 'MSE', color='grey')\n",
        "plt.title('Impact of the two individual components of CARL')\n",
        "plt.xticks(X_axis, models)\n",
        "plt.xlabel('Models')\n",
        "plt.ylabel('Error')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "id": "R1nVse_jDcMn"
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Analysis_CARL_AutomotiveFiles",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}