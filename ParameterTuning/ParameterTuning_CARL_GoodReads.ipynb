{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ms1901/CF_Project/blob/main/ParameterTuning/ParameterTuning_CARL_GoodReads.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4f364b6b",
        "outputId": "e07ddf14-4e93-47d6-9f51-d8c4775526a8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.7/dist-packages (2.8.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (4.2.0)\n",
            "Collecting tf-estimator-nightly==2.8.0.dev2021122109\n",
            "  Downloading tf_estimator_nightly-2.8.0.dev2021122109-py2.py3-none-any.whl (462 kB)\n",
            "\u001b[K     |████████████████████████████████| 462 kB 5.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorboard<2.9,>=2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.8.0)\n",
            "Requirement already satisfied: keras<2.9,>=2.8.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.8.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.25.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.14.0)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (14.0.1)\n",
            "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.0.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tensorflow) (57.4.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.21.6)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.2)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.17.3)\n",
            "Requirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.5.3)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.15.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.44.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow) (0.37.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow) (1.5.2)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (1.8.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (1.35.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (0.6.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (3.3.6)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (0.4.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (2.23.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (1.0.1)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (4.2.4)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (4.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow) (4.11.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow) (3.8.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (0.4.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (3.0.4)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow) (3.2.0)\n",
            "Installing collected packages: tf-estimator-nightly\n",
            "Successfully installed tf-estimator-nightly-2.8.0.dev2021122109\n"
          ]
        }
      ],
      "source": [
        "pip install --upgrade tensorflow"
      ],
      "id": "4f364b6b"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ac9149d"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import scipy.sparse as sp\n",
        "import math\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import sys\n",
        "from time import time\n",
        "import os\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ],
      "id": "2ac9149d"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q5LbbrqP0Ilv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "792cc2f4-8121-45aa-b2ca-dfa5ca156f2e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "id": "Q5LbbrqP0Ilv"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RVxfHDx8tGaF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bdd45efb-cab5-43a4-f39f-d4a561b9a2c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<module 'tensorflow._api.v2.version' from '/usr/local/lib/python3.7/dist-packages/tensorflow/_api/v2/version/__init__.py'>\n"
          ]
        }
      ],
      "source": [
        "print(tf.version)"
      ],
      "id": "RVxfHDx8tGaF"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZdenhkrjwCSg",
        "outputId": "0d72e65d-7935-47e2-94d7-7c87eee73906"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/CF_end_proejct/Goodreads\n"
          ]
        }
      ],
      "source": [
        "%cd /content/drive/MyDrive/CF_end_proejct/Goodreads"
      ],
      "id": "ZdenhkrjwCSg"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RGS6qYUmtLs_",
        "outputId": "1715409a-6cfd-44c4-f637-50b2b3ed9c4c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['/device:CPU:0', '/device:GPU:0']\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.python.client import device_lib\n",
        "\n",
        "def get_available_devices():\n",
        "    local_device_protos = device_lib.list_local_devices()\n",
        "    return [x.name for x in local_device_protos]\n",
        "\n",
        "print(get_available_devices())"
      ],
      "id": "RGS6qYUmtLs_"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "590f617f"
      },
      "source": [
        "ExtractData"
      ],
      "id": "590f617f"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4c63801c"
      },
      "outputs": [],
      "source": [
        "class Dataset(object):\n",
        "    \"'extract dataset from file'\"\n",
        "\n",
        "    def __init__(self, max_length, path, word_id_path):\n",
        "        self.word_id_dict = self.load_word_dict(path + word_id_path)\n",
        "        print(\"wordId_dict finished\")\n",
        "        \n",
        "        self.userReview_dict = self.load_reviews(max_length, len(self.word_id_dict), path + \"dataPreprocessingUserReviews.out\")\n",
        "        self.itemReview_dict = self.load_reviews(max_length, len(self.word_id_dict), path + \"dataPreprocessingItemReviews.out\")\n",
        "        print(\"load reviews finished\")\n",
        "        \n",
        "        self.num_users, self.num_items = len(self.userReview_dict), len(self.itemReview_dict)\n",
        "        \n",
        "        self.trainMtrx = self.load_ratingFile_as_mtrx(path + \"dataPreprocessingTrainInteraction.out\")\n",
        "        self.valRatings = self.load_ratingFile_as_list(path + \"dataPreprocessingValInteraction.out\")\n",
        "        self.testRatings = self.load_ratingFile_as_list(path + \"dataPreprocessingTestInteraction.out\")\n",
        "\n",
        "    def load_word_dict(self, path):\n",
        "        wordId_dict = {}\n",
        "\n",
        "        with open(path, \"r\") as f:\n",
        "            line = f.readline().replace(\"\\n\", \"\")\n",
        "            while line != None and line != \"\":\n",
        "                arr = line.split(\"\\t\")\n",
        "                wordId_dict[arr[0]] = int(arr[1])\n",
        "                line = f.readline().replace(\"\\n\", \"\")\n",
        "\n",
        "        return wordId_dict\n",
        "\n",
        "    def load_reviews(self, max_doc_length, padding_word_id, path):\n",
        "        entity_review_dict = {}\n",
        "\n",
        "        with open(path, \"r\") as f:\n",
        "            line = f.readline().replace(\"\\n\", \"\")\n",
        "            while line != None and line != \"\":\n",
        "                review = []\n",
        "                arr = line.split(\"\\t\")\n",
        "                entity = int(arr[0])\n",
        "                word_list = arr[1].split(\" \")\n",
        "\n",
        "                for i in range(len(word_list)):\n",
        "                    if (word_list[i] == \"\" or word_list[i] == None or (not self.word_id_dict[word_list[i]] in self.word_id_dict.keys())):\n",
        "                        continue\n",
        "                    review.append(self.word_id_dict.get(word_list[i]))\n",
        "                    if (len(review) >= max_doc_length):\n",
        "                        break\n",
        "                if (len(review) < max_doc_length):\n",
        "                    review = self.padding_word(max_doc_length, padding_word_id, review)\n",
        "                entity_review_dict[entity] = review\n",
        "                line = f.readline().replace(\"\\n\", \"\")\n",
        "        return entity_review_dict\n",
        "\n",
        "    def padding_word(self, max_size, max_word_idx, review):\n",
        "        review.extend([max_word_idx]*(max_size - len(review)))\n",
        "        return review\n",
        "\n",
        "    def load_ratingFile_as_mtrx(self, file_path):\n",
        "        mtrx = sp.dok_matrix((self.num_users, self.num_items), dtype=np.float32)\n",
        "        with open(file_path, \"r\") as f:\n",
        "            line = f.readline()\n",
        "            line = line.strip()\n",
        "            while line != None and line != \"\":\n",
        "                arr = line.split(\"\\t\")\n",
        "                user, item, rating = int(arr[0]), int(arr[1]), float(arr[2])\n",
        "                if (rating > 0):\n",
        "                    mtrx[user, item] = rating\n",
        "                line = f.readline()\n",
        "\n",
        "        return mtrx\n",
        "\n",
        "    def load_ratingFile_as_list(self, file_path):\n",
        "        rateList = []\n",
        "\n",
        "        with open(file_path, \"r\") as f:\n",
        "            line = f.readline()\n",
        "            while line != None and line != \"\":\n",
        "                arr = line.split(\"\\t\")\n",
        "                user, item = int(arr[0]), int(arr[1])\n",
        "                rate = float(arr[2])\n",
        "                rateList.append([user, item, rate])\n",
        "                line = f.readline()\n",
        "\n",
        "        return rateList"
      ],
      "id": "4c63801c"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "77e2f302"
      },
      "source": [
        "GetTest"
      ],
      "id": "77e2f302"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "523a402f"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "def get_test_list(batch_size, test_rating, user_reviews, item_reviews):\n",
        "    user_test_batchs, item_test_batchs, user_input_test_batchs, item_input_test_batchs, rating_input_test_batchs = [], [], [], [], []\n",
        "    for count in range(int(math.ceil(len(test_rating) / float(batch_size)))):\n",
        "        user_test, item_test, user_input_test, item_input_test, rating_input_test = [], [], [], [], []\n",
        "        for idx in range(batch_size):\n",
        "            index = (count * batch_size + idx)\n",
        "            if (index >= len(test_rating)):\n",
        "                break\n",
        "            rating = test_rating[index]\n",
        "            user_test.append(rating[0])\n",
        "            item_test.append(rating[1])\n",
        "            user_input_test.append(user_reviews.get(rating[0]))\n",
        "            item_input_test.append(item_reviews.get(rating[1]))\n",
        "            rating_input_test.append([rating[2]])\n",
        "        user_test_batchs.append(user_test)\n",
        "        item_test_batchs.append(item_test)\n",
        "        user_input_test_batchs.append(user_input_test)\n",
        "        item_input_test_batchs.append(item_input_test)\n",
        "        rating_input_test_batchs.append(rating_input_test)\n",
        "        #print count, len(item_input_test_batchs[count])\n",
        "    return user_test_batchs, item_test_batchs, user_input_test_batchs, item_input_test_batchs, rating_input_test_batchs"
      ],
      "id": "523a402f"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "74573912"
      },
      "source": [
        "CARL"
      ],
      "id": "74573912"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "70fbf310"
      },
      "outputs": [],
      "source": [
        "def ini_word_embed(num_words, latent_dim):\n",
        "    word_embeds = np.random.rand(num_words, latent_dim)\n",
        "    return word_embeds\n"
      ],
      "id": "70fbf310"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "15e0a2c1"
      },
      "outputs": [],
      "source": [
        "def word2vec_word_embed(num_words, latent_dim, path, word_id_dict):\n",
        "    word2vect_embed_mtrx = np.zeros((num_words, latent_dim))\n",
        "    with open(path, \"r\") as f:\n",
        "        line = f.readline()\n",
        "        while line != None and line != \"\":\n",
        "            arr = line.split(\"\\t\")\n",
        "            row_id = word_id_dict.get(arr[0])\n",
        "            vect = arr[1].strip().split(\" \")\n",
        "            for i in range(len(vect)):\n",
        "                word2vect_embed_mtrx[row_id, i] = float(vect[i])\n",
        "            line = f.readline()\n",
        "\n",
        "    return word2vect_embed_mtrx"
      ],
      "id": "15e0a2c1"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cabc3fef"
      },
      "outputs": [],
      "source": [
        "def get_train_instance(train):\n",
        "    user_input, item_input, rates = [], [], []\n",
        "\n",
        "    for (u, i) in train.keys():\n",
        "        # positive instance\n",
        "        user_input.append(u)\n",
        "        item_input.append(i)\n",
        "        rates.append(train[u,i])\n",
        "    return user_input, item_input, rates\n",
        "\n"
      ],
      "id": "cabc3fef"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e6abcbc9"
      },
      "outputs": [],
      "source": [
        "def get_train_instance_batch_change(count, batch_size, user_input, item_input, ratings, user_reviews, item_reviews):\n",
        "    users_batch, items_batch, user_input_batch, item_input_batch, labels_batch = [], [], [], [], []\n",
        "\n",
        "    for idx in range(batch_size):\n",
        "        index = (count*batch_size + idx) % len(user_input)\n",
        "        users_batch.append(user_input[index])\n",
        "        items_batch.append(item_input[index])\n",
        "        user_input_batch.append(user_reviews.get(user_input[index]))\n",
        "        item_input_batch.append(item_reviews.get(item_input[index]))\n",
        "        labels_batch.append([ratings[index]])\n",
        "\n",
        "    return users_batch, items_batch, user_input_batch, item_input_batch, labels_batch"
      ],
      "id": "e6abcbc9"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "38ffbde1"
      },
      "outputs": [],
      "source": [
        "#review.py\n",
        "def  cnn_model_average(filters, user_reviews_representation_expnd, item_reviews_representation_expnd, W_u, W_i, W_u_1, W_i_1, rand_matrix):\n",
        "    #convolution layer\n",
        "    convU = tf.nn.conv2d(user_reviews_representation_expnd, W_u, strides=[1, 1, word_latent_dim, 1], padding='SAME')\n",
        "    convI = tf.nn.conv2d(item_reviews_representation_expnd, W_i, strides=[1, 1, word_latent_dim, 1], padding='SAME')\n",
        "\n",
        "    hU = tf.nn.relu(tf.squeeze(convU, 2))\n",
        "    hI = tf.nn.relu(tf.squeeze(convI, 2))\n",
        "\n",
        "    # attentive layer\n",
        "    sec_dim = int(hU.get_shape()[1])\n",
        "    tmphU = tf.reshape(hU, [-1, filters])\n",
        "    hU_mul_rand = tf.reshape(tf.matmul(tmphU, rand_matrix), [-1, sec_dim, filters])\n",
        "    f = tf.matmul(hU_mul_rand, hI, transpose_b=True)\n",
        "    f = tf.expand_dims(f, -1)\n",
        "    att1 = tf.tanh(f)\n",
        "\n",
        "    pool_user = tf.reduce_mean(att1, 2)\n",
        "    pool_item = tf.reduce_mean(att1, 1)\n",
        "\n",
        "    user_flat = tf.squeeze(pool_user, -1)\n",
        "    item_flat = tf.squeeze(pool_item, -1)\n",
        "\n",
        "    weight_user = tf.nn.softmax(user_flat)\n",
        "    weight_item = tf.nn.softmax(item_flat)\n",
        "\n",
        "    weight_user_exp = tf.expand_dims(weight_user, -1)\n",
        "    weight_item_exp = tf.expand_dims(weight_item, -1)\n",
        "\n",
        "    hU = tf.expand_dims(hU * weight_user_exp, -1)\n",
        "    hI = tf.expand_dims(hI * weight_item_exp, -1)\n",
        "\n",
        "    #abstracting layer\n",
        "    hU_1 = tf.nn.relu(tf.nn.conv2d(hU, W_u_1, strides=[1, 1, 1, 1], padding='VALID'))\n",
        "    hI_1 = tf.nn.relu(tf.nn.conv2d(hI, W_i_1, strides=[1, 1, 1, 1], padding='VALID'))\n",
        "\n",
        "    sec_dim = hU_1.get_shape()[1]\n",
        "\n",
        "    oU = tf.nn.avg_pool(hU_1, ksize=[1, sec_dim, 1, 1], strides=[1, 1, 1, 1],\n",
        "        padding='VALID')\n",
        "    oI = tf.nn.avg_pool(hI_1, ksize=[1, sec_dim, 1, 1], strides=[1, 1, 1, 1],\n",
        "        padding='VALID')\n",
        "\n",
        "    att_user = tf.squeeze(oU)\n",
        "    att_item = tf.squeeze(oI)\n",
        "    #print \"attention\", att_user.get_shape(), att_item.get_shape()\n",
        "\n",
        "    return att_user, att_item"
      ],
      "id": "38ffbde1"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6967e9a6"
      },
      "outputs": [],
      "source": [
        "def eval_model(users, items, users_inputs, items_inputs, dropout_rate, predict_rating, sess, user_batch, item_batch, user_input_batch, item_input_batch, rate_tests, rmses, maes):\n",
        "    print(sess)\n",
        "    print(\"user batch. \"+str(user_batch))\n",
        "    print(\"item batch \"+str(item_batch))\n",
        "    print(\"user_input_batch. \"+str(user_input_batch))\n",
        "    print(\"item_input_batch \"+str( item_input_batch))\n",
        "\n",
        "    print(\"before session run\")\n",
        "    predicts = sess.run(predict_rating, feed_dict={users: user_batch, items: item_batch, users_inputs: user_input_batch, items_inputs: item_input_batch, dropout_rate:1.0})\n",
        "    print(\"after session run\")\n",
        "    #print(predicts)\n",
        "    row, col = predicts.shape\n",
        "    for r in range(row):\n",
        "        rmses.append(pow((predicts[r, 0] - rate_tests[r][0]), 2))\n",
        "        maes.append(abs((predicts[r, 0] - rate_tests[r][0])))\n",
        "    print(rmses)\n",
        "    return rmses, maes\n"
      ],
      "id": "6967e9a6"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "55f18043"
      },
      "outputs": [],
      "source": [
        "#combination of interaction.py and review.py\n",
        "def train_model():\n",
        "    mae_list=[]\n",
        "    mse_list=[]\n",
        "    tf.compat.v1.disable_eager_execution()\n",
        "    #I and R-----\n",
        "    users = tf.compat.v1.placeholder(tf.int32, shape=[None])\n",
        "    items = tf.compat.v1.placeholder(tf.int32, shape=[None])\n",
        "    users_inputs = tf.compat.v1.placeholder(tf.int32, shape=[None, max_doc_length])\n",
        "    items_inputs = tf.compat.v1.placeholder(tf.int32, shape=[None, max_doc_length])\n",
        "    ratings = tf.compat.v1.placeholder(tf.float32, shape=[None, 1])\n",
        "    \n",
        "    #R-----\n",
        "    dropout_rate = tf.compat.v1.placeholder(tf.float32)\n",
        "    \n",
        "    #I-----\n",
        "    user_entity_embedding = tf.Variable(tf.random.normal([num_users, latent_dim], mean=0, stddev=0.02), name=\"user_entity_embeddings\")\n",
        "    item_entity_embedding = tf.Variable(tf.random.normal([num_items, latent_dim], mean=0, stddev=0.02), name=\"item_entity_embeddings\")\n",
        "    user_entity_embeds = tf.nn.embedding_lookup(user_entity_embedding, users)\n",
        "    item_entity_embeds = tf.nn.embedding_lookup(item_entity_embedding, items)\n",
        "    \n",
        "    #R-----\n",
        "    user_bias = tf.Variable(tf.random.normal([num_users, 1], mean=0, stddev=0.02), name=\"user_bias\")\n",
        "    item_bias = tf.Variable(tf.random.normal([num_items, 1], mean=0, stddev=0.02), name=\"item_bias\")\n",
        "    user_bs = tf.nn.embedding_lookup(user_bias, users)\n",
        "    item_bs = tf.nn.embedding_lookup(item_bias, items)\n",
        "    \n",
        "    text_embedding = tf.Variable(word_embedding_mtrx, dtype=tf.float32, name=\"review_text_embeds\")\n",
        "    padding_embedding = tf.Variable(np.zeros([1, word_latent_dim]), dtype=tf.float32)\n",
        "    text_mask = tf.constant([1.0] * text_embedding.get_shape()[0] + [0.0])\n",
        "    word_embeddings = tf.concat([text_embedding, padding_embedding], 0)\n",
        "    word_embeddings = word_embeddings * tf.expand_dims(text_mask, -1)\n",
        "    user_reviews_representation = tf.nn.embedding_lookup(word_embeddings, users_inputs)\n",
        "    user_reviews_representation_expnd = tf.expand_dims(user_reviews_representation, -1)\n",
        "    item_reviews_representation = tf.nn.embedding_lookup(word_embeddings, items_inputs)\n",
        "    item_reviews_representation_expnd = tf.expand_dims(item_reviews_representation, -1)\n",
        "    \n",
        "    # CNN layers\n",
        "    W_u = tf.Variable(tf.random.truncated_normal([window_size, word_latent_dim, 1, num_filters], stddev=0.3), name=\"review_W_u\")\n",
        "    W_i = tf.Variable(tf.random.truncated_normal([window_size, word_latent_dim, 1, num_filters], stddev=0.3), name=\"review_W_i\")\n",
        "    W_u_1 = tf.Variable(tf.random.truncated_normal([window_size, num_filters, 1, num_filters], stddev=0.3), name=\"review_W_u_1\")\n",
        "    W_i_1 = tf.Variable(tf.random.truncated_normal([window_size, num_filters, 1, num_filters], stddev=0.3), name=\"review_W_i_1\")\n",
        "    # b = tf.Variable(tf.constant(0.1, shape=[self.num_filters]))\n",
        "    rand_matrix = tf.Variable(tf.random.truncated_normal([num_filters, num_filters], stddev=0.3), name=\"review_rand_matrix\")\n",
        "\n",
        "    user_embeds, item_embeds = cnn_model_average(num_filters, user_reviews_representation_expnd, item_reviews_representation_expnd, W_u, W_i, W_u_1, W_i_1, rand_matrix)\n",
        "    \n",
        "    #shared MLP layer\n",
        "    W_mlp = tf.Variable(tf.random.normal([num_filters, latent_dim], mean=0, stddev=0.02), name=\"review_W_mlp\")\n",
        "    W_mlp = tf.nn.dropout(W_mlp, dropout_rate)\n",
        "    b_mlp = tf.Variable(tf.constant(0.1, shape=[latent_dim]), name=\"review_b_mlp\")\n",
        "\n",
        "    user_embeds = tf.nn.relu(tf.matmul(user_embeds, W_mlp) + b_mlp)\n",
        "    item_embeds = tf.nn.relu(tf.matmul(item_embeds, W_mlp) + b_mlp)\n",
        "\n",
        "    embeds_sum = tf.concat([tf.multiply(user_embeds, item_embeds), user_embeds, item_embeds], 1, name=\"concat_embed\")\n",
        "    #I\n",
        "    entity_embeds_sum = tf.concat([tf.multiply(user_entity_embeds, item_entity_embeds), user_entity_embeds, item_entity_embeds],1)\n",
        "\n",
        "    #FM layer\n",
        "    #R---\n",
        "    w_0 = tf.Variable(tf.zeros(1), name=\"review_w_0\")\n",
        "    w_1 = tf.Variable(tf.random.truncated_normal([1, latent_dim * 3], stddev=0.3), name=\"review_w_1\")\n",
        "    v = tf.Variable(tf.random.truncated_normal([latent_dim * 3, latent_dim * 3], stddev=0.3), name=\"review_v\")\n",
        "    \n",
        "    #I---\n",
        "    w_entity_0 = tf.Variable(tf.zeros(1), name=\"entity_w_0\")\n",
        "    w_entity_1 = tf.Variable(tf.random.truncated_normal([1, latent_dim*3], stddev=0.3), name=\"entity_w_1\")\n",
        "    v_entity = tf.Variable(tf.random.truncated_normal([latent_dim*3, v_dim], stddev=0.3), name=\"entity_v\")\n",
        "    \n",
        "    #I---\n",
        "    J_e_1 = w_entity_0 + tf.matmul(entity_embeds_sum, w_entity_1, transpose_b=True)\n",
        "    #R---\n",
        "    J_1 = w_0 + tf.matmul(embeds_sum, w_1, transpose_b=True)\n",
        "    \n",
        "    #I---\n",
        "    entity_embeds_sum_1 = tf.expand_dims(entity_embeds_sum, -1)\n",
        "    entity_embeds_sum_2 = tf.expand_dims(entity_embeds_sum, 1)\n",
        "    #R---\n",
        "    embeds_sum_1 = tf.expand_dims(embeds_sum, -1)\n",
        "    embeds_sum_2 = tf.expand_dims(embeds_sum, 1)\n",
        "    \n",
        "    #I---\n",
        "    J_e_2 = tf.reduce_sum(tf.reduce_sum(tf.multiply(tf.matmul(entity_embeds_sum_1, entity_embeds_sum_2), tf.matmul(v_entity, v_entity, transpose_b=True)),2), 1, keepdims=True)\n",
        "    #J_e_3 = tf.trace(tf.multiply(tf.matmul(entity_embeds_sum_1, entity_embeds_sum_2), tf.matmul(v_entity, v_entity, transpose_b=True))) \n",
        "    \n",
        "    #R---\n",
        "    J_2 = tf.reduce_sum(tf.reduce_sum(tf.multiply(tf.matmul(embeds_sum_1, embeds_sum_2), tf.matmul(v, v, transpose_b=True)),2), 1, keepdims=True)\n",
        "    #J_3 = tf.trace(tf.multiply(tf.matmul(embeds_sum_1, embeds_sum_2), tf.matmul(v, v, transpose_b=True)))\n",
        "    #R---\n",
        "    J_total = (J_1 + 0.5 * (J_2))\n",
        "    \n",
        "    #I---\n",
        "    J_e_total = (J_e_1 + 0.5 * (J_e_2))\n",
        "    \n",
        "    \n",
        "    numerator = J_total + J_e_total\n",
        "    predict_rating = tf.divide(J_total, numerator) * J_total + tf.divide(J_e_total,numerator) * J_e_total + user_bs + item_bs\n",
        "    loss = tf.reduce_mean(tf.math.squared_difference(predict_rating, ratings))\n",
        "    loss += lambda_1 * (tf.nn.l2_loss(W_i_1) + tf.nn.l2_loss(W_u_1) + tf.nn.l2_loss(user_entity_embedding) + tf.nn.l2_loss(item_entity_embedding) + tf.nn.l2_loss(W_u) + tf.nn.l2_loss(W_i) + tf.nn.l2_loss(v) + tf.nn.l2_loss(v_entity) + tf.nn.l2_loss(rand_matrix) + tf.nn.l2_loss(user_bs) + tf.nn.l2_loss(item_bs))\n",
        "    \n",
        "    train_step = tf.compat.v1.train.RMSPropOptimizer(learning_rate).minimize(loss)\n",
        "    saver = tf.compat.v1.train.Saver()\n",
        "    \n",
        "    init = tf.compat.v1.global_variables_initializer()\n",
        "    with tf.compat.v1.Session() as sess:\n",
        "        sess.run(init)\n",
        "\n",
        "        for e in range(epochs):\n",
        "            t = time()\n",
        "            loss_total = 0.0\n",
        "            count = 0.0\n",
        "            \n",
        "            for i in range(int(math.ceil(len(user_input) / float(batch_size)))):\n",
        "                user_batch, item_batch, user_input_batch, item_input_batch, rates_batch = get_train_instance_batch_change(i, batch_size,user_input, item_input, rateings, user_reviews,item_reviews)\n",
        "                _, loss_val, words = sess.run([train_step, loss, word_embeddings], feed_dict={users: user_batch, items: item_batch, users_inputs: user_input_batch, items_inputs: item_input_batch, ratings: rates_batch, dropout_rate:drop_out})\n",
        "                loss_total += loss_val\n",
        "                count += 1.0\n",
        "\n",
        "            t1 = time()            \n",
        "            #LOOK FROM HERE \n",
        "            val_mses, val_maes = [], []\n",
        "            for i in range(len(user_input_val)):\n",
        "                #eval_model(users, items, users_inputs, items_inputs, dropout_rate, predict_rating, sess, user_vals[i], item_vals[i], user_input_val[i], item_input_val[i], rating_input_val[i], val_mses, val_maes)\n",
        "                #predicts = sess.run(predict_rating, feed_dict={users: user_batch, items: item_batch, users_inputs: user_input_batch, items_inputs: item_input_batch, dropout_rate:1.0})\n",
        "                predicts = sess.run(predict_rating, feed_dict={users: user_vals[i], items: item_vals[i], users_inputs: user_input_val[i], items_inputs: item_input_val[i], ratings: rating_input_val[i], dropout_rate: drop_out})\n",
        "                #print(predicts)\n",
        "                row, col = predicts.shape\n",
        "                for r in range(row):\n",
        "                    val_mses.append(pow((predicts[r, 0] - rating_input_val[i][r][0]), 2))\n",
        "                    val_maes.append(abs((predicts[r, 0] - rating_input_val[i][r][0])))\n",
        "            val_mse = np.array(val_mses).mean()            \n",
        "            t2 = time()            \n",
        "            mses, maes = [], []\n",
        "            for i in range(len(user_input_test)):\n",
        "                #eval_model(users, items, users_inputs, items_inputs, dropout_rate, predict_rating, sess, user_tests[i], item_tests[i], user_input_test[i], item_input_test[i], rating_input_test[i], mses, maes)\n",
        "                predicts_test = sess.run(predict_rating, feed_dict={users: user_tests[i], items: item_tests[i], users_inputs: user_input_test[i], items_inputs: item_input_test[i], ratings: rating_input_test[i], dropout_rate: drop_out})\n",
        "                #print(predicts)\n",
        "                row, col = predicts_test.shape\n",
        "                for r in range(row):\n",
        "                    mses.append(pow((predicts_test[r, 0] - rating_input_test[i][r][0]), 2))\n",
        "                    maes.append(abs((predicts_test[r, 0] - rating_input_test[i][r][0])))\n",
        "                    \n",
        "            mse = np.array(mses).mean()\n",
        "            mae = np.array(maes).mean()\n",
        "            t3 = time()\n",
        "            mae_list.append(mae)\n",
        "            mse_list.append(mse)\n",
        "            print(\"epoch%d train time: %.3fs test time: %.3f  loss = %.3f val_mse = %.3f mse = %.3f mae = %.3f\"%(e, (t1 - t), (t3 - t2), loss_total/count, val_mse, mse, mae))\n",
        "        \n",
        "        avg_mae=sum(mae_list) / len(mae_list)\n",
        "        avg_mse=sum(mse_list)/len(mse_list)\n",
        "        avg_mae_list.append(avg_mae)\n",
        "        avg_mse_list.append(avg_mse)\n",
        "        print(\"MAE \"+str(avg_mae))\n",
        "        print(\"MSE \"+str(avg_mse))"
      ],
      "id": "55f18043"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a924fe38",
        "outputId": "ef020579-fee0-45ea-8f3b-3782d4d444d9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "wordId_dict finished\n",
            "load reviews finished\n",
            "load data: 5.114s\n",
            "506 2581\n",
            "15\n",
            "shape (19930, 300)\n",
            "18880 18880 18880\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/training/rmsprop.py:188: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "epoch0 train time: 17.107s test time: 0.626  loss = 192.471 val_mse = 1.566 mse = 1.501 mae = 0.887\n",
            "epoch1 train time: 4.740s test time: 0.541  loss = 121.049 val_mse = 1.570 mse = 1.508 mae = 0.878\n",
            "epoch2 train time: 4.744s test time: 0.542  loss = 59.462 val_mse = 1.564 mse = 1.503 mae = 0.877\n",
            "epoch3 train time: 4.751s test time: 0.538  loss = 25.787 val_mse = 1.556 mse = 1.498 mae = 0.876\n",
            "epoch4 train time: 4.853s test time: 0.538  loss = 9.666 val_mse = 1.546 mse = 1.490 mae = 0.874\n",
            "epoch5 train time: 4.785s test time: 0.544  loss = 3.384 val_mse = 1.536 mse = 1.483 mae = 0.872\n",
            "epoch6 train time: 4.794s test time: 0.545  loss = 1.710 val_mse = 1.525 mse = 1.475 mae = 0.871\n",
            "epoch7 train time: 4.768s test time: 0.540  loss = 1.476 val_mse = 1.513 mse = 1.467 mae = 0.869\n",
            "epoch8 train time: 4.787s test time: 0.547  loss = 1.403 val_mse = 1.502 mse = 1.458 mae = 0.868\n",
            "epoch9 train time: 4.798s test time: 0.548  loss = 1.346 val_mse = 1.490 mse = 1.451 mae = 0.866\n",
            "epoch10 train time: 4.806s test time: 0.546  loss = 1.299 val_mse = 1.479 mse = 1.443 mae = 0.865\n",
            "epoch11 train time: 5.294s test time: 0.545  loss = 1.259 val_mse = 1.467 mse = 1.435 mae = 0.864\n",
            "epoch12 train time: 4.810s test time: 0.544  loss = 1.226 val_mse = 1.455 mse = 1.427 mae = 0.862\n",
            "epoch13 train time: 4.841s test time: 0.547  loss = 1.197 val_mse = 1.444 mse = 1.420 mae = 0.861\n",
            "epoch14 train time: 4.824s test time: 0.549  loss = 1.172 val_mse = 1.432 mse = 1.412 mae = 0.860\n",
            "epoch15 train time: 4.826s test time: 0.548  loss = 1.149 val_mse = 1.420 mse = 1.405 mae = 0.859\n",
            "epoch16 train time: 4.843s test time: 0.542  loss = 1.128 val_mse = 1.409 mse = 1.398 mae = 0.858\n",
            "epoch17 train time: 4.853s test time: 0.553  loss = 1.110 val_mse = 1.398 mse = 1.392 mae = 0.857\n",
            "epoch18 train time: 4.858s test time: 0.549  loss = 1.093 val_mse = 1.388 mse = 1.386 mae = 0.855\n",
            "epoch19 train time: 4.853s test time: 0.548  loss = 1.077 val_mse = 1.378 mse = 1.380 mae = 0.854\n",
            "epoch20 train time: 4.850s test time: 0.551  loss = 1.062 val_mse = 1.369 mse = 1.375 mae = 0.852\n",
            "epoch21 train time: 4.866s test time: 0.554  loss = 1.049 val_mse = 1.360 mse = 1.371 mae = 0.851\n",
            "epoch22 train time: 4.885s test time: 0.557  loss = 1.035 val_mse = 1.352 mse = 1.367 mae = 0.849\n",
            "epoch23 train time: 4.890s test time: 0.552  loss = 1.023 val_mse = 1.344 mse = 1.363 mae = 0.847\n",
            "epoch24 train time: 4.888s test time: 0.558  loss = 1.012 val_mse = 1.337 mse = 1.360 mae = 0.845\n",
            "epoch25 train time: 4.883s test time: 0.554  loss = 1.001 val_mse = 1.331 mse = 1.357 mae = 0.843\n",
            "epoch26 train time: 4.883s test time: 0.559  loss = 0.990 val_mse = 1.325 mse = 1.355 mae = 0.841\n",
            "epoch27 train time: 4.898s test time: 0.563  loss = 0.981 val_mse = 1.320 mse = 1.353 mae = 0.840\n",
            "epoch28 train time: 4.910s test time: 0.558  loss = 0.971 val_mse = 1.316 mse = 1.352 mae = 0.838\n",
            "epoch29 train time: 4.910s test time: 0.556  loss = 0.963 val_mse = 1.311 mse = 1.350 mae = 0.836\n",
            "epoch30 train time: 4.913s test time: 0.550  loss = 0.954 val_mse = 1.304 mse = 1.346 mae = 0.835\n",
            "epoch31 train time: 4.909s test time: 0.559  loss = 0.947 val_mse = 1.302 mse = 1.346 mae = 0.834\n",
            "epoch32 train time: 4.903s test time: 0.550  loss = 0.941 val_mse = 1.293 mse = 1.340 mae = 0.833\n",
            "epoch33 train time: 4.914s test time: 0.562  loss = 0.933 val_mse = 1.300 mse = 1.348 mae = 0.832\n",
            "epoch34 train time: 4.917s test time: 0.549  loss = 0.928 val_mse = 1.285 mse = 1.338 mae = 0.832\n",
            "epoch35 train time: 4.924s test time: 0.553  loss = 0.922 val_mse = 1.283 mse = 1.335 mae = 0.830\n",
            "epoch36 train time: 4.918s test time: 0.562  loss = 0.916 val_mse = 1.290 mse = 1.347 mae = 0.832\n",
            "epoch37 train time: 4.922s test time: 0.559  loss = 0.911 val_mse = 1.281 mse = 1.338 mae = 0.831\n",
            "epoch38 train time: 4.923s test time: 0.563  loss = 0.908 val_mse = 1.274 mse = 1.338 mae = 0.833\n",
            "epoch39 train time: 4.937s test time: 0.567  loss = 0.902 val_mse = 1.275 mse = 1.332 mae = 0.830\n",
            "epoch40 train time: 4.929s test time: 0.553  loss = 0.900 val_mse = 1.265 mse = 1.332 mae = 0.833\n",
            "epoch41 train time: 4.946s test time: 0.559  loss = 0.898 val_mse = 1.269 mse = 1.326 mae = 0.828\n",
            "epoch42 train time: 4.931s test time: 0.553  loss = 0.896 val_mse = 1.252 mse = 1.318 mae = 0.833\n",
            "epoch43 train time: 4.932s test time: 0.562  loss = 0.895 val_mse = 1.267 mse = 1.325 mae = 0.829\n",
            "epoch44 train time: 4.932s test time: 0.552  loss = 0.893 val_mse = 1.246 mse = 1.316 mae = 0.833\n",
            "epoch45 train time: 4.943s test time: 0.552  loss = 0.891 val_mse = 1.274 mse = 1.334 mae = 0.831\n",
            "epoch46 train time: 4.943s test time: 0.563  loss = 0.890 val_mse = 1.251 mse = 1.322 mae = 0.833\n",
            "epoch47 train time: 4.935s test time: 0.552  loss = 0.891 val_mse = 1.255 mse = 1.317 mae = 0.830\n",
            "epoch48 train time: 4.922s test time: 0.552  loss = 0.889 val_mse = 1.250 mse = 1.317 mae = 0.836\n",
            "epoch49 train time: 4.929s test time: 0.563  loss = 0.890 val_mse = 1.252 mse = 1.317 mae = 0.836\n",
            "epoch50 train time: 4.939s test time: 0.559  loss = 0.891 val_mse = 1.244 mse = 1.315 mae = 0.835\n",
            "epoch51 train time: 4.942s test time: 0.566  loss = 0.887 val_mse = 1.254 mse = 1.316 mae = 0.835\n",
            "epoch52 train time: 4.939s test time: 0.555  loss = 0.890 val_mse = 1.243 mse = 1.312 mae = 0.835\n",
            "epoch53 train time: 4.942s test time: 0.557  loss = 0.889 val_mse = 1.255 mse = 1.324 mae = 0.838\n",
            "epoch54 train time: 4.939s test time: 0.568  loss = 0.889 val_mse = 1.244 mse = 1.315 mae = 0.836\n",
            "epoch55 train time: 4.951s test time: 0.552  loss = 0.887 val_mse = 1.254 mse = 1.321 mae = 0.834\n",
            "epoch56 train time: 4.951s test time: 0.552  loss = 0.888 val_mse = 1.236 mse = 1.310 mae = 0.839\n",
            "epoch57 train time: 4.947s test time: 0.565  loss = 0.885 val_mse = 1.257 mse = 1.329 mae = 0.841\n",
            "epoch58 train time: 4.940s test time: 0.554  loss = 0.888 val_mse = 1.245 mse = 1.314 mae = 0.838\n",
            "epoch59 train time: 4.949s test time: 0.563  loss = 0.890 val_mse = 1.248 mse = 1.319 mae = 0.839\n",
            "epoch60 train time: 4.944s test time: 0.561  loss = 0.888 val_mse = 1.233 mse = 1.306 mae = 0.837\n",
            "epoch61 train time: 4.960s test time: 0.555  loss = 0.883 val_mse = 1.253 mse = 1.326 mae = 0.839\n",
            "epoch62 train time: 4.950s test time: 0.557  loss = 0.884 val_mse = 1.242 mse = 1.320 mae = 0.840\n",
            "epoch63 train time: 4.933s test time: 0.556  loss = 0.887 val_mse = 1.254 mse = 1.325 mae = 0.839\n",
            "epoch64 train time: 4.951s test time: 0.556  loss = 0.880 val_mse = 1.241 mse = 1.313 mae = 0.838\n",
            "epoch65 train time: 4.939s test time: 0.555  loss = 0.885 val_mse = 1.245 mse = 1.321 mae = 0.841\n",
            "epoch66 train time: 4.940s test time: 0.554  loss = 0.880 val_mse = 1.243 mse = 1.319 mae = 0.838\n",
            "epoch67 train time: 4.945s test time: 0.564  loss = 0.878 val_mse = 1.254 mse = 1.326 mae = 0.839\n",
            "epoch68 train time: 4.954s test time: 0.562  loss = 0.884 val_mse = 1.235 mse = 1.312 mae = 0.841\n",
            "epoch69 train time: 4.946s test time: 0.557  loss = 0.878 val_mse = 1.243 mse = 1.316 mae = 0.841\n",
            "epoch70 train time: 4.965s test time: 0.554  loss = 0.882 val_mse = 1.233 mse = 1.313 mae = 0.838\n",
            "epoch71 train time: 4.963s test time: 0.561  loss = 0.876 val_mse = 1.251 mse = 1.325 mae = 0.842\n",
            "epoch72 train time: 4.953s test time: 0.549  loss = 0.873 val_mse = 1.237 mse = 1.313 mae = 0.838\n",
            "epoch73 train time: 4.951s test time: 0.558  loss = 0.878 val_mse = 1.252 mse = 1.324 mae = 0.843\n",
            "epoch74 train time: 4.953s test time: 0.552  loss = 0.881 val_mse = 1.228 mse = 1.304 mae = 0.840\n",
            "epoch75 train time: 4.958s test time: 0.554  loss = 0.874 val_mse = 1.241 mse = 1.324 mae = 0.843\n",
            "epoch76 train time: 4.944s test time: 0.562  loss = 0.875 val_mse = 1.234 mse = 1.309 mae = 0.839\n",
            "epoch77 train time: 4.955s test time: 0.560  loss = 0.876 val_mse = 1.245 mse = 1.321 mae = 0.841\n",
            "epoch78 train time: 4.948s test time: 0.565  loss = 0.878 val_mse = 1.227 mse = 1.309 mae = 0.839\n",
            "epoch79 train time: 4.954s test time: 0.557  loss = 0.873 val_mse = 1.240 mse = 1.316 mae = 0.839\n",
            "epoch80 train time: 4.954s test time: 0.557  loss = 0.871 val_mse = 1.230 mse = 1.309 mae = 0.840\n",
            "epoch81 train time: 4.980s test time: 0.569  loss = 0.875 val_mse = 1.245 mse = 1.317 mae = 0.840\n",
            "epoch82 train time: 4.966s test time: 0.571  loss = 0.873 val_mse = 1.228 mse = 1.311 mae = 0.838\n",
            "epoch83 train time: 4.958s test time: 0.563  loss = 0.869 val_mse = 1.241 mse = 1.324 mae = 0.844\n",
            "epoch84 train time: 4.965s test time: 0.562  loss = 0.871 val_mse = 1.227 mse = 1.311 mae = 0.838\n",
            "epoch85 train time: 4.952s test time: 0.564  loss = 0.867 val_mse = 1.244 mse = 1.317 mae = 0.842\n",
            "epoch86 train time: 4.968s test time: 0.558  loss = 0.871 val_mse = 1.226 mse = 1.311 mae = 0.841\n",
            "epoch87 train time: 4.976s test time: 0.557  loss = 0.870 val_mse = 1.240 mse = 1.314 mae = 0.841\n",
            "epoch88 train time: 4.974s test time: 0.553  loss = 0.868 val_mse = 1.228 mse = 1.307 mae = 0.835\n",
            "epoch89 train time: 4.951s test time: 0.554  loss = 0.867 val_mse = 1.243 mse = 1.320 mae = 0.842\n",
            "epoch90 train time: 4.953s test time: 0.561  loss = 0.865 val_mse = 1.228 mse = 1.310 mae = 0.838\n",
            "epoch91 train time: 4.955s test time: 0.554  loss = 0.865 val_mse = 1.236 mse = 1.319 mae = 0.841\n",
            "epoch92 train time: 4.961s test time: 0.560  loss = 0.866 val_mse = 1.223 mse = 1.302 mae = 0.839\n",
            "epoch93 train time: 4.948s test time: 0.565  loss = 0.867 val_mse = 1.238 mse = 1.315 mae = 0.839\n",
            "epoch94 train time: 4.961s test time: 0.557  loss = 0.864 val_mse = 1.227 mse = 1.308 mae = 0.837\n",
            "epoch95 train time: 4.958s test time: 0.564  loss = 0.863 val_mse = 1.241 mse = 1.321 mae = 0.843\n",
            "epoch96 train time: 4.962s test time: 0.555  loss = 0.871 val_mse = 1.217 mse = 1.303 mae = 0.839\n",
            "epoch97 train time: 4.975s test time: 0.558  loss = 0.862 val_mse = 1.232 mse = 1.313 mae = 0.840\n",
            "epoch98 train time: 4.954s test time: 0.567  loss = 0.861 val_mse = 1.226 mse = 1.306 mae = 0.838\n",
            "epoch99 train time: 4.954s test time: 0.562  loss = 0.860 val_mse = 1.231 mse = 1.316 mae = 0.840\n",
            "epoch100 train time: 4.967s test time: 0.561  loss = 0.859 val_mse = 1.216 mse = 1.302 mae = 0.838\n",
            "epoch101 train time: 4.956s test time: 0.558  loss = 0.862 val_mse = 1.231 mse = 1.316 mae = 0.843\n",
            "epoch102 train time: 4.957s test time: 0.558  loss = 0.862 val_mse = 1.220 mse = 1.303 mae = 0.839\n",
            "epoch103 train time: 4.962s test time: 0.551  loss = 0.854 val_mse = 1.233 mse = 1.317 mae = 0.841\n",
            "epoch104 train time: 4.966s test time: 0.559  loss = 0.857 val_mse = 1.217 mse = 1.303 mae = 0.838\n",
            "epoch105 train time: 4.955s test time: 0.556  loss = 0.857 val_mse = 1.230 mse = 1.314 mae = 0.843\n",
            "epoch106 train time: 4.960s test time: 0.557  loss = 0.857 val_mse = 1.215 mse = 1.300 mae = 0.838\n",
            "epoch107 train time: 4.963s test time: 0.558  loss = 0.855 val_mse = 1.233 mse = 1.313 mae = 0.839\n",
            "epoch108 train time: 4.952s test time: 0.552  loss = 0.856 val_mse = 1.218 mse = 1.303 mae = 0.836\n",
            "epoch109 train time: 4.952s test time: 0.558  loss = 0.854 val_mse = 1.226 mse = 1.314 mae = 0.842\n",
            "epoch110 train time: 4.966s test time: 0.564  loss = 0.851 val_mse = 1.213 mse = 1.298 mae = 0.837\n",
            "epoch111 train time: 4.962s test time: 0.554  loss = 0.852 val_mse = 1.229 mse = 1.312 mae = 0.840\n",
            "epoch112 train time: 4.968s test time: 0.564  loss = 0.851 val_mse = 1.215 mse = 1.301 mae = 0.834\n",
            "epoch113 train time: 4.966s test time: 0.567  loss = 0.848 val_mse = 1.230 mse = 1.314 mae = 0.842\n",
            "epoch114 train time: 4.956s test time: 0.552  loss = 0.849 val_mse = 1.214 mse = 1.301 mae = 0.837\n",
            "epoch115 train time: 4.966s test time: 0.556  loss = 0.852 val_mse = 1.223 mse = 1.311 mae = 0.841\n",
            "epoch116 train time: 4.954s test time: 0.552  loss = 0.847 val_mse = 1.207 mse = 1.297 mae = 0.838\n",
            "epoch117 train time: 4.958s test time: 0.552  loss = 0.846 val_mse = 1.226 mse = 1.309 mae = 0.838\n",
            "epoch118 train time: 4.961s test time: 0.558  loss = 0.846 val_mse = 1.207 mse = 1.297 mae = 0.836\n",
            "epoch119 train time: 4.952s test time: 0.559  loss = 0.845 val_mse = 1.230 mse = 1.312 mae = 0.841\n",
            "epoch120 train time: 4.955s test time: 0.568  loss = 0.845 val_mse = 1.208 mse = 1.299 mae = 0.838\n",
            "epoch121 train time: 4.949s test time: 0.562  loss = 0.844 val_mse = 1.217 mse = 1.310 mae = 0.840\n",
            "epoch122 train time: 4.955s test time: 0.556  loss = 0.843 val_mse = 1.210 mse = 1.295 mae = 0.836\n",
            "epoch123 train time: 4.963s test time: 0.555  loss = 0.842 val_mse = 1.220 mse = 1.308 mae = 0.838\n",
            "epoch124 train time: 4.951s test time: 0.566  loss = 0.842 val_mse = 1.210 mse = 1.296 mae = 0.835\n",
            "epoch125 train time: 4.954s test time: 0.555  loss = 0.838 val_mse = 1.222 mse = 1.310 mae = 0.839\n",
            "epoch126 train time: 4.960s test time: 0.565  loss = 0.846 val_mse = 1.209 mse = 1.297 mae = 0.837\n",
            "epoch127 train time: 4.963s test time: 0.564  loss = 0.842 val_mse = 1.214 mse = 1.306 mae = 0.840\n",
            "epoch128 train time: 4.959s test time: 0.564  loss = 0.839 val_mse = 1.214 mse = 1.293 mae = 0.835\n",
            "epoch129 train time: 4.956s test time: 0.557  loss = 0.841 val_mse = 1.216 mse = 1.303 mae = 0.836\n",
            "epoch130 train time: 4.966s test time: 0.559  loss = 0.838 val_mse = 1.211 mse = 1.299 mae = 0.835\n",
            "epoch131 train time: 4.974s test time: 0.554  loss = 0.837 val_mse = 1.220 mse = 1.310 mae = 0.841\n",
            "epoch132 train time: 4.966s test time: 0.556  loss = 0.840 val_mse = 1.204 mse = 1.298 mae = 0.836\n",
            "epoch133 train time: 4.962s test time: 0.555  loss = 0.837 val_mse = 1.215 mse = 1.303 mae = 0.836\n",
            "epoch134 train time: 4.963s test time: 0.556  loss = 0.839 val_mse = 1.208 mse = 1.295 mae = 0.837\n",
            "epoch135 train time: 4.951s test time: 0.554  loss = 0.839 val_mse = 1.218 mse = 1.307 mae = 0.839\n",
            "epoch136 train time: 4.959s test time: 0.561  loss = 0.837 val_mse = 1.211 mse = 1.292 mae = 0.832\n",
            "epoch137 train time: 4.957s test time: 0.563  loss = 0.836 val_mse = 1.221 mse = 1.307 mae = 0.841\n",
            "epoch138 train time: 4.971s test time: 0.563  loss = 0.839 val_mse = 1.206 mse = 1.295 mae = 0.836\n",
            "epoch139 train time: 4.997s test time: 0.560  loss = 0.838 val_mse = 1.215 mse = 1.306 mae = 0.838\n",
            "epoch140 train time: 4.987s test time: 0.562  loss = 0.837 val_mse = 1.209 mse = 1.297 mae = 0.835\n",
            "epoch141 train time: 4.980s test time: 0.558  loss = 0.839 val_mse = 1.220 mse = 1.311 mae = 0.838\n",
            "epoch142 train time: 4.971s test time: 0.556  loss = 0.840 val_mse = 1.209 mse = 1.292 mae = 0.836\n",
            "epoch143 train time: 4.947s test time: 0.561  loss = 0.838 val_mse = 1.219 mse = 1.308 mae = 0.840\n",
            "epoch144 train time: 4.959s test time: 0.560  loss = 0.837 val_mse = 1.207 mse = 1.293 mae = 0.834\n",
            "epoch145 train time: 4.954s test time: 0.553  loss = 0.834 val_mse = 1.215 mse = 1.304 mae = 0.836\n",
            "epoch146 train time: 4.973s test time: 0.560  loss = 0.840 val_mse = 1.209 mse = 1.299 mae = 0.838\n",
            "epoch147 train time: 4.972s test time: 0.562  loss = 0.839 val_mse = 1.210 mse = 1.301 mae = 0.836\n",
            "epoch148 train time: 4.956s test time: 0.552  loss = 0.834 val_mse = 1.206 mse = 1.294 mae = 0.834\n",
            "epoch149 train time: 4.951s test time: 0.558  loss = 0.834 val_mse = 1.220 mse = 1.308 mae = 0.842\n",
            "epoch150 train time: 4.942s test time: 0.561  loss = 0.842 val_mse = 1.207 mse = 1.297 mae = 0.839\n",
            "epoch151 train time: 4.961s test time: 0.558  loss = 0.839 val_mse = 1.219 mse = 1.309 mae = 0.838\n",
            "epoch152 train time: 4.974s test time: 0.562  loss = 0.835 val_mse = 1.208 mse = 1.297 mae = 0.836\n",
            "epoch153 train time: 4.970s test time: 0.554  loss = 0.838 val_mse = 1.212 mse = 1.299 mae = 0.836\n",
            "epoch154 train time: 4.970s test time: 0.562  loss = 0.836 val_mse = 1.207 mse = 1.294 mae = 0.834\n",
            "epoch155 train time: 4.950s test time: 0.561  loss = 0.834 val_mse = 1.221 mse = 1.304 mae = 0.840\n",
            "epoch156 train time: 4.953s test time: 0.557  loss = 0.837 val_mse = 1.198 mse = 1.290 mae = 0.836\n",
            "epoch157 train time: 4.942s test time: 0.564  loss = 0.838 val_mse = 1.223 mse = 1.308 mae = 0.838\n",
            "epoch158 train time: 4.960s test time: 0.565  loss = 0.839 val_mse = 1.209 mse = 1.297 mae = 0.837\n",
            "epoch159 train time: 4.948s test time: 0.558  loss = 0.837 val_mse = 1.210 mse = 1.300 mae = 0.838\n",
            "epoch160 train time: 4.959s test time: 0.556  loss = 0.833 val_mse = 1.205 mse = 1.293 mae = 0.832\n",
            "epoch161 train time: 4.954s test time: 0.558  loss = 0.833 val_mse = 1.221 mse = 1.306 mae = 0.837\n",
            "epoch162 train time: 4.951s test time: 0.556  loss = 0.837 val_mse = 1.207 mse = 1.295 mae = 0.836\n",
            "epoch163 train time: 4.964s test time: 0.561  loss = 0.836 val_mse = 1.214 mse = 1.303 mae = 0.837\n",
            "epoch164 train time: 4.958s test time: 0.556  loss = 0.831 val_mse = 1.204 mse = 1.293 mae = 0.834\n",
            "epoch165 train time: 4.957s test time: 0.550  loss = 0.832 val_mse = 1.212 mse = 1.299 mae = 0.836\n",
            "epoch166 train time: 4.953s test time: 0.567  loss = 0.835 val_mse = 1.205 mse = 1.288 mae = 0.837\n",
            "epoch167 train time: 4.934s test time: 0.552  loss = 0.835 val_mse = 1.207 mse = 1.300 mae = 0.837\n",
            "epoch168 train time: 4.964s test time: 0.553  loss = 0.834 val_mse = 1.212 mse = 1.299 mae = 0.836\n",
            "epoch169 train time: 4.942s test time: 0.558  loss = 0.834 val_mse = 1.220 mse = 1.308 mae = 0.838\n",
            "epoch170 train time: 4.959s test time: 0.554  loss = 0.835 val_mse = 1.203 mse = 1.294 mae = 0.836\n",
            "epoch171 train time: 4.966s test time: 0.561  loss = 0.835 val_mse = 1.212 mse = 1.305 mae = 0.838\n",
            "epoch172 train time: 4.959s test time: 0.562  loss = 0.834 val_mse = 1.209 mse = 1.295 mae = 0.836\n",
            "epoch173 train time: 4.964s test time: 0.555  loss = 0.833 val_mse = 1.216 mse = 1.302 mae = 0.838\n",
            "epoch174 train time: 4.978s test time: 0.553  loss = 0.832 val_mse = 1.206 mse = 1.293 mae = 0.837\n",
            "epoch175 train time: 4.964s test time: 0.562  loss = 0.836 val_mse = 1.217 mse = 1.303 mae = 0.837\n",
            "epoch176 train time: 4.960s test time: 0.554  loss = 0.830 val_mse = 1.203 mse = 1.291 mae = 0.836\n",
            "epoch177 train time: 4.948s test time: 0.558  loss = 0.832 val_mse = 1.218 mse = 1.301 mae = 0.837\n",
            "epoch178 train time: 4.946s test time: 0.556  loss = 0.833 val_mse = 1.203 mse = 1.292 mae = 0.834\n",
            "epoch179 train time: 4.974s test time: 0.553  loss = 0.830 val_mse = 1.212 mse = 1.304 mae = 0.840\n",
            "MAE 0.8409811153816871\n",
            "MSE 1.3281734798779705\n"
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    print(sys.path[0])\n",
        "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
        "    word_latent_dim = 300\n",
        "    latent_dim = 15\n",
        "    max_doc_length = 300\n",
        "    num_filters = 50\n",
        "    window_size = 3\n",
        "    v_dim = 50\n",
        "    learning_rate = 0.001\n",
        "    lambda_1 = 0.05 #normally we set from [0.05, 0.01, 0.005, 0.001]\n",
        "    drop_out = 0.8\n",
        "    batch_size = 200\n",
        "    epochs = 180\n",
        "    avg_mae_list = []\n",
        "    avg_mse_list = []\n",
        "    \n",
        "    # loading data\n",
        "    firTime = time()\n",
        "    dataSet = Dataset(max_doc_length, sys.path[0], \"dataPreprocessingWordDict.out\")\n",
        "    word_dict, user_reviews, item_reviews, train, valRatings, testRatings = dataSet.word_id_dict, dataSet.userReview_dict, dataSet.itemReview_dict, dataSet.trainMtrx, dataSet.valRatings, dataSet.testRatings\n",
        "    secTime = time()\n",
        "\n",
        "    num_users, num_items = train.shape\n",
        "    print(\"load data: %.3fs\" % (secTime - firTime))\n",
        "    print(num_users, num_items)\n",
        "    print(latent_dim)\n",
        "\n",
        "    #load word embeddings\n",
        "    word_embedding_mtrx = ini_word_embed(len(word_dict), word_latent_dim)\n",
        "    # word_embedding_mtrx = word2vec_word_embed(len(word_dict), word_latent_dim,\n",
        "    #                                           \"Directory of pretrained WordEmbedding.out\",\n",
        "    #                                           word_dict)\n",
        "\n",
        "    print( \"shape\", word_embedding_mtrx.shape)\n",
        "\n",
        "    # get train instances\n",
        "    user_input, item_input, rateings = get_train_instance(train)\n",
        "    print (len(user_input), len(item_input), len(rateings))\n",
        "\n",
        "    # get test/val instances\n",
        "    user_vals, item_vals, user_input_val, item_input_val, rating_input_val = get_test_list(200, valRatings, user_reviews, item_reviews)\n",
        "    user_tests, item_tests, user_input_test, item_input_test, rating_input_test = get_test_list(200, testRatings, user_reviews, item_reviews)\n",
        "\n",
        "    #train & eval model\n",
        "    train_model()"
      ],
      "id": "a924fe38"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wAgPEu_TDLgj"
      },
      "source": [
        "Varying EPOCHS"
      ],
      "id": "wAgPEu_TDLgj"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EB5kGs3IC0dl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94f46612-5157-47e0-e5d8-9ad6082a56c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "wordId_dict finished\n",
            "load reviews finished\n",
            "load data: 1.538s\n",
            "506 2581\n",
            "15\n",
            "shape (19930, 300)\n",
            "18880 18880 18880\n",
            "epoch0 train time: 5.118s test time: 0.539  loss = 44.523 val_mse = 1.579 mse = 1.514 mae = 0.883\n",
            "epoch1 train time: 4.714s test time: 0.541  loss = 30.372 val_mse = 1.576 mse = 1.515 mae = 0.882\n",
            "epoch2 train time: 4.741s test time: 0.546  loss = 16.172 val_mse = 1.552 mse = 1.497 mae = 0.876\n",
            "epoch3 train time: 4.734s test time: 0.546  loss = 7.889 val_mse = 1.528 mse = 1.480 mae = 0.871\n",
            "epoch4 train time: 4.753s test time: 0.548  loss = 3.721 val_mse = 1.504 mse = 1.463 mae = 0.866\n",
            "epoch5 train time: 4.780s test time: 0.545  loss = 1.957 val_mse = 1.481 mse = 1.447 mae = 0.862\n",
            "epoch6 train time: 4.790s test time: 0.542  loss = 1.394 val_mse = 1.457 mse = 1.431 mae = 0.859\n",
            "epoch7 train time: 4.764s test time: 0.551  loss = 1.276 val_mse = 1.434 mse = 1.416 mae = 0.856\n",
            "epoch8 train time: 4.794s test time: 0.551  loss = 1.233 val_mse = 1.410 mse = 1.401 mae = 0.854\n",
            "epoch9 train time: 4.787s test time: 0.555  loss = 1.195 val_mse = 1.380 mse = 1.380 mae = 0.855\n",
            "epoch10 train time: 4.785s test time: 0.550  loss = 1.159 val_mse = 1.356 mse = 1.366 mae = 0.854\n",
            "epoch11 train time: 4.794s test time: 0.546  loss = 1.126 val_mse = 1.335 mse = 1.354 mae = 0.853\n",
            "epoch12 train time: 4.822s test time: 0.559  loss = 1.096 val_mse = 1.320 mse = 1.346 mae = 0.850\n",
            "epoch13 train time: 4.816s test time: 0.547  loss = 1.067 val_mse = 1.307 mse = 1.340 mae = 0.847\n",
            "epoch14 train time: 4.820s test time: 0.550  loss = 1.040 val_mse = 1.295 mse = 1.335 mae = 0.843\n",
            "epoch15 train time: 4.818s test time: 0.548  loss = 1.016 val_mse = 1.282 mse = 1.327 mae = 0.841\n",
            "epoch16 train time: 4.820s test time: 0.559  loss = 0.993 val_mse = 1.267 mse = 1.319 mae = 0.839\n",
            "epoch17 train time: 4.838s test time: 0.549  loss = 0.972 val_mse = 1.260 mse = 1.317 mae = 0.838\n",
            "epoch18 train time: 4.831s test time: 0.558  loss = 0.954 val_mse = 1.248 mse = 1.311 mae = 0.837\n",
            "epoch19 train time: 4.851s test time: 0.546  loss = 0.938 val_mse = 1.241 mse = 1.307 mae = 0.838\n",
            "epoch20 train time: 4.858s test time: 0.549  loss = 0.925 val_mse = 1.240 mse = 1.309 mae = 0.835\n",
            "epoch21 train time: 4.859s test time: 0.552  loss = 0.912 val_mse = 1.247 mse = 1.317 mae = 0.833\n",
            "epoch22 train time: 4.850s test time: 0.551  loss = 0.903 val_mse = 1.220 mse = 1.298 mae = 0.840\n",
            "epoch23 train time: 4.852s test time: 0.550  loss = 0.895 val_mse = 1.218 mse = 1.298 mae = 0.843\n",
            "epoch24 train time: 4.857s test time: 0.556  loss = 0.891 val_mse = 1.202 mse = 1.282 mae = 0.840\n",
            "epoch25 train time: 4.864s test time: 0.563  loss = 0.885 val_mse = 1.210 mse = 1.297 mae = 0.845\n",
            "epoch26 train time: 4.888s test time: 0.559  loss = 0.881 val_mse = 1.218 mse = 1.300 mae = 0.838\n",
            "epoch27 train time: 4.871s test time: 0.551  loss = 0.880 val_mse = 1.216 mse = 1.306 mae = 0.841\n",
            "epoch28 train time: 4.878s test time: 0.561  loss = 0.869 val_mse = 1.195 mse = 1.289 mae = 0.848\n",
            "epoch29 train time: 4.883s test time: 0.555  loss = 0.868 val_mse = 1.215 mse = 1.303 mae = 0.848\n",
            "epoch30 train time: 4.874s test time: 0.556  loss = 0.866 val_mse = 1.198 mse = 1.294 mae = 0.847\n",
            "epoch31 train time: 4.894s test time: 0.557  loss = 0.869 val_mse = 1.204 mse = 1.297 mae = 0.849\n",
            "epoch32 train time: 4.879s test time: 0.551  loss = 0.862 val_mse = 1.222 mse = 1.300 mae = 0.832\n",
            "epoch33 train time: 4.888s test time: 0.555  loss = 0.855 val_mse = 1.219 mse = 1.306 mae = 0.839\n",
            "epoch34 train time: 4.893s test time: 0.560  loss = 0.851 val_mse = 1.203 mse = 1.295 mae = 0.846\n",
            "epoch35 train time: 4.883s test time: 0.558  loss = 0.851 val_mse = 1.241 mse = 1.326 mae = 0.832\n",
            "epoch36 train time: 4.886s test time: 0.555  loss = 0.850 val_mse = 1.226 mse = 1.300 mae = 0.841\n",
            "epoch37 train time: 4.901s test time: 0.556  loss = 0.851 val_mse = 1.210 mse = 1.307 mae = 0.846\n",
            "epoch38 train time: 4.895s test time: 0.556  loss = 0.843 val_mse = 1.237 mse = 1.315 mae = 0.832\n",
            "epoch39 train time: 4.905s test time: 0.558  loss = 0.841 val_mse = 1.228 mse = 1.308 mae = 0.840\n",
            "epoch40 train time: 4.909s test time: 0.559  loss = 0.841 val_mse = 1.207 mse = 1.304 mae = 0.847\n",
            "epoch41 train time: 4.913s test time: 0.561  loss = 0.842 val_mse = 1.209 mse = 1.307 mae = 0.844\n",
            "epoch42 train time: 4.922s test time: 0.552  loss = 0.837 val_mse = 1.244 mse = 1.323 mae = 0.833\n",
            "epoch43 train time: 4.921s test time: 0.561  loss = 0.832 val_mse = 1.221 mse = 1.314 mae = 0.837\n",
            "epoch44 train time: 4.918s test time: 0.562  loss = 0.831 val_mse = 1.210 mse = 1.297 mae = 0.845\n",
            "epoch45 train time: 4.908s test time: 0.556  loss = 0.831 val_mse = 1.238 mse = 1.322 mae = 0.835\n",
            "epoch46 train time: 4.913s test time: 0.556  loss = 0.836 val_mse = 1.238 mse = 1.317 mae = 0.833\n",
            "epoch47 train time: 4.893s test time: 0.555  loss = 0.835 val_mse = 1.234 mse = 1.321 mae = 0.838\n",
            "epoch48 train time: 4.914s test time: 0.558  loss = 0.827 val_mse = 1.242 mse = 1.319 mae = 0.832\n",
            "epoch49 train time: 4.929s test time: 0.552  loss = 0.825 val_mse = 1.207 mse = 1.298 mae = 0.846\n",
            "epoch50 train time: 4.921s test time: 0.558  loss = 0.827 val_mse = 1.217 mse = 1.302 mae = 0.843\n",
            "epoch51 train time: 4.914s test time: 0.558  loss = 0.826 val_mse = 1.211 mse = 1.307 mae = 0.841\n",
            "epoch52 train time: 4.922s test time: 0.552  loss = 0.820 val_mse = 1.207 mse = 1.298 mae = 0.842\n",
            "epoch53 train time: 4.928s test time: 0.567  loss = 0.826 val_mse = 1.204 mse = 1.303 mae = 0.846\n",
            "epoch54 train time: 4.924s test time: 0.555  loss = 0.819 val_mse = 1.211 mse = 1.301 mae = 0.839\n",
            "epoch55 train time: 4.927s test time: 0.570  loss = 0.821 val_mse = 1.210 mse = 1.303 mae = 0.840\n",
            "epoch56 train time: 4.931s test time: 0.564  loss = 0.822 val_mse = 1.212 mse = 1.294 mae = 0.841\n",
            "epoch57 train time: 4.925s test time: 0.554  loss = 0.823 val_mse = 1.197 mse = 1.303 mae = 0.842\n",
            "epoch58 train time: 4.930s test time: 0.570  loss = 0.813 val_mse = 1.238 mse = 1.319 mae = 0.830\n",
            "epoch59 train time: 4.930s test time: 0.559  loss = 0.823 val_mse = 1.214 mse = 1.305 mae = 0.840\n",
            "MAE 0.845240419804884\n",
            "MSE 1.3344836261132609\n",
            "epoch0 train time: 5.330s test time: 0.575  loss = 43.505 val_mse = 1.574 mse = 1.511 mae = 0.883\n",
            "epoch1 train time: 4.934s test time: 0.558  loss = 30.375 val_mse = 1.562 mse = 1.503 mae = 0.878\n",
            "epoch2 train time: 4.934s test time: 0.563  loss = 16.183 val_mse = 1.540 mse = 1.486 mae = 0.873\n",
            "epoch3 train time: 4.938s test time: 0.564  loss = 7.907 val_mse = 1.518 mse = 1.470 mae = 0.869\n",
            "epoch4 train time: 4.912s test time: 0.553  loss = 3.740 val_mse = 1.495 mse = 1.454 mae = 0.866\n",
            "epoch5 train time: 4.922s test time: 0.565  loss = 1.974 val_mse = 1.473 mse = 1.439 mae = 0.863\n",
            "epoch6 train time: 4.901s test time: 0.552  loss = 1.411 val_mse = 1.451 mse = 1.425 mae = 0.861\n",
            "epoch7 train time: 4.900s test time: 0.571  loss = 1.293 val_mse = 1.430 mse = 1.410 mae = 0.859\n",
            "epoch8 train time: 4.925s test time: 0.559  loss = 1.249 val_mse = 1.408 mse = 1.397 mae = 0.857\n",
            "epoch9 train time: 4.920s test time: 0.556  loss = 1.210 val_mse = 1.386 mse = 1.383 mae = 0.856\n",
            "epoch10 train time: 4.937s test time: 0.562  loss = 1.175 val_mse = 1.365 mse = 1.370 mae = 0.854\n",
            "epoch11 train time: 4.917s test time: 0.567  loss = 1.142 val_mse = 1.345 mse = 1.359 mae = 0.853\n",
            "epoch12 train time: 4.912s test time: 0.561  loss = 1.112 val_mse = 1.326 mse = 1.348 mae = 0.852\n",
            "epoch13 train time: 4.932s test time: 0.563  loss = 1.083 val_mse = 1.307 mse = 1.337 mae = 0.850\n",
            "epoch14 train time: 4.921s test time: 0.555  loss = 1.056 val_mse = 1.292 mse = 1.330 mae = 0.848\n",
            "epoch15 train time: 4.917s test time: 0.560  loss = 1.030 val_mse = 1.281 mse = 1.325 mae = 0.844\n",
            "epoch16 train time: 4.915s test time: 0.557  loss = 1.007 val_mse = 1.271 mse = 1.321 mae = 0.842\n",
            "epoch17 train time: 4.924s test time: 0.562  loss = 0.985 val_mse = 1.261 mse = 1.317 mae = 0.839\n",
            "epoch18 train time: 4.915s test time: 0.563  loss = 0.964 val_mse = 1.247 mse = 1.309 mae = 0.838\n",
            "epoch19 train time: 4.932s test time: 0.560  loss = 0.948 val_mse = 1.235 mse = 1.304 mae = 0.840\n",
            "epoch20 train time: 4.912s test time: 0.561  loss = 0.933 val_mse = 1.231 mse = 1.300 mae = 0.839\n",
            "epoch21 train time: 4.919s test time: 0.554  loss = 0.921 val_mse = 1.238 mse = 1.307 mae = 0.836\n",
            "epoch22 train time: 4.928s test time: 0.564  loss = 0.908 val_mse = 1.239 mse = 1.313 mae = 0.834\n",
            "epoch23 train time: 4.922s test time: 0.560  loss = 0.900 val_mse = 1.212 mse = 1.297 mae = 0.842\n",
            "epoch24 train time: 4.924s test time: 0.560  loss = 0.891 val_mse = 1.222 mse = 1.303 mae = 0.834\n",
            "epoch25 train time: 4.920s test time: 0.559  loss = 0.886 val_mse = 1.217 mse = 1.301 mae = 0.839\n",
            "epoch26 train time: 4.928s test time: 0.555  loss = 0.890 val_mse = 1.198 mse = 1.292 mae = 0.850\n",
            "epoch27 train time: 4.926s test time: 0.561  loss = 0.886 val_mse = 1.222 mse = 1.307 mae = 0.841\n",
            "epoch28 train time: 4.931s test time: 0.566  loss = 0.875 val_mse = 1.198 mse = 1.292 mae = 0.840\n",
            "epoch29 train time: 4.939s test time: 0.556  loss = 0.871 val_mse = 1.224 mse = 1.310 mae = 0.838\n",
            "epoch30 train time: 4.933s test time: 0.558  loss = 0.867 val_mse = 1.221 mse = 1.310 mae = 0.845\n",
            "epoch31 train time: 4.927s test time: 0.556  loss = 0.867 val_mse = 1.232 mse = 1.314 mae = 0.842\n",
            "epoch32 train time: 4.920s test time: 0.555  loss = 0.867 val_mse = 1.206 mse = 1.298 mae = 0.845\n",
            "epoch33 train time: 4.924s test time: 0.561  loss = 0.861 val_mse = 1.224 mse = 1.313 mae = 0.839\n",
            "epoch34 train time: 4.941s test time: 0.553  loss = 0.855 val_mse = 1.248 mse = 1.329 mae = 0.837\n",
            "epoch35 train time: 4.916s test time: 0.558  loss = 0.853 val_mse = 1.238 mse = 1.331 mae = 0.835\n",
            "epoch36 train time: 4.917s test time: 0.560  loss = 0.849 val_mse = 1.209 mse = 1.296 mae = 0.843\n",
            "epoch37 train time: 4.931s test time: 0.554  loss = 0.853 val_mse = 1.248 mse = 1.317 mae = 0.829\n",
            "epoch38 train time: 4.922s test time: 0.557  loss = 0.842 val_mse = 1.204 mse = 1.299 mae = 0.839\n",
            "epoch39 train time: 4.927s test time: 0.557  loss = 0.843 val_mse = 1.231 mse = 1.322 mae = 0.835\n",
            "epoch40 train time: 4.934s test time: 0.564  loss = 0.842 val_mse = 1.247 mse = 1.336 mae = 0.833\n",
            "epoch41 train time: 4.933s test time: 0.569  loss = 0.840 val_mse = 1.205 mse = 1.305 mae = 0.842\n",
            "epoch42 train time: 5.372s test time: 1.075  loss = 0.835 val_mse = 1.200 mse = 1.294 mae = 0.838\n",
            "epoch43 train time: 5.919s test time: 0.662  loss = 0.835 val_mse = 1.244 mse = 1.325 mae = 0.831\n",
            "epoch44 train time: 5.711s test time: 0.724  loss = 0.839 val_mse = 1.211 mse = 1.297 mae = 0.839\n",
            "epoch45 train time: 5.519s test time: 0.743  loss = 0.837 val_mse = 1.234 mse = 1.320 mae = 0.831\n",
            "epoch46 train time: 4.914s test time: 0.561  loss = 0.830 val_mse = 1.264 mse = 1.335 mae = 0.832\n",
            "epoch47 train time: 4.917s test time: 0.557  loss = 0.833 val_mse = 1.240 mse = 1.317 mae = 0.829\n",
            "epoch48 train time: 4.912s test time: 0.564  loss = 0.836 val_mse = 1.230 mse = 1.307 mae = 0.836\n",
            "epoch49 train time: 4.915s test time: 0.558  loss = 0.832 val_mse = 1.233 mse = 1.321 mae = 0.831\n",
            "epoch50 train time: 4.927s test time: 0.564  loss = 0.825 val_mse = 1.226 mse = 1.310 mae = 0.835\n",
            "epoch51 train time: 4.925s test time: 0.569  loss = 0.831 val_mse = 1.232 mse = 1.316 mae = 0.836\n",
            "epoch52 train time: 4.941s test time: 0.555  loss = 0.833 val_mse = 1.231 mse = 1.309 mae = 0.833\n",
            "epoch53 train time: 4.916s test time: 0.553  loss = 0.827 val_mse = 1.231 mse = 1.324 mae = 0.836\n",
            "epoch54 train time: 4.917s test time: 0.553  loss = 0.822 val_mse = 1.226 mse = 1.305 mae = 0.834\n",
            "epoch55 train time: 4.941s test time: 0.568  loss = 0.823 val_mse = 1.225 mse = 1.317 mae = 0.833\n",
            "epoch56 train time: 4.933s test time: 0.565  loss = 0.816 val_mse = 1.228 mse = 1.309 mae = 0.835\n",
            "epoch57 train time: 4.921s test time: 0.559  loss = 0.818 val_mse = 1.229 mse = 1.309 mae = 0.833\n",
            "epoch58 train time: 4.916s test time: 0.576  loss = 0.819 val_mse = 1.217 mse = 1.306 mae = 0.831\n",
            "epoch59 train time: 4.912s test time: 0.559  loss = 0.819 val_mse = 1.229 mse = 1.312 mae = 0.833\n",
            "epoch60 train time: 5.082s test time: 0.560  loss = 0.819 val_mse = 1.229 mse = 1.308 mae = 0.832\n",
            "epoch61 train time: 4.914s test time: 0.555  loss = 0.815 val_mse = 1.222 mse = 1.313 mae = 0.833\n",
            "epoch62 train time: 4.904s test time: 0.564  loss = 0.812 val_mse = 1.231 mse = 1.317 mae = 0.834\n",
            "epoch63 train time: 5.223s test time: 0.552  loss = 0.814 val_mse = 1.224 mse = 1.314 mae = 0.834\n",
            "epoch64 train time: 4.904s test time: 0.554  loss = 0.812 val_mse = 1.212 mse = 1.301 mae = 0.831\n",
            "epoch65 train time: 4.918s test time: 0.553  loss = 0.811 val_mse = 1.225 mse = 1.317 mae = 0.832\n",
            "epoch66 train time: 4.929s test time: 0.558  loss = 0.810 val_mse = 1.222 mse = 1.303 mae = 0.831\n",
            "epoch67 train time: 4.911s test time: 0.557  loss = 0.814 val_mse = 1.219 mse = 1.311 mae = 0.833\n",
            "epoch68 train time: 4.925s test time: 0.554  loss = 0.809 val_mse = 1.219 mse = 1.309 mae = 0.837\n",
            "epoch69 train time: 4.916s test time: 0.566  loss = 0.813 val_mse = 1.218 mse = 1.313 mae = 0.834\n",
            "epoch70 train time: 4.923s test time: 0.557  loss = 0.808 val_mse = 1.210 mse = 1.303 mae = 0.833\n",
            "epoch71 train time: 4.913s test time: 0.552  loss = 0.808 val_mse = 1.218 mse = 1.308 mae = 0.832\n",
            "epoch72 train time: 4.926s test time: 0.563  loss = 0.808 val_mse = 1.215 mse = 1.298 mae = 0.833\n",
            "epoch73 train time: 4.931s test time: 0.553  loss = 0.810 val_mse = 1.214 mse = 1.305 mae = 0.835\n",
            "epoch74 train time: 4.913s test time: 0.557  loss = 0.812 val_mse = 1.213 mse = 1.304 mae = 0.837\n",
            "epoch75 train time: 4.918s test time: 0.562  loss = 0.812 val_mse = 1.211 mse = 1.310 mae = 0.834\n",
            "epoch76 train time: 4.918s test time: 0.556  loss = 0.805 val_mse = 1.222 mse = 1.306 mae = 0.831\n",
            "epoch77 train time: 4.918s test time: 0.562  loss = 0.803 val_mse = 1.213 mse = 1.310 mae = 0.832\n",
            "epoch78 train time: 4.915s test time: 0.562  loss = 0.801 val_mse = 1.211 mse = 1.300 mae = 0.835\n",
            "epoch79 train time: 4.925s test time: 0.561  loss = 0.806 val_mse = 1.216 mse = 1.312 mae = 0.834\n",
            "epoch80 train time: 4.915s test time: 0.572  loss = 0.806 val_mse = 1.220 mse = 1.301 mae = 0.832\n",
            "epoch81 train time: 4.921s test time: 0.558  loss = 0.807 val_mse = 1.212 mse = 1.306 mae = 0.831\n",
            "epoch82 train time: 4.926s test time: 0.557  loss = 0.804 val_mse = 1.216 mse = 1.303 mae = 0.832\n",
            "epoch83 train time: 4.926s test time: 0.555  loss = 0.805 val_mse = 1.218 mse = 1.307 mae = 0.829\n",
            "epoch84 train time: 4.933s test time: 0.555  loss = 0.807 val_mse = 1.214 mse = 1.299 mae = 0.834\n",
            "epoch85 train time: 4.929s test time: 0.558  loss = 0.805 val_mse = 1.207 mse = 1.304 mae = 0.832\n",
            "epoch86 train time: 4.941s test time: 0.555  loss = 0.801 val_mse = 1.204 mse = 1.295 mae = 0.833\n",
            "epoch87 train time: 4.926s test time: 0.566  loss = 0.802 val_mse = 1.213 mse = 1.303 mae = 0.829\n",
            "epoch88 train time: 4.915s test time: 0.561  loss = 0.805 val_mse = 1.218 mse = 1.302 mae = 0.832\n",
            "epoch89 train time: 4.922s test time: 0.560  loss = 0.801 val_mse = 1.204 mse = 1.305 mae = 0.834\n",
            "epoch90 train time: 4.906s test time: 0.555  loss = 0.801 val_mse = 1.206 mse = 1.294 mae = 0.831\n",
            "epoch91 train time: 4.918s test time: 0.558  loss = 0.799 val_mse = 1.205 mse = 1.298 mae = 0.831\n",
            "epoch92 train time: 4.916s test time: 0.555  loss = 0.796 val_mse = 1.213 mse = 1.304 mae = 0.831\n",
            "epoch93 train time: 4.926s test time: 0.566  loss = 0.797 val_mse = 1.205 mse = 1.304 mae = 0.832\n",
            "epoch94 train time: 4.914s test time: 0.559  loss = 0.796 val_mse = 1.204 mse = 1.297 mae = 0.832\n",
            "epoch95 train time: 4.913s test time: 0.553  loss = 0.795 val_mse = 1.208 mse = 1.306 mae = 0.833\n",
            "epoch96 train time: 4.923s test time: 0.553  loss = 0.795 val_mse = 1.203 mse = 1.297 mae = 0.831\n",
            "epoch97 train time: 4.922s test time: 0.569  loss = 0.797 val_mse = 1.200 mse = 1.296 mae = 0.834\n",
            "epoch98 train time: 4.923s test time: 0.566  loss = 0.793 val_mse = 1.195 mse = 1.292 mae = 0.831\n",
            "epoch99 train time: 4.916s test time: 0.556  loss = 0.793 val_mse = 1.208 mse = 1.295 mae = 0.830\n",
            "epoch100 train time: 4.909s test time: 0.562  loss = 0.795 val_mse = 1.200 mse = 1.295 mae = 0.830\n",
            "epoch101 train time: 4.932s test time: 0.554  loss = 0.793 val_mse = 1.211 mse = 1.306 mae = 0.830\n",
            "epoch102 train time: 4.929s test time: 0.553  loss = 0.798 val_mse = 1.204 mse = 1.292 mae = 0.832\n",
            "epoch103 train time: 4.902s test time: 0.566  loss = 0.795 val_mse = 1.202 mse = 1.307 mae = 0.832\n",
            "epoch104 train time: 4.922s test time: 0.554  loss = 0.795 val_mse = 1.200 mse = 1.297 mae = 0.832\n",
            "epoch105 train time: 4.916s test time: 0.556  loss = 0.798 val_mse = 1.196 mse = 1.297 mae = 0.833\n",
            "epoch106 train time: 4.919s test time: 0.558  loss = 0.791 val_mse = 1.191 mse = 1.288 mae = 0.830\n",
            "epoch107 train time: 4.932s test time: 0.561  loss = 0.793 val_mse = 1.212 mse = 1.308 mae = 0.830\n",
            "epoch108 train time: 4.942s test time: 0.567  loss = 0.796 val_mse = 1.208 mse = 1.297 mae = 0.831\n",
            "epoch109 train time: 4.937s test time: 0.559  loss = 0.794 val_mse = 1.196 mse = 1.297 mae = 0.832\n",
            "epoch110 train time: 4.942s test time: 0.561  loss = 0.789 val_mse = 1.211 mse = 1.301 mae = 0.827\n",
            "epoch111 train time: 4.934s test time: 0.566  loss = 0.794 val_mse = 1.198 mse = 1.296 mae = 0.831\n",
            "epoch112 train time: 4.936s test time: 0.562  loss = 0.793 val_mse = 1.202 mse = 1.296 mae = 0.828\n",
            "epoch113 train time: 4.945s test time: 0.561  loss = 0.792 val_mse = 1.201 mse = 1.299 mae = 0.829\n",
            "epoch114 train time: 4.928s test time: 0.563  loss = 0.790 val_mse = 1.195 mse = 1.290 mae = 0.830\n",
            "epoch115 train time: 4.930s test time: 0.563  loss = 0.793 val_mse = 1.205 mse = 1.300 mae = 0.831\n",
            "epoch116 train time: 4.922s test time: 0.556  loss = 0.794 val_mse = 1.197 mse = 1.293 mae = 0.831\n",
            "epoch117 train time: 4.909s test time: 0.556  loss = 0.792 val_mse = 1.196 mse = 1.293 mae = 0.831\n",
            "epoch118 train time: 4.911s test time: 0.565  loss = 0.794 val_mse = 1.208 mse = 1.300 mae = 0.830\n",
            "epoch119 train time: 4.922s test time: 0.558  loss = 0.789 val_mse = 1.195 mse = 1.300 mae = 0.830\n",
            "MAE 0.8374417539447163\n",
            "MSE 1.3195535680625576\n",
            "epoch0 train time: 5.353s test time: 0.558  loss = 43.721 val_mse = 1.580 mse = 1.515 mae = 0.882\n",
            "epoch1 train time: 4.930s test time: 0.564  loss = 30.373 val_mse = 1.571 mse = 1.511 mae = 0.881\n",
            "epoch2 train time: 4.925s test time: 0.558  loss = 16.143 val_mse = 1.547 mse = 1.494 mae = 0.876\n",
            "epoch3 train time: 4.924s test time: 0.561  loss = 7.850 val_mse = 1.525 mse = 1.478 mae = 0.871\n",
            "epoch4 train time: 4.922s test time: 0.560  loss = 3.688 val_mse = 1.502 mse = 1.462 mae = 0.866\n",
            "epoch5 train time: 4.909s test time: 0.555  loss = 1.933 val_mse = 1.479 mse = 1.446 mae = 0.861\n",
            "epoch6 train time: 4.905s test time: 0.560  loss = 1.375 val_mse = 1.456 mse = 1.430 mae = 0.858\n",
            "epoch7 train time: 4.913s test time: 0.556  loss = 1.259 val_mse = 1.434 mse = 1.416 mae = 0.855\n",
            "epoch8 train time: 4.925s test time: 0.559  loss = 1.216 val_mse = 1.411 mse = 1.401 mae = 0.853\n",
            "epoch9 train time: 4.912s test time: 0.554  loss = 1.179 val_mse = 1.388 mse = 1.387 mae = 0.852\n",
            "epoch10 train time: 4.905s test time: 0.549  loss = 1.145 val_mse = 1.363 mse = 1.370 mae = 0.851\n",
            "epoch11 train time: 4.903s test time: 0.560  loss = 1.113 val_mse = 1.344 mse = 1.359 mae = 0.849\n",
            "epoch12 train time: 4.907s test time: 0.557  loss = 1.084 val_mse = 1.325 mse = 1.349 mae = 0.848\n",
            "epoch13 train time: 4.924s test time: 0.555  loss = 1.056 val_mse = 1.305 mse = 1.337 mae = 0.847\n",
            "epoch14 train time: 4.898s test time: 0.552  loss = 1.030 val_mse = 1.296 mse = 1.333 mae = 0.843\n",
            "epoch15 train time: 4.909s test time: 0.561  loss = 1.006 val_mse = 1.285 mse = 1.328 mae = 0.841\n",
            "epoch16 train time: 4.904s test time: 0.554  loss = 0.983 val_mse = 1.274 mse = 1.323 mae = 0.838\n",
            "epoch17 train time: 4.921s test time: 0.552  loss = 0.963 val_mse = 1.265 mse = 1.320 mae = 0.837\n",
            "epoch18 train time: 4.911s test time: 0.565  loss = 0.945 val_mse = 1.249 mse = 1.310 mae = 0.837\n",
            "epoch19 train time: 4.940s test time: 0.559  loss = 0.931 val_mse = 1.243 mse = 1.308 mae = 0.838\n",
            "epoch20 train time: 4.931s test time: 0.555  loss = 0.919 val_mse = 1.246 mse = 1.311 mae = 0.835\n",
            "epoch21 train time: 4.912s test time: 0.566  loss = 0.908 val_mse = 1.247 mse = 1.316 mae = 0.834\n",
            "epoch22 train time: 4.908s test time: 0.557  loss = 0.898 val_mse = 1.224 mse = 1.302 mae = 0.839\n",
            "epoch23 train time: 4.937s test time: 0.567  loss = 0.890 val_mse = 1.243 mse = 1.318 mae = 0.836\n",
            "epoch24 train time: 4.930s test time: 0.557  loss = 0.885 val_mse = 1.249 mse = 1.324 mae = 0.831\n",
            "epoch25 train time: 4.914s test time: 0.571  loss = 0.882 val_mse = 1.234 mse = 1.306 mae = 0.839\n",
            "epoch26 train time: 4.925s test time: 0.565  loss = 0.882 val_mse = 1.216 mse = 1.289 mae = 0.839\n",
            "epoch27 train time: 4.916s test time: 0.557  loss = 0.873 val_mse = 1.213 mse = 1.296 mae = 0.846\n",
            "epoch28 train time: 4.912s test time: 0.566  loss = 0.872 val_mse = 1.215 mse = 1.303 mae = 0.843\n",
            "epoch29 train time: 4.946s test time: 0.566  loss = 0.874 val_mse = 1.253 mse = 1.325 mae = 0.838\n",
            "epoch30 train time: 4.929s test time: 0.555  loss = 0.866 val_mse = 1.215 mse = 1.303 mae = 0.842\n",
            "epoch31 train time: 4.928s test time: 0.555  loss = 0.861 val_mse = 1.208 mse = 1.300 mae = 0.848\n",
            "epoch32 train time: 4.925s test time: 0.552  loss = 0.856 val_mse = 1.231 mse = 1.320 mae = 0.833\n",
            "epoch33 train time: 4.924s test time: 0.564  loss = 0.859 val_mse = 1.260 mse = 1.337 mae = 0.837\n",
            "epoch34 train time: 4.928s test time: 0.564  loss = 0.858 val_mse = 1.229 mse = 1.309 mae = 0.832\n",
            "epoch35 train time: 4.925s test time: 0.561  loss = 0.853 val_mse = 1.259 mse = 1.328 mae = 0.832\n",
            "epoch36 train time: 4.909s test time: 0.572  loss = 0.853 val_mse = 1.244 mse = 1.323 mae = 0.832\n",
            "epoch37 train time: 4.923s test time: 0.555  loss = 0.854 val_mse = 1.216 mse = 1.311 mae = 0.846\n",
            "epoch38 train time: 4.913s test time: 0.562  loss = 0.847 val_mse = 1.232 mse = 1.311 mae = 0.835\n",
            "epoch39 train time: 4.910s test time: 0.568  loss = 0.844 val_mse = 1.244 mse = 1.333 mae = 0.835\n",
            "epoch40 train time: 4.906s test time: 0.572  loss = 0.840 val_mse = 1.226 mse = 1.316 mae = 0.835\n",
            "epoch41 train time: 4.916s test time: 0.563  loss = 0.842 val_mse = 1.245 mse = 1.332 mae = 0.835\n",
            "epoch42 train time: 4.907s test time: 0.554  loss = 0.839 val_mse = 1.230 mse = 1.311 mae = 0.832\n",
            "epoch43 train time: 4.923s test time: 0.561  loss = 0.836 val_mse = 1.244 mse = 1.334 mae = 0.837\n",
            "epoch44 train time: 4.918s test time: 0.560  loss = 0.835 val_mse = 1.233 mse = 1.312 mae = 0.832\n",
            "epoch45 train time: 4.907s test time: 0.553  loss = 0.839 val_mse = 1.221 mse = 1.314 mae = 0.846\n",
            "epoch46 train time: 4.927s test time: 0.569  loss = 0.835 val_mse = 1.224 mse = 1.307 mae = 0.836\n",
            "epoch47 train time: 4.937s test time: 0.558  loss = 0.835 val_mse = 1.250 mse = 1.329 mae = 0.831\n",
            "epoch48 train time: 4.936s test time: 0.560  loss = 0.829 val_mse = 1.234 mse = 1.313 mae = 0.828\n",
            "epoch49 train time: 4.906s test time: 0.557  loss = 0.824 val_mse = 1.255 mse = 1.339 mae = 0.837\n",
            "epoch50 train time: 4.912s test time: 0.561  loss = 0.827 val_mse = 1.232 mse = 1.316 mae = 0.831\n",
            "epoch51 train time: 4.917s test time: 0.561  loss = 0.827 val_mse = 1.238 mse = 1.326 mae = 0.837\n",
            "epoch52 train time: 4.934s test time: 0.559  loss = 0.823 val_mse = 1.237 mse = 1.312 mae = 0.829\n",
            "epoch53 train time: 4.931s test time: 0.561  loss = 0.825 val_mse = 1.239 mse = 1.324 mae = 0.835\n",
            "epoch54 train time: 4.912s test time: 0.556  loss = 0.823 val_mse = 1.232 mse = 1.306 mae = 0.828\n",
            "epoch55 train time: 4.911s test time: 0.557  loss = 0.820 val_mse = 1.215 mse = 1.318 mae = 0.847\n",
            "epoch56 train time: 4.897s test time: 0.555  loss = 0.825 val_mse = 1.201 mse = 1.289 mae = 0.834\n",
            "epoch57 train time: 4.913s test time: 0.554  loss = 0.817 val_mse = 1.220 mse = 1.314 mae = 0.842\n",
            "epoch58 train time: 4.926s test time: 0.557  loss = 0.818 val_mse = 1.203 mse = 1.295 mae = 0.835\n",
            "epoch59 train time: 4.922s test time: 0.559  loss = 0.817 val_mse = 1.227 mse = 1.315 mae = 0.842\n",
            "epoch60 train time: 4.928s test time: 0.559  loss = 0.817 val_mse = 1.200 mse = 1.288 mae = 0.836\n",
            "epoch61 train time: 4.930s test time: 0.566  loss = 0.812 val_mse = 1.208 mse = 1.314 mae = 0.847\n",
            "epoch62 train time: 4.923s test time: 0.566  loss = 0.816 val_mse = 1.208 mse = 1.307 mae = 0.840\n",
            "epoch63 train time: 4.917s test time: 0.557  loss = 0.813 val_mse = 1.212 mse = 1.312 mae = 0.844\n",
            "epoch64 train time: 4.915s test time: 0.569  loss = 0.812 val_mse = 1.201 mse = 1.286 mae = 0.838\n",
            "epoch65 train time: 4.922s test time: 0.558  loss = 0.813 val_mse = 1.212 mse = 1.317 mae = 0.841\n",
            "epoch66 train time: 4.929s test time: 0.562  loss = 0.809 val_mse = 1.204 mse = 1.293 mae = 0.837\n",
            "epoch67 train time: 4.904s test time: 0.557  loss = 0.810 val_mse = 1.215 mse = 1.312 mae = 0.843\n",
            "epoch68 train time: 4.917s test time: 0.557  loss = 0.811 val_mse = 1.202 mse = 1.287 mae = 0.837\n",
            "epoch69 train time: 4.905s test time: 0.560  loss = 0.804 val_mse = 1.216 mse = 1.312 mae = 0.836\n",
            "epoch70 train time: 4.896s test time: 0.553  loss = 0.804 val_mse = 1.200 mse = 1.284 mae = 0.837\n",
            "epoch71 train time: 4.923s test time: 0.557  loss = 0.806 val_mse = 1.216 mse = 1.309 mae = 0.838\n",
            "epoch72 train time: 4.909s test time: 0.557  loss = 0.800 val_mse = 1.198 mse = 1.290 mae = 0.835\n",
            "epoch73 train time: 4.909s test time: 0.557  loss = 0.804 val_mse = 1.211 mse = 1.300 mae = 0.837\n",
            "epoch74 train time: 4.926s test time: 0.560  loss = 0.800 val_mse = 1.201 mse = 1.294 mae = 0.836\n",
            "epoch75 train time: 4.917s test time: 0.560  loss = 0.798 val_mse = 1.212 mse = 1.316 mae = 0.842\n",
            "epoch76 train time: 4.919s test time: 0.560  loss = 0.799 val_mse = 1.199 mse = 1.291 mae = 0.835\n",
            "epoch77 train time: 4.915s test time: 0.562  loss = 0.800 val_mse = 1.211 mse = 1.305 mae = 0.841\n",
            "epoch78 train time: 4.920s test time: 0.564  loss = 0.799 val_mse = 1.194 mse = 1.285 mae = 0.833\n",
            "epoch79 train time: 4.934s test time: 0.558  loss = 0.801 val_mse = 1.213 mse = 1.311 mae = 0.841\n",
            "epoch80 train time: 4.901s test time: 0.561  loss = 0.796 val_mse = 1.191 mse = 1.291 mae = 0.835\n",
            "epoch81 train time: 4.907s test time: 0.555  loss = 0.794 val_mse = 1.212 mse = 1.309 mae = 0.838\n",
            "epoch82 train time: 4.909s test time: 0.559  loss = 0.797 val_mse = 1.202 mse = 1.290 mae = 0.832\n",
            "epoch83 train time: 4.918s test time: 0.561  loss = 0.796 val_mse = 1.217 mse = 1.306 mae = 0.837\n",
            "epoch84 train time: 4.899s test time: 0.563  loss = 0.794 val_mse = 1.198 mse = 1.298 mae = 0.837\n",
            "epoch85 train time: 4.916s test time: 0.555  loss = 0.794 val_mse = 1.212 mse = 1.305 mae = 0.837\n",
            "epoch86 train time: 4.908s test time: 0.558  loss = 0.791 val_mse = 1.185 mse = 1.286 mae = 0.834\n",
            "epoch87 train time: 4.904s test time: 0.561  loss = 0.791 val_mse = 1.214 mse = 1.312 mae = 0.836\n",
            "epoch88 train time: 4.917s test time: 0.555  loss = 0.792 val_mse = 1.196 mse = 1.290 mae = 0.832\n",
            "epoch89 train time: 4.925s test time: 0.554  loss = 0.789 val_mse = 1.206 mse = 1.303 mae = 0.835\n",
            "epoch90 train time: 4.938s test time: 0.564  loss = 0.787 val_mse = 1.190 mse = 1.293 mae = 0.838\n",
            "epoch91 train time: 4.926s test time: 0.563  loss = 0.790 val_mse = 1.206 mse = 1.303 mae = 0.837\n",
            "epoch92 train time: 4.941s test time: 0.562  loss = 0.788 val_mse = 1.190 mse = 1.286 mae = 0.835\n",
            "epoch93 train time: 4.924s test time: 0.560  loss = 0.786 val_mse = 1.205 mse = 1.302 mae = 0.835\n",
            "epoch94 train time: 4.921s test time: 0.557  loss = 0.790 val_mse = 1.194 mse = 1.289 mae = 0.831\n",
            "epoch95 train time: 4.927s test time: 0.560  loss = 0.792 val_mse = 1.213 mse = 1.302 mae = 0.837\n",
            "epoch96 train time: 4.918s test time: 0.557  loss = 0.789 val_mse = 1.186 mse = 1.286 mae = 0.834\n",
            "epoch97 train time: 4.920s test time: 0.558  loss = 0.786 val_mse = 1.213 mse = 1.313 mae = 0.835\n",
            "epoch98 train time: 4.910s test time: 0.560  loss = 0.790 val_mse = 1.192 mse = 1.286 mae = 0.831\n",
            "epoch99 train time: 4.906s test time: 0.561  loss = 0.786 val_mse = 1.206 mse = 1.299 mae = 0.836\n",
            "epoch100 train time: 4.900s test time: 0.559  loss = 0.789 val_mse = 1.193 mse = 1.287 mae = 0.831\n",
            "epoch101 train time: 4.911s test time: 0.568  loss = 0.787 val_mse = 1.204 mse = 1.300 mae = 0.833\n",
            "epoch102 train time: 4.915s test time: 0.560  loss = 0.784 val_mse = 1.179 mse = 1.282 mae = 0.835\n",
            "epoch103 train time: 4.933s test time: 0.561  loss = 0.787 val_mse = 1.207 mse = 1.304 mae = 0.835\n",
            "epoch104 train time: 4.925s test time: 0.563  loss = 0.785 val_mse = 1.189 mse = 1.291 mae = 0.835\n",
            "epoch105 train time: 4.927s test time: 0.558  loss = 0.786 val_mse = 1.201 mse = 1.303 mae = 0.839\n",
            "epoch106 train time: 4.921s test time: 0.552  loss = 0.786 val_mse = 1.186 mse = 1.287 mae = 0.834\n",
            "epoch107 train time: 4.925s test time: 0.565  loss = 0.790 val_mse = 1.200 mse = 1.305 mae = 0.838\n",
            "epoch108 train time: 4.911s test time: 0.557  loss = 0.787 val_mse = 1.193 mse = 1.291 mae = 0.832\n",
            "epoch109 train time: 4.922s test time: 0.568  loss = 0.786 val_mse = 1.202 mse = 1.302 mae = 0.836\n",
            "epoch110 train time: 4.913s test time: 0.558  loss = 0.785 val_mse = 1.181 mse = 1.287 mae = 0.833\n",
            "epoch111 train time: 4.909s test time: 0.562  loss = 0.784 val_mse = 1.200 mse = 1.304 mae = 0.839\n",
            "epoch112 train time: 4.921s test time: 0.557  loss = 0.785 val_mse = 1.188 mse = 1.289 mae = 0.828\n",
            "epoch113 train time: 4.909s test time: 0.554  loss = 0.781 val_mse = 1.209 mse = 1.304 mae = 0.834\n",
            "epoch114 train time: 4.912s test time: 0.555  loss = 0.783 val_mse = 1.184 mse = 1.282 mae = 0.833\n",
            "epoch115 train time: 4.927s test time: 0.566  loss = 0.781 val_mse = 1.200 mse = 1.303 mae = 0.830\n",
            "epoch116 train time: 4.911s test time: 0.555  loss = 0.781 val_mse = 1.188 mse = 1.286 mae = 0.835\n",
            "epoch117 train time: 4.935s test time: 0.558  loss = 0.782 val_mse = 1.197 mse = 1.298 mae = 0.834\n",
            "epoch118 train time: 4.925s test time: 0.575  loss = 0.781 val_mse = 1.184 mse = 1.288 mae = 0.832\n",
            "epoch119 train time: 4.915s test time: 0.560  loss = 0.783 val_mse = 1.200 mse = 1.292 mae = 0.831\n",
            "epoch120 train time: 4.915s test time: 0.565  loss = 0.782 val_mse = 1.189 mse = 1.287 mae = 0.833\n",
            "epoch121 train time: 4.931s test time: 0.554  loss = 0.782 val_mse = 1.201 mse = 1.304 mae = 0.834\n",
            "epoch122 train time: 4.917s test time: 0.565  loss = 0.780 val_mse = 1.184 mse = 1.283 mae = 0.830\n",
            "epoch123 train time: 4.912s test time: 0.553  loss = 0.783 val_mse = 1.198 mse = 1.299 mae = 0.836\n",
            "epoch124 train time: 4.918s test time: 0.566  loss = 0.782 val_mse = 1.191 mse = 1.285 mae = 0.828\n",
            "epoch125 train time: 4.906s test time: 0.564  loss = 0.782 val_mse = 1.205 mse = 1.306 mae = 0.835\n",
            "epoch126 train time: 4.920s test time: 0.559  loss = 0.780 val_mse = 1.185 mse = 1.289 mae = 0.834\n",
            "epoch127 train time: 4.912s test time: 0.557  loss = 0.778 val_mse = 1.197 mse = 1.297 mae = 0.834\n",
            "epoch128 train time: 4.915s test time: 0.555  loss = 0.781 val_mse = 1.180 mse = 1.287 mae = 0.833\n",
            "epoch129 train time: 4.916s test time: 0.565  loss = 0.783 val_mse = 1.195 mse = 1.299 mae = 0.835\n",
            "epoch130 train time: 4.948s test time: 0.557  loss = 0.783 val_mse = 1.192 mse = 1.294 mae = 0.831\n",
            "epoch131 train time: 4.933s test time: 0.564  loss = 0.782 val_mse = 1.196 mse = 1.301 mae = 0.834\n",
            "epoch132 train time: 4.911s test time: 0.559  loss = 0.781 val_mse = 1.185 mse = 1.285 mae = 0.829\n",
            "epoch133 train time: 4.917s test time: 0.563  loss = 0.778 val_mse = 1.200 mse = 1.307 mae = 0.833\n",
            "epoch134 train time: 4.910s test time: 0.562  loss = 0.780 val_mse = 1.180 mse = 1.281 mae = 0.830\n",
            "epoch135 train time: 4.939s test time: 0.557  loss = 0.780 val_mse = 1.203 mse = 1.301 mae = 0.831\n",
            "epoch136 train time: 4.915s test time: 0.558  loss = 0.781 val_mse = 1.182 mse = 1.281 mae = 0.830\n",
            "epoch137 train time: 4.923s test time: 0.561  loss = 0.778 val_mse = 1.193 mse = 1.295 mae = 0.833\n",
            "epoch138 train time: 4.913s test time: 0.560  loss = 0.778 val_mse = 1.185 mse = 1.289 mae = 0.832\n",
            "epoch139 train time: 4.903s test time: 0.561  loss = 0.778 val_mse = 1.198 mse = 1.298 mae = 0.834\n",
            "epoch140 train time: 4.906s test time: 0.555  loss = 0.778 val_mse = 1.178 mse = 1.288 mae = 0.830\n",
            "epoch141 train time: 4.915s test time: 0.553  loss = 0.779 val_mse = 1.196 mse = 1.299 mae = 0.831\n",
            "epoch142 train time: 4.911s test time: 0.569  loss = 0.780 val_mse = 1.189 mse = 1.285 mae = 0.827\n",
            "epoch143 train time: 4.904s test time: 0.554  loss = 0.778 val_mse = 1.196 mse = 1.302 mae = 0.834\n",
            "epoch144 train time: 4.926s test time: 0.557  loss = 0.780 val_mse = 1.179 mse = 1.277 mae = 0.829\n",
            "epoch145 train time: 4.921s test time: 0.561  loss = 0.781 val_mse = 1.193 mse = 1.294 mae = 0.834\n",
            "epoch146 train time: 4.913s test time: 0.562  loss = 0.778 val_mse = 1.182 mse = 1.278 mae = 0.828\n",
            "epoch147 train time: 4.920s test time: 0.562  loss = 0.779 val_mse = 1.197 mse = 1.296 mae = 0.831\n",
            "epoch148 train time: 4.940s test time: 0.557  loss = 0.776 val_mse = 1.188 mse = 1.286 mae = 0.828\n",
            "epoch149 train time: 4.920s test time: 0.558  loss = 0.776 val_mse = 1.192 mse = 1.296 mae = 0.837\n",
            "epoch150 train time: 4.942s test time: 0.567  loss = 0.778 val_mse = 1.185 mse = 1.282 mae = 0.825\n",
            "epoch151 train time: 4.908s test time: 0.557  loss = 0.774 val_mse = 1.191 mse = 1.301 mae = 0.833\n",
            "epoch152 train time: 4.921s test time: 0.568  loss = 0.778 val_mse = 1.180 mse = 1.280 mae = 0.829\n",
            "epoch153 train time: 4.916s test time: 0.559  loss = 0.775 val_mse = 1.191 mse = 1.290 mae = 0.830\n",
            "epoch154 train time: 4.917s test time: 0.561  loss = 0.774 val_mse = 1.183 mse = 1.286 mae = 0.829\n",
            "epoch155 train time: 4.911s test time: 0.559  loss = 0.773 val_mse = 1.188 mse = 1.295 mae = 0.836\n",
            "epoch156 train time: 4.904s test time: 0.565  loss = 0.774 val_mse = 1.184 mse = 1.285 mae = 0.829\n",
            "epoch157 train time: 4.928s test time: 0.566  loss = 0.776 val_mse = 1.194 mse = 1.298 mae = 0.831\n",
            "epoch158 train time: 4.919s test time: 0.553  loss = 0.776 val_mse = 1.180 mse = 1.280 mae = 0.829\n",
            "epoch159 train time: 4.908s test time: 0.562  loss = 0.774 val_mse = 1.193 mse = 1.298 mae = 0.832\n",
            "epoch160 train time: 4.937s test time: 0.563  loss = 0.772 val_mse = 1.176 mse = 1.279 mae = 0.829\n",
            "epoch161 train time: 4.932s test time: 0.560  loss = 0.771 val_mse = 1.193 mse = 1.297 mae = 0.833\n",
            "epoch162 train time: 4.931s test time: 0.555  loss = 0.776 val_mse = 1.178 mse = 1.280 mae = 0.828\n",
            "epoch163 train time: 4.923s test time: 0.564  loss = 0.773 val_mse = 1.196 mse = 1.296 mae = 0.831\n",
            "epoch164 train time: 4.933s test time: 0.565  loss = 0.774 val_mse = 1.183 mse = 1.283 mae = 0.828\n",
            "epoch165 train time: 4.910s test time: 0.563  loss = 0.772 val_mse = 1.192 mse = 1.292 mae = 0.833\n",
            "epoch166 train time: 4.927s test time: 0.554  loss = 0.771 val_mse = 1.184 mse = 1.283 mae = 0.827\n",
            "epoch167 train time: 4.929s test time: 0.558  loss = 0.769 val_mse = 1.190 mse = 1.293 mae = 0.832\n",
            "epoch168 train time: 4.909s test time: 0.565  loss = 0.767 val_mse = 1.178 mse = 1.278 mae = 0.829\n",
            "epoch169 train time: 4.921s test time: 0.554  loss = 0.768 val_mse = 1.193 mse = 1.297 mae = 0.834\n",
            "epoch170 train time: 4.921s test time: 0.557  loss = 0.771 val_mse = 1.186 mse = 1.282 mae = 0.827\n",
            "epoch171 train time: 4.918s test time: 0.556  loss = 0.769 val_mse = 1.194 mse = 1.296 mae = 0.832\n",
            "epoch172 train time: 4.944s test time: 0.564  loss = 0.768 val_mse = 1.178 mse = 1.283 mae = 0.828\n",
            "epoch173 train time: 4.906s test time: 0.566  loss = 0.769 val_mse = 1.193 mse = 1.294 mae = 0.832\n",
            "epoch174 train time: 4.932s test time: 0.556  loss = 0.769 val_mse = 1.178 mse = 1.281 mae = 0.827\n",
            "epoch175 train time: 4.916s test time: 0.561  loss = 0.770 val_mse = 1.192 mse = 1.299 mae = 0.834\n",
            "epoch176 train time: 4.910s test time: 0.554  loss = 0.769 val_mse = 1.172 mse = 1.279 mae = 0.827\n",
            "epoch177 train time: 4.923s test time: 0.574  loss = 0.767 val_mse = 1.196 mse = 1.295 mae = 0.830\n",
            "epoch178 train time: 4.929s test time: 0.561  loss = 0.770 val_mse = 1.177 mse = 1.282 mae = 0.828\n",
            "epoch179 train time: 4.938s test time: 0.555  loss = 0.768 val_mse = 1.189 mse = 1.292 mae = 0.830\n",
            "MAE 0.8364452190112996\n",
            "MSE 1.309650268364305\n",
            "epoch0 train time: 5.387s test time: 0.563  loss = 43.325 val_mse = 1.588 mse = 1.523 mae = 0.886\n",
            "epoch1 train time: 4.926s test time: 0.564  loss = 30.384 val_mse = 1.566 mse = 1.507 mae = 0.880\n",
            "epoch2 train time: 4.940s test time: 0.564  loss = 16.167 val_mse = 1.538 mse = 1.487 mae = 0.874\n",
            "epoch3 train time: 4.918s test time: 0.556  loss = 7.876 val_mse = 1.512 mse = 1.468 mae = 0.869\n",
            "epoch4 train time: 4.932s test time: 0.560  loss = 3.703 val_mse = 1.488 mse = 1.451 mae = 0.864\n",
            "epoch5 train time: 4.928s test time: 0.558  loss = 1.935 val_mse = 1.464 mse = 1.434 mae = 0.861\n",
            "epoch6 train time: 4.934s test time: 0.564  loss = 1.368 val_mse = 1.440 mse = 1.418 mae = 0.859\n",
            "epoch7 train time: 4.933s test time: 0.563  loss = 1.245 val_mse = 1.415 mse = 1.403 mae = 0.857\n",
            "epoch8 train time: 4.928s test time: 0.553  loss = 1.199 val_mse = 1.391 mse = 1.388 mae = 0.856\n",
            "epoch9 train time: 4.932s test time: 0.567  loss = 1.160 val_mse = 1.367 mse = 1.373 mae = 0.855\n",
            "epoch10 train time: 4.926s test time: 0.556  loss = 1.124 val_mse = 1.340 mse = 1.356 mae = 0.856\n",
            "epoch11 train time: 4.930s test time: 0.559  loss = 1.092 val_mse = 1.319 mse = 1.344 mae = 0.856\n",
            "epoch12 train time: 4.929s test time: 0.565  loss = 1.063 val_mse = 1.305 mse = 1.337 mae = 0.852\n",
            "epoch13 train time: 4.946s test time: 0.567  loss = 1.035 val_mse = 1.293 mse = 1.331 mae = 0.848\n",
            "epoch14 train time: 4.920s test time: 0.561  loss = 1.010 val_mse = 1.282 mse = 1.326 mae = 0.845\n",
            "epoch15 train time: 4.914s test time: 0.564  loss = 0.986 val_mse = 1.270 mse = 1.319 mae = 0.841\n",
            "epoch16 train time: 4.916s test time: 0.561  loss = 0.965 val_mse = 1.259 mse = 1.317 mae = 0.840\n",
            "epoch17 train time: 4.912s test time: 0.570  loss = 0.948 val_mse = 1.241 mse = 1.303 mae = 0.839\n",
            "epoch18 train time: 4.893s test time: 0.554  loss = 0.932 val_mse = 1.241 mse = 1.308 mae = 0.839\n",
            "epoch19 train time: 4.921s test time: 0.560  loss = 0.921 val_mse = 1.251 mse = 1.313 mae = 0.836\n",
            "epoch20 train time: 4.915s test time: 0.557  loss = 0.911 val_mse = 1.244 mse = 1.316 mae = 0.837\n",
            "epoch21 train time: 4.921s test time: 0.565  loss = 0.900 val_mse = 1.224 mse = 1.299 mae = 0.841\n",
            "epoch22 train time: 4.922s test time: 0.561  loss = 0.892 val_mse = 1.213 mse = 1.294 mae = 0.842\n",
            "epoch23 train time: 4.915s test time: 0.565  loss = 0.886 val_mse = 1.213 mse = 1.295 mae = 0.846\n",
            "epoch24 train time: 4.933s test time: 0.559  loss = 0.879 val_mse = 1.215 mse = 1.303 mae = 0.847\n",
            "epoch25 train time: 4.938s test time: 0.557  loss = 0.886 val_mse = 1.227 mse = 1.308 mae = 0.840\n",
            "epoch26 train time: 4.928s test time: 0.566  loss = 0.877 val_mse = 1.210 mse = 1.301 mae = 0.844\n",
            "epoch27 train time: 4.933s test time: 0.568  loss = 0.875 val_mse = 1.194 mse = 1.298 mae = 0.854\n",
            "epoch28 train time: 4.917s test time: 0.558  loss = 0.865 val_mse = 1.217 mse = 1.298 mae = 0.843\n",
            "epoch29 train time: 4.926s test time: 0.566  loss = 0.867 val_mse = 1.232 mse = 1.315 mae = 0.839\n",
            "epoch30 train time: 4.924s test time: 0.559  loss = 0.858 val_mse = 1.244 mse = 1.328 mae = 0.835\n",
            "epoch31 train time: 4.933s test time: 0.555  loss = 0.858 val_mse = 1.237 mse = 1.325 mae = 0.833\n",
            "epoch32 train time: 4.910s test time: 0.561  loss = 0.855 val_mse = 1.236 mse = 1.322 mae = 0.836\n",
            "epoch33 train time: 4.903s test time: 0.561  loss = 0.853 val_mse = 1.205 mse = 1.301 mae = 0.848\n",
            "epoch34 train time: 4.935s test time: 0.557  loss = 0.850 val_mse = 1.236 mse = 1.318 mae = 0.836\n",
            "epoch35 train time: 4.920s test time: 0.558  loss = 0.852 val_mse = 1.226 mse = 1.313 mae = 0.833\n",
            "epoch36 train time: 4.927s test time: 0.559  loss = 0.846 val_mse = 1.261 mse = 1.336 mae = 0.834\n",
            "epoch37 train time: 4.928s test time: 0.561  loss = 0.852 val_mse = 1.213 mse = 1.298 mae = 0.841\n",
            "epoch38 train time: 4.929s test time: 0.556  loss = 0.845 val_mse = 1.226 mse = 1.311 mae = 0.834\n",
            "epoch39 train time: 4.938s test time: 0.560  loss = 0.838 val_mse = 1.255 mse = 1.331 mae = 0.834\n",
            "epoch40 train time: 4.925s test time: 0.577  loss = 0.844 val_mse = 1.246 mse = 1.330 mae = 0.829\n",
            "epoch41 train time: 4.927s test time: 0.557  loss = 0.839 val_mse = 1.231 mse = 1.316 mae = 0.833\n",
            "epoch42 train time: 4.925s test time: 0.561  loss = 0.836 val_mse = 1.245 mse = 1.322 mae = 0.831\n",
            "epoch43 train time: 4.923s test time: 0.556  loss = 0.834 val_mse = 1.246 mse = 1.325 mae = 0.836\n",
            "epoch44 train time: 4.920s test time: 0.560  loss = 0.840 val_mse = 1.239 mse = 1.323 mae = 0.833\n",
            "epoch45 train time: 4.922s test time: 0.566  loss = 0.832 val_mse = 1.228 mse = 1.313 mae = 0.834\n",
            "epoch46 train time: 4.930s test time: 0.553  loss = 0.829 val_mse = 1.239 mse = 1.321 mae = 0.835\n",
            "epoch47 train time: 4.915s test time: 0.561  loss = 0.833 val_mse = 1.236 mse = 1.319 mae = 0.834\n",
            "epoch48 train time: 4.935s test time: 0.563  loss = 0.829 val_mse = 1.242 mse = 1.324 mae = 0.832\n",
            "epoch49 train time: 4.923s test time: 0.563  loss = 0.832 val_mse = 1.241 mse = 1.333 mae = 0.839\n",
            "epoch50 train time: 4.936s test time: 0.560  loss = 0.831 val_mse = 1.246 mse = 1.323 mae = 0.831\n",
            "epoch51 train time: 4.925s test time: 0.568  loss = 0.829 val_mse = 1.206 mse = 1.301 mae = 0.849\n",
            "epoch52 train time: 4.923s test time: 0.564  loss = 0.831 val_mse = 1.239 mse = 1.328 mae = 0.833\n",
            "epoch53 train time: 4.948s test time: 0.564  loss = 0.825 val_mse = 1.212 mse = 1.302 mae = 0.846\n",
            "epoch54 train time: 4.930s test time: 0.558  loss = 0.827 val_mse = 1.224 mse = 1.307 mae = 0.835\n",
            "epoch55 train time: 4.947s test time: 0.566  loss = 0.822 val_mse = 1.240 mse = 1.326 mae = 0.832\n",
            "epoch56 train time: 4.933s test time: 0.562  loss = 0.825 val_mse = 1.242 mse = 1.324 mae = 0.833\n",
            "epoch57 train time: 4.921s test time: 0.555  loss = 0.821 val_mse = 1.240 mse = 1.324 mae = 0.830\n",
            "epoch58 train time: 4.942s test time: 0.563  loss = 0.819 val_mse = 1.236 mse = 1.321 mae = 0.833\n",
            "epoch59 train time: 4.932s test time: 0.556  loss = 0.818 val_mse = 1.225 mse = 1.315 mae = 0.833\n",
            "epoch60 train time: 4.944s test time: 0.566  loss = 0.819 val_mse = 1.229 mse = 1.313 mae = 0.830\n",
            "epoch61 train time: 4.921s test time: 0.555  loss = 0.815 val_mse = 1.236 mse = 1.326 mae = 0.834\n",
            "epoch62 train time: 4.935s test time: 0.559  loss = 0.814 val_mse = 1.229 mse = 1.323 mae = 0.834\n",
            "epoch63 train time: 4.927s test time: 0.558  loss = 0.816 val_mse = 1.223 mse = 1.308 mae = 0.833\n",
            "epoch64 train time: 4.936s test time: 0.557  loss = 0.816 val_mse = 1.233 mse = 1.320 mae = 0.831\n",
            "epoch65 train time: 4.946s test time: 0.569  loss = 0.810 val_mse = 1.208 mse = 1.299 mae = 0.846\n",
            "epoch66 train time: 4.939s test time: 0.568  loss = 0.814 val_mse = 1.223 mse = 1.316 mae = 0.835\n",
            "epoch67 train time: 4.925s test time: 0.562  loss = 0.810 val_mse = 1.230 mse = 1.323 mae = 0.835\n",
            "epoch68 train time: 4.945s test time: 0.563  loss = 0.810 val_mse = 1.237 mse = 1.324 mae = 0.833\n",
            "epoch69 train time: 4.919s test time: 0.557  loss = 0.810 val_mse = 1.219 mse = 1.309 mae = 0.832\n",
            "epoch70 train time: 4.933s test time: 0.559  loss = 0.807 val_mse = 1.230 mse = 1.318 mae = 0.835\n",
            "epoch71 train time: 4.917s test time: 0.565  loss = 0.811 val_mse = 1.198 mse = 1.298 mae = 0.845\n",
            "epoch72 train time: 4.892s test time: 0.561  loss = 0.813 val_mse = 1.228 mse = 1.313 mae = 0.831\n",
            "epoch73 train time: 4.923s test time: 0.553  loss = 0.809 val_mse = 1.235 mse = 1.314 mae = 0.830\n",
            "epoch74 train time: 4.921s test time: 0.559  loss = 0.807 val_mse = 1.216 mse = 1.313 mae = 0.835\n",
            "epoch75 train time: 4.922s test time: 0.564  loss = 0.804 val_mse = 1.224 mse = 1.318 mae = 0.831\n",
            "epoch76 train time: 4.938s test time: 0.559  loss = 0.802 val_mse = 1.233 mse = 1.315 mae = 0.831\n",
            "epoch77 train time: 4.947s test time: 0.560  loss = 0.805 val_mse = 1.194 mse = 1.302 mae = 0.847\n",
            "epoch78 train time: 4.928s test time: 0.562  loss = 0.802 val_mse = 1.217 mse = 1.316 mae = 0.832\n",
            "epoch79 train time: 4.947s test time: 0.570  loss = 0.803 val_mse = 1.230 mse = 1.319 mae = 0.836\n",
            "epoch80 train time: 4.950s test time: 0.558  loss = 0.808 val_mse = 1.212 mse = 1.309 mae = 0.836\n",
            "epoch81 train time: 4.932s test time: 0.560  loss = 0.799 val_mse = 1.218 mse = 1.315 mae = 0.832\n",
            "epoch82 train time: 4.926s test time: 0.563  loss = 0.801 val_mse = 1.215 mse = 1.307 mae = 0.830\n",
            "epoch83 train time: 4.932s test time: 0.561  loss = 0.802 val_mse = 1.219 mse = 1.303 mae = 0.835\n",
            "epoch84 train time: 4.945s test time: 0.564  loss = 0.804 val_mse = 1.212 mse = 1.310 mae = 0.833\n",
            "epoch85 train time: 4.935s test time: 0.568  loss = 0.798 val_mse = 1.203 mse = 1.303 mae = 0.834\n",
            "epoch86 train time: 4.927s test time: 0.573  loss = 0.798 val_mse = 1.219 mse = 1.309 mae = 0.833\n",
            "epoch87 train time: 4.912s test time: 0.564  loss = 0.797 val_mse = 1.207 mse = 1.307 mae = 0.833\n",
            "epoch88 train time: 4.927s test time: 0.561  loss = 0.796 val_mse = 1.206 mse = 1.303 mae = 0.834\n",
            "epoch89 train time: 4.931s test time: 0.559  loss = 0.799 val_mse = 1.212 mse = 1.306 mae = 0.833\n",
            "epoch90 train time: 4.933s test time: 0.558  loss = 0.797 val_mse = 1.209 mse = 1.306 mae = 0.834\n",
            "epoch91 train time: 4.928s test time: 0.555  loss = 0.798 val_mse = 1.206 mse = 1.302 mae = 0.833\n",
            "epoch92 train time: 4.947s test time: 0.570  loss = 0.795 val_mse = 1.213 mse = 1.308 mae = 0.836\n",
            "epoch93 train time: 4.934s test time: 0.562  loss = 0.798 val_mse = 1.205 mse = 1.300 mae = 0.835\n",
            "epoch94 train time: 4.923s test time: 0.559  loss = 0.800 val_mse = 1.205 mse = 1.304 mae = 0.832\n",
            "epoch95 train time: 4.938s test time: 0.567  loss = 0.794 val_mse = 1.212 mse = 1.306 mae = 0.834\n",
            "epoch96 train time: 4.930s test time: 0.560  loss = 0.797 val_mse = 1.216 mse = 1.308 mae = 0.832\n",
            "epoch97 train time: 4.940s test time: 0.559  loss = 0.798 val_mse = 1.200 mse = 1.303 mae = 0.835\n",
            "epoch98 train time: 4.932s test time: 0.569  loss = 0.795 val_mse = 1.210 mse = 1.307 mae = 0.833\n",
            "epoch99 train time: 4.926s test time: 0.558  loss = 0.795 val_mse = 1.207 mse = 1.297 mae = 0.829\n",
            "epoch100 train time: 4.917s test time: 0.554  loss = 0.795 val_mse = 1.204 mse = 1.300 mae = 0.831\n",
            "epoch101 train time: 4.933s test time: 0.554  loss = 0.790 val_mse = 1.204 mse = 1.297 mae = 0.831\n",
            "epoch102 train time: 4.932s test time: 0.570  loss = 0.792 val_mse = 1.206 mse = 1.303 mae = 0.832\n",
            "epoch103 train time: 4.918s test time: 0.557  loss = 0.787 val_mse = 1.206 mse = 1.302 mae = 0.832\n",
            "epoch104 train time: 4.933s test time: 0.558  loss = 0.788 val_mse = 1.211 mse = 1.298 mae = 0.830\n",
            "epoch105 train time: 4.922s test time: 0.560  loss = 0.790 val_mse = 1.207 mse = 1.305 mae = 0.833\n",
            "epoch106 train time: 4.929s test time: 0.553  loss = 0.790 val_mse = 1.211 mse = 1.306 mae = 0.830\n",
            "epoch107 train time: 4.931s test time: 0.574  loss = 0.791 val_mse = 1.200 mse = 1.299 mae = 0.832\n",
            "epoch108 train time: 4.930s test time: 0.567  loss = 0.792 val_mse = 1.212 mse = 1.303 mae = 0.832\n",
            "epoch109 train time: 4.928s test time: 0.557  loss = 0.791 val_mse = 1.196 mse = 1.302 mae = 0.832\n",
            "epoch110 train time: 4.925s test time: 0.566  loss = 0.788 val_mse = 1.208 mse = 1.303 mae = 0.832\n",
            "epoch111 train time: 4.928s test time: 0.563  loss = 0.791 val_mse = 1.195 mse = 1.299 mae = 0.834\n",
            "epoch112 train time: 4.935s test time: 0.555  loss = 0.786 val_mse = 1.200 mse = 1.298 mae = 0.832\n",
            "epoch113 train time: 4.934s test time: 0.569  loss = 0.786 val_mse = 1.192 mse = 1.298 mae = 0.834\n",
            "epoch114 train time: 4.922s test time: 0.560  loss = 0.788 val_mse = 1.198 mse = 1.301 mae = 0.833\n",
            "epoch115 train time: 4.937s test time: 0.566  loss = 0.783 val_mse = 1.196 mse = 1.301 mae = 0.832\n",
            "epoch116 train time: 4.934s test time: 0.558  loss = 0.785 val_mse = 1.204 mse = 1.304 mae = 0.833\n",
            "epoch117 train time: 4.927s test time: 0.559  loss = 0.784 val_mse = 1.193 mse = 1.299 mae = 0.834\n",
            "epoch118 train time: 4.929s test time: 0.564  loss = 0.784 val_mse = 1.201 mse = 1.299 mae = 0.831\n",
            "epoch119 train time: 4.942s test time: 0.566  loss = 0.781 val_mse = 1.193 mse = 1.297 mae = 0.834\n",
            "epoch120 train time: 4.934s test time: 0.553  loss = 0.785 val_mse = 1.202 mse = 1.300 mae = 0.831\n",
            "epoch121 train time: 4.926s test time: 0.574  loss = 0.784 val_mse = 1.192 mse = 1.295 mae = 0.832\n",
            "epoch122 train time: 4.931s test time: 0.567  loss = 0.783 val_mse = 1.204 mse = 1.302 mae = 0.832\n",
            "epoch123 train time: 4.947s test time: 0.552  loss = 0.781 val_mse = 1.192 mse = 1.298 mae = 0.834\n",
            "epoch124 train time: 4.947s test time: 0.567  loss = 0.784 val_mse = 1.196 mse = 1.298 mae = 0.829\n",
            "epoch125 train time: 4.931s test time: 0.558  loss = 0.781 val_mse = 1.191 mse = 1.291 mae = 0.832\n",
            "epoch126 train time: 4.931s test time: 0.557  loss = 0.784 val_mse = 1.203 mse = 1.297 mae = 0.833\n",
            "epoch127 train time: 4.933s test time: 0.561  loss = 0.783 val_mse = 1.189 mse = 1.298 mae = 0.829\n",
            "epoch128 train time: 4.917s test time: 0.556  loss = 0.783 val_mse = 1.196 mse = 1.299 mae = 0.831\n",
            "epoch129 train time: 4.915s test time: 0.560  loss = 0.781 val_mse = 1.187 mse = 1.293 mae = 0.831\n",
            "epoch130 train time: 4.929s test time: 0.557  loss = 0.779 val_mse = 1.198 mse = 1.300 mae = 0.832\n",
            "epoch131 train time: 4.919s test time: 0.558  loss = 0.781 val_mse = 1.190 mse = 1.296 mae = 0.833\n",
            "epoch132 train time: 4.933s test time: 0.566  loss = 0.780 val_mse = 1.193 mse = 1.293 mae = 0.830\n",
            "epoch133 train time: 4.935s test time: 0.574  loss = 0.783 val_mse = 1.187 mse = 1.294 mae = 0.832\n",
            "epoch134 train time: 4.951s test time: 0.564  loss = 0.782 val_mse = 1.201 mse = 1.297 mae = 0.830\n",
            "epoch135 train time: 4.942s test time: 0.558  loss = 0.778 val_mse = 1.184 mse = 1.289 mae = 0.834\n",
            "epoch136 train time: 4.910s test time: 0.556  loss = 0.781 val_mse = 1.191 mse = 1.297 mae = 0.831\n",
            "epoch137 train time: 4.941s test time: 0.563  loss = 0.779 val_mse = 1.183 mse = 1.292 mae = 0.831\n",
            "epoch138 train time: 4.926s test time: 0.556  loss = 0.780 val_mse = 1.197 mse = 1.298 mae = 0.834\n",
            "epoch139 train time: 4.956s test time: 0.558  loss = 0.780 val_mse = 1.186 mse = 1.288 mae = 0.832\n",
            "epoch140 train time: 4.930s test time: 0.557  loss = 0.781 val_mse = 1.196 mse = 1.292 mae = 0.826\n",
            "epoch141 train time: 4.935s test time: 0.555  loss = 0.775 val_mse = 1.191 mse = 1.296 mae = 0.830\n",
            "epoch142 train time: 4.939s test time: 0.562  loss = 0.779 val_mse = 1.193 mse = 1.294 mae = 0.832\n",
            "epoch143 train time: 4.958s test time: 0.563  loss = 0.775 val_mse = 1.185 mse = 1.284 mae = 0.829\n",
            "epoch144 train time: 4.940s test time: 0.555  loss = 0.776 val_mse = 1.193 mse = 1.295 mae = 0.831\n",
            "epoch145 train time: 4.927s test time: 0.556  loss = 0.775 val_mse = 1.195 mse = 1.294 mae = 0.830\n",
            "epoch146 train time: 4.919s test time: 0.555  loss = 0.776 val_mse = 1.193 mse = 1.295 mae = 0.831\n",
            "epoch147 train time: 5.715s test time: 0.739  loss = 0.773 val_mse = 1.187 mse = 1.289 mae = 0.828\n",
            "epoch148 train time: 5.879s test time: 0.681  loss = 0.774 val_mse = 1.191 mse = 1.289 mae = 0.830\n",
            "epoch149 train time: 5.529s test time: 0.763  loss = 0.773 val_mse = 1.191 mse = 1.291 mae = 0.829\n",
            "epoch150 train time: 5.499s test time: 0.683  loss = 0.777 val_mse = 1.194 mse = 1.295 mae = 0.830\n",
            "epoch151 train time: 5.500s test time: 0.717  loss = 0.774 val_mse = 1.187 mse = 1.288 mae = 0.828\n",
            "epoch152 train time: 5.656s test time: 0.725  loss = 0.774 val_mse = 1.197 mse = 1.296 mae = 0.833\n",
            "epoch153 train time: 5.543s test time: 0.688  loss = 0.776 val_mse = 1.179 mse = 1.290 mae = 0.829\n",
            "epoch154 train time: 5.489s test time: 0.684  loss = 0.774 val_mse = 1.190 mse = 1.292 mae = 0.830\n",
            "epoch155 train time: 5.145s test time: 0.557  loss = 0.774 val_mse = 1.185 mse = 1.292 mae = 0.830\n",
            "epoch156 train time: 4.911s test time: 0.554  loss = 0.774 val_mse = 1.188 mse = 1.288 mae = 0.830\n",
            "epoch157 train time: 4.904s test time: 0.558  loss = 0.769 val_mse = 1.185 mse = 1.290 mae = 0.830\n",
            "epoch158 train time: 4.912s test time: 0.563  loss = 0.773 val_mse = 1.189 mse = 1.293 mae = 0.833\n",
            "epoch159 train time: 4.938s test time: 0.571  loss = 0.771 val_mse = 1.187 mse = 1.293 mae = 0.832\n",
            "epoch160 train time: 4.945s test time: 0.573  loss = 0.772 val_mse = 1.182 mse = 1.292 mae = 0.832\n",
            "epoch161 train time: 4.939s test time: 0.560  loss = 0.770 val_mse = 1.182 mse = 1.288 mae = 0.830\n",
            "epoch162 train time: 4.929s test time: 0.560  loss = 0.769 val_mse = 1.191 mse = 1.293 mae = 0.832\n",
            "epoch163 train time: 4.947s test time: 0.565  loss = 0.770 val_mse = 1.187 mse = 1.290 mae = 0.828\n",
            "epoch164 train time: 4.922s test time: 0.560  loss = 0.767 val_mse = 1.189 mse = 1.294 mae = 0.831\n",
            "epoch165 train time: 4.930s test time: 0.563  loss = 0.768 val_mse = 1.183 mse = 1.287 mae = 0.828\n",
            "epoch166 train time: 4.933s test time: 0.552  loss = 0.769 val_mse = 1.183 mse = 1.291 mae = 0.830\n",
            "epoch167 train time: 4.941s test time: 0.568  loss = 0.767 val_mse = 1.182 mse = 1.289 mae = 0.830\n",
            "epoch168 train time: 4.938s test time: 0.554  loss = 0.771 val_mse = 1.186 mse = 1.290 mae = 0.826\n",
            "epoch169 train time: 4.928s test time: 0.553  loss = 0.766 val_mse = 1.190 mse = 1.292 mae = 0.829\n",
            "epoch170 train time: 4.911s test time: 0.795  loss = 0.770 val_mse = 1.188 mse = 1.290 mae = 0.831\n",
            "epoch171 train time: 4.908s test time: 0.568  loss = 0.767 val_mse = 1.180 mse = 1.287 mae = 0.829\n",
            "epoch172 train time: 4.928s test time: 0.553  loss = 0.766 val_mse = 1.184 mse = 1.294 mae = 0.831\n",
            "epoch173 train time: 4.918s test time: 0.566  loss = 0.764 val_mse = 1.178 mse = 1.285 mae = 0.831\n",
            "epoch174 train time: 4.945s test time: 0.563  loss = 0.768 val_mse = 1.187 mse = 1.292 mae = 0.828\n",
            "epoch175 train time: 4.937s test time: 0.559  loss = 0.764 val_mse = 1.174 mse = 1.285 mae = 0.832\n",
            "epoch176 train time: 4.944s test time: 0.563  loss = 0.764 val_mse = 1.188 mse = 1.289 mae = 0.831\n",
            "epoch177 train time: 4.931s test time: 0.561  loss = 0.765 val_mse = 1.180 mse = 1.286 mae = 0.827\n",
            "epoch178 train time: 4.926s test time: 0.559  loss = 0.765 val_mse = 1.183 mse = 1.291 mae = 0.829\n",
            "epoch179 train time: 4.928s test time: 0.565  loss = 0.763 val_mse = 1.180 mse = 1.284 mae = 0.828\n",
            "epoch180 train time: 4.936s test time: 0.562  loss = 0.766 val_mse = 1.186 mse = 1.289 mae = 0.829\n",
            "epoch181 train time: 4.938s test time: 0.559  loss = 0.765 val_mse = 1.179 mse = 1.286 mae = 0.829\n",
            "epoch182 train time: 4.933s test time: 0.568  loss = 0.764 val_mse = 1.185 mse = 1.288 mae = 0.829\n",
            "epoch183 train time: 4.918s test time: 0.554  loss = 0.764 val_mse = 1.175 mse = 1.285 mae = 0.834\n",
            "epoch184 train time: 4.909s test time: 0.558  loss = 0.769 val_mse = 1.185 mse = 1.298 mae = 0.827\n",
            "epoch185 train time: 4.939s test time: 0.570  loss = 0.765 val_mse = 1.184 mse = 1.287 mae = 0.828\n",
            "epoch186 train time: 4.919s test time: 0.566  loss = 0.766 val_mse = 1.183 mse = 1.284 mae = 0.829\n",
            "epoch187 train time: 4.924s test time: 0.555  loss = 0.767 val_mse = 1.184 mse = 1.288 mae = 0.828\n",
            "epoch188 train time: 4.926s test time: 0.561  loss = 0.769 val_mse = 1.177 mse = 1.283 mae = 0.828\n",
            "epoch189 train time: 5.263s test time: 0.564  loss = 0.766 val_mse = 1.180 mse = 1.286 mae = 0.830\n",
            "epoch190 train time: 4.948s test time: 0.559  loss = 0.767 val_mse = 1.186 mse = 1.291 mae = 0.828\n",
            "epoch191 train time: 4.913s test time: 0.557  loss = 0.765 val_mse = 1.181 mse = 1.287 mae = 0.831\n",
            "epoch192 train time: 4.919s test time: 0.555  loss = 0.765 val_mse = 1.179 mse = 1.289 mae = 0.828\n",
            "epoch193 train time: 4.925s test time: 0.568  loss = 0.765 val_mse = 1.178 mse = 1.289 mae = 0.832\n",
            "epoch194 train time: 4.924s test time: 0.550  loss = 0.766 val_mse = 1.182 mse = 1.289 mae = 0.828\n",
            "epoch195 train time: 4.914s test time: 0.563  loss = 0.764 val_mse = 1.178 mse = 1.286 mae = 0.831\n",
            "epoch196 train time: 4.918s test time: 0.560  loss = 0.766 val_mse = 1.182 mse = 1.291 mae = 0.829\n",
            "epoch197 train time: 4.921s test time: 0.558  loss = 0.768 val_mse = 1.180 mse = 1.286 mae = 0.828\n",
            "epoch198 train time: 4.909s test time: 0.565  loss = 0.768 val_mse = 1.188 mse = 1.290 mae = 0.828\n",
            "epoch199 train time: 4.910s test time: 0.555  loss = 0.767 val_mse = 1.178 mse = 1.293 mae = 0.830\n",
            "epoch200 train time: 4.925s test time: 0.562  loss = 0.765 val_mse = 1.186 mse = 1.291 mae = 0.827\n",
            "epoch201 train time: 4.916s test time: 0.563  loss = 0.766 val_mse = 1.184 mse = 1.289 mae = 0.831\n",
            "epoch202 train time: 4.926s test time: 0.565  loss = 0.766 val_mse = 1.183 mse = 1.286 mae = 0.828\n",
            "epoch203 train time: 4.934s test time: 0.563  loss = 0.764 val_mse = 1.182 mse = 1.288 mae = 0.829\n",
            "epoch204 train time: 4.918s test time: 0.555  loss = 0.766 val_mse = 1.188 mse = 1.295 mae = 0.830\n",
            "epoch205 train time: 4.933s test time: 0.568  loss = 0.762 val_mse = 1.178 mse = 1.287 mae = 0.829\n",
            "epoch206 train time: 4.915s test time: 0.569  loss = 0.765 val_mse = 1.185 mse = 1.287 mae = 0.828\n",
            "epoch207 train time: 4.902s test time: 0.562  loss = 0.764 val_mse = 1.176 mse = 1.288 mae = 0.830\n",
            "epoch208 train time: 4.922s test time: 0.557  loss = 0.768 val_mse = 1.184 mse = 1.286 mae = 0.830\n",
            "epoch209 train time: 4.935s test time: 0.556  loss = 0.769 val_mse = 1.180 mse = 1.292 mae = 0.828\n",
            "epoch210 train time: 4.932s test time: 0.557  loss = 0.766 val_mse = 1.182 mse = 1.287 mae = 0.828\n",
            "epoch211 train time: 4.945s test time: 0.560  loss = 0.765 val_mse = 1.177 mse = 1.288 mae = 0.828\n",
            "epoch212 train time: 4.904s test time: 0.559  loss = 0.764 val_mse = 1.177 mse = 1.288 mae = 0.828\n",
            "epoch213 train time: 4.916s test time: 0.561  loss = 0.764 val_mse = 1.182 mse = 1.293 mae = 0.830\n",
            "epoch214 train time: 4.926s test time: 0.557  loss = 0.766 val_mse = 1.183 mse = 1.287 mae = 0.827\n",
            "epoch215 train time: 4.900s test time: 0.553  loss = 0.766 val_mse = 1.184 mse = 1.292 mae = 0.829\n",
            "epoch216 train time: 4.915s test time: 0.564  loss = 0.766 val_mse = 1.183 mse = 1.286 mae = 0.827\n",
            "epoch217 train time: 4.916s test time: 0.558  loss = 0.765 val_mse = 1.181 mse = 1.289 mae = 0.830\n",
            "epoch218 train time: 4.913s test time: 0.564  loss = 0.770 val_mse = 1.181 mse = 1.285 mae = 0.829\n",
            "epoch219 train time: 4.927s test time: 0.562  loss = 0.766 val_mse = 1.183 mse = 1.284 mae = 0.829\n",
            "epoch220 train time: 4.930s test time: 0.558  loss = 0.767 val_mse = 1.185 mse = 1.291 mae = 0.830\n",
            "epoch221 train time: 4.942s test time: 0.561  loss = 0.767 val_mse = 1.178 mse = 1.283 mae = 0.827\n",
            "epoch222 train time: 4.946s test time: 0.558  loss = 0.769 val_mse = 1.186 mse = 1.285 mae = 0.830\n",
            "epoch223 train time: 4.923s test time: 0.564  loss = 0.766 val_mse = 1.189 mse = 1.292 mae = 0.829\n",
            "epoch224 train time: 4.934s test time: 0.558  loss = 0.767 val_mse = 1.182 mse = 1.284 mae = 0.828\n",
            "epoch225 train time: 4.913s test time: 0.560  loss = 0.766 val_mse = 1.180 mse = 1.292 mae = 0.827\n",
            "epoch226 train time: 4.918s test time: 0.565  loss = 0.764 val_mse = 1.178 mse = 1.284 mae = 0.829\n",
            "epoch227 train time: 4.896s test time: 0.556  loss = 0.766 val_mse = 1.179 mse = 1.285 mae = 0.830\n",
            "epoch228 train time: 4.907s test time: 0.562  loss = 0.767 val_mse = 1.184 mse = 1.284 mae = 0.828\n",
            "epoch229 train time: 4.898s test time: 0.556  loss = 0.767 val_mse = 1.179 mse = 1.282 mae = 0.829\n",
            "epoch230 train time: 4.925s test time: 0.557  loss = 0.767 val_mse = 1.186 mse = 1.291 mae = 0.828\n",
            "epoch231 train time: 4.927s test time: 0.560  loss = 0.765 val_mse = 1.181 mse = 1.284 mae = 0.827\n",
            "epoch232 train time: 4.930s test time: 0.555  loss = 0.766 val_mse = 1.184 mse = 1.286 mae = 0.827\n",
            "epoch233 train time: 4.933s test time: 0.559  loss = 0.766 val_mse = 1.179 mse = 1.287 mae = 0.831\n",
            "epoch234 train time: 4.936s test time: 0.571  loss = 0.769 val_mse = 1.181 mse = 1.290 mae = 0.830\n",
            "epoch235 train time: 4.917s test time: 0.559  loss = 0.766 val_mse = 1.186 mse = 1.283 mae = 0.825\n",
            "epoch236 train time: 4.925s test time: 0.557  loss = 0.767 val_mse = 1.183 mse = 1.280 mae = 0.828\n",
            "epoch237 train time: 4.922s test time: 0.555  loss = 0.765 val_mse = 1.179 mse = 1.294 mae = 0.827\n",
            "epoch238 train time: 4.924s test time: 0.559  loss = 0.765 val_mse = 1.178 mse = 1.282 mae = 0.827\n",
            "epoch239 train time: 4.929s test time: 0.558  loss = 0.766 val_mse = 1.183 mse = 1.288 mae = 0.828\n",
            "MAE 0.8339613501772636\n",
            "MSE 1.3063634430115583\n"
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    print(sys.path[0])\n",
        "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
        "    word_latent_dim = 300\n",
        "    latent_dim = 15\n",
        "    max_doc_length = 300\n",
        "    num_filters = 50\n",
        "    window_size = 3\n",
        "    v_dim = 50\n",
        "    learning_rate = 0.001\n",
        "    lambda_1 = 0.01 #normally we set from [0.05, 0.01, 0.005, 0.001]\n",
        "    drop_out = 0.8\n",
        "    batch_size = 200\n",
        "    epochs = 60\n",
        "    \n",
        "    # loading data\n",
        "    firTime = time()\n",
        "    dataSet = Dataset(max_doc_length, sys.path[0], \"dataPreprocessingWordDict.out\")\n",
        "    word_dict, user_reviews, item_reviews, train, valRatings, testRatings = dataSet.word_id_dict, dataSet.userReview_dict, dataSet.itemReview_dict, dataSet.trainMtrx, dataSet.valRatings, dataSet.testRatings\n",
        "    secTime = time()\n",
        "\n",
        "    num_users, num_items = train.shape\n",
        "    print(\"load data: %.3fs\" % (secTime - firTime))\n",
        "    print(num_users, num_items)\n",
        "    print(latent_dim)\n",
        "\n",
        "    #load word embeddings\n",
        "    word_embedding_mtrx = ini_word_embed(len(word_dict), word_latent_dim)\n",
        "    # word_embedding_mtrx = word2vec_word_embed(len(word_dict), word_latent_dim,\n",
        "    #                                           \"Directory of pretrained WordEmbedding.out\",\n",
        "    #                                           word_dict)\n",
        "\n",
        "    print( \"shape\", word_embedding_mtrx.shape)\n",
        "\n",
        "    # get train instances\n",
        "    user_input, item_input, rateings = get_train_instance(train)\n",
        "    print (len(user_input), len(item_input), len(rateings))\n",
        "\n",
        "    # get test/val instances\n",
        "    user_vals, item_vals, user_input_val, item_input_val, rating_input_val = get_test_list(200, valRatings, user_reviews, item_reviews)\n",
        "    user_tests, item_tests, user_input_test, item_input_test, rating_input_test = get_test_list(200, testRatings, user_reviews, item_reviews)\n",
        "\n",
        "    #train & eval model\n",
        "    train_model()\n",
        "\n",
        "    epochs = 120\n",
        "    #train & eval model\n",
        "    train_model()\n",
        "\n",
        "    epochs = 180\n",
        "    #train & eval model\n",
        "    train_model()\n",
        "\n",
        "    epochs = 240\n",
        "    #train & eval model\n",
        "    train_model()"
      ],
      "id": "EB5kGs3IC0dl"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q5MbTZYUC0Rf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 426
        },
        "outputId": "f04a5236-5491-49de-f191-528a5722863a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "avg mae over epochs = [60, 120, 180, 240]: [0.845240419804884, 0.8374417539447163, 0.8364452190112996, 0.8339613501772636]\n",
            "avg mse over epochs = [60, 120, 180, 240]: [1.3344836261132609, 1.3195535680625576, 1.309650268364305, 1.3063634430115583]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 750x350 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "minimum avg mae over epochs = 0.8339613501772636 at epoch = 240 and mse = 1.3063634430115583\n",
            "minimum avg mse over epochs = 1.3063634430115583 at epoch = 240 and mae = 0.8339613501772636\n"
          ]
        }
      ],
      "source": [
        "avg_mae_list = [0.845240419804884,0.8374417539447163,0.8364452190112996,0.8339613501772636]\n",
        "avg_mse_list = [1.3344836261132609,1.3195535680625576,1.309650268364305,1.3063634430115583]\n",
        "epochs_list = [60, 120, 180, 240]\n",
        "print(f'avg mae over epochs = [60, 120, 180, 240]: {avg_mae_list}')\n",
        "print(f'avg mse over epochs = [60, 120, 180, 240]: {avg_mse_list}')\n",
        "    \n",
        "plt.rcParams.update({'figure.figsize':(7.5,3.5), 'figure.dpi':100})\n",
        "fig, axes = plt.subplots(1, 2)\n",
        "axes[0].plot(epochs_list, avg_mae_list, 'm--o')\n",
        "axes[0].set_title('Avg MAE over varying epochs')\n",
        "axes[0].set_xlabel('epochs')\n",
        "axes[0].set_ylabel('MAE')\n",
        "\n",
        "axes[1].plot(epochs_list, avg_mse_list, 'g--o')\n",
        "axes[1].set_title('Avg MSE over varying epochs')\n",
        "axes[1].set_xlabel('epochs')\n",
        "axes[1].set_ylabel('MSE')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "index = 0\n",
        "min = 100\n",
        "min_index = 0\n",
        "for i in avg_mae_list:\n",
        "  if i<min:\n",
        "    min = i\n",
        "    min_index = index\n",
        "  index+=1\n",
        "print(f'minimum avg mae over epochs = {min} at epoch = {epochs_list[min_index]} and mse = {avg_mse_list[min_index]}')\n",
        "\n",
        "index = 0\n",
        "min = 100\n",
        "min_index = 0\n",
        "for i in avg_mse_list:\n",
        "  if i<min:\n",
        "    min = i\n",
        "    min_index = index\n",
        "  index+=1\n",
        "print(f'minimum avg mse over epochs = {min} at epoch = {epochs_list[min_index]} and mae = {avg_mae_list[min_index]}')"
      ],
      "id": "q5MbTZYUC0Rf"
    },
    {
      "cell_type": "markdown",
      "source": [
        "epochs = 240"
      ],
      "metadata": {
        "id": "1qN19JbJcAhA"
      },
      "id": "1qN19JbJcAhA"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZSd7YbCrC0vw"
      },
      "source": [
        "Varying LAMBDA"
      ],
      "id": "ZSd7YbCrC0vw"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z8BUPB5jDcSF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3143f084-064c-4081-f3f5-14f8b3261839"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "wordId_dict finished\n",
            "load reviews finished\n",
            "load data: 3.139s\n",
            "506 2581\n",
            "15\n",
            "shape (19930, 300)\n",
            "18880 18880 18880\n",
            "epoch0 train time: 5.331s test time: 0.728  loss = 8.513 val_mse = 1.588 mse = 1.521 mae = 0.884\n",
            "epoch1 train time: 5.548s test time: 0.682  loss = 4.655 val_mse = 1.566 mse = 1.509 mae = 0.881\n",
            "epoch2 train time: 5.396s test time: 0.551  loss = 3.166 val_mse = 1.522 mse = 1.478 mae = 0.872\n",
            "epoch3 train time: 4.878s test time: 0.556  loss = 2.033 val_mse = 1.474 mse = 1.447 mae = 0.864\n",
            "epoch4 train time: 4.886s test time: 0.549  loss = 1.409 val_mse = 1.431 mse = 1.422 mae = 0.858\n",
            "epoch5 train time: 4.885s test time: 0.550  loss = 1.104 val_mse = 1.393 mse = 1.403 mae = 0.854\n",
            "epoch6 train time: 4.908s test time: 0.550  loss = 0.974 val_mse = 1.359 mse = 1.385 mae = 0.851\n",
            "epoch7 train time: 4.911s test time: 0.557  loss = 0.923 val_mse = 1.323 mse = 1.365 mae = 0.848\n",
            "epoch8 train time: 4.897s test time: 0.549  loss = 0.896 val_mse = 1.286 mse = 1.342 mae = 0.846\n",
            "epoch9 train time: 4.909s test time: 0.556  loss = 0.875 val_mse = 1.269 mse = 1.336 mae = 0.844\n",
            "epoch10 train time: 4.914s test time: 0.557  loss = 0.860 val_mse = 1.241 mse = 1.318 mae = 0.848\n",
            "epoch11 train time: 4.921s test time: 0.552  loss = 0.845 val_mse = 1.237 mse = 1.320 mae = 0.840\n",
            "epoch12 train time: 4.919s test time: 0.563  loss = 0.834 val_mse = 1.230 mse = 1.318 mae = 0.839\n",
            "epoch13 train time: 4.926s test time: 0.551  loss = 0.828 val_mse = 1.221 mse = 1.316 mae = 0.840\n",
            "epoch14 train time: 4.955s test time: 0.561  loss = 0.821 val_mse = 1.207 mse = 1.307 mae = 0.842\n",
            "epoch15 train time: 4.931s test time: 0.565  loss = 0.818 val_mse = 1.203 mse = 1.305 mae = 0.843\n",
            "epoch16 train time: 4.940s test time: 0.560  loss = 0.812 val_mse = 1.200 mse = 1.303 mae = 0.846\n",
            "epoch17 train time: 5.109s test time: 0.550  loss = 0.811 val_mse = 1.205 mse = 1.303 mae = 0.843\n",
            "epoch18 train time: 4.941s test time: 0.554  loss = 0.804 val_mse = 1.196 mse = 1.297 mae = 0.836\n",
            "epoch19 train time: 4.948s test time: 0.556  loss = 0.801 val_mse = 1.186 mse = 1.293 mae = 0.838\n",
            "epoch20 train time: 4.945s test time: 0.558  loss = 0.798 val_mse = 1.171 mse = 1.288 mae = 0.844\n",
            "epoch21 train time: 4.950s test time: 0.555  loss = 0.798 val_mse = 1.193 mse = 1.300 mae = 0.836\n",
            "epoch22 train time: 4.926s test time: 0.553  loss = 0.793 val_mse = 1.188 mse = 1.303 mae = 0.834\n",
            "epoch23 train time: 4.934s test time: 0.557  loss = 0.792 val_mse = 1.180 mse = 1.303 mae = 0.840\n",
            "epoch24 train time: 4.964s test time: 0.555  loss = 0.790 val_mse = 1.178 mse = 1.294 mae = 0.841\n",
            "epoch25 train time: 4.952s test time: 0.551  loss = 0.790 val_mse = 1.184 mse = 1.300 mae = 0.837\n",
            "epoch26 train time: 4.963s test time: 0.560  loss = 0.788 val_mse = 1.187 mse = 1.309 mae = 0.837\n",
            "epoch27 train time: 4.961s test time: 0.561  loss = 0.788 val_mse = 1.195 mse = 1.308 mae = 0.834\n",
            "epoch28 train time: 4.955s test time: 0.555  loss = 0.784 val_mse = 1.191 mse = 1.301 mae = 0.832\n",
            "epoch29 train time: 4.957s test time: 0.557  loss = 0.782 val_mse = 1.178 mse = 1.301 mae = 0.841\n",
            "epoch30 train time: 4.962s test time: 0.560  loss = 0.783 val_mse = 1.175 mse = 1.295 mae = 0.843\n",
            "epoch31 train time: 4.959s test time: 0.560  loss = 0.782 val_mse = 1.192 mse = 1.305 mae = 0.833\n",
            "epoch32 train time: 4.954s test time: 0.561  loss = 0.779 val_mse = 1.180 mse = 1.304 mae = 0.835\n",
            "epoch33 train time: 4.945s test time: 0.552  loss = 0.777 val_mse = 1.184 mse = 1.305 mae = 0.832\n",
            "epoch34 train time: 4.949s test time: 0.567  loss = 0.775 val_mse = 1.180 mse = 1.301 mae = 0.835\n",
            "epoch35 train time: 5.202s test time: 0.561  loss = 0.775 val_mse = 1.187 mse = 1.305 mae = 0.829\n",
            "epoch36 train time: 4.967s test time: 0.555  loss = 0.774 val_mse = 1.181 mse = 1.304 mae = 0.835\n",
            "epoch37 train time: 4.956s test time: 0.558  loss = 0.774 val_mse = 1.185 mse = 1.309 mae = 0.833\n",
            "epoch38 train time: 4.961s test time: 0.568  loss = 0.773 val_mse = 1.185 mse = 1.302 mae = 0.834\n",
            "epoch39 train time: 4.958s test time: 0.561  loss = 0.773 val_mse = 1.174 mse = 1.298 mae = 0.839\n",
            "epoch40 train time: 5.535s test time: 0.555  loss = 0.770 val_mse = 1.182 mse = 1.303 mae = 0.835\n",
            "epoch41 train time: 5.005s test time: 0.833  loss = 0.771 val_mse = 1.172 mse = 1.297 mae = 0.841\n",
            "epoch42 train time: 6.304s test time: 0.768  loss = 0.769 val_mse = 1.164 mse = 1.300 mae = 0.837\n",
            "epoch43 train time: 5.991s test time: 0.716  loss = 0.766 val_mse = 1.173 mse = 1.296 mae = 0.838\n",
            "epoch44 train time: 5.903s test time: 0.552  loss = 0.764 val_mse = 1.182 mse = 1.307 mae = 0.831\n",
            "epoch45 train time: 4.972s test time: 0.557  loss = 0.766 val_mse = 1.190 mse = 1.302 mae = 0.826\n",
            "epoch46 train time: 4.976s test time: 0.561  loss = 0.764 val_mse = 1.186 mse = 1.315 mae = 0.830\n",
            "epoch47 train time: 4.966s test time: 0.557  loss = 0.765 val_mse = 1.199 mse = 1.311 mae = 0.825\n",
            "epoch48 train time: 4.945s test time: 0.561  loss = 0.760 val_mse = 1.187 mse = 1.312 mae = 0.830\n",
            "epoch49 train time: 4.958s test time: 0.555  loss = 0.760 val_mse = 1.196 mse = 1.306 mae = 0.825\n",
            "epoch50 train time: 4.945s test time: 0.557  loss = 0.758 val_mse = 1.203 mse = 1.319 mae = 0.825\n",
            "epoch51 train time: 4.965s test time: 0.566  loss = 0.758 val_mse = 1.192 mse = 1.310 mae = 0.829\n",
            "epoch52 train time: 4.972s test time: 0.572  loss = 0.758 val_mse = 1.201 mse = 1.317 mae = 0.824\n",
            "epoch53 train time: 4.983s test time: 0.561  loss = 0.754 val_mse = 1.185 mse = 1.312 mae = 0.829\n",
            "epoch54 train time: 4.968s test time: 0.568  loss = 0.754 val_mse = 1.183 mse = 1.308 mae = 0.825\n",
            "epoch55 train time: 4.981s test time: 0.563  loss = 0.752 val_mse = 1.179 mse = 1.310 mae = 0.830\n",
            "epoch56 train time: 4.974s test time: 0.560  loss = 0.752 val_mse = 1.173 mse = 1.305 mae = 0.826\n",
            "epoch57 train time: 4.965s test time: 0.559  loss = 0.747 val_mse = 1.174 mse = 1.309 mae = 0.829\n",
            "epoch58 train time: 4.968s test time: 0.567  loss = 0.748 val_mse = 1.171 mse = 1.310 mae = 0.827\n",
            "epoch59 train time: 4.977s test time: 0.558  loss = 0.746 val_mse = 1.171 mse = 1.306 mae = 0.827\n",
            "epoch60 train time: 4.977s test time: 0.563  loss = 0.745 val_mse = 1.160 mse = 1.307 mae = 0.826\n",
            "epoch61 train time: 5.084s test time: 0.553  loss = 0.741 val_mse = 1.166 mse = 1.306 mae = 0.827\n",
            "epoch62 train time: 4.955s test time: 0.552  loss = 0.740 val_mse = 1.157 mse = 1.308 mae = 0.826\n",
            "epoch63 train time: 5.048s test time: 0.556  loss = 0.736 val_mse = 1.158 mse = 1.304 mae = 0.827\n",
            "epoch64 train time: 4.970s test time: 0.558  loss = 0.734 val_mse = 1.142 mse = 1.307 mae = 0.829\n",
            "epoch65 train time: 4.977s test time: 0.556  loss = 0.730 val_mse = 1.145 mse = 1.306 mae = 0.830\n",
            "epoch66 train time: 4.959s test time: 0.561  loss = 0.729 val_mse = 1.143 mse = 1.304 mae = 0.826\n",
            "epoch67 train time: 4.983s test time: 0.562  loss = 0.725 val_mse = 1.144 mse = 1.308 mae = 0.828\n",
            "epoch68 train time: 4.974s test time: 0.554  loss = 0.724 val_mse = 1.141 mse = 1.308 mae = 0.827\n",
            "epoch69 train time: 4.972s test time: 0.560  loss = 0.719 val_mse = 1.137 mse = 1.310 mae = 0.828\n",
            "epoch70 train time: 4.970s test time: 0.558  loss = 0.718 val_mse = 1.127 mse = 1.302 mae = 0.830\n",
            "epoch71 train time: 4.975s test time: 0.554  loss = 0.715 val_mse = 1.125 mse = 1.302 mae = 0.831\n",
            "epoch72 train time: 4.969s test time: 0.559  loss = 0.714 val_mse = 1.124 mse = 1.299 mae = 0.831\n",
            "epoch73 train time: 4.981s test time: 0.563  loss = 0.711 val_mse = 1.123 mse = 1.302 mae = 0.833\n",
            "epoch74 train time: 5.253s test time: 0.562  loss = 0.708 val_mse = 1.113 mse = 1.297 mae = 0.833\n",
            "epoch75 train time: 4.979s test time: 0.556  loss = 0.706 val_mse = 1.116 mse = 1.305 mae = 0.836\n",
            "epoch76 train time: 4.976s test time: 0.561  loss = 0.705 val_mse = 1.104 mse = 1.308 mae = 0.835\n",
            "epoch77 train time: 4.961s test time: 0.558  loss = 0.702 val_mse = 1.108 mse = 1.310 mae = 0.838\n",
            "epoch78 train time: 4.983s test time: 0.556  loss = 0.699 val_mse = 1.098 mse = 1.312 mae = 0.839\n",
            "epoch79 train time: 4.967s test time: 0.569  loss = 0.699 val_mse = 1.103 mse = 1.311 mae = 0.839\n",
            "epoch80 train time: 4.973s test time: 0.555  loss = 0.696 val_mse = 1.093 mse = 1.317 mae = 0.841\n",
            "epoch81 train time: 4.980s test time: 0.559  loss = 0.694 val_mse = 1.101 mse = 1.315 mae = 0.843\n",
            "epoch82 train time: 5.001s test time: 0.560  loss = 0.692 val_mse = 1.092 mse = 1.318 mae = 0.843\n",
            "epoch83 train time: 4.994s test time: 0.567  loss = 0.689 val_mse = 1.097 mse = 1.317 mae = 0.845\n",
            "epoch84 train time: 4.975s test time: 0.566  loss = 0.688 val_mse = 1.086 mse = 1.323 mae = 0.847\n",
            "epoch85 train time: 4.963s test time: 0.565  loss = 0.686 val_mse = 1.089 mse = 1.324 mae = 0.846\n",
            "epoch86 train time: 4.966s test time: 0.558  loss = 0.684 val_mse = 1.085 mse = 1.329 mae = 0.846\n",
            "epoch87 train time: 4.973s test time: 0.562  loss = 0.680 val_mse = 1.087 mse = 1.332 mae = 0.849\n",
            "epoch88 train time: 4.972s test time: 0.560  loss = 0.681 val_mse = 1.084 mse = 1.333 mae = 0.845\n",
            "epoch89 train time: 4.974s test time: 0.552  loss = 0.677 val_mse = 1.085 mse = 1.330 mae = 0.851\n",
            "epoch90 train time: 4.975s test time: 0.560  loss = 0.677 val_mse = 1.078 mse = 1.337 mae = 0.848\n",
            "epoch91 train time: 4.960s test time: 0.561  loss = 0.673 val_mse = 1.078 mse = 1.334 mae = 0.851\n",
            "epoch92 train time: 4.974s test time: 0.557  loss = 0.671 val_mse = 1.078 mse = 1.340 mae = 0.852\n",
            "epoch93 train time: 4.981s test time: 0.563  loss = 0.668 val_mse = 1.078 mse = 1.339 mae = 0.852\n",
            "epoch94 train time: 4.972s test time: 0.570  loss = 0.667 val_mse = 1.080 mse = 1.349 mae = 0.853\n",
            "epoch95 train time: 4.986s test time: 0.555  loss = 0.664 val_mse = 1.073 mse = 1.340 mae = 0.856\n",
            "epoch96 train time: 4.986s test time: 0.563  loss = 0.663 val_mse = 1.077 mse = 1.344 mae = 0.859\n",
            "epoch97 train time: 4.994s test time: 0.561  loss = 0.658 val_mse = 1.072 mse = 1.342 mae = 0.861\n",
            "epoch98 train time: 4.990s test time: 0.555  loss = 0.659 val_mse = 1.074 mse = 1.347 mae = 0.858\n",
            "epoch99 train time: 4.989s test time: 0.566  loss = 0.654 val_mse = 1.070 mse = 1.342 mae = 0.861\n",
            "epoch100 train time: 4.990s test time: 0.560  loss = 0.650 val_mse = 1.067 mse = 1.351 mae = 0.859\n",
            "epoch101 train time: 4.984s test time: 0.563  loss = 0.647 val_mse = 1.067 mse = 1.348 mae = 0.862\n",
            "epoch102 train time: 4.981s test time: 0.555  loss = 0.644 val_mse = 1.063 mse = 1.355 mae = 0.865\n",
            "epoch103 train time: 4.967s test time: 0.557  loss = 0.640 val_mse = 1.066 mse = 1.351 mae = 0.867\n",
            "epoch104 train time: 4.966s test time: 0.555  loss = 0.638 val_mse = 1.054 mse = 1.354 mae = 0.863\n",
            "epoch105 train time: 4.978s test time: 0.563  loss = 0.632 val_mse = 1.060 mse = 1.355 mae = 0.869\n",
            "epoch106 train time: 4.972s test time: 0.567  loss = 0.629 val_mse = 1.046 mse = 1.363 mae = 0.864\n",
            "epoch107 train time: 4.979s test time: 0.552  loss = 0.622 val_mse = 1.045 mse = 1.360 mae = 0.865\n",
            "epoch108 train time: 4.987s test time: 0.565  loss = 0.619 val_mse = 1.040 mse = 1.356 mae = 0.871\n",
            "epoch109 train time: 4.982s test time: 0.562  loss = 0.610 val_mse = 1.031 mse = 1.367 mae = 0.870\n",
            "epoch110 train time: 4.980s test time: 0.555  loss = 0.607 val_mse = 1.022 mse = 1.370 mae = 0.864\n",
            "epoch111 train time: 4.986s test time: 0.570  loss = 0.599 val_mse = 1.008 mse = 1.370 mae = 0.871\n",
            "epoch112 train time: 4.974s test time: 0.560  loss = 0.592 val_mse = 0.998 mse = 1.375 mae = 0.866\n",
            "epoch113 train time: 4.982s test time: 0.556  loss = 0.584 val_mse = 0.991 mse = 1.372 mae = 0.872\n",
            "epoch114 train time: 4.976s test time: 0.567  loss = 0.576 val_mse = 0.993 mse = 1.394 mae = 0.867\n",
            "epoch115 train time: 4.958s test time: 0.557  loss = 0.567 val_mse = 0.971 mse = 1.377 mae = 0.875\n",
            "epoch116 train time: 4.979s test time: 0.553  loss = 0.559 val_mse = 0.969 mse = 1.394 mae = 0.871\n",
            "epoch117 train time: 4.976s test time: 0.565  loss = 0.550 val_mse = 0.947 mse = 1.392 mae = 0.876\n",
            "epoch118 train time: 4.969s test time: 0.563  loss = 0.538 val_mse = 0.946 mse = 1.411 mae = 0.874\n",
            "epoch119 train time: 4.952s test time: 0.554  loss = 0.531 val_mse = 0.928 mse = 1.407 mae = 0.879\n",
            "epoch120 train time: 4.981s test time: 0.557  loss = 0.518 val_mse = 0.929 mse = 1.437 mae = 0.878\n",
            "epoch121 train time: 4.980s test time: 0.558  loss = 0.510 val_mse = 0.912 mse = 1.433 mae = 0.885\n",
            "epoch122 train time: 4.982s test time: 0.554  loss = 0.496 val_mse = 0.910 mse = 1.451 mae = 0.885\n",
            "epoch123 train time: 4.973s test time: 0.559  loss = 0.487 val_mse = 0.884 mse = 1.457 mae = 0.891\n",
            "epoch124 train time: 4.972s test time: 0.561  loss = 0.476 val_mse = 0.884 mse = 1.475 mae = 0.890\n",
            "epoch125 train time: 4.977s test time: 0.562  loss = 0.466 val_mse = 0.864 mse = 1.476 mae = 0.898\n",
            "epoch126 train time: 4.973s test time: 0.562  loss = 0.454 val_mse = 0.876 mse = 1.520 mae = 0.897\n",
            "epoch127 train time: 4.975s test time: 0.565  loss = 0.445 val_mse = 0.836 mse = 1.512 mae = 0.906\n",
            "epoch128 train time: 4.966s test time: 0.556  loss = 0.434 val_mse = 0.838 mse = 1.524 mae = 0.901\n",
            "epoch129 train time: 4.985s test time: 0.560  loss = 0.426 val_mse = 0.815 mse = 1.532 mae = 0.914\n",
            "epoch130 train time: 4.978s test time: 0.558  loss = 0.411 val_mse = 0.841 mse = 1.601 mae = 0.916\n",
            "epoch131 train time: 4.989s test time: 0.552  loss = 0.407 val_mse = 0.815 mse = 1.592 mae = 0.923\n",
            "epoch132 train time: 4.966s test time: 0.563  loss = 0.396 val_mse = 0.792 mse = 1.549 mae = 0.916\n",
            "epoch133 train time: 4.959s test time: 0.561  loss = 0.389 val_mse = 0.781 mse = 1.606 mae = 0.929\n",
            "epoch134 train time: 4.962s test time: 0.557  loss = 0.380 val_mse = 0.767 mse = 1.599 mae = 0.924\n",
            "epoch135 train time: 4.973s test time: 0.562  loss = 0.373 val_mse = 0.760 mse = 1.635 mae = 0.938\n",
            "epoch136 train time: 4.977s test time: 0.561  loss = 0.362 val_mse = 0.767 mse = 1.670 mae = 0.937\n",
            "epoch137 train time: 4.981s test time: 0.565  loss = 0.361 val_mse = 0.769 mse = 1.725 mae = 0.950\n",
            "epoch138 train time: 4.971s test time: 0.561  loss = 0.346 val_mse = 0.763 mse = 1.746 mae = 0.951\n",
            "epoch139 train time: 4.996s test time: 0.559  loss = 0.343 val_mse = 0.743 mse = 1.747 mae = 0.956\n",
            "epoch140 train time: 4.977s test time: 0.562  loss = 0.338 val_mse = 0.744 mse = 1.757 mae = 0.955\n",
            "epoch141 train time: 4.967s test time: 0.571  loss = 0.330 val_mse = 0.739 mse = 1.803 mae = 0.962\n",
            "epoch142 train time: 4.961s test time: 0.555  loss = 0.321 val_mse = 0.739 mse = 1.864 mae = 0.971\n",
            "epoch143 train time: 4.996s test time: 0.557  loss = 0.317 val_mse = 0.706 mse = 1.800 mae = 0.965\n",
            "epoch144 train time: 4.983s test time: 0.571  loss = 0.311 val_mse = 0.699 mse = 1.791 mae = 0.964\n",
            "epoch145 train time: 4.970s test time: 0.556  loss = 0.310 val_mse = 0.700 mse = 1.893 mae = 0.976\n",
            "epoch146 train time: 4.958s test time: 0.558  loss = 0.302 val_mse = 0.701 mse = 1.862 mae = 0.972\n",
            "epoch147 train time: 4.970s test time: 0.558  loss = 0.296 val_mse = 0.690 mse = 1.935 mae = 0.983\n",
            "epoch148 train time: 4.978s test time: 0.553  loss = 0.292 val_mse = 0.697 mse = 1.969 mae = 0.985\n",
            "epoch149 train time: 4.968s test time: 0.560  loss = 0.284 val_mse = 0.672 mse = 2.046 mae = 0.991\n",
            "epoch150 train time: 4.973s test time: 0.561  loss = 0.284 val_mse = 0.670 mse = 1.873 mae = 0.978\n",
            "epoch151 train time: 4.987s test time: 0.560  loss = 0.278 val_mse = 0.662 mse = 2.202 mae = 0.996\n",
            "epoch152 train time: 4.985s test time: 0.567  loss = 0.271 val_mse = 0.664 mse = 2.021 mae = 0.986\n",
            "epoch153 train time: 4.982s test time: 0.565  loss = 0.271 val_mse = 0.654 mse = 2.442 mae = 1.001\n",
            "epoch154 train time: 4.981s test time: 0.563  loss = 0.269 val_mse = 0.649 mse = 1.963 mae = 0.990\n",
            "epoch155 train time: 4.985s test time: 0.560  loss = 0.263 val_mse = 0.635 mse = 3.438 mae = 1.012\n",
            "epoch156 train time: 4.984s test time: 0.560  loss = 0.264 val_mse = 0.650 mse = 2.428 mae = 1.003\n",
            "epoch157 train time: 4.976s test time: 0.555  loss = 0.261 val_mse = 0.631 mse = 3.838 mae = 1.020\n",
            "epoch158 train time: 4.979s test time: 0.555  loss = 0.253 val_mse = 0.651 mse = 2.740 mae = 1.019\n",
            "epoch159 train time: 4.962s test time: 0.564  loss = 0.259 val_mse = 0.643 mse = 3.386 mae = 1.025\n",
            "epoch160 train time: 4.972s test time: 0.554  loss = 0.252 val_mse = 0.698 mse = 22.334 mae = 1.094\n",
            "epoch161 train time: 4.962s test time: 0.566  loss = 0.246 val_mse = 0.656 mse = 4.756 mae = 1.055\n",
            "epoch162 train time: 4.964s test time: 0.567  loss = 0.245 val_mse = 0.641 mse = 23.031 mae = 1.078\n",
            "epoch163 train time: 4.977s test time: 0.554  loss = 0.247 val_mse = 0.631 mse = 16.967 mae = 1.076\n",
            "epoch164 train time: 4.979s test time: 0.556  loss = 0.248 val_mse = 0.634 mse = 51.620 mae = 1.115\n",
            "epoch165 train time: 4.993s test time: 0.558  loss = 0.240 val_mse = 0.615 mse = 3.132 mae = 1.031\n",
            "epoch166 train time: 4.982s test time: 0.560  loss = 0.243 val_mse = 0.634 mse = 5.974 mae = 1.054\n",
            "epoch167 train time: 4.964s test time: 0.561  loss = 0.232 val_mse = 0.631 mse = 2.670 mae = 1.040\n",
            "epoch168 train time: 4.990s test time: 0.563  loss = 0.235 val_mse = 0.624 mse = 4.579 mae = 1.053\n",
            "epoch169 train time: 4.974s test time: 0.560  loss = 0.230 val_mse = 0.617 mse = 4.097 mae = 1.050\n",
            "epoch170 train time: 4.963s test time: 0.562  loss = 0.234 val_mse = 0.637 mse = 2.820 mae = 1.046\n",
            "epoch171 train time: 4.964s test time: 0.559  loss = 0.225 val_mse = 0.614 mse = 2.509 mae = 1.040\n",
            "epoch172 train time: 4.972s test time: 0.553  loss = 0.224 val_mse = 0.626 mse = 2.869 mae = 1.056\n",
            "epoch173 train time: 4.979s test time: 0.560  loss = 0.222 val_mse = 0.609 mse = 3.019 mae = 1.048\n",
            "epoch174 train time: 4.965s test time: 0.565  loss = 0.227 val_mse = 0.624 mse = 3.438 mae = 1.061\n",
            "epoch175 train time: 4.960s test time: 0.554  loss = 0.221 val_mse = 0.597 mse = 2.640 mae = 1.044\n",
            "epoch176 train time: 4.984s test time: 0.559  loss = 0.222 val_mse = 0.621 mse = 2.838 mae = 1.059\n",
            "epoch177 train time: 4.972s test time: 0.564  loss = 0.220 val_mse = 0.577 mse = 2.728 mae = 1.049\n",
            "epoch178 train time: 4.974s test time: 0.551  loss = 0.220 val_mse = 0.596 mse = 2.537 mae = 1.053\n",
            "epoch179 train time: 4.969s test time: 0.564  loss = 0.216 val_mse = 0.584 mse = 2.463 mae = 1.048\n",
            "epoch180 train time: 4.987s test time: 0.556  loss = 0.216 val_mse = 0.619 mse = 2.973 mae = 1.073\n",
            "epoch181 train time: 4.985s test time: 0.557  loss = 0.215 val_mse = 0.583 mse = 2.921 mae = 1.054\n",
            "epoch182 train time: 4.976s test time: 0.556  loss = 0.211 val_mse = 0.648 mse = 2.899 mae = 1.075\n",
            "epoch183 train time: 4.983s test time: 0.555  loss = 0.211 val_mse = 0.578 mse = 2.846 mae = 1.058\n",
            "epoch184 train time: 4.991s test time: 0.558  loss = 0.205 val_mse = 0.626 mse = 2.925 mae = 1.072\n",
            "epoch185 train time: 4.988s test time: 0.564  loss = 0.209 val_mse = 0.605 mse = 2.768 mae = 1.058\n",
            "epoch186 train time: 4.989s test time: 0.559  loss = 0.212 val_mse = 0.616 mse = 5.517 mae = 1.085\n",
            "epoch187 train time: 4.980s test time: 0.558  loss = 0.200 val_mse = 0.604 mse = 4.665 mae = 1.078\n",
            "epoch188 train time: 4.985s test time: 0.560  loss = 0.205 val_mse = 0.604 mse = 5.028 mae = 1.072\n",
            "epoch189 train time: 4.983s test time: 0.554  loss = 0.201 val_mse = 0.583 mse = 88.043 mae = 1.172\n",
            "epoch190 train time: 4.985s test time: 0.559  loss = 0.201 val_mse = 0.595 mse = 5832.914 mae = 2.158\n",
            "epoch191 train time: 4.979s test time: 0.555  loss = 0.198 val_mse = 0.603 mse = 6.034 mae = 1.080\n",
            "epoch192 train time: 4.975s test time: 0.553  loss = 0.197 val_mse = 0.602 mse = 26.521 mae = 1.125\n",
            "epoch193 train time: 4.971s test time: 0.559  loss = 0.204 val_mse = 0.579 mse = 3.626 mae = 1.074\n",
            "epoch194 train time: 4.982s test time: 0.561  loss = 0.196 val_mse = 0.615 mse = 123.930 mae = 1.216\n",
            "epoch195 train time: 4.990s test time: 0.564  loss = 0.197 val_mse = 0.596 mse = 4.711 mae = 1.085\n",
            "epoch196 train time: 4.975s test time: 0.554  loss = 0.199 val_mse = 0.611 mse = 285.774 mae = 1.307\n",
            "epoch197 train time: 4.977s test time: 0.567  loss = 0.199 val_mse = 0.636 mse = 5.867 mae = 1.096\n",
            "epoch198 train time: 4.984s test time: 0.561  loss = 0.191 val_mse = 0.614 mse = 4.674 mae = 1.077\n",
            "epoch199 train time: 4.967s test time: 0.570  loss = 0.196 val_mse = 0.588 mse = 7.448 mae = 1.081\n",
            "epoch200 train time: 4.979s test time: 0.559  loss = 0.188 val_mse = 0.594 mse = 21.532 mae = 1.114\n",
            "epoch201 train time: 4.979s test time: 0.557  loss = 0.193 val_mse = 0.577 mse = 6.932 mae = 1.084\n",
            "epoch202 train time: 4.966s test time: 0.563  loss = 0.190 val_mse = 0.584 mse = 4.373 mae = 1.069\n",
            "epoch203 train time: 4.957s test time: 0.561  loss = 0.189 val_mse = 0.589 mse = 3.455 mae = 1.074\n",
            "epoch204 train time: 4.984s test time: 0.559  loss = 0.191 val_mse = 0.595 mse = 2.658 mae = 1.061\n",
            "epoch205 train time: 4.989s test time: 0.563  loss = 0.189 val_mse = 0.587 mse = 3.727 mae = 1.077\n",
            "epoch206 train time: 4.980s test time: 0.561  loss = 0.188 val_mse = 0.591 mse = 4.343 mae = 1.082\n",
            "epoch207 train time: 4.974s test time: 0.558  loss = 0.192 val_mse = 0.603 mse = 8203.496 mae = 2.377\n",
            "epoch208 train time: 4.977s test time: 0.554  loss = 0.188 val_mse = 0.609 mse = 12.804 mae = 1.109\n",
            "epoch209 train time: 4.971s test time: 0.559  loss = 0.187 val_mse = 0.672 mse = 7.737 mae = 1.112\n",
            "epoch210 train time: 4.985s test time: 0.561  loss = 0.184 val_mse = 0.648 mse = 3.010 mae = 1.075\n",
            "epoch211 train time: 4.987s test time: 0.567  loss = 0.183 val_mse = 0.633 mse = 6.097 mae = 1.101\n",
            "epoch212 train time: 4.989s test time: 0.563  loss = 0.183 val_mse = 0.632 mse = 104.812 mae = 1.227\n",
            "epoch213 train time: 4.982s test time: 0.567  loss = 0.183 val_mse = 0.613 mse = 8.726 mae = 1.118\n",
            "epoch214 train time: 4.987s test time: 0.559  loss = 0.180 val_mse = 0.569 mse = 17.866 mae = 1.126\n",
            "epoch215 train time: 4.962s test time: 0.560  loss = 0.181 val_mse = 0.586 mse = 6.720 mae = 1.108\n",
            "epoch216 train time: 4.981s test time: 0.559  loss = 0.179 val_mse = 0.595 mse = 4.084 mae = 1.086\n",
            "epoch217 train time: 4.975s test time: 0.566  loss = 0.182 val_mse = 0.600 mse = 5.543 mae = 1.103\n",
            "epoch218 train time: 4.961s test time: 0.557  loss = 0.178 val_mse = 0.640 mse = 19.051 mae = 1.134\n",
            "epoch219 train time: 4.990s test time: 0.562  loss = 0.177 val_mse = 0.661 mse = 14.830 mae = 1.134\n",
            "epoch220 train time: 4.976s test time: 0.558  loss = 0.178 val_mse = 0.665 mse = 3.621 mae = 1.086\n",
            "epoch221 train time: 5.002s test time: 0.567  loss = 0.178 val_mse = 0.676 mse = 28.193 mae = 1.156\n",
            "epoch222 train time: 4.965s test time: 0.560  loss = 0.176 val_mse = 0.719 mse = 6.560 mae = 1.105\n",
            "epoch223 train time: 4.997s test time: 0.576  loss = 0.173 val_mse = 0.737 mse = 39.414 mae = 1.178\n",
            "epoch224 train time: 4.974s test time: 0.569  loss = 0.172 val_mse = 0.724 mse = 8.905 mae = 1.123\n",
            "epoch225 train time: 4.976s test time: 0.560  loss = 0.175 val_mse = 0.696 mse = 26.880 mae = 1.172\n",
            "epoch226 train time: 4.972s test time: 0.570  loss = 0.174 val_mse = 0.699 mse = 5.851 mae = 1.101\n",
            "epoch227 train time: 4.977s test time: 0.566  loss = 0.175 val_mse = 0.673 mse = 34.304 mae = 1.159\n",
            "epoch228 train time: 4.967s test time: 0.552  loss = 0.172 val_mse = 1.140 mse = 41.879 mae = 1.170\n",
            "epoch229 train time: 4.965s test time: 0.564  loss = 0.175 val_mse = 0.961 mse = 176.013 mae = 1.266\n",
            "epoch230 train time: 4.976s test time: 0.556  loss = 0.176 val_mse = 0.936 mse = 3.920 mae = 1.088\n",
            "epoch231 train time: 4.968s test time: 0.560  loss = 0.173 val_mse = 0.691 mse = 5.146 mae = 1.098\n",
            "epoch232 train time: 4.978s test time: 0.564  loss = 0.172 val_mse = 0.612 mse = 14.723 mae = 1.120\n",
            "epoch233 train time: 4.971s test time: 0.562  loss = 0.170 val_mse = 0.607 mse = 31.495 mae = 1.153\n",
            "epoch234 train time: 4.974s test time: 0.556  loss = 0.173 val_mse = 0.610 mse = 3.274 mae = 1.089\n",
            "epoch235 train time: 4.979s test time: 0.558  loss = 0.171 val_mse = 0.596 mse = 2.956 mae = 1.088\n",
            "epoch236 train time: 4.987s test time: 0.562  loss = 0.169 val_mse = 0.605 mse = 4.139 mae = 1.093\n",
            "epoch237 train time: 4.967s test time: 0.564  loss = 0.168 val_mse = 0.608 mse = 2.693 mae = 1.083\n",
            "epoch238 train time: 4.983s test time: 0.563  loss = 0.167 val_mse = 0.695 mse = 3.573 mae = 1.090\n",
            "epoch239 train time: 4.981s test time: 0.562  loss = 0.171 val_mse = 0.614 mse = 2.999 mae = 1.079\n",
            "MAE 0.9561675719966913\n",
            "MSE 65.55600156321124\n",
            "epoch0 train time: 5.419s test time: 0.555  loss = 23.316 val_mse = 1.592 mse = 1.521 mae = 0.886\n",
            "epoch1 train time: 4.932s test time: 0.567  loss = 16.767 val_mse = 1.578 mse = 1.512 mae = 0.883\n",
            "epoch2 train time: 4.931s test time: 0.561  loss = 9.416 val_mse = 1.551 mse = 1.492 mae = 0.876\n",
            "epoch3 train time: 4.933s test time: 0.556  loss = 4.911 val_mse = 1.521 mse = 1.470 mae = 0.869\n",
            "epoch4 train time: 4.942s test time: 0.563  loss = 2.597 val_mse = 1.489 mse = 1.448 mae = 0.863\n",
            "epoch5 train time: 4.926s test time: 0.562  loss = 1.583 val_mse = 1.456 mse = 1.425 mae = 0.858\n",
            "epoch6 train time: 4.933s test time: 0.562  loss = 1.235 val_mse = 1.418 mse = 1.399 mae = 0.855\n",
            "epoch7 train time: 4.932s test time: 0.559  loss = 1.146 val_mse = 1.384 mse = 1.378 mae = 0.854\n",
            "epoch8 train time: 4.931s test time: 0.562  loss = 1.109 val_mse = 1.352 mse = 1.360 mae = 0.854\n",
            "epoch9 train time: 4.925s test time: 0.567  loss = 1.075 val_mse = 1.325 mse = 1.345 mae = 0.853\n",
            "epoch10 train time: 4.934s test time: 0.569  loss = 1.045 val_mse = 1.308 mse = 1.338 mae = 0.848\n",
            "epoch11 train time: 4.960s test time: 0.555  loss = 1.016 val_mse = 1.289 mse = 1.327 mae = 0.844\n",
            "epoch12 train time: 4.933s test time: 0.558  loss = 0.991 val_mse = 1.274 mse = 1.321 mae = 0.842\n",
            "epoch13 train time: 4.942s test time: 0.569  loss = 0.967 val_mse = 1.261 mse = 1.314 mae = 0.838\n",
            "epoch14 train time: 4.931s test time: 0.558  loss = 0.946 val_mse = 1.250 mse = 1.310 mae = 0.838\n",
            "epoch15 train time: 4.941s test time: 0.561  loss = 0.928 val_mse = 1.237 mse = 1.303 mae = 0.837\n",
            "epoch16 train time: 4.947s test time: 0.558  loss = 0.914 val_mse = 1.229 mse = 1.303 mae = 0.841\n",
            "epoch17 train time: 4.928s test time: 0.564  loss = 0.902 val_mse = 1.228 mse = 1.301 mae = 0.841\n",
            "epoch18 train time: 4.921s test time: 0.558  loss = 0.891 val_mse = 1.221 mse = 1.305 mae = 0.842\n",
            "epoch19 train time: 4.944s test time: 0.552  loss = 0.888 val_mse = 1.213 mse = 1.297 mae = 0.843\n",
            "epoch20 train time: 4.951s test time: 0.559  loss = 0.881 val_mse = 1.210 mse = 1.294 mae = 0.845\n",
            "epoch21 train time: 4.925s test time: 0.558  loss = 0.877 val_mse = 1.210 mse = 1.299 mae = 0.841\n",
            "epoch22 train time: 4.943s test time: 0.553  loss = 0.869 val_mse = 1.225 mse = 1.305 mae = 0.837\n",
            "epoch23 train time: 4.935s test time: 0.560  loss = 0.867 val_mse = 1.209 mse = 1.297 mae = 0.842\n",
            "epoch24 train time: 4.944s test time: 0.556  loss = 0.860 val_mse = 1.221 mse = 1.308 mae = 0.839\n",
            "epoch25 train time: 4.930s test time: 0.562  loss = 0.858 val_mse = 1.206 mse = 1.301 mae = 0.846\n",
            "epoch26 train time: 4.947s test time: 0.561  loss = 0.857 val_mse = 1.220 mse = 1.308 mae = 0.842\n",
            "epoch27 train time: 4.936s test time: 0.553  loss = 0.849 val_mse = 1.206 mse = 1.302 mae = 0.843\n",
            "epoch28 train time: 4.961s test time: 0.558  loss = 0.844 val_mse = 1.222 mse = 1.307 mae = 0.835\n",
            "epoch29 train time: 4.940s test time: 0.561  loss = 0.839 val_mse = 1.217 mse = 1.313 mae = 0.844\n",
            "epoch30 train time: 4.926s test time: 0.558  loss = 0.838 val_mse = 1.222 mse = 1.313 mae = 0.838\n",
            "epoch31 train time: 4.931s test time: 0.562  loss = 0.836 val_mse = 1.242 mse = 1.333 mae = 0.834\n",
            "epoch32 train time: 4.927s test time: 0.560  loss = 0.831 val_mse = 1.242 mse = 1.326 mae = 0.832\n",
            "epoch33 train time: 4.949s test time: 0.564  loss = 0.836 val_mse = 1.206 mse = 1.314 mae = 0.848\n",
            "epoch34 train time: 4.930s test time: 0.560  loss = 0.831 val_mse = 1.216 mse = 1.303 mae = 0.833\n",
            "epoch35 train time: 4.917s test time: 0.564  loss = 0.828 val_mse = 1.207 mse = 1.302 mae = 0.842\n",
            "epoch36 train time: 4.936s test time: 0.564  loss = 0.824 val_mse = 1.221 mse = 1.300 mae = 0.833\n",
            "epoch37 train time: 4.943s test time: 0.558  loss = 0.822 val_mse = 1.210 mse = 1.305 mae = 0.841\n",
            "epoch38 train time: 4.950s test time: 0.564  loss = 0.821 val_mse = 1.220 mse = 1.307 mae = 0.834\n",
            "epoch39 train time: 4.950s test time: 0.566  loss = 0.823 val_mse = 1.215 mse = 1.306 mae = 0.842\n",
            "epoch40 train time: 4.957s test time: 0.558  loss = 0.817 val_mse = 1.224 mse = 1.313 mae = 0.834\n",
            "epoch41 train time: 4.953s test time: 0.563  loss = 0.814 val_mse = 1.207 mse = 1.310 mae = 0.845\n",
            "epoch42 train time: 4.959s test time: 0.562  loss = 0.820 val_mse = 1.233 mse = 1.313 mae = 0.833\n",
            "epoch43 train time: 4.957s test time: 0.575  loss = 0.818 val_mse = 1.213 mse = 1.315 mae = 0.845\n",
            "epoch44 train time: 4.947s test time: 0.568  loss = 0.814 val_mse = 1.217 mse = 1.298 mae = 0.833\n",
            "epoch45 train time: 4.941s test time: 0.563  loss = 0.814 val_mse = 1.238 mse = 1.326 mae = 0.835\n",
            "epoch46 train time: 4.925s test time: 0.558  loss = 0.808 val_mse = 1.238 mse = 1.327 mae = 0.831\n",
            "epoch47 train time: 4.934s test time: 0.557  loss = 0.810 val_mse = 1.211 mse = 1.305 mae = 0.847\n",
            "epoch48 train time: 4.923s test time: 0.556  loss = 0.808 val_mse = 1.227 mse = 1.310 mae = 0.833\n",
            "epoch49 train time: 4.948s test time: 0.559  loss = 0.806 val_mse = 1.210 mse = 1.308 mae = 0.841\n",
            "epoch50 train time: 4.935s test time: 0.560  loss = 0.808 val_mse = 1.220 mse = 1.299 mae = 0.826\n",
            "epoch51 train time: 4.937s test time: 0.561  loss = 0.807 val_mse = 1.199 mse = 1.304 mae = 0.841\n",
            "epoch52 train time: 4.944s test time: 0.558  loss = 0.801 val_mse = 1.206 mse = 1.298 mae = 0.839\n",
            "epoch53 train time: 4.935s test time: 0.561  loss = 0.801 val_mse = 1.217 mse = 1.310 mae = 0.833\n",
            "epoch54 train time: 4.936s test time: 0.568  loss = 0.796 val_mse = 1.194 mse = 1.302 mae = 0.841\n",
            "epoch55 train time: 4.951s test time: 0.562  loss = 0.799 val_mse = 1.220 mse = 1.313 mae = 0.835\n",
            "epoch56 train time: 4.948s test time: 0.555  loss = 0.798 val_mse = 1.198 mse = 1.295 mae = 0.838\n",
            "epoch57 train time: 4.952s test time: 0.564  loss = 0.798 val_mse = 1.211 mse = 1.309 mae = 0.840\n",
            "epoch58 train time: 4.931s test time: 0.562  loss = 0.794 val_mse = 1.199 mse = 1.293 mae = 0.834\n",
            "epoch59 train time: 4.950s test time: 0.555  loss = 0.795 val_mse = 1.218 mse = 1.306 mae = 0.838\n",
            "epoch60 train time: 4.944s test time: 0.566  loss = 0.794 val_mse = 1.188 mse = 1.295 mae = 0.837\n",
            "epoch61 train time: 4.914s test time: 0.558  loss = 0.793 val_mse = 1.204 mse = 1.310 mae = 0.844\n",
            "epoch62 train time: 4.933s test time: 0.555  loss = 0.791 val_mse = 1.212 mse = 1.301 mae = 0.830\n",
            "epoch63 train time: 4.938s test time: 0.556  loss = 0.789 val_mse = 1.225 mse = 1.320 mae = 0.836\n",
            "epoch64 train time: 4.954s test time: 0.558  loss = 0.792 val_mse = 1.219 mse = 1.309 mae = 0.831\n",
            "epoch65 train time: 4.948s test time: 0.559  loss = 0.793 val_mse = 1.237 mse = 1.328 mae = 0.835\n",
            "epoch66 train time: 4.969s test time: 0.556  loss = 0.791 val_mse = 1.218 mse = 1.306 mae = 0.829\n",
            "epoch67 train time: 4.926s test time: 0.561  loss = 0.787 val_mse = 1.223 mse = 1.321 mae = 0.832\n",
            "epoch68 train time: 4.964s test time: 0.575  loss = 0.784 val_mse = 1.216 mse = 1.304 mae = 0.830\n",
            "epoch69 train time: 4.944s test time: 0.560  loss = 0.787 val_mse = 1.219 mse = 1.314 mae = 0.834\n",
            "epoch70 train time: 4.965s test time: 0.553  loss = 0.786 val_mse = 1.220 mse = 1.310 mae = 0.831\n",
            "epoch71 train time: 4.928s test time: 0.560  loss = 0.787 val_mse = 1.221 mse = 1.311 mae = 0.832\n",
            "epoch72 train time: 4.963s test time: 0.559  loss = 0.786 val_mse = 1.217 mse = 1.309 mae = 0.828\n",
            "epoch73 train time: 4.946s test time: 0.562  loss = 0.786 val_mse = 1.220 mse = 1.313 mae = 0.833\n",
            "epoch74 train time: 4.923s test time: 0.568  loss = 0.788 val_mse = 1.211 mse = 1.294 mae = 0.828\n",
            "epoch75 train time: 4.949s test time: 0.552  loss = 0.781 val_mse = 1.213 mse = 1.309 mae = 0.834\n",
            "epoch76 train time: 4.942s test time: 0.563  loss = 0.781 val_mse = 1.211 mse = 1.299 mae = 0.827\n",
            "epoch77 train time: 4.929s test time: 0.560  loss = 0.782 val_mse = 1.223 mse = 1.317 mae = 0.834\n",
            "epoch78 train time: 4.942s test time: 0.560  loss = 0.783 val_mse = 1.212 mse = 1.296 mae = 0.826\n",
            "epoch79 train time: 4.934s test time: 0.567  loss = 0.781 val_mse = 1.220 mse = 1.322 mae = 0.835\n",
            "epoch80 train time: 4.932s test time: 0.556  loss = 0.779 val_mse = 1.209 mse = 1.295 mae = 0.827\n",
            "epoch81 train time: 4.929s test time: 0.563  loss = 0.777 val_mse = 1.211 mse = 1.310 mae = 0.831\n",
            "epoch82 train time: 4.931s test time: 0.563  loss = 0.779 val_mse = 1.212 mse = 1.302 mae = 0.832\n",
            "epoch83 train time: 4.940s test time: 0.556  loss = 0.776 val_mse = 1.198 mse = 1.299 mae = 0.838\n",
            "epoch84 train time: 4.947s test time: 0.563  loss = 0.774 val_mse = 1.199 mse = 1.297 mae = 0.832\n",
            "epoch85 train time: 4.945s test time: 0.564  loss = 0.778 val_mse = 1.200 mse = 1.306 mae = 0.838\n",
            "epoch86 train time: 4.934s test time: 0.564  loss = 0.776 val_mse = 1.205 mse = 1.301 mae = 0.831\n",
            "epoch87 train time: 4.936s test time: 0.569  loss = 0.776 val_mse = 1.200 mse = 1.295 mae = 0.832\n",
            "epoch88 train time: 4.929s test time: 0.560  loss = 0.770 val_mse = 1.192 mse = 1.295 mae = 0.832\n",
            "epoch89 train time: 4.934s test time: 0.561  loss = 0.770 val_mse = 1.206 mse = 1.309 mae = 0.836\n",
            "epoch90 train time: 4.919s test time: 0.562  loss = 0.772 val_mse = 1.188 mse = 1.286 mae = 0.834\n",
            "epoch91 train time: 4.930s test time: 0.556  loss = 0.769 val_mse = 1.194 mse = 1.300 mae = 0.838\n",
            "epoch92 train time: 4.930s test time: 0.573  loss = 0.770 val_mse = 1.186 mse = 1.290 mae = 0.831\n",
            "epoch93 train time: 4.945s test time: 0.553  loss = 0.768 val_mse = 1.198 mse = 1.304 mae = 0.838\n",
            "epoch94 train time: 4.958s test time: 0.560  loss = 0.768 val_mse = 1.183 mse = 1.286 mae = 0.832\n",
            "epoch95 train time: 4.940s test time: 0.570  loss = 0.768 val_mse = 1.195 mse = 1.308 mae = 0.838\n",
            "epoch96 train time: 4.945s test time: 0.559  loss = 0.766 val_mse = 1.192 mse = 1.290 mae = 0.831\n",
            "epoch97 train time: 4.937s test time: 0.567  loss = 0.766 val_mse = 1.189 mse = 1.303 mae = 0.839\n",
            "epoch98 train time: 4.955s test time: 0.567  loss = 0.764 val_mse = 1.197 mse = 1.288 mae = 0.832\n",
            "epoch99 train time: 4.944s test time: 0.553  loss = 0.770 val_mse = 1.192 mse = 1.303 mae = 0.836\n",
            "epoch100 train time: 4.946s test time: 0.559  loss = 0.765 val_mse = 1.190 mse = 1.289 mae = 0.829\n",
            "epoch101 train time: 4.951s test time: 0.568  loss = 0.766 val_mse = 1.196 mse = 1.301 mae = 0.834\n",
            "epoch102 train time: 4.933s test time: 0.565  loss = 0.764 val_mse = 1.187 mse = 1.290 mae = 0.833\n",
            "epoch103 train time: 4.925s test time: 0.563  loss = 0.764 val_mse = 1.190 mse = 1.296 mae = 0.834\n",
            "epoch104 train time: 4.921s test time: 0.557  loss = 0.762 val_mse = 1.187 mse = 1.288 mae = 0.830\n",
            "epoch105 train time: 4.945s test time: 0.560  loss = 0.763 val_mse = 1.190 mse = 1.301 mae = 0.837\n",
            "epoch106 train time: 4.935s test time: 0.573  loss = 0.765 val_mse = 1.180 mse = 1.286 mae = 0.830\n",
            "epoch107 train time: 4.925s test time: 0.554  loss = 0.760 val_mse = 1.191 mse = 1.300 mae = 0.836\n",
            "epoch108 train time: 4.945s test time: 0.563  loss = 0.764 val_mse = 1.191 mse = 1.291 mae = 0.831\n",
            "epoch109 train time: 4.935s test time: 0.561  loss = 0.763 val_mse = 1.186 mse = 1.300 mae = 0.834\n",
            "epoch110 train time: 4.956s test time: 0.567  loss = 0.761 val_mse = 1.184 mse = 1.285 mae = 0.832\n",
            "epoch111 train time: 4.937s test time: 0.572  loss = 0.762 val_mse = 1.192 mse = 1.299 mae = 0.834\n",
            "epoch112 train time: 4.946s test time: 0.565  loss = 0.764 val_mse = 1.192 mse = 1.290 mae = 0.828\n",
            "epoch113 train time: 4.942s test time: 0.563  loss = 0.761 val_mse = 1.187 mse = 1.300 mae = 0.835\n",
            "epoch114 train time: 4.944s test time: 0.572  loss = 0.760 val_mse = 1.188 mse = 1.288 mae = 0.830\n",
            "epoch115 train time: 4.949s test time: 0.560  loss = 0.762 val_mse = 1.196 mse = 1.301 mae = 0.836\n",
            "epoch116 train time: 4.937s test time: 0.557  loss = 0.764 val_mse = 1.183 mse = 1.286 mae = 0.827\n",
            "epoch117 train time: 4.929s test time: 0.558  loss = 0.762 val_mse = 1.193 mse = 1.301 mae = 0.835\n",
            "epoch118 train time: 4.926s test time: 0.563  loss = 0.760 val_mse = 1.186 mse = 1.284 mae = 0.827\n",
            "epoch119 train time: 4.925s test time: 0.565  loss = 0.761 val_mse = 1.194 mse = 1.298 mae = 0.832\n",
            "epoch120 train time: 4.933s test time: 0.560  loss = 0.759 val_mse = 1.181 mse = 1.286 mae = 0.829\n",
            "epoch121 train time: 4.938s test time: 0.559  loss = 0.761 val_mse = 1.191 mse = 1.296 mae = 0.836\n",
            "epoch122 train time: 4.947s test time: 0.560  loss = 0.759 val_mse = 1.175 mse = 1.282 mae = 0.829\n",
            "epoch123 train time: 4.943s test time: 0.558  loss = 0.758 val_mse = 1.188 mse = 1.297 mae = 0.831\n",
            "epoch124 train time: 4.942s test time: 0.561  loss = 0.759 val_mse = 1.187 mse = 1.286 mae = 0.828\n",
            "epoch125 train time: 4.950s test time: 0.561  loss = 0.758 val_mse = 1.189 mse = 1.295 mae = 0.830\n",
            "epoch126 train time: 4.947s test time: 0.556  loss = 0.759 val_mse = 1.180 mse = 1.284 mae = 0.829\n",
            "epoch127 train time: 4.936s test time: 0.563  loss = 0.756 val_mse = 1.181 mse = 1.295 mae = 0.834\n",
            "epoch128 train time: 4.947s test time: 0.564  loss = 0.756 val_mse = 1.186 mse = 1.286 mae = 0.830\n",
            "epoch129 train time: 4.923s test time: 0.556  loss = 0.755 val_mse = 1.187 mse = 1.299 mae = 0.835\n",
            "epoch130 train time: 4.950s test time: 0.561  loss = 0.758 val_mse = 1.187 mse = 1.283 mae = 0.828\n",
            "epoch131 train time: 4.917s test time: 0.561  loss = 0.756 val_mse = 1.190 mse = 1.298 mae = 0.832\n",
            "epoch132 train time: 4.952s test time: 0.555  loss = 0.755 val_mse = 1.175 mse = 1.286 mae = 0.830\n",
            "epoch133 train time: 4.945s test time: 0.570  loss = 0.755 val_mse = 1.185 mse = 1.299 mae = 0.831\n",
            "epoch134 train time: 4.945s test time: 0.552  loss = 0.754 val_mse = 1.186 mse = 1.285 mae = 0.831\n",
            "epoch135 train time: 4.939s test time: 0.562  loss = 0.755 val_mse = 1.184 mse = 1.298 mae = 0.834\n",
            "epoch136 train time: 4.942s test time: 0.561  loss = 0.754 val_mse = 1.174 mse = 1.284 mae = 0.829\n",
            "epoch137 train time: 4.942s test time: 0.557  loss = 0.755 val_mse = 1.190 mse = 1.297 mae = 0.833\n",
            "epoch138 train time: 4.946s test time: 0.566  loss = 0.753 val_mse = 1.180 mse = 1.285 mae = 0.829\n",
            "epoch139 train time: 4.945s test time: 0.566  loss = 0.755 val_mse = 1.191 mse = 1.301 mae = 0.831\n",
            "epoch140 train time: 4.947s test time: 0.560  loss = 0.755 val_mse = 1.179 mse = 1.281 mae = 0.829\n",
            "epoch141 train time: 4.956s test time: 0.558  loss = 0.754 val_mse = 1.182 mse = 1.302 mae = 0.835\n",
            "epoch142 train time: 4.954s test time: 0.554  loss = 0.753 val_mse = 1.177 mse = 1.287 mae = 0.829\n",
            "epoch143 train time: 4.932s test time: 0.558  loss = 0.756 val_mse = 1.186 mse = 1.293 mae = 0.832\n",
            "epoch144 train time: 4.944s test time: 0.560  loss = 0.753 val_mse = 1.175 mse = 1.283 mae = 0.829\n",
            "epoch145 train time: 4.927s test time: 0.555  loss = 0.751 val_mse = 1.181 mse = 1.296 mae = 0.833\n",
            "epoch146 train time: 4.927s test time: 0.567  loss = 0.753 val_mse = 1.176 mse = 1.283 mae = 0.827\n",
            "epoch147 train time: 4.946s test time: 0.554  loss = 0.753 val_mse = 1.191 mse = 1.296 mae = 0.831\n",
            "epoch148 train time: 4.935s test time: 0.561  loss = 0.754 val_mse = 1.179 mse = 1.283 mae = 0.825\n",
            "epoch149 train time: 4.943s test time: 0.565  loss = 0.753 val_mse = 1.185 mse = 1.290 mae = 0.831\n",
            "epoch150 train time: 4.947s test time: 0.559  loss = 0.756 val_mse = 1.177 mse = 1.280 mae = 0.828\n",
            "epoch151 train time: 4.921s test time: 0.551  loss = 0.754 val_mse = 1.183 mse = 1.297 mae = 0.832\n",
            "epoch152 train time: 4.950s test time: 0.561  loss = 0.754 val_mse = 1.174 mse = 1.280 mae = 0.829\n",
            "epoch153 train time: 4.939s test time: 0.557  loss = 0.754 val_mse = 1.189 mse = 1.300 mae = 0.832\n",
            "epoch154 train time: 4.940s test time: 0.559  loss = 0.756 val_mse = 1.179 mse = 1.281 mae = 0.829\n",
            "epoch155 train time: 4.942s test time: 0.561  loss = 0.756 val_mse = 1.182 mse = 1.294 mae = 0.832\n",
            "epoch156 train time: 4.934s test time: 0.555  loss = 0.755 val_mse = 1.177 mse = 1.287 mae = 0.829\n",
            "epoch157 train time: 4.932s test time: 0.566  loss = 0.755 val_mse = 1.182 mse = 1.298 mae = 0.834\n",
            "epoch158 train time: 4.943s test time: 0.560  loss = 0.753 val_mse = 1.175 mse = 1.283 mae = 0.827\n",
            "epoch159 train time: 4.934s test time: 0.561  loss = 0.754 val_mse = 1.184 mse = 1.294 mae = 0.829\n",
            "epoch160 train time: 4.929s test time: 0.557  loss = 0.753 val_mse = 1.180 mse = 1.283 mae = 0.826\n",
            "epoch161 train time: 4.933s test time: 0.555  loss = 0.754 val_mse = 1.186 mse = 1.294 mae = 0.831\n",
            "epoch162 train time: 4.933s test time: 0.553  loss = 0.753 val_mse = 1.175 mse = 1.280 mae = 0.827\n",
            "epoch163 train time: 4.932s test time: 0.554  loss = 0.753 val_mse = 1.188 mse = 1.295 mae = 0.831\n",
            "epoch164 train time: 4.927s test time: 0.561  loss = 0.758 val_mse = 1.180 mse = 1.283 mae = 0.828\n",
            "epoch165 train time: 4.955s test time: 0.566  loss = 0.756 val_mse = 1.182 mse = 1.294 mae = 0.834\n",
            "epoch166 train time: 4.923s test time: 0.557  loss = 0.753 val_mse = 1.173 mse = 1.280 mae = 0.830\n",
            "epoch167 train time: 4.942s test time: 0.560  loss = 0.752 val_mse = 1.180 mse = 1.295 mae = 0.831\n",
            "epoch168 train time: 4.942s test time: 0.568  loss = 0.753 val_mse = 1.178 mse = 1.285 mae = 0.827\n",
            "epoch169 train time: 4.942s test time: 0.555  loss = 0.751 val_mse = 1.187 mse = 1.294 mae = 0.830\n",
            "epoch170 train time: 4.938s test time: 0.560  loss = 0.751 val_mse = 1.173 mse = 1.284 mae = 0.830\n",
            "epoch171 train time: 4.945s test time: 0.571  loss = 0.755 val_mse = 1.184 mse = 1.292 mae = 0.829\n",
            "epoch172 train time: 4.929s test time: 0.563  loss = 0.753 val_mse = 1.179 mse = 1.281 mae = 0.825\n",
            "epoch173 train time: 4.925s test time: 0.566  loss = 0.752 val_mse = 1.181 mse = 1.295 mae = 0.831\n",
            "epoch174 train time: 4.937s test time: 0.554  loss = 0.753 val_mse = 1.181 mse = 1.283 mae = 0.827\n",
            "epoch175 train time: 4.964s test time: 0.555  loss = 0.754 val_mse = 1.181 mse = 1.295 mae = 0.828\n",
            "epoch176 train time: 4.935s test time: 0.561  loss = 0.749 val_mse = 1.173 mse = 1.285 mae = 0.828\n",
            "epoch177 train time: 4.961s test time: 0.556  loss = 0.753 val_mse = 1.177 mse = 1.296 mae = 0.832\n",
            "epoch178 train time: 4.951s test time: 0.560  loss = 0.751 val_mse = 1.173 mse = 1.282 mae = 0.829\n",
            "epoch179 train time: 4.951s test time: 0.581  loss = 0.751 val_mse = 1.180 mse = 1.297 mae = 0.830\n",
            "epoch180 train time: 4.949s test time: 0.557  loss = 0.752 val_mse = 1.175 mse = 1.279 mae = 0.829\n",
            "epoch181 train time: 4.945s test time: 0.560  loss = 0.754 val_mse = 1.189 mse = 1.297 mae = 0.833\n",
            "epoch182 train time: 4.949s test time: 0.560  loss = 0.753 val_mse = 1.174 mse = 1.283 mae = 0.828\n",
            "epoch183 train time: 4.960s test time: 0.555  loss = 0.752 val_mse = 1.182 mse = 1.296 mae = 0.829\n",
            "epoch184 train time: 4.940s test time: 0.565  loss = 0.751 val_mse = 1.177 mse = 1.285 mae = 0.826\n",
            "epoch185 train time: 4.951s test time: 0.555  loss = 0.752 val_mse = 1.179 mse = 1.292 mae = 0.830\n",
            "epoch186 train time: 4.928s test time: 0.563  loss = 0.751 val_mse = 1.173 mse = 1.283 mae = 0.828\n",
            "epoch187 train time: 4.915s test time: 0.551  loss = 0.749 val_mse = 1.176 mse = 1.288 mae = 0.828\n",
            "epoch188 train time: 4.909s test time: 0.565  loss = 0.749 val_mse = 1.174 mse = 1.278 mae = 0.826\n",
            "epoch189 train time: 4.939s test time: 0.569  loss = 0.750 val_mse = 1.180 mse = 1.291 mae = 0.831\n",
            "epoch190 train time: 4.943s test time: 0.566  loss = 0.750 val_mse = 1.170 mse = 1.282 mae = 0.824\n",
            "epoch191 train time: 4.952s test time: 0.561  loss = 0.751 val_mse = 1.178 mse = 1.291 mae = 0.833\n",
            "epoch192 train time: 4.945s test time: 0.563  loss = 0.752 val_mse = 1.174 mse = 1.281 mae = 0.827\n",
            "epoch193 train time: 4.940s test time: 0.562  loss = 0.750 val_mse = 1.179 mse = 1.292 mae = 0.829\n",
            "epoch194 train time: 4.939s test time: 0.563  loss = 0.749 val_mse = 1.176 mse = 1.283 mae = 0.825\n",
            "epoch195 train time: 4.944s test time: 0.563  loss = 0.750 val_mse = 1.178 mse = 1.290 mae = 0.830\n",
            "epoch196 train time: 4.958s test time: 0.563  loss = 0.749 val_mse = 1.173 mse = 1.283 mae = 0.828\n",
            "epoch197 train time: 4.946s test time: 0.558  loss = 0.751 val_mse = 1.175 mse = 1.292 mae = 0.831\n",
            "epoch198 train time: 4.942s test time: 0.560  loss = 0.751 val_mse = 1.173 mse = 1.280 mae = 0.826\n",
            "epoch199 train time: 4.951s test time: 0.557  loss = 0.751 val_mse = 1.171 mse = 1.293 mae = 0.830\n",
            "epoch200 train time: 4.937s test time: 0.565  loss = 0.749 val_mse = 1.171 mse = 1.281 mae = 0.824\n",
            "epoch201 train time: 4.930s test time: 0.556  loss = 0.749 val_mse = 1.173 mse = 1.290 mae = 0.830\n",
            "epoch202 train time: 4.923s test time: 0.561  loss = 0.748 val_mse = 1.168 mse = 1.276 mae = 0.825\n",
            "epoch203 train time: 4.901s test time: 0.559  loss = 0.750 val_mse = 1.178 mse = 1.291 mae = 0.830\n",
            "epoch204 train time: 4.921s test time: 0.559  loss = 0.751 val_mse = 1.176 mse = 1.277 mae = 0.825\n",
            "epoch205 train time: 4.951s test time: 0.562  loss = 0.748 val_mse = 1.181 mse = 1.291 mae = 0.826\n",
            "epoch206 train time: 4.954s test time: 0.562  loss = 0.749 val_mse = 1.171 mse = 1.282 mae = 0.827\n",
            "epoch207 train time: 4.941s test time: 0.569  loss = 0.750 val_mse = 1.181 mse = 1.291 mae = 0.829\n",
            "epoch208 train time: 4.926s test time: 0.560  loss = 0.747 val_mse = 1.171 mse = 1.277 mae = 0.825\n",
            "epoch209 train time: 4.934s test time: 0.563  loss = 0.750 val_mse = 1.173 mse = 1.288 mae = 0.829\n",
            "epoch210 train time: 4.948s test time: 0.558  loss = 0.748 val_mse = 1.170 mse = 1.278 mae = 0.825\n",
            "epoch211 train time: 4.938s test time: 0.562  loss = 0.748 val_mse = 1.170 mse = 1.285 mae = 0.828\n",
            "epoch212 train time: 4.942s test time: 0.555  loss = 0.747 val_mse = 1.171 mse = 1.278 mae = 0.824\n",
            "epoch213 train time: 4.923s test time: 0.568  loss = 0.746 val_mse = 1.175 mse = 1.285 mae = 0.828\n",
            "epoch214 train time: 4.930s test time: 0.561  loss = 0.748 val_mse = 1.170 mse = 1.278 mae = 0.826\n",
            "epoch215 train time: 4.930s test time: 0.561  loss = 0.748 val_mse = 1.178 mse = 1.291 mae = 0.828\n",
            "epoch216 train time: 4.929s test time: 0.563  loss = 0.746 val_mse = 1.168 mse = 1.279 mae = 0.826\n",
            "epoch217 train time: 4.931s test time: 0.562  loss = 0.747 val_mse = 1.177 mse = 1.288 mae = 0.830\n",
            "epoch218 train time: 4.933s test time: 0.561  loss = 0.747 val_mse = 1.173 mse = 1.277 mae = 0.825\n",
            "epoch219 train time: 4.924s test time: 0.557  loss = 0.746 val_mse = 1.173 mse = 1.286 mae = 0.831\n",
            "epoch220 train time: 4.928s test time: 0.552  loss = 0.748 val_mse = 1.168 mse = 1.276 mae = 0.825\n",
            "epoch221 train time: 4.918s test time: 0.576  loss = 0.745 val_mse = 1.171 mse = 1.290 mae = 0.828\n",
            "epoch222 train time: 4.930s test time: 0.563  loss = 0.745 val_mse = 1.166 mse = 1.275 mae = 0.824\n",
            "epoch223 train time: 4.923s test time: 0.567  loss = 0.744 val_mse = 1.172 mse = 1.285 mae = 0.828\n",
            "epoch224 train time: 4.906s test time: 0.562  loss = 0.747 val_mse = 1.166 mse = 1.277 mae = 0.827\n",
            "epoch225 train time: 4.950s test time: 0.562  loss = 0.746 val_mse = 1.172 mse = 1.286 mae = 0.826\n",
            "epoch226 train time: 4.928s test time: 0.562  loss = 0.746 val_mse = 1.171 mse = 1.277 mae = 0.824\n",
            "epoch227 train time: 4.938s test time: 0.561  loss = 0.747 val_mse = 1.173 mse = 1.286 mae = 0.827\n",
            "epoch228 train time: 4.938s test time: 0.557  loss = 0.747 val_mse = 1.170 mse = 1.280 mae = 0.824\n",
            "epoch229 train time: 4.933s test time: 0.571  loss = 0.747 val_mse = 1.169 mse = 1.285 mae = 0.828\n",
            "epoch230 train time: 4.916s test time: 0.561  loss = 0.746 val_mse = 1.167 mse = 1.278 mae = 0.825\n",
            "epoch231 train time: 4.933s test time: 0.567  loss = 0.745 val_mse = 1.172 mse = 1.283 mae = 0.826\n",
            "epoch232 train time: 4.932s test time: 0.560  loss = 0.744 val_mse = 1.164 mse = 1.282 mae = 0.827\n",
            "epoch233 train time: 4.934s test time: 0.552  loss = 0.746 val_mse = 1.171 mse = 1.286 mae = 0.829\n",
            "epoch234 train time: 4.950s test time: 0.559  loss = 0.748 val_mse = 1.172 mse = 1.278 mae = 0.824\n",
            "epoch235 train time: 4.926s test time: 0.568  loss = 0.747 val_mse = 1.171 mse = 1.284 mae = 0.829\n",
            "epoch236 train time: 4.947s test time: 0.564  loss = 0.747 val_mse = 1.175 mse = 1.279 mae = 0.824\n",
            "epoch237 train time: 4.935s test time: 0.563  loss = 0.745 val_mse = 1.173 mse = 1.286 mae = 0.830\n",
            "epoch238 train time: 4.950s test time: 0.561  loss = 0.746 val_mse = 1.164 mse = 1.271 mae = 0.824\n",
            "epoch239 train time: 4.942s test time: 0.560  loss = 0.744 val_mse = 1.172 mse = 1.287 mae = 0.829\n",
            "MAE 0.833382876876455\n",
            "MSE 1.3009332724232503\n",
            "epoch0 train time: 5.461s test time: 0.557  loss = 43.271 val_mse = 1.589 mse = 1.520 mae = 0.885\n",
            "epoch1 train time: 4.971s test time: 0.556  loss = 30.638 val_mse = 1.571 mse = 1.509 mae = 0.881\n",
            "epoch2 train time: 4.977s test time: 0.558  loss = 16.348 val_mse = 1.548 mse = 1.492 mae = 0.875\n",
            "epoch3 train time: 4.977s test time: 0.552  loss = 7.992 val_mse = 1.525 mse = 1.475 mae = 0.869\n",
            "epoch4 train time: 4.976s test time: 0.551  loss = 3.769 val_mse = 1.501 mse = 1.457 mae = 0.865\n",
            "epoch5 train time: 4.975s test time: 0.555  loss = 1.971 val_mse = 1.476 mse = 1.440 mae = 0.861\n",
            "epoch6 train time: 4.970s test time: 0.560  loss = 1.396 val_mse = 1.451 mse = 1.423 mae = 0.857\n",
            "epoch7 train time: 4.969s test time: 0.575  loss = 1.274 val_mse = 1.426 mse = 1.406 mae = 0.855\n",
            "epoch8 train time: 4.973s test time: 0.565  loss = 1.231 val_mse = 1.396 mse = 1.386 mae = 0.855\n",
            "epoch9 train time: 4.974s test time: 0.555  loss = 1.191 val_mse = 1.369 mse = 1.369 mae = 0.855\n",
            "epoch10 train time: 4.979s test time: 0.554  loss = 1.155 val_mse = 1.345 mse = 1.356 mae = 0.854\n",
            "epoch11 train time: 4.957s test time: 0.556  loss = 1.121 val_mse = 1.327 mse = 1.346 mae = 0.851\n",
            "epoch12 train time: 4.978s test time: 0.557  loss = 1.089 val_mse = 1.313 mse = 1.340 mae = 0.847\n",
            "epoch13 train time: 4.987s test time: 0.561  loss = 1.059 val_mse = 1.300 mse = 1.333 mae = 0.844\n",
            "epoch14 train time: 4.973s test time: 0.562  loss = 1.032 val_mse = 1.288 mse = 1.328 mae = 0.841\n",
            "epoch15 train time: 4.991s test time: 0.551  loss = 1.006 val_mse = 1.278 mse = 1.322 mae = 0.838\n",
            "epoch16 train time: 4.967s test time: 0.556  loss = 0.984 val_mse = 1.262 mse = 1.314 mae = 0.838\n",
            "epoch17 train time: 4.980s test time: 0.562  loss = 0.964 val_mse = 1.253 mse = 1.311 mae = 0.837\n",
            "epoch18 train time: 4.964s test time: 0.556  loss = 0.946 val_mse = 1.241 mse = 1.306 mae = 0.838\n",
            "epoch19 train time: 4.959s test time: 0.563  loss = 0.930 val_mse = 1.233 mse = 1.304 mae = 0.839\n",
            "epoch20 train time: 4.971s test time: 0.564  loss = 0.919 val_mse = 1.231 mse = 1.302 mae = 0.839\n",
            "epoch21 train time: 4.961s test time: 0.553  loss = 0.907 val_mse = 1.222 mse = 1.301 mae = 0.841\n",
            "epoch22 train time: 5.009s test time: 0.556  loss = 0.895 val_mse = 1.213 mse = 1.295 mae = 0.839\n",
            "epoch23 train time: 4.991s test time: 0.558  loss = 0.889 val_mse = 1.208 mse = 1.298 mae = 0.844\n",
            "epoch24 train time: 4.982s test time: 0.554  loss = 0.885 val_mse = 1.208 mse = 1.298 mae = 0.845\n",
            "epoch25 train time: 4.982s test time: 0.562  loss = 0.883 val_mse = 1.202 mse = 1.292 mae = 0.847\n",
            "epoch26 train time: 4.978s test time: 0.563  loss = 0.878 val_mse = 1.223 mse = 1.306 mae = 0.848\n",
            "epoch27 train time: 4.983s test time: 0.559  loss = 0.877 val_mse = 1.208 mse = 1.301 mae = 0.851\n",
            "epoch28 train time: 4.976s test time: 0.553  loss = 0.869 val_mse = 1.210 mse = 1.291 mae = 0.844\n",
            "epoch29 train time: 4.964s test time: 0.558  loss = 0.864 val_mse = 1.206 mse = 1.298 mae = 0.846\n",
            "epoch30 train time: 4.971s test time: 0.558  loss = 0.860 val_mse = 1.207 mse = 1.297 mae = 0.848\n",
            "epoch31 train time: 4.964s test time: 0.565  loss = 0.858 val_mse = 1.221 mse = 1.307 mae = 0.839\n",
            "epoch32 train time: 4.955s test time: 0.573  loss = 0.857 val_mse = 1.233 mse = 1.319 mae = 0.838\n",
            "epoch33 train time: 4.977s test time: 0.555  loss = 0.858 val_mse = 1.244 mse = 1.330 mae = 0.837\n",
            "epoch34 train time: 4.992s test time: 0.555  loss = 0.851 val_mse = 1.224 mse = 1.315 mae = 0.839\n",
            "epoch35 train time: 4.982s test time: 0.555  loss = 0.844 val_mse = 1.256 mse = 1.331 mae = 0.833\n",
            "epoch36 train time: 4.959s test time: 0.559  loss = 0.847 val_mse = 1.237 mse = 1.325 mae = 0.835\n",
            "epoch37 train time: 4.968s test time: 0.558  loss = 0.849 val_mse = 1.261 mse = 1.334 mae = 0.835\n",
            "epoch38 train time: 4.965s test time: 0.565  loss = 0.843 val_mse = 1.255 mse = 1.323 mae = 0.830\n",
            "epoch39 train time: 4.979s test time: 0.559  loss = 0.844 val_mse = 1.238 mse = 1.308 mae = 0.833\n",
            "epoch40 train time: 4.962s test time: 0.569  loss = 0.841 val_mse = 1.242 mse = 1.328 mae = 0.834\n",
            "epoch41 train time: 4.976s test time: 0.564  loss = 0.837 val_mse = 1.264 mse = 1.329 mae = 0.831\n",
            "epoch42 train time: 4.981s test time: 0.564  loss = 0.836 val_mse = 1.218 mse = 1.309 mae = 0.839\n",
            "epoch43 train time: 4.979s test time: 0.565  loss = 0.837 val_mse = 1.245 mse = 1.319 mae = 0.833\n",
            "epoch44 train time: 4.974s test time: 0.557  loss = 0.831 val_mse = 1.224 mse = 1.316 mae = 0.837\n",
            "epoch45 train time: 4.976s test time: 0.553  loss = 0.831 val_mse = 1.242 mse = 1.322 mae = 0.835\n",
            "epoch46 train time: 5.001s test time: 0.563  loss = 0.833 val_mse = 1.218 mse = 1.308 mae = 0.835\n",
            "epoch47 train time: 4.964s test time: 0.561  loss = 0.831 val_mse = 1.238 mse = 1.310 mae = 0.834\n",
            "epoch48 train time: 4.960s test time: 0.553  loss = 0.827 val_mse = 1.225 mse = 1.316 mae = 0.835\n",
            "epoch49 train time: 4.976s test time: 0.568  loss = 0.825 val_mse = 1.252 mse = 1.327 mae = 0.833\n",
            "epoch50 train time: 4.991s test time: 0.564  loss = 0.824 val_mse = 1.221 mse = 1.311 mae = 0.837\n",
            "epoch51 train time: 4.973s test time: 0.561  loss = 0.827 val_mse = 1.239 mse = 1.316 mae = 0.833\n",
            "epoch52 train time: 4.988s test time: 0.559  loss = 0.826 val_mse = 1.225 mse = 1.314 mae = 0.835\n",
            "epoch53 train time: 4.982s test time: 0.568  loss = 0.823 val_mse = 1.230 mse = 1.319 mae = 0.833\n",
            "epoch54 train time: 4.992s test time: 0.555  loss = 0.821 val_mse = 1.232 mse = 1.312 mae = 0.833\n",
            "epoch55 train time: 4.980s test time: 0.564  loss = 0.818 val_mse = 1.241 mse = 1.323 mae = 0.833\n",
            "epoch56 train time: 4.986s test time: 0.559  loss = 0.823 val_mse = 1.220 mse = 1.314 mae = 0.837\n",
            "epoch57 train time: 4.986s test time: 0.555  loss = 0.824 val_mse = 1.244 mse = 1.321 mae = 0.833\n",
            "epoch58 train time: 4.978s test time: 0.558  loss = 0.821 val_mse = 1.221 mse = 1.304 mae = 0.831\n",
            "epoch59 train time: 4.978s test time: 0.556  loss = 0.816 val_mse = 1.244 mse = 1.325 mae = 0.835\n",
            "epoch60 train time: 4.980s test time: 0.562  loss = 0.819 val_mse = 1.221 mse = 1.313 mae = 0.835\n",
            "epoch61 train time: 4.968s test time: 0.562  loss = 0.818 val_mse = 1.231 mse = 1.316 mae = 0.835\n",
            "epoch62 train time: 4.976s test time: 0.571  loss = 0.814 val_mse = 1.217 mse = 1.310 mae = 0.835\n",
            "epoch63 train time: 4.977s test time: 0.554  loss = 0.817 val_mse = 1.233 mse = 1.312 mae = 0.833\n",
            "epoch64 train time: 4.994s test time: 0.556  loss = 0.814 val_mse = 1.211 mse = 1.311 mae = 0.838\n",
            "epoch65 train time: 4.976s test time: 0.555  loss = 0.815 val_mse = 1.232 mse = 1.316 mae = 0.833\n",
            "epoch66 train time: 4.985s test time: 0.563  loss = 0.810 val_mse = 1.211 mse = 1.312 mae = 0.834\n",
            "epoch67 train time: 4.970s test time: 0.562  loss = 0.814 val_mse = 1.231 mse = 1.307 mae = 0.830\n",
            "epoch68 train time: 4.976s test time: 0.563  loss = 0.811 val_mse = 1.207 mse = 1.311 mae = 0.836\n",
            "epoch69 train time: 4.989s test time: 0.564  loss = 0.810 val_mse = 1.231 mse = 1.310 mae = 0.836\n",
            "epoch70 train time: 4.974s test time: 0.561  loss = 0.811 val_mse = 1.199 mse = 1.299 mae = 0.833\n",
            "epoch71 train time: 4.982s test time: 0.569  loss = 0.808 val_mse = 1.233 mse = 1.314 mae = 0.831\n",
            "epoch72 train time: 4.977s test time: 0.553  loss = 0.808 val_mse = 1.213 mse = 1.304 mae = 0.833\n",
            "epoch73 train time: 4.972s test time: 0.558  loss = 0.808 val_mse = 1.224 mse = 1.309 mae = 0.834\n",
            "epoch74 train time: 4.964s test time: 0.561  loss = 0.807 val_mse = 1.204 mse = 1.304 mae = 0.833\n",
            "epoch75 train time: 4.993s test time: 0.557  loss = 0.806 val_mse = 1.224 mse = 1.301 mae = 0.832\n",
            "epoch76 train time: 4.972s test time: 0.558  loss = 0.804 val_mse = 1.201 mse = 1.303 mae = 0.835\n",
            "epoch77 train time: 4.966s test time: 0.563  loss = 0.801 val_mse = 1.224 mse = 1.306 mae = 0.832\n",
            "epoch78 train time: 4.966s test time: 0.559  loss = 0.803 val_mse = 1.206 mse = 1.308 mae = 0.834\n",
            "epoch79 train time: 4.973s test time: 0.565  loss = 0.798 val_mse = 1.220 mse = 1.304 mae = 0.831\n",
            "epoch80 train time: 4.993s test time: 0.570  loss = 0.804 val_mse = 1.204 mse = 1.301 mae = 0.836\n",
            "epoch81 train time: 4.997s test time: 0.557  loss = 0.801 val_mse = 1.225 mse = 1.312 mae = 0.829\n",
            "epoch82 train time: 4.993s test time: 0.562  loss = 0.796 val_mse = 1.199 mse = 1.297 mae = 0.834\n",
            "epoch83 train time: 4.975s test time: 0.564  loss = 0.798 val_mse = 1.219 mse = 1.302 mae = 0.833\n",
            "epoch84 train time: 4.983s test time: 0.555  loss = 0.799 val_mse = 1.200 mse = 1.296 mae = 0.833\n",
            "epoch85 train time: 4.989s test time: 0.559  loss = 0.796 val_mse = 1.229 mse = 1.318 mae = 0.832\n",
            "epoch86 train time: 4.983s test time: 0.557  loss = 0.799 val_mse = 1.213 mse = 1.305 mae = 0.836\n",
            "epoch87 train time: 4.959s test time: 0.564  loss = 0.795 val_mse = 1.221 mse = 1.307 mae = 0.829\n",
            "epoch88 train time: 4.972s test time: 0.567  loss = 0.790 val_mse = 1.202 mse = 1.298 mae = 0.835\n",
            "epoch89 train time: 4.973s test time: 0.559  loss = 0.788 val_mse = 1.215 mse = 1.299 mae = 0.832\n",
            "epoch90 train time: 4.978s test time: 0.557  loss = 0.787 val_mse = 1.200 mse = 1.304 mae = 0.835\n",
            "epoch91 train time: 4.980s test time: 0.560  loss = 0.786 val_mse = 1.207 mse = 1.302 mae = 0.834\n",
            "epoch92 train time: 4.965s test time: 0.559  loss = 0.790 val_mse = 1.210 mse = 1.304 mae = 0.831\n",
            "epoch93 train time: 4.977s test time: 0.562  loss = 0.787 val_mse = 1.218 mse = 1.301 mae = 0.832\n",
            "epoch94 train time: 4.984s test time: 0.558  loss = 0.786 val_mse = 1.198 mse = 1.299 mae = 0.832\n",
            "epoch95 train time: 4.978s test time: 0.559  loss = 0.783 val_mse = 1.209 mse = 1.298 mae = 0.832\n",
            "epoch96 train time: 4.983s test time: 0.552  loss = 0.788 val_mse = 1.188 mse = 1.295 mae = 0.833\n",
            "epoch97 train time: 4.986s test time: 0.553  loss = 0.781 val_mse = 1.208 mse = 1.302 mae = 0.832\n",
            "epoch98 train time: 4.980s test time: 0.551  loss = 0.782 val_mse = 1.206 mse = 1.302 mae = 0.835\n",
            "epoch99 train time: 4.980s test time: 0.554  loss = 0.789 val_mse = 1.204 mse = 1.302 mae = 0.835\n",
            "epoch100 train time: 4.971s test time: 0.558  loss = 0.787 val_mse = 1.191 mse = 1.296 mae = 0.835\n",
            "epoch101 train time: 4.975s test time: 0.558  loss = 0.785 val_mse = 1.223 mse = 1.307 mae = 0.828\n",
            "epoch102 train time: 4.985s test time: 0.556  loss = 0.780 val_mse = 1.198 mse = 1.300 mae = 0.833\n",
            "epoch103 train time: 4.957s test time: 0.556  loss = 0.783 val_mse = 1.201 mse = 1.295 mae = 0.832\n",
            "epoch104 train time: 4.983s test time: 0.559  loss = 0.781 val_mse = 1.193 mse = 1.298 mae = 0.831\n",
            "epoch105 train time: 4.968s test time: 0.563  loss = 0.784 val_mse = 1.208 mse = 1.303 mae = 0.832\n",
            "epoch106 train time: 4.976s test time: 0.562  loss = 0.783 val_mse = 1.199 mse = 1.300 mae = 0.830\n",
            "epoch107 train time: 4.969s test time: 0.560  loss = 0.782 val_mse = 1.215 mse = 1.304 mae = 0.832\n",
            "epoch108 train time: 4.988s test time: 0.556  loss = 0.784 val_mse = 1.190 mse = 1.295 mae = 0.831\n",
            "epoch109 train time: 4.973s test time: 0.564  loss = 0.780 val_mse = 1.204 mse = 1.292 mae = 0.830\n",
            "epoch110 train time: 4.972s test time: 0.560  loss = 0.781 val_mse = 1.196 mse = 1.296 mae = 0.832\n",
            "epoch111 train time: 4.976s test time: 0.557  loss = 0.783 val_mse = 1.203 mse = 1.295 mae = 0.831\n",
            "epoch112 train time: 4.989s test time: 0.563  loss = 0.780 val_mse = 1.193 mse = 1.295 mae = 0.832\n",
            "epoch113 train time: 4.975s test time: 0.556  loss = 0.783 val_mse = 1.209 mse = 1.299 mae = 0.830\n",
            "epoch114 train time: 4.981s test time: 0.559  loss = 0.782 val_mse = 1.193 mse = 1.297 mae = 0.832\n",
            "epoch115 train time: 4.983s test time: 0.563  loss = 0.779 val_mse = 1.206 mse = 1.299 mae = 0.831\n",
            "epoch116 train time: 4.971s test time: 0.559  loss = 0.781 val_mse = 1.189 mse = 1.293 mae = 0.834\n",
            "epoch117 train time: 4.959s test time: 0.560  loss = 0.781 val_mse = 1.203 mse = 1.300 mae = 0.830\n",
            "epoch118 train time: 4.984s test time: 0.567  loss = 0.778 val_mse = 1.193 mse = 1.289 mae = 0.832\n",
            "epoch119 train time: 4.971s test time: 0.553  loss = 0.780 val_mse = 1.204 mse = 1.299 mae = 0.831\n",
            "epoch120 train time: 4.972s test time: 0.563  loss = 0.781 val_mse = 1.188 mse = 1.291 mae = 0.831\n",
            "epoch121 train time: 4.981s test time: 0.558  loss = 0.780 val_mse = 1.201 mse = 1.289 mae = 0.828\n",
            "epoch122 train time: 4.990s test time: 0.558  loss = 0.782 val_mse = 1.197 mse = 1.296 mae = 0.830\n",
            "epoch123 train time: 4.976s test time: 0.553  loss = 0.781 val_mse = 1.208 mse = 1.295 mae = 0.827\n",
            "epoch124 train time: 4.971s test time: 0.556  loss = 0.780 val_mse = 1.189 mse = 1.292 mae = 0.832\n",
            "epoch125 train time: 4.987s test time: 0.568  loss = 0.782 val_mse = 1.204 mse = 1.300 mae = 0.831\n",
            "epoch126 train time: 4.983s test time: 0.566  loss = 0.780 val_mse = 1.193 mse = 1.295 mae = 0.828\n",
            "epoch127 train time: 4.988s test time: 0.572  loss = 0.784 val_mse = 1.199 mse = 1.295 mae = 0.831\n",
            "epoch128 train time: 4.969s test time: 0.558  loss = 0.783 val_mse = 1.191 mse = 1.294 mae = 0.829\n",
            "epoch129 train time: 4.978s test time: 0.559  loss = 0.779 val_mse = 1.202 mse = 1.297 mae = 0.829\n",
            "epoch130 train time: 4.968s test time: 0.569  loss = 0.780 val_mse = 1.183 mse = 1.287 mae = 0.830\n",
            "epoch131 train time: 4.986s test time: 0.554  loss = 0.779 val_mse = 1.205 mse = 1.292 mae = 0.828\n",
            "epoch132 train time: 4.966s test time: 0.563  loss = 0.779 val_mse = 1.185 mse = 1.296 mae = 0.831\n",
            "epoch133 train time: 4.973s test time: 0.560  loss = 0.777 val_mse = 1.205 mse = 1.303 mae = 0.831\n",
            "epoch134 train time: 4.973s test time: 0.556  loss = 0.781 val_mse = 1.189 mse = 1.294 mae = 0.829\n",
            "epoch135 train time: 4.991s test time: 0.554  loss = 0.778 val_mse = 1.198 mse = 1.294 mae = 0.829\n",
            "epoch136 train time: 4.980s test time: 0.569  loss = 0.781 val_mse = 1.186 mse = 1.292 mae = 0.832\n",
            "epoch137 train time: 4.992s test time: 0.560  loss = 0.778 val_mse = 1.196 mse = 1.296 mae = 0.830\n",
            "epoch138 train time: 4.969s test time: 0.557  loss = 0.778 val_mse = 1.185 mse = 1.290 mae = 0.829\n",
            "epoch139 train time: 4.991s test time: 0.561  loss = 0.779 val_mse = 1.196 mse = 1.297 mae = 0.832\n",
            "epoch140 train time: 4.981s test time: 0.555  loss = 0.784 val_mse = 1.193 mse = 1.295 mae = 0.831\n",
            "epoch141 train time: 4.981s test time: 0.560  loss = 0.783 val_mse = 1.207 mse = 1.297 mae = 0.827\n",
            "epoch142 train time: 4.983s test time: 0.556  loss = 0.778 val_mse = 1.189 mse = 1.293 mae = 0.830\n",
            "epoch143 train time: 4.987s test time: 0.555  loss = 0.780 val_mse = 1.196 mse = 1.287 mae = 0.828\n",
            "epoch144 train time: 4.969s test time: 0.559  loss = 0.781 val_mse = 1.186 mse = 1.294 mae = 0.829\n",
            "epoch145 train time: 4.972s test time: 0.574  loss = 0.777 val_mse = 1.194 mse = 1.297 mae = 0.831\n",
            "epoch146 train time: 4.968s test time: 0.559  loss = 0.778 val_mse = 1.187 mse = 1.289 mae = 0.829\n",
            "epoch147 train time: 4.986s test time: 0.558  loss = 0.778 val_mse = 1.204 mse = 1.295 mae = 0.828\n",
            "epoch148 train time: 4.982s test time: 0.562  loss = 0.782 val_mse = 1.188 mse = 1.290 mae = 0.829\n",
            "epoch149 train time: 4.973s test time: 0.551  loss = 0.778 val_mse = 1.202 mse = 1.290 mae = 0.828\n",
            "epoch150 train time: 4.972s test time: 0.559  loss = 0.782 val_mse = 1.193 mse = 1.293 mae = 0.830\n",
            "epoch151 train time: 4.978s test time: 0.558  loss = 0.781 val_mse = 1.197 mse = 1.293 mae = 0.829\n",
            "epoch152 train time: 4.969s test time: 0.563  loss = 0.778 val_mse = 1.185 mse = 1.295 mae = 0.827\n",
            "epoch153 train time: 4.972s test time: 0.560  loss = 0.776 val_mse = 1.197 mse = 1.293 mae = 0.828\n",
            "epoch154 train time: 4.980s test time: 0.569  loss = 0.776 val_mse = 1.181 mse = 1.290 mae = 0.830\n",
            "epoch155 train time: 4.974s test time: 0.554  loss = 0.777 val_mse = 1.192 mse = 1.291 mae = 0.828\n",
            "epoch156 train time: 4.995s test time: 0.560  loss = 0.776 val_mse = 1.179 mse = 1.289 mae = 0.829\n",
            "epoch157 train time: 4.987s test time: 0.564  loss = 0.774 val_mse = 1.192 mse = 1.293 mae = 0.831\n",
            "epoch158 train time: 4.976s test time: 0.557  loss = 0.777 val_mse = 1.177 mse = 1.290 mae = 0.829\n",
            "epoch159 train time: 4.975s test time: 0.556  loss = 0.775 val_mse = 1.193 mse = 1.289 mae = 0.828\n",
            "epoch160 train time: 4.984s test time: 0.562  loss = 0.776 val_mse = 1.184 mse = 1.288 mae = 0.831\n",
            "epoch161 train time: 4.977s test time: 0.559  loss = 0.777 val_mse = 1.195 mse = 1.287 mae = 0.828\n",
            "epoch162 train time: 4.967s test time: 0.557  loss = 0.776 val_mse = 1.192 mse = 1.296 mae = 0.826\n",
            "epoch163 train time: 4.976s test time: 0.562  loss = 0.774 val_mse = 1.196 mse = 1.293 mae = 0.831\n",
            "epoch164 train time: 4.973s test time: 0.559  loss = 0.776 val_mse = 1.178 mse = 1.282 mae = 0.828\n",
            "epoch165 train time: 4.977s test time: 0.565  loss = 0.775 val_mse = 1.190 mse = 1.291 mae = 0.829\n",
            "epoch166 train time: 4.987s test time: 0.558  loss = 0.778 val_mse = 1.188 mse = 1.286 mae = 0.830\n",
            "epoch167 train time: 4.992s test time: 0.551  loss = 0.777 val_mse = 1.192 mse = 1.292 mae = 0.828\n",
            "epoch168 train time: 4.977s test time: 0.559  loss = 0.773 val_mse = 1.186 mse = 1.293 mae = 0.828\n",
            "epoch169 train time: 4.990s test time: 0.565  loss = 0.773 val_mse = 1.191 mse = 1.289 mae = 0.829\n",
            "epoch170 train time: 4.978s test time: 0.559  loss = 0.777 val_mse = 1.180 mse = 1.284 mae = 0.828\n",
            "epoch171 train time: 4.968s test time: 0.566  loss = 0.777 val_mse = 1.195 mse = 1.292 mae = 0.829\n",
            "epoch172 train time: 4.983s test time: 0.559  loss = 0.776 val_mse = 1.179 mse = 1.288 mae = 0.828\n",
            "epoch173 train time: 4.952s test time: 0.562  loss = 0.773 val_mse = 1.195 mse = 1.289 mae = 0.828\n",
            "epoch174 train time: 4.968s test time: 0.552  loss = 0.775 val_mse = 1.184 mse = 1.286 mae = 0.828\n",
            "epoch175 train time: 4.969s test time: 0.554  loss = 0.772 val_mse = 1.189 mse = 1.285 mae = 0.827\n",
            "epoch176 train time: 4.996s test time: 0.550  loss = 0.772 val_mse = 1.181 mse = 1.291 mae = 0.829\n",
            "epoch177 train time: 4.971s test time: 0.560  loss = 0.773 val_mse = 1.189 mse = 1.283 mae = 0.828\n",
            "epoch178 train time: 4.979s test time: 0.560  loss = 0.774 val_mse = 1.188 mse = 1.292 mae = 0.827\n",
            "epoch179 train time: 4.971s test time: 0.553  loss = 0.770 val_mse = 1.190 mse = 1.291 mae = 0.829\n",
            "epoch180 train time: 4.978s test time: 0.554  loss = 0.773 val_mse = 1.184 mse = 1.291 mae = 0.828\n",
            "epoch181 train time: 4.967s test time: 0.558  loss = 0.775 val_mse = 1.193 mse = 1.295 mae = 0.829\n",
            "epoch182 train time: 4.986s test time: 0.561  loss = 0.774 val_mse = 1.180 mse = 1.281 mae = 0.827\n",
            "epoch183 train time: 4.991s test time: 0.572  loss = 0.773 val_mse = 1.189 mse = 1.286 mae = 0.828\n",
            "epoch184 train time: 4.977s test time: 0.557  loss = 0.775 val_mse = 1.180 mse = 1.291 mae = 0.828\n",
            "epoch185 train time: 4.977s test time: 0.551  loss = 0.771 val_mse = 1.188 mse = 1.286 mae = 0.826\n",
            "epoch186 train time: 4.974s test time: 0.572  loss = 0.772 val_mse = 1.180 mse = 1.282 mae = 0.828\n",
            "epoch187 train time: 4.988s test time: 0.556  loss = 0.773 val_mse = 1.191 mse = 1.294 mae = 0.827\n",
            "epoch188 train time: 4.982s test time: 0.553  loss = 0.771 val_mse = 1.178 mse = 1.287 mae = 0.830\n",
            "epoch189 train time: 4.978s test time: 0.559  loss = 0.771 val_mse = 1.187 mse = 1.292 mae = 0.827\n",
            "epoch190 train time: 4.969s test time: 0.555  loss = 0.769 val_mse = 1.182 mse = 1.288 mae = 0.828\n",
            "epoch191 train time: 4.967s test time: 0.563  loss = 0.769 val_mse = 1.182 mse = 1.285 mae = 0.828\n",
            "epoch192 train time: 4.950s test time: 0.552  loss = 0.768 val_mse = 1.177 mse = 1.291 mae = 0.829\n",
            "epoch193 train time: 4.980s test time: 0.551  loss = 0.767 val_mse = 1.186 mse = 1.289 mae = 0.827\n",
            "epoch194 train time: 4.981s test time: 0.557  loss = 0.769 val_mse = 1.184 mse = 1.292 mae = 0.831\n",
            "epoch195 train time: 4.979s test time: 0.566  loss = 0.770 val_mse = 1.185 mse = 1.292 mae = 0.828\n",
            "epoch196 train time: 4.992s test time: 0.556  loss = 0.768 val_mse = 1.175 mse = 1.291 mae = 0.830\n",
            "epoch197 train time: 4.982s test time: 0.552  loss = 0.767 val_mse = 1.191 mse = 1.293 mae = 0.828\n",
            "epoch198 train time: 4.968s test time: 0.563  loss = 0.767 val_mse = 1.176 mse = 1.290 mae = 0.829\n",
            "epoch199 train time: 4.959s test time: 0.557  loss = 0.765 val_mse = 1.182 mse = 1.287 mae = 0.827\n",
            "epoch200 train time: 4.988s test time: 0.556  loss = 0.765 val_mse = 1.184 mse = 1.292 mae = 0.828\n",
            "epoch201 train time: 4.984s test time: 0.552  loss = 0.767 val_mse = 1.189 mse = 1.289 mae = 0.828\n",
            "epoch202 train time: 4.975s test time: 0.552  loss = 0.765 val_mse = 1.180 mse = 1.291 mae = 0.831\n",
            "epoch203 train time: 4.971s test time: 0.554  loss = 0.766 val_mse = 1.181 mse = 1.283 mae = 0.828\n",
            "epoch204 train time: 4.971s test time: 0.561  loss = 0.767 val_mse = 1.179 mse = 1.289 mae = 0.828\n",
            "epoch205 train time: 4.982s test time: 0.556  loss = 0.765 val_mse = 1.189 mse = 1.289 mae = 0.829\n",
            "epoch206 train time: 4.962s test time: 0.569  loss = 0.767 val_mse = 1.185 mse = 1.295 mae = 0.830\n",
            "epoch207 train time: 4.969s test time: 0.561  loss = 0.766 val_mse = 1.186 mse = 1.289 mae = 0.828\n",
            "epoch208 train time: 4.960s test time: 0.553  loss = 0.768 val_mse = 1.180 mse = 1.285 mae = 0.830\n",
            "epoch209 train time: 4.969s test time: 0.555  loss = 0.766 val_mse = 1.191 mse = 1.293 mae = 0.829\n",
            "epoch210 train time: 4.978s test time: 0.572  loss = 0.769 val_mse = 1.176 mse = 1.285 mae = 0.830\n",
            "epoch211 train time: 5.003s test time: 0.552  loss = 0.765 val_mse = 1.181 mse = 1.285 mae = 0.828\n",
            "epoch212 train time: 4.977s test time: 0.568  loss = 0.763 val_mse = 1.177 mse = 1.287 mae = 0.830\n",
            "epoch213 train time: 4.957s test time: 0.555  loss = 0.763 val_mse = 1.182 mse = 1.288 mae = 0.829\n",
            "epoch214 train time: 4.986s test time: 0.553  loss = 0.766 val_mse = 1.178 mse = 1.283 mae = 0.830\n",
            "epoch215 train time: 4.956s test time: 0.562  loss = 0.765 val_mse = 1.187 mse = 1.290 mae = 0.827\n",
            "epoch216 train time: 4.986s test time: 0.556  loss = 0.765 val_mse = 1.176 mse = 1.286 mae = 0.830\n",
            "epoch217 train time: 4.966s test time: 0.552  loss = 0.767 val_mse = 1.185 mse = 1.286 mae = 0.828\n",
            "epoch218 train time: 4.978s test time: 0.563  loss = 0.767 val_mse = 1.180 mse = 1.287 mae = 0.831\n",
            "epoch219 train time: 4.975s test time: 0.554  loss = 0.764 val_mse = 1.181 mse = 1.292 mae = 0.828\n",
            "epoch220 train time: 4.979s test time: 0.559  loss = 0.766 val_mse = 1.178 mse = 1.281 mae = 0.829\n",
            "epoch221 train time: 4.976s test time: 0.553  loss = 0.766 val_mse = 1.188 mse = 1.289 mae = 0.828\n",
            "epoch222 train time: 4.980s test time: 0.558  loss = 0.765 val_mse = 1.176 mse = 1.285 mae = 0.828\n",
            "epoch223 train time: 4.978s test time: 0.553  loss = 0.763 val_mse = 1.183 mse = 1.286 mae = 0.828\n",
            "epoch224 train time: 4.976s test time: 0.561  loss = 0.764 val_mse = 1.179 mse = 1.288 mae = 0.828\n",
            "epoch225 train time: 4.979s test time: 0.555  loss = 0.765 val_mse = 1.183 mse = 1.285 mae = 0.828\n",
            "epoch226 train time: 4.971s test time: 0.551  loss = 0.767 val_mse = 1.175 mse = 1.290 mae = 0.831\n",
            "epoch227 train time: 4.974s test time: 0.557  loss = 0.766 val_mse = 1.184 mse = 1.289 mae = 0.827\n",
            "epoch228 train time: 4.956s test time: 0.552  loss = 0.764 val_mse = 1.171 mse = 1.288 mae = 0.831\n",
            "epoch229 train time: 4.970s test time: 0.568  loss = 0.764 val_mse = 1.187 mse = 1.287 mae = 0.826\n",
            "epoch230 train time: 4.947s test time: 0.561  loss = 0.767 val_mse = 1.181 mse = 1.288 mae = 0.828\n",
            "epoch231 train time: 4.963s test time: 0.558  loss = 0.764 val_mse = 1.184 mse = 1.287 mae = 0.826\n",
            "epoch232 train time: 4.977s test time: 0.562  loss = 0.765 val_mse = 1.179 mse = 1.286 mae = 0.826\n",
            "epoch233 train time: 4.977s test time: 0.562  loss = 0.765 val_mse = 1.180 mse = 1.286 mae = 0.828\n",
            "epoch234 train time: 4.981s test time: 0.552  loss = 0.763 val_mse = 1.173 mse = 1.285 mae = 0.830\n",
            "epoch235 train time: 4.976s test time: 0.559  loss = 0.763 val_mse = 1.184 mse = 1.293 mae = 0.827\n",
            "epoch236 train time: 4.986s test time: 0.560  loss = 0.766 val_mse = 1.182 mse = 1.287 mae = 0.831\n",
            "epoch237 train time: 4.969s test time: 0.567  loss = 0.766 val_mse = 1.181 mse = 1.287 mae = 0.828\n",
            "epoch238 train time: 4.978s test time: 0.561  loss = 0.766 val_mse = 1.178 mse = 1.292 mae = 0.831\n",
            "epoch239 train time: 4.992s test time: 0.562  loss = 0.762 val_mse = 1.178 mse = 1.287 mae = 0.826\n",
            "MAE 0.8332786701043388\n",
            "MSE 1.3050463728962762\n",
            "epoch0 train time: 5.428s test time: 0.553  loss = 191.796 val_mse = 1.563 mse = 1.506 mae = 0.887\n",
            "epoch1 train time: 4.959s test time: 0.560  loss = 121.804 val_mse = 1.563 mse = 1.504 mae = 0.877\n",
            "epoch2 train time: 4.933s test time: 0.560  loss = 59.916 val_mse = 1.556 mse = 1.499 mae = 0.875\n",
            "epoch3 train time: 4.958s test time: 0.553  loss = 26.017 val_mse = 1.548 mse = 1.493 mae = 0.874\n",
            "epoch4 train time: 4.956s test time: 0.558  loss = 9.768 val_mse = 1.540 mse = 1.486 mae = 0.873\n",
            "epoch5 train time: 4.947s test time: 0.566  loss = 3.436 val_mse = 1.530 mse = 1.479 mae = 0.871\n",
            "epoch6 train time: 4.969s test time: 0.556  loss = 1.742 val_mse = 1.519 mse = 1.472 mae = 0.869\n",
            "epoch7 train time: 4.959s test time: 0.558  loss = 1.496 val_mse = 1.508 mse = 1.464 mae = 0.868\n",
            "epoch8 train time: 4.951s test time: 0.563  loss = 1.415 val_mse = 1.497 mse = 1.456 mae = 0.867\n",
            "epoch9 train time: 4.915s test time: 0.557  loss = 1.351 val_mse = 1.486 mse = 1.448 mae = 0.865\n",
            "epoch10 train time: 4.946s test time: 0.567  loss = 1.300 val_mse = 1.474 mse = 1.440 mae = 0.864\n",
            "epoch11 train time: 4.942s test time: 0.568  loss = 1.257 val_mse = 1.462 mse = 1.432 mae = 0.863\n",
            "epoch12 train time: 4.932s test time: 0.557  loss = 1.220 val_mse = 1.450 mse = 1.424 mae = 0.862\n",
            "epoch13 train time: 4.938s test time: 0.558  loss = 1.188 val_mse = 1.438 mse = 1.417 mae = 0.861\n",
            "epoch14 train time: 4.928s test time: 0.561  loss = 1.161 val_mse = 1.427 mse = 1.409 mae = 0.860\n",
            "epoch15 train time: 4.931s test time: 0.557  loss = 1.137 val_mse = 1.416 mse = 1.402 mae = 0.859\n",
            "epoch16 train time: 4.922s test time: 0.556  loss = 1.115 val_mse = 1.405 mse = 1.396 mae = 0.857\n",
            "epoch17 train time: 4.931s test time: 0.566  loss = 1.096 val_mse = 1.395 mse = 1.389 mae = 0.856\n",
            "epoch18 train time: 4.932s test time: 0.572  loss = 1.078 val_mse = 1.384 mse = 1.383 mae = 0.855\n",
            "epoch19 train time: 4.928s test time: 0.552  loss = 1.062 val_mse = 1.375 mse = 1.378 mae = 0.854\n",
            "epoch20 train time: 4.940s test time: 0.555  loss = 1.047 val_mse = 1.366 mse = 1.373 mae = 0.852\n",
            "epoch21 train time: 4.935s test time: 0.557  loss = 1.033 val_mse = 1.357 mse = 1.368 mae = 0.850\n",
            "epoch22 train time: 4.949s test time: 0.558  loss = 1.020 val_mse = 1.350 mse = 1.365 mae = 0.849\n",
            "epoch23 train time: 4.947s test time: 0.567  loss = 1.008 val_mse = 1.343 mse = 1.361 mae = 0.847\n",
            "epoch24 train time: 4.954s test time: 0.567  loss = 0.996 val_mse = 1.336 mse = 1.359 mae = 0.845\n",
            "epoch25 train time: 4.943s test time: 0.569  loss = 0.986 val_mse = 1.330 mse = 1.356 mae = 0.843\n",
            "epoch26 train time: 4.946s test time: 0.559  loss = 0.976 val_mse = 1.323 mse = 1.352 mae = 0.842\n",
            "epoch27 train time: 4.935s test time: 0.552  loss = 0.966 val_mse = 1.317 mse = 1.350 mae = 0.840\n",
            "epoch28 train time: 4.951s test time: 0.558  loss = 0.957 val_mse = 1.312 mse = 1.347 mae = 0.838\n",
            "epoch29 train time: 4.956s test time: 0.555  loss = 0.948 val_mse = 1.307 mse = 1.346 mae = 0.837\n",
            "epoch30 train time: 4.934s test time: 0.556  loss = 0.941 val_mse = 1.302 mse = 1.344 mae = 0.835\n",
            "epoch31 train time: 4.927s test time: 0.555  loss = 0.934 val_mse = 1.298 mse = 1.343 mae = 0.834\n",
            "epoch32 train time: 4.940s test time: 0.569  loss = 0.927 val_mse = 1.287 mse = 1.336 mae = 0.833\n",
            "epoch33 train time: 4.935s test time: 0.557  loss = 0.921 val_mse = 1.288 mse = 1.339 mae = 0.833\n",
            "epoch34 train time: 4.942s test time: 0.566  loss = 0.916 val_mse = 1.283 mse = 1.336 mae = 0.832\n",
            "epoch35 train time: 4.941s test time: 0.565  loss = 0.911 val_mse = 1.278 mse = 1.334 mae = 0.833\n",
            "epoch36 train time: 4.944s test time: 0.559  loss = 0.907 val_mse = 1.274 mse = 1.332 mae = 0.832\n",
            "epoch37 train time: 4.941s test time: 0.555  loss = 0.902 val_mse = 1.272 mse = 1.330 mae = 0.832\n",
            "epoch38 train time: 4.947s test time: 0.561  loss = 0.899 val_mse = 1.280 mse = 1.341 mae = 0.832\n",
            "epoch39 train time: 4.962s test time: 0.569  loss = 0.897 val_mse = 1.270 mse = 1.330 mae = 0.831\n",
            "epoch40 train time: 4.953s test time: 0.566  loss = 0.896 val_mse = 1.268 mse = 1.332 mae = 0.832\n",
            "epoch41 train time: 4.972s test time: 0.558  loss = 0.893 val_mse = 1.265 mse = 1.324 mae = 0.830\n",
            "epoch42 train time: 4.931s test time: 0.567  loss = 0.892 val_mse = 1.256 mse = 1.322 mae = 0.829\n",
            "epoch43 train time: 4.940s test time: 0.557  loss = 0.891 val_mse = 1.268 mse = 1.322 mae = 0.831\n",
            "epoch44 train time: 4.934s test time: 0.555  loss = 0.891 val_mse = 1.252 mse = 1.322 mae = 0.830\n",
            "epoch45 train time: 4.949s test time: 0.558  loss = 0.888 val_mse = 1.270 mse = 1.327 mae = 0.831\n",
            "epoch46 train time: 4.941s test time: 0.560  loss = 0.888 val_mse = 1.251 mse = 1.319 mae = 0.831\n",
            "epoch47 train time: 4.944s test time: 0.561  loss = 0.890 val_mse = 1.255 mse = 1.319 mae = 0.833\n",
            "epoch48 train time: 4.939s test time: 0.561  loss = 0.885 val_mse = 1.248 mse = 1.319 mae = 0.832\n",
            "epoch49 train time: 4.956s test time: 0.551  loss = 0.887 val_mse = 1.257 mse = 1.321 mae = 0.836\n",
            "epoch50 train time: 4.944s test time: 0.569  loss = 0.887 val_mse = 1.240 mse = 1.313 mae = 0.833\n",
            "epoch51 train time: 4.957s test time: 0.557  loss = 0.887 val_mse = 1.259 mse = 1.322 mae = 0.839\n",
            "epoch52 train time: 4.942s test time: 0.557  loss = 0.888 val_mse = 1.234 mse = 1.310 mae = 0.833\n",
            "epoch53 train time: 4.956s test time: 0.556  loss = 0.886 val_mse = 1.252 mse = 1.318 mae = 0.838\n",
            "epoch54 train time: 4.942s test time: 0.556  loss = 0.882 val_mse = 1.250 mse = 1.318 mae = 0.833\n",
            "epoch55 train time: 4.939s test time: 0.553  loss = 0.885 val_mse = 1.263 mse = 1.325 mae = 0.839\n",
            "epoch56 train time: 4.942s test time: 0.562  loss = 0.884 val_mse = 1.237 mse = 1.318 mae = 0.832\n",
            "epoch57 train time: 4.944s test time: 0.554  loss = 0.882 val_mse = 1.268 mse = 1.328 mae = 0.842\n",
            "epoch58 train time: 4.945s test time: 0.558  loss = 0.890 val_mse = 1.238 mse = 1.315 mae = 0.834\n",
            "epoch59 train time: 4.949s test time: 0.555  loss = 0.882 val_mse = 1.262 mse = 1.322 mae = 0.841\n",
            "epoch60 train time: 4.935s test time: 0.557  loss = 0.883 val_mse = 1.230 mse = 1.314 mae = 0.836\n",
            "epoch61 train time: 4.957s test time: 0.562  loss = 0.886 val_mse = 1.268 mse = 1.326 mae = 0.844\n",
            "epoch62 train time: 4.954s test time: 0.565  loss = 0.884 val_mse = 1.234 mse = 1.317 mae = 0.838\n",
            "epoch63 train time: 4.939s test time: 0.554  loss = 0.882 val_mse = 1.260 mse = 1.321 mae = 0.841\n",
            "epoch64 train time: 4.955s test time: 0.561  loss = 0.883 val_mse = 1.229 mse = 1.308 mae = 0.834\n",
            "epoch65 train time: 4.931s test time: 0.559  loss = 0.878 val_mse = 1.261 mse = 1.323 mae = 0.842\n",
            "epoch66 train time: 4.933s test time: 0.557  loss = 0.878 val_mse = 1.239 mse = 1.318 mae = 0.838\n",
            "epoch67 train time: 4.943s test time: 0.555  loss = 0.884 val_mse = 1.265 mse = 1.328 mae = 0.845\n",
            "epoch68 train time: 4.955s test time: 0.557  loss = 0.883 val_mse = 1.228 mse = 1.306 mae = 0.836\n",
            "epoch69 train time: 4.950s test time: 0.560  loss = 0.877 val_mse = 1.260 mse = 1.325 mae = 0.842\n",
            "epoch70 train time: 4.951s test time: 0.560  loss = 0.877 val_mse = 1.228 mse = 1.311 mae = 0.836\n",
            "epoch71 train time: 4.943s test time: 0.561  loss = 0.882 val_mse = 1.259 mse = 1.326 mae = 0.844\n",
            "epoch72 train time: 4.927s test time: 0.565  loss = 0.881 val_mse = 1.221 mse = 1.303 mae = 0.835\n",
            "epoch73 train time: 4.931s test time: 0.554  loss = 0.877 val_mse = 1.261 mse = 1.328 mae = 0.843\n",
            "epoch74 train time: 4.946s test time: 0.558  loss = 0.876 val_mse = 1.234 mse = 1.310 mae = 0.836\n",
            "epoch75 train time: 4.948s test time: 0.553  loss = 0.875 val_mse = 1.253 mse = 1.324 mae = 0.844\n",
            "epoch76 train time: 4.931s test time: 0.558  loss = 0.877 val_mse = 1.228 mse = 1.309 mae = 0.834\n",
            "epoch77 train time: 4.966s test time: 0.557  loss = 0.874 val_mse = 1.258 mse = 1.325 mae = 0.845\n",
            "epoch78 train time: 4.950s test time: 0.558  loss = 0.876 val_mse = 1.221 mse = 1.303 mae = 0.836\n",
            "epoch79 train time: 4.958s test time: 0.554  loss = 0.875 val_mse = 1.256 mse = 1.322 mae = 0.839\n",
            "epoch80 train time: 4.955s test time: 0.559  loss = 0.872 val_mse = 1.226 mse = 1.310 mae = 0.838\n",
            "epoch81 train time: 4.956s test time: 0.559  loss = 0.873 val_mse = 1.258 mse = 1.327 mae = 0.847\n",
            "epoch82 train time: 5.153s test time: 0.564  loss = 0.877 val_mse = 1.216 mse = 1.299 mae = 0.835\n",
            "epoch83 train time: 4.948s test time: 0.558  loss = 0.870 val_mse = 1.253 mse = 1.326 mae = 0.843\n",
            "epoch84 train time: 4.953s test time: 0.559  loss = 0.869 val_mse = 1.228 mse = 1.309 mae = 0.835\n",
            "epoch85 train time: 4.938s test time: 0.561  loss = 0.872 val_mse = 1.254 mse = 1.324 mae = 0.844\n",
            "epoch86 train time: 4.941s test time: 0.558  loss = 0.868 val_mse = 1.219 mse = 1.304 mae = 0.837\n",
            "epoch87 train time: 4.927s test time: 0.554  loss = 0.872 val_mse = 1.252 mse = 1.325 mae = 0.844\n",
            "epoch88 train time: 4.933s test time: 0.560  loss = 0.871 val_mse = 1.218 mse = 1.302 mae = 0.834\n",
            "epoch89 train time: 4.928s test time: 0.554  loss = 0.863 val_mse = 1.256 mse = 1.322 mae = 0.844\n",
            "epoch90 train time: 4.938s test time: 0.559  loss = 0.870 val_mse = 1.224 mse = 1.304 mae = 0.835\n",
            "epoch91 train time: 4.943s test time: 0.553  loss = 0.865 val_mse = 1.257 mse = 1.332 mae = 0.845\n",
            "epoch92 train time: 4.962s test time: 0.563  loss = 0.871 val_mse = 1.216 mse = 1.304 mae = 0.837\n",
            "epoch93 train time: 4.950s test time: 0.562  loss = 0.864 val_mse = 1.248 mse = 1.320 mae = 0.841\n",
            "epoch94 train time: 4.940s test time: 0.565  loss = 0.866 val_mse = 1.219 mse = 1.301 mae = 0.836\n",
            "epoch95 train time: 4.953s test time: 0.557  loss = 0.864 val_mse = 1.249 mse = 1.318 mae = 0.840\n",
            "epoch96 train time: 4.950s test time: 0.552  loss = 0.862 val_mse = 1.219 mse = 1.308 mae = 0.835\n",
            "epoch97 train time: 4.949s test time: 0.557  loss = 0.867 val_mse = 1.248 mse = 1.321 mae = 0.842\n",
            "epoch98 train time: 4.923s test time: 0.552  loss = 0.864 val_mse = 1.217 mse = 1.301 mae = 0.835\n",
            "epoch99 train time: 4.936s test time: 0.561  loss = 0.859 val_mse = 1.250 mse = 1.318 mae = 0.844\n",
            "epoch100 train time: 4.947s test time: 0.562  loss = 0.864 val_mse = 1.218 mse = 1.301 mae = 0.834\n",
            "epoch101 train time: 4.935s test time: 0.555  loss = 0.864 val_mse = 1.246 mse = 1.320 mae = 0.844\n",
            "epoch102 train time: 4.941s test time: 0.579  loss = 0.864 val_mse = 1.216 mse = 1.299 mae = 0.835\n",
            "epoch103 train time: 4.935s test time: 0.553  loss = 0.861 val_mse = 1.252 mse = 1.327 mae = 0.842\n",
            "epoch104 train time: 4.948s test time: 0.554  loss = 0.860 val_mse = 1.217 mse = 1.303 mae = 0.837\n",
            "epoch105 train time: 4.936s test time: 0.575  loss = 0.863 val_mse = 1.240 mse = 1.319 mae = 0.843\n",
            "epoch106 train time: 4.958s test time: 0.556  loss = 0.861 val_mse = 1.211 mse = 1.297 mae = 0.835\n",
            "epoch107 train time: 4.932s test time: 0.567  loss = 0.858 val_mse = 1.241 mse = 1.319 mae = 0.843\n",
            "epoch108 train time: 4.949s test time: 0.562  loss = 0.861 val_mse = 1.211 mse = 1.302 mae = 0.834\n",
            "epoch109 train time: 4.951s test time: 0.556  loss = 0.859 val_mse = 1.247 mse = 1.320 mae = 0.840\n",
            "epoch110 train time: 4.953s test time: 0.566  loss = 0.859 val_mse = 1.214 mse = 1.299 mae = 0.836\n",
            "epoch111 train time: 4.946s test time: 0.556  loss = 0.859 val_mse = 1.243 mse = 1.316 mae = 0.841\n",
            "epoch112 train time: 4.937s test time: 0.554  loss = 0.858 val_mse = 1.206 mse = 1.295 mae = 0.834\n",
            "epoch113 train time: 4.938s test time: 0.571  loss = 0.858 val_mse = 1.237 mse = 1.315 mae = 0.841\n",
            "epoch114 train time: 4.942s test time: 0.554  loss = 0.859 val_mse = 1.213 mse = 1.301 mae = 0.833\n",
            "epoch115 train time: 4.940s test time: 0.567  loss = 0.854 val_mse = 1.244 mse = 1.320 mae = 0.841\n",
            "epoch116 train time: 4.945s test time: 0.554  loss = 0.856 val_mse = 1.208 mse = 1.299 mae = 0.834\n",
            "epoch117 train time: 4.930s test time: 0.552  loss = 0.855 val_mse = 1.240 mse = 1.317 mae = 0.841\n",
            "epoch118 train time: 4.941s test time: 0.573  loss = 0.859 val_mse = 1.207 mse = 1.294 mae = 0.834\n",
            "epoch119 train time: 4.963s test time: 0.558  loss = 0.853 val_mse = 1.240 mse = 1.317 mae = 0.841\n",
            "epoch120 train time: 4.937s test time: 0.554  loss = 0.855 val_mse = 1.211 mse = 1.301 mae = 0.833\n",
            "epoch121 train time: 4.940s test time: 0.560  loss = 0.853 val_mse = 1.244 mse = 1.315 mae = 0.842\n",
            "epoch122 train time: 4.942s test time: 0.566  loss = 0.855 val_mse = 1.207 mse = 1.298 mae = 0.836\n",
            "epoch123 train time: 4.954s test time: 0.562  loss = 0.857 val_mse = 1.239 mse = 1.316 mae = 0.842\n",
            "epoch124 train time: 4.952s test time: 0.559  loss = 0.860 val_mse = 1.201 mse = 1.295 mae = 0.833\n",
            "epoch125 train time: 4.940s test time: 0.558  loss = 0.851 val_mse = 1.235 mse = 1.318 mae = 0.843\n",
            "epoch126 train time: 4.932s test time: 0.556  loss = 0.851 val_mse = 1.211 mse = 1.298 mae = 0.832\n",
            "epoch127 train time: 4.950s test time: 0.562  loss = 0.854 val_mse = 1.234 mse = 1.315 mae = 0.842\n",
            "epoch128 train time: 4.941s test time: 0.557  loss = 0.854 val_mse = 1.203 mse = 1.294 mae = 0.834\n",
            "epoch129 train time: 4.937s test time: 0.560  loss = 0.855 val_mse = 1.241 mse = 1.321 mae = 0.843\n",
            "epoch130 train time: 4.939s test time: 0.556  loss = 0.854 val_mse = 1.205 mse = 1.295 mae = 0.831\n",
            "epoch131 train time: 4.938s test time: 0.561  loss = 0.850 val_mse = 1.234 mse = 1.314 mae = 0.841\n",
            "epoch132 train time: 4.946s test time: 0.558  loss = 0.855 val_mse = 1.205 mse = 1.293 mae = 0.832\n",
            "epoch133 train time: 4.966s test time: 0.561  loss = 0.852 val_mse = 1.232 mse = 1.312 mae = 0.841\n",
            "epoch134 train time: 4.953s test time: 0.568  loss = 0.851 val_mse = 1.203 mse = 1.294 mae = 0.833\n",
            "epoch135 train time: 4.940s test time: 0.559  loss = 0.851 val_mse = 1.231 mse = 1.310 mae = 0.839\n",
            "epoch136 train time: 4.959s test time: 0.559  loss = 0.852 val_mse = 1.202 mse = 1.294 mae = 0.833\n",
            "epoch137 train time: 4.957s test time: 0.574  loss = 0.854 val_mse = 1.233 mse = 1.313 mae = 0.844\n",
            "epoch138 train time: 4.961s test time: 0.567  loss = 0.851 val_mse = 1.208 mse = 1.299 mae = 0.833\n",
            "epoch139 train time: 4.957s test time: 0.561  loss = 0.851 val_mse = 1.239 mse = 1.319 mae = 0.840\n",
            "epoch140 train time: 4.943s test time: 0.559  loss = 0.852 val_mse = 1.208 mse = 1.292 mae = 0.832\n",
            "epoch141 train time: 4.938s test time: 0.563  loss = 0.850 val_mse = 1.232 mse = 1.315 mae = 0.841\n",
            "epoch142 train time: 4.945s test time: 0.558  loss = 0.848 val_mse = 1.206 mse = 1.295 mae = 0.834\n",
            "epoch143 train time: 4.944s test time: 0.554  loss = 0.853 val_mse = 1.234 mse = 1.313 mae = 0.843\n",
            "epoch144 train time: 4.946s test time: 0.558  loss = 0.856 val_mse = 1.203 mse = 1.294 mae = 0.832\n",
            "epoch145 train time: 4.940s test time: 0.568  loss = 0.849 val_mse = 1.231 mse = 1.313 mae = 0.840\n",
            "epoch146 train time: 4.941s test time: 0.561  loss = 0.852 val_mse = 1.209 mse = 1.294 mae = 0.830\n",
            "epoch147 train time: 4.935s test time: 0.560  loss = 0.849 val_mse = 1.236 mse = 1.315 mae = 0.841\n",
            "epoch148 train time: 4.939s test time: 0.562  loss = 0.851 val_mse = 1.204 mse = 1.295 mae = 0.832\n",
            "epoch149 train time: 4.952s test time: 0.556  loss = 0.852 val_mse = 1.231 mse = 1.312 mae = 0.842\n",
            "epoch150 train time: 4.959s test time: 0.558  loss = 0.851 val_mse = 1.206 mse = 1.296 mae = 0.831\n",
            "epoch151 train time: 4.946s test time: 0.563  loss = 0.849 val_mse = 1.237 mse = 1.316 mae = 0.841\n",
            "epoch152 train time: 4.960s test time: 0.561  loss = 0.853 val_mse = 1.205 mse = 1.291 mae = 0.833\n",
            "epoch153 train time: 4.947s test time: 0.561  loss = 0.850 val_mse = 1.228 mse = 1.314 mae = 0.837\n",
            "epoch154 train time: 4.953s test time: 0.561  loss = 0.849 val_mse = 1.207 mse = 1.296 mae = 0.831\n",
            "epoch155 train time: 4.943s test time: 0.558  loss = 0.849 val_mse = 1.231 mse = 1.314 mae = 0.840\n",
            "epoch156 train time: 4.952s test time: 0.556  loss = 0.851 val_mse = 1.206 mse = 1.288 mae = 0.831\n",
            "epoch157 train time: 4.926s test time: 0.564  loss = 0.849 val_mse = 1.230 mse = 1.314 mae = 0.843\n",
            "epoch158 train time: 4.935s test time: 0.552  loss = 0.851 val_mse = 1.207 mse = 1.293 mae = 0.831\n",
            "epoch159 train time: 4.952s test time: 0.557  loss = 0.850 val_mse = 1.235 mse = 1.311 mae = 0.838\n",
            "epoch160 train time: 4.939s test time: 0.566  loss = 0.851 val_mse = 1.214 mse = 1.301 mae = 0.834\n",
            "epoch161 train time: 4.937s test time: 0.566  loss = 0.849 val_mse = 1.233 mse = 1.314 mae = 0.840\n",
            "epoch162 train time: 4.949s test time: 0.560  loss = 0.849 val_mse = 1.210 mse = 1.294 mae = 0.830\n",
            "epoch163 train time: 4.923s test time: 0.566  loss = 0.850 val_mse = 1.229 mse = 1.309 mae = 0.843\n",
            "epoch164 train time: 4.966s test time: 0.565  loss = 0.851 val_mse = 1.206 mse = 1.296 mae = 0.835\n",
            "epoch165 train time: 4.953s test time: 0.555  loss = 0.849 val_mse = 1.238 mse = 1.314 mae = 0.840\n",
            "epoch166 train time: 4.947s test time: 0.557  loss = 0.852 val_mse = 1.208 mse = 1.294 mae = 0.833\n",
            "epoch167 train time: 4.947s test time: 0.568  loss = 0.849 val_mse = 1.233 mse = 1.314 mae = 0.841\n",
            "epoch168 train time: 4.932s test time: 0.554  loss = 0.850 val_mse = 1.207 mse = 1.290 mae = 0.831\n",
            "epoch169 train time: 4.936s test time: 0.559  loss = 0.849 val_mse = 1.229 mse = 1.310 mae = 0.843\n",
            "epoch170 train time: 4.933s test time: 0.557  loss = 0.851 val_mse = 1.208 mse = 1.288 mae = 0.832\n",
            "epoch171 train time: 4.961s test time: 0.560  loss = 0.847 val_mse = 1.239 mse = 1.314 mae = 0.840\n",
            "epoch172 train time: 4.936s test time: 0.557  loss = 0.854 val_mse = 1.204 mse = 1.291 mae = 0.833\n",
            "epoch173 train time: 4.945s test time: 0.558  loss = 0.847 val_mse = 1.232 mse = 1.315 mae = 0.841\n",
            "epoch174 train time: 4.932s test time: 0.562  loss = 0.847 val_mse = 1.208 mse = 1.294 mae = 0.830\n",
            "epoch175 train time: 4.943s test time: 0.562  loss = 0.848 val_mse = 1.230 mse = 1.312 mae = 0.843\n",
            "epoch176 train time: 4.948s test time: 0.557  loss = 0.852 val_mse = 1.206 mse = 1.292 mae = 0.835\n",
            "epoch177 train time: 4.940s test time: 0.565  loss = 0.848 val_mse = 1.229 mse = 1.311 mae = 0.838\n",
            "epoch178 train time: 4.943s test time: 0.560  loss = 0.847 val_mse = 1.215 mse = 1.296 mae = 0.832\n",
            "epoch179 train time: 4.953s test time: 0.562  loss = 0.847 val_mse = 1.233 mse = 1.316 mae = 0.845\n",
            "epoch180 train time: 4.941s test time: 0.557  loss = 0.851 val_mse = 1.211 mse = 1.298 mae = 0.834\n",
            "epoch181 train time: 4.964s test time: 0.568  loss = 0.847 val_mse = 1.228 mse = 1.306 mae = 0.839\n",
            "epoch182 train time: 4.949s test time: 0.557  loss = 0.847 val_mse = 1.205 mse = 1.295 mae = 0.832\n",
            "epoch183 train time: 4.959s test time: 0.567  loss = 0.847 val_mse = 1.229 mse = 1.312 mae = 0.842\n",
            "epoch184 train time: 4.934s test time: 0.565  loss = 0.850 val_mse = 1.208 mse = 1.293 mae = 0.834\n",
            "epoch185 train time: 4.952s test time: 0.555  loss = 0.845 val_mse = 1.229 mse = 1.310 mae = 0.839\n",
            "epoch186 train time: 4.946s test time: 0.565  loss = 0.845 val_mse = 1.209 mse = 1.296 mae = 0.833\n",
            "epoch187 train time: 4.942s test time: 0.550  loss = 0.846 val_mse = 1.227 mse = 1.309 mae = 0.842\n",
            "epoch188 train time: 4.934s test time: 0.563  loss = 0.845 val_mse = 1.209 mse = 1.294 mae = 0.831\n",
            "epoch189 train time: 4.928s test time: 0.554  loss = 0.842 val_mse = 1.234 mse = 1.315 mae = 0.841\n",
            "epoch190 train time: 4.931s test time: 0.560  loss = 0.846 val_mse = 1.201 mse = 1.291 mae = 0.833\n",
            "epoch191 train time: 4.942s test time: 0.567  loss = 0.843 val_mse = 1.227 mse = 1.310 mae = 0.840\n",
            "epoch192 train time: 4.950s test time: 0.553  loss = 0.842 val_mse = 1.206 mse = 1.296 mae = 0.831\n",
            "epoch193 train time: 4.950s test time: 0.558  loss = 0.843 val_mse = 1.228 mse = 1.307 mae = 0.841\n",
            "epoch194 train time: 4.942s test time: 0.558  loss = 0.841 val_mse = 1.204 mse = 1.292 mae = 0.832\n",
            "epoch195 train time: 4.947s test time: 0.558  loss = 0.838 val_mse = 1.230 mse = 1.314 mae = 0.840\n",
            "epoch196 train time: 4.958s test time: 0.557  loss = 0.841 val_mse = 1.204 mse = 1.295 mae = 0.833\n",
            "epoch197 train time: 4.950s test time: 0.558  loss = 0.841 val_mse = 1.228 mse = 1.306 mae = 0.840\n",
            "epoch198 train time: 4.959s test time: 0.558  loss = 0.838 val_mse = 1.198 mse = 1.291 mae = 0.834\n",
            "epoch199 train time: 4.955s test time: 0.559  loss = 0.839 val_mse = 1.229 mse = 1.314 mae = 0.841\n",
            "epoch200 train time: 5.431s test time: 0.807  loss = 0.840 val_mse = 1.208 mse = 1.289 mae = 0.832\n",
            "epoch201 train time: 5.808s test time: 0.734  loss = 0.834 val_mse = 1.224 mse = 1.310 mae = 0.842\n",
            "epoch202 train time: 5.439s test time: 0.559  loss = 0.837 val_mse = 1.200 mse = 1.290 mae = 0.833\n",
            "epoch203 train time: 4.944s test time: 0.559  loss = 0.836 val_mse = 1.229 mse = 1.308 mae = 0.842\n",
            "epoch204 train time: 4.934s test time: 0.554  loss = 0.833 val_mse = 1.201 mse = 1.292 mae = 0.832\n",
            "epoch205 train time: 4.948s test time: 0.560  loss = 0.836 val_mse = 1.225 mse = 1.305 mae = 0.841\n",
            "epoch206 train time: 4.932s test time: 0.557  loss = 0.832 val_mse = 1.198 mse = 1.288 mae = 0.831\n",
            "epoch207 train time: 4.937s test time: 0.567  loss = 0.829 val_mse = 1.225 mse = 1.310 mae = 0.843\n",
            "epoch208 train time: 4.936s test time: 0.560  loss = 0.834 val_mse = 1.198 mse = 1.286 mae = 0.833\n",
            "epoch209 train time: 4.943s test time: 0.560  loss = 0.831 val_mse = 1.222 mse = 1.302 mae = 0.840\n",
            "epoch210 train time: 4.924s test time: 0.563  loss = 0.830 val_mse = 1.201 mse = 1.293 mae = 0.836\n",
            "epoch211 train time: 4.941s test time: 0.561  loss = 0.834 val_mse = 1.220 mse = 1.307 mae = 0.843\n",
            "epoch212 train time: 4.944s test time: 0.557  loss = 0.830 val_mse = 1.199 mse = 1.291 mae = 0.832\n",
            "epoch213 train time: 4.932s test time: 0.563  loss = 0.829 val_mse = 1.223 mse = 1.306 mae = 0.842\n",
            "epoch214 train time: 4.925s test time: 0.562  loss = 0.829 val_mse = 1.195 mse = 1.295 mae = 0.835\n",
            "epoch215 train time: 4.920s test time: 0.559  loss = 0.828 val_mse = 1.221 mse = 1.303 mae = 0.841\n",
            "epoch216 train time: 4.937s test time: 0.562  loss = 0.826 val_mse = 1.197 mse = 1.286 mae = 0.834\n",
            "epoch217 train time: 4.925s test time: 0.558  loss = 0.828 val_mse = 1.222 mse = 1.301 mae = 0.841\n",
            "epoch218 train time: 4.940s test time: 0.552  loss = 0.822 val_mse = 1.198 mse = 1.296 mae = 0.833\n",
            "epoch219 train time: 4.941s test time: 0.563  loss = 0.824 val_mse = 1.225 mse = 1.309 mae = 0.842\n",
            "epoch220 train time: 4.944s test time: 0.554  loss = 0.825 val_mse = 1.191 mse = 1.281 mae = 0.834\n",
            "epoch221 train time: 4.952s test time: 0.566  loss = 0.823 val_mse = 1.217 mse = 1.301 mae = 0.842\n",
            "epoch222 train time: 4.937s test time: 0.555  loss = 0.821 val_mse = 1.196 mse = 1.293 mae = 0.834\n",
            "epoch223 train time: 4.945s test time: 0.559  loss = 0.821 val_mse = 1.215 mse = 1.311 mae = 0.841\n",
            "epoch224 train time: 4.932s test time: 0.568  loss = 0.818 val_mse = 1.194 mse = 1.293 mae = 0.833\n",
            "epoch225 train time: 4.942s test time: 0.561  loss = 0.816 val_mse = 1.223 mse = 1.313 mae = 0.841\n",
            "epoch226 train time: 4.936s test time: 0.555  loss = 0.818 val_mse = 1.191 mse = 1.289 mae = 0.834\n",
            "epoch227 train time: 4.941s test time: 0.559  loss = 0.820 val_mse = 1.216 mse = 1.309 mae = 0.836\n",
            "epoch228 train time: 4.940s test time: 0.566  loss = 0.814 val_mse = 1.192 mse = 1.292 mae = 0.833\n",
            "epoch229 train time: 4.938s test time: 0.567  loss = 0.819 val_mse = 1.222 mse = 1.307 mae = 0.840\n",
            "epoch230 train time: 4.942s test time: 0.562  loss = 0.814 val_mse = 1.195 mse = 1.289 mae = 0.832\n",
            "epoch231 train time: 4.936s test time: 0.563  loss = 0.819 val_mse = 1.216 mse = 1.305 mae = 0.842\n",
            "epoch232 train time: 4.962s test time: 0.567  loss = 0.817 val_mse = 1.195 mse = 1.295 mae = 0.836\n",
            "epoch233 train time: 4.938s test time: 0.559  loss = 0.813 val_mse = 1.217 mse = 1.306 mae = 0.842\n",
            "epoch234 train time: 4.944s test time: 0.552  loss = 0.815 val_mse = 1.188 mse = 1.289 mae = 0.834\n",
            "epoch235 train time: 4.941s test time: 0.562  loss = 0.814 val_mse = 1.216 mse = 1.305 mae = 0.839\n",
            "epoch236 train time: 4.928s test time: 0.560  loss = 0.815 val_mse = 1.188 mse = 1.288 mae = 0.836\n",
            "epoch237 train time: 4.934s test time: 0.557  loss = 0.815 val_mse = 1.215 mse = 1.314 mae = 0.840\n",
            "epoch238 train time: 4.931s test time: 0.556  loss = 0.813 val_mse = 1.194 mse = 1.297 mae = 0.835\n",
            "epoch239 train time: 4.947s test time: 0.557  loss = 0.815 val_mse = 1.213 mse = 1.308 mae = 0.844\n",
            "MAE 0.8397565477635661\n",
            "MSE 1.3220048676510137\n"
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    print(sys.path[0])\n",
        "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
        "    word_latent_dim = 300\n",
        "    latent_dim = 15\n",
        "    max_doc_length = 300\n",
        "    num_filters = 50\n",
        "    window_size = 3\n",
        "    v_dim = 50\n",
        "    learning_rate = 0.001\n",
        "    lambda_list = [0.001, 0.005, 0.01, 0.05]\n",
        "    lambda_1 = 0.001 #normally we set from [0.05, 0.01, 0.005, 0.001]\n",
        "    drop_out = 0.8\n",
        "    batch_size = 200\n",
        "    epochs = 240\n",
        "    avg_mae_list = []\n",
        "    avg_mse_list = []\n",
        "    \n",
        "    # loading data\n",
        "    firTime = time()\n",
        "    dataSet = Dataset(max_doc_length, sys.path[0], \"dataPreprocessingWordDict.out\")\n",
        "    word_dict, user_reviews, item_reviews, train, valRatings, testRatings = dataSet.word_id_dict, dataSet.userReview_dict, dataSet.itemReview_dict, dataSet.trainMtrx, dataSet.valRatings, dataSet.testRatings\n",
        "    secTime = time()\n",
        "\n",
        "    num_users, num_items = train.shape\n",
        "    print(\"load data: %.3fs\" % (secTime - firTime))\n",
        "    print(num_users, num_items)\n",
        "    print(latent_dim)\n",
        "\n",
        "    #load word embeddings\n",
        "    word_embedding_mtrx = ini_word_embed(len(word_dict), word_latent_dim)\n",
        "    # word_embedding_mtrx = word2vec_word_embed(len(word_dict), word_latent_dim,\n",
        "    #                                           \"Directory of pretrained WordEmbedding.out\",\n",
        "    #                                           word_dict)\n",
        "\n",
        "    print( \"shape\", word_embedding_mtrx.shape)\n",
        "\n",
        "    # get train instances\n",
        "    user_input, item_input, rateings = get_train_instance(train)\n",
        "    print (len(user_input), len(item_input), len(rateings))\n",
        "\n",
        "    # get test/val instances\n",
        "    user_vals, item_vals, user_input_val, item_input_val, rating_input_val = get_test_list(200, valRatings, user_reviews, item_reviews)\n",
        "    user_tests, item_tests, user_input_test, item_input_test, rating_input_test = get_test_list(200, testRatings, user_reviews, item_reviews)\n",
        "\n",
        "    #train & eval model\n",
        "    train_model()\n",
        "\n",
        "    lambda_1 = 0.005\n",
        "    #train & eval model\n",
        "    train_model()\n",
        "\n",
        "    lambda_1 = 0.01\n",
        "    #train & eval model\n",
        "    train_model()\n",
        "\n",
        "    lambda_1 = 0.05\n",
        "    #train & eval model\n",
        "    train_model()\n"
      ],
      "id": "z8BUPB5jDcSF"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-wWPKRUTDcPb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 426
        },
        "outputId": "d911a693-d6ab-4156-b465-094c20370e23"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "avg mae over lambda = [0.001, 0.005, 0.01, 0.05]: [0.9561675719966913, 0.833382876876455, 0.8332786701043388, 0.8397565477635661]\n",
            "avg mse over lambda = [0.001, 0.005, 0.01, 0.05]: [65.55600156321124, 1.3009332724232503, 1.3050463728962762, 1.3220048676510137]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 750x350 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "minimum avg mae over lambda = 0.8332786701043388 at lambda = 0.01 and mse = 1.3050463728962762\n",
            "minimum avg mse over lambda = 1.3009332724232503 at lambda = 0.005 and mae = 0.833382876876455\n"
          ]
        }
      ],
      "source": [
        "avg_mae_list = [0.9561675719966913,0.833382876876455,0.8332786701043388,0.8397565477635661]\n",
        "avg_mse_list = [65.55600156321124,1.3009332724232503,1.3050463728962762,1.3220048676510137]\n",
        "lambda_list = [0.001, 0.005, 0.01, 0.05]\n",
        "print(f'avg mae over lambda = [0.001, 0.005, 0.01, 0.05]: {avg_mae_list}')\n",
        "print(f'avg mse over lambda = [0.001, 0.005, 0.01, 0.05]: {avg_mse_list}')\n",
        "    \n",
        "plt.rcParams.update({'figure.figsize':(7.5,3.5), 'figure.dpi':100})\n",
        "fig, axes = plt.subplots(1, 2)\n",
        "axes[0].plot(lambda_list, avg_mae_list, 'm--o')\n",
        "axes[0].set_title('Avg MAE over varying lambda')\n",
        "axes[0].set_xlabel('lambda')\n",
        "axes[0].set_ylabel('MAE')\n",
        "\n",
        "axes[1].plot(lambda_list, avg_mse_list, 'g--o')\n",
        "axes[1].set_title('Avg MSE over varying lambda')\n",
        "axes[1].set_xlabel('lambda')\n",
        "axes[1].set_ylabel('MSE')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "index = 0\n",
        "min = 100\n",
        "min_index = 0\n",
        "for i in avg_mae_list:\n",
        "  if i<min:\n",
        "    min = i\n",
        "    min_index = index\n",
        "  index+=1\n",
        "print(f'minimum avg mae over lambda = {min} at lambda = {lambda_list[min_index]} and mse = {avg_mse_list[min_index]}')\n",
        "\n",
        "index = 0\n",
        "min = 100\n",
        "min_index = 0\n",
        "for i in avg_mse_list:\n",
        "  if i<min:\n",
        "    min = i\n",
        "    min_index = index\n",
        "  index+=1\n",
        "print(f'minimum avg mse over lambda = {min} at lambda = {lambda_list[min_index]} and mae = {avg_mae_list[min_index]}')"
      ],
      "id": "-wWPKRUTDcPb"
    },
    {
      "cell_type": "markdown",
      "source": [
        "lambda = 0.01"
      ],
      "metadata": {
        "id": "wPKOTWhjA02p"
      },
      "id": "wPKOTWhjA02p"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RY4t0iAbDeBU"
      },
      "source": [
        "Varying latent dimensions"
      ],
      "id": "RY4t0iAbDeBU"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ckPxBumcDdkb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "909fb56f-5500-4aca-9589-e33b3b573c42"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "wordId_dict finished\n",
            "load reviews finished\n",
            "load data: 4.245s\n",
            "506 2581\n",
            "15\n",
            "shape (19930, 300)\n",
            "18880 18880 18880\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/training/rmsprop.py:188: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "epoch0 train time: 25.925s test time: 1.336  loss = 1000.541 val_mse = 1.653 mse = 1.585 mae = 0.910\n",
            "epoch1 train time: 12.905s test time: 1.259  loss = 294.003 val_mse = 3.526 mse = 3.451 mae = 1.421\n",
            "epoch2 train time: 13.014s test time: 1.270  loss = 214.555 val_mse = 1.446 mse = 1.439 mae = 0.897\n",
            "epoch3 train time: 13.127s test time: 1.281  loss = 151.437 val_mse = 2.153 mse = 2.103 mae = 1.030\n",
            "epoch4 train time: 13.230s test time: 1.293  loss = 103.477 val_mse = 1.489 mse = 1.504 mae = 1.005\n",
            "epoch5 train time: 13.191s test time: 1.273  loss = 75.018 val_mse = 3.390 mse = 3.302 mae = 1.392\n",
            "epoch6 train time: 13.104s test time: 1.270  loss = 49.841 val_mse = 1.469 mse = 1.465 mae = 0.866\n",
            "epoch7 train time: 13.133s test time: 1.270  loss = 32.212 val_mse = 1.526 mse = 1.492 mae = 0.866\n",
            "epoch8 train time: 13.018s test time: 1.261  loss = 18.381 val_mse = 1.469 mse = 1.459 mae = 0.863\n",
            "epoch9 train time: 13.015s test time: 1.265  loss = 9.355 val_mse = 1.386 mse = 1.386 mae = 0.866\n",
            "epoch10 train time: 13.036s test time: 1.261  loss = 4.334 val_mse = 1.549 mse = 1.532 mae = 0.869\n",
            "epoch11 train time: 13.015s test time: 1.257  loss = 2.296 val_mse = 1.398 mse = 1.411 mae = 0.855\n",
            "epoch12 train time: 13.014s test time: 1.255  loss = 1.486 val_mse = 3.139 mse = 3.077 mae = 1.340\n",
            "epoch13 train time: 12.990s test time: 1.254  loss = 1.268 val_mse = 1.327 mse = 1.356 mae = 0.853\n",
            "epoch14 train time: 13.043s test time: 1.256  loss = 1.181 val_mse = 1.318 mse = 1.343 mae = 0.852\n",
            "epoch15 train time: 13.050s test time: 1.251  loss = 1.098 val_mse = 1.289 mse = 1.319 mae = 0.855\n",
            "epoch16 train time: 12.955s test time: 1.250  loss = 1.062 val_mse = 1.284 mse = 1.315 mae = 0.857\n",
            "epoch17 train time: 12.940s test time: 1.256  loss = 1.016 val_mse = 1.271 mse = 1.320 mae = 0.854\n",
            "epoch18 train time: 12.939s test time: 1.250  loss = 0.981 val_mse = 1.299 mse = 1.340 mae = 0.842\n",
            "epoch19 train time: 12.938s test time: 1.252  loss = 0.975 val_mse = 1.265 mse = 1.317 mae = 0.857\n",
            "epoch20 train time: 12.911s test time: 1.252  loss = 0.965 val_mse = 1.275 mse = 1.323 mae = 0.856\n",
            "epoch21 train time: 12.927s test time: 1.250  loss = 0.956 val_mse = 1.264 mse = 1.320 mae = 0.870\n",
            "epoch22 train time: 12.902s test time: 1.247  loss = 0.952 val_mse = 1.237 mse = 1.305 mae = 0.867\n",
            "epoch23 train time: 12.914s test time: 1.253  loss = 0.944 val_mse = 1.265 mse = 1.311 mae = 0.847\n",
            "epoch24 train time: 12.893s test time: 1.245  loss = 0.929 val_mse = 1.389 mse = 1.419 mae = 0.841\n",
            "epoch25 train time: 12.887s test time: 1.247  loss = 0.922 val_mse = 1.326 mse = 1.371 mae = 0.840\n",
            "epoch26 train time: 12.921s test time: 1.253  loss = 0.924 val_mse = 1.248 mse = 1.306 mae = 0.848\n",
            "epoch27 train time: 12.918s test time: 1.252  loss = 0.903 val_mse = 1.245 mse = 1.301 mae = 0.845\n",
            "epoch28 train time: 12.912s test time: 1.247  loss = 0.910 val_mse = 1.253 mse = 1.309 mae = 0.858\n",
            "epoch29 train time: 12.892s test time: 1.253  loss = 0.909 val_mse = 1.250 mse = 1.305 mae = 0.851\n",
            "epoch30 train time: 12.878s test time: 1.246  loss = 0.899 val_mse = 1.254 mse = 1.307 mae = 0.852\n",
            "epoch31 train time: 12.882s test time: 1.252  loss = 0.900 val_mse = 1.241 mse = 1.313 mae = 0.855\n",
            "epoch32 train time: 12.899s test time: 1.251  loss = 0.888 val_mse = 1.246 mse = 1.310 mae = 0.858\n",
            "epoch33 train time: 12.897s test time: 1.250  loss = 0.891 val_mse = 1.240 mse = 1.307 mae = 0.849\n",
            "epoch34 train time: 12.882s test time: 1.251  loss = 0.884 val_mse = 1.235 mse = 1.306 mae = 0.858\n",
            "epoch35 train time: 12.895s test time: 1.255  loss = 0.895 val_mse = 1.234 mse = 1.306 mae = 0.854\n",
            "epoch36 train time: 12.877s test time: 1.253  loss = 0.874 val_mse = 1.231 mse = 1.305 mae = 0.852\n",
            "epoch37 train time: 12.897s test time: 1.249  loss = 0.889 val_mse = 1.251 mse = 1.315 mae = 0.858\n",
            "epoch38 train time: 12.900s test time: 1.248  loss = 0.883 val_mse = 1.220 mse = 1.302 mae = 0.854\n",
            "epoch39 train time: 12.885s test time: 1.248  loss = 0.876 val_mse = 1.228 mse = 1.297 mae = 0.851\n",
            "epoch40 train time: 12.881s test time: 1.244  loss = 0.875 val_mse = 1.217 mse = 1.303 mae = 0.855\n",
            "epoch41 train time: 12.859s test time: 1.243  loss = 0.875 val_mse = 1.249 mse = 1.316 mae = 0.857\n",
            "epoch42 train time: 12.877s test time: 1.246  loss = 0.875 val_mse = 1.240 mse = 1.329 mae = 0.854\n",
            "epoch43 train time: 12.882s test time: 1.244  loss = 0.871 val_mse = 1.240 mse = 1.304 mae = 0.846\n",
            "epoch44 train time: 12.864s test time: 1.244  loss = 0.875 val_mse = 1.219 mse = 1.302 mae = 0.852\n",
            "epoch45 train time: 12.863s test time: 1.244  loss = 0.869 val_mse = 1.226 mse = 1.304 mae = 0.856\n",
            "epoch46 train time: 12.892s test time: 1.242  loss = 0.869 val_mse = 1.232 mse = 1.307 mae = 0.852\n",
            "epoch47 train time: 12.856s test time: 1.252  loss = 0.869 val_mse = 1.217 mse = 1.305 mae = 0.852\n",
            "epoch48 train time: 12.893s test time: 1.253  loss = 0.860 val_mse = 1.228 mse = 1.308 mae = 0.849\n",
            "epoch49 train time: 12.894s test time: 1.247  loss = 0.865 val_mse = 1.233 mse = 1.308 mae = 0.852\n",
            "epoch50 train time: 12.895s test time: 1.245  loss = 0.865 val_mse = 1.232 mse = 1.305 mae = 0.844\n",
            "epoch51 train time: 12.894s test time: 1.248  loss = 0.863 val_mse = 1.230 mse = 1.319 mae = 0.841\n",
            "epoch52 train time: 12.910s test time: 1.246  loss = 0.860 val_mse = 1.224 mse = 1.303 mae = 0.849\n",
            "epoch53 train time: 12.879s test time: 1.248  loss = 0.859 val_mse = 1.236 mse = 1.322 mae = 0.852\n",
            "epoch54 train time: 12.887s test time: 1.242  loss = 0.859 val_mse = 1.233 mse = 1.306 mae = 0.845\n",
            "epoch55 train time: 12.912s test time: 1.250  loss = 0.861 val_mse = 1.224 mse = 1.301 mae = 0.855\n",
            "epoch56 train time: 12.867s test time: 1.249  loss = 0.855 val_mse = 1.240 mse = 1.311 mae = 0.849\n",
            "epoch57 train time: 12.863s test time: 1.242  loss = 0.855 val_mse = 1.213 mse = 1.299 mae = 0.848\n",
            "epoch58 train time: 12.862s test time: 1.253  loss = 0.848 val_mse = 1.228 mse = 1.305 mae = 0.843\n",
            "epoch59 train time: 12.859s test time: 1.247  loss = 0.847 val_mse = 1.215 mse = 1.303 mae = 0.847\n",
            "epoch60 train time: 12.878s test time: 1.250  loss = 0.846 val_mse = 1.233 mse = 1.304 mae = 0.848\n",
            "epoch61 train time: 12.880s test time: 1.243  loss = 0.847 val_mse = 1.220 mse = 1.299 mae = 0.851\n",
            "epoch62 train time: 12.859s test time: 1.245  loss = 0.847 val_mse = 1.228 mse = 1.302 mae = 0.847\n",
            "epoch63 train time: 12.865s test time: 1.251  loss = 0.851 val_mse = 1.201 mse = 1.290 mae = 0.852\n",
            "epoch64 train time: 12.889s test time: 1.246  loss = 0.847 val_mse = 1.240 mse = 1.325 mae = 0.836\n",
            "epoch65 train time: 12.877s test time: 1.247  loss = 0.841 val_mse = 1.222 mse = 1.304 mae = 0.844\n",
            "epoch66 train time: 12.869s test time: 1.246  loss = 0.849 val_mse = 1.219 mse = 1.308 mae = 0.855\n",
            "epoch67 train time: 12.873s test time: 1.246  loss = 0.845 val_mse = 1.204 mse = 1.296 mae = 0.845\n",
            "epoch68 train time: 12.883s test time: 1.250  loss = 0.840 val_mse = 1.212 mse = 1.306 mae = 0.850\n",
            "epoch69 train time: 12.887s test time: 1.247  loss = 0.844 val_mse = 1.217 mse = 1.289 mae = 0.847\n",
            "epoch70 train time: 12.890s test time: 1.245  loss = 0.843 val_mse = 1.221 mse = 1.305 mae = 0.843\n",
            "epoch71 train time: 12.900s test time: 1.252  loss = 0.839 val_mse = 1.237 mse = 1.311 mae = 0.837\n",
            "epoch72 train time: 12.875s test time: 1.247  loss = 0.836 val_mse = 1.265 mse = 1.347 mae = 0.832\n",
            "epoch73 train time: 12.877s test time: 1.252  loss = 0.836 val_mse = 1.236 mse = 1.312 mae = 0.842\n",
            "epoch74 train time: 12.865s test time: 1.245  loss = 0.843 val_mse = 1.229 mse = 1.312 mae = 0.837\n",
            "epoch75 train time: 12.873s test time: 1.245  loss = 0.832 val_mse = 1.216 mse = 1.306 mae = 0.846\n",
            "epoch76 train time: 12.884s test time: 1.247  loss = 0.842 val_mse = 1.243 mse = 1.313 mae = 0.835\n",
            "epoch77 train time: 12.885s test time: 1.245  loss = 0.834 val_mse = 1.213 mse = 1.291 mae = 0.843\n",
            "epoch78 train time: 12.868s test time: 1.247  loss = 0.835 val_mse = 1.212 mse = 1.303 mae = 0.851\n",
            "epoch79 train time: 12.870s test time: 1.248  loss = 0.835 val_mse = 1.240 mse = 1.313 mae = 0.835\n",
            "epoch80 train time: 12.883s test time: 1.244  loss = 0.836 val_mse = 1.248 mse = 1.325 mae = 0.832\n",
            "epoch81 train time: 12.865s test time: 1.248  loss = 0.826 val_mse = 1.211 mse = 1.293 mae = 0.847\n",
            "epoch82 train time: 12.855s test time: 1.243  loss = 0.828 val_mse = 1.241 mse = 1.334 mae = 0.836\n",
            "epoch83 train time: 12.856s test time: 1.242  loss = 0.831 val_mse = 1.202 mse = 1.299 mae = 0.845\n",
            "epoch84 train time: 12.862s test time: 1.248  loss = 0.829 val_mse = 1.235 mse = 1.309 mae = 0.831\n",
            "epoch85 train time: 12.853s test time: 1.246  loss = 0.824 val_mse = 1.227 mse = 1.300 mae = 0.835\n",
            "epoch86 train time: 12.852s test time: 1.243  loss = 0.832 val_mse = 1.227 mse = 1.315 mae = 0.842\n",
            "epoch87 train time: 12.861s test time: 1.246  loss = 0.823 val_mse = 1.225 mse = 1.314 mae = 0.838\n",
            "epoch88 train time: 12.852s test time: 1.240  loss = 0.829 val_mse = 1.233 mse = 1.308 mae = 0.838\n",
            "epoch89 train time: 12.856s test time: 1.246  loss = 0.821 val_mse = 1.232 mse = 1.322 mae = 0.843\n",
            "epoch90 train time: 12.845s test time: 1.249  loss = 0.823 val_mse = 1.254 mse = 1.336 mae = 0.835\n",
            "epoch91 train time: 12.853s test time: 1.248  loss = 0.826 val_mse = 1.229 mse = 1.310 mae = 0.835\n",
            "epoch92 train time: 12.853s test time: 1.245  loss = 0.821 val_mse = 1.235 mse = 1.325 mae = 0.835\n",
            "epoch93 train time: 12.854s test time: 1.247  loss = 0.818 val_mse = 1.235 mse = 1.313 mae = 0.836\n",
            "epoch94 train time: 12.847s test time: 1.247  loss = 0.830 val_mse = 1.244 mse = 1.330 mae = 0.834\n",
            "epoch95 train time: 12.854s test time: 1.243  loss = 0.817 val_mse = 1.217 mse = 1.307 mae = 0.838\n",
            "epoch96 train time: 12.863s test time: 1.247  loss = 0.825 val_mse = 1.224 mse = 1.307 mae = 0.836\n",
            "epoch97 train time: 12.862s test time: 1.246  loss = 0.819 val_mse = 1.205 mse = 1.293 mae = 0.838\n",
            "epoch98 train time: 12.863s test time: 1.244  loss = 0.824 val_mse = 1.218 mse = 1.308 mae = 0.839\n",
            "epoch99 train time: 12.843s test time: 1.243  loss = 0.819 val_mse = 1.210 mse = 1.290 mae = 0.839\n",
            "epoch100 train time: 12.873s test time: 1.247  loss = 0.822 val_mse = 1.213 mse = 1.295 mae = 0.838\n",
            "epoch101 train time: 12.859s test time: 1.244  loss = 0.814 val_mse = 1.200 mse = 1.289 mae = 0.844\n",
            "epoch102 train time: 12.880s test time: 1.246  loss = 0.818 val_mse = 1.242 mse = 1.319 mae = 0.829\n",
            "epoch103 train time: 12.910s test time: 1.243  loss = 0.815 val_mse = 1.198 mse = 1.294 mae = 0.839\n",
            "epoch104 train time: 12.850s test time: 1.245  loss = 0.819 val_mse = 1.222 mse = 1.313 mae = 0.835\n",
            "epoch105 train time: 12.848s test time: 1.242  loss = 0.813 val_mse = 1.233 mse = 1.321 mae = 0.836\n",
            "epoch106 train time: 12.845s test time: 1.250  loss = 0.819 val_mse = 1.244 mse = 1.322 mae = 0.830\n",
            "epoch107 train time: 12.837s test time: 1.241  loss = 0.808 val_mse = 1.201 mse = 1.290 mae = 0.841\n",
            "epoch108 train time: 12.839s test time: 1.241  loss = 0.818 val_mse = 1.235 mse = 1.320 mae = 0.833\n",
            "epoch109 train time: 12.864s test time: 1.244  loss = 0.815 val_mse = 1.195 mse = 1.293 mae = 0.838\n",
            "epoch110 train time: 12.846s test time: 1.246  loss = 0.820 val_mse = 1.227 mse = 1.309 mae = 0.833\n",
            "epoch111 train time: 12.873s test time: 1.248  loss = 0.811 val_mse = 1.199 mse = 1.297 mae = 0.842\n",
            "epoch112 train time: 12.836s test time: 1.244  loss = 0.815 val_mse = 1.194 mse = 1.292 mae = 0.848\n",
            "epoch113 train time: 12.868s test time: 1.242  loss = 0.807 val_mse = 1.211 mse = 1.296 mae = 0.837\n",
            "epoch114 train time: 12.845s test time: 1.249  loss = 0.816 val_mse = 1.216 mse = 1.303 mae = 0.837\n",
            "epoch115 train time: 12.853s test time: 1.242  loss = 0.815 val_mse = 1.201 mse = 1.293 mae = 0.837\n",
            "epoch116 train time: 12.833s test time: 1.246  loss = 0.816 val_mse = 1.240 mse = 1.317 mae = 0.831\n",
            "epoch117 train time: 12.857s test time: 1.242  loss = 0.808 val_mse = 1.199 mse = 1.300 mae = 0.839\n",
            "epoch118 train time: 12.848s test time: 1.244  loss = 0.809 val_mse = 1.209 mse = 1.301 mae = 0.846\n",
            "epoch119 train time: 12.847s test time: 1.243  loss = 0.813 val_mse = 1.208 mse = 1.299 mae = 0.839\n",
            "epoch120 train time: 12.861s test time: 1.244  loss = 0.810 val_mse = 1.194 mse = 1.287 mae = 0.841\n",
            "epoch121 train time: 12.845s test time: 1.242  loss = 0.809 val_mse = 1.189 mse = 1.295 mae = 0.843\n",
            "epoch122 train time: 12.851s test time: 1.250  loss = 0.809 val_mse = 1.202 mse = 1.290 mae = 0.839\n",
            "epoch123 train time: 12.845s test time: 1.244  loss = 0.808 val_mse = 1.209 mse = 1.295 mae = 0.834\n",
            "epoch124 train time: 12.847s test time: 1.245  loss = 0.808 val_mse = 1.205 mse = 1.301 mae = 0.843\n",
            "epoch125 train time: 12.854s test time: 1.249  loss = 0.803 val_mse = 1.196 mse = 1.297 mae = 0.836\n",
            "epoch126 train time: 12.846s test time: 1.242  loss = 0.806 val_mse = 1.203 mse = 1.303 mae = 0.843\n",
            "epoch127 train time: 12.879s test time: 1.243  loss = 0.801 val_mse = 1.195 mse = 1.300 mae = 0.838\n",
            "epoch128 train time: 12.845s test time: 1.246  loss = 0.803 val_mse = 1.194 mse = 1.293 mae = 0.846\n",
            "epoch129 train time: 12.852s test time: 1.242  loss = 0.804 val_mse = 1.189 mse = 1.285 mae = 0.834\n",
            "epoch130 train time: 12.841s test time: 1.243  loss = 0.803 val_mse = 1.195 mse = 1.295 mae = 0.843\n",
            "epoch131 train time: 12.864s test time: 1.248  loss = 0.803 val_mse = 1.192 mse = 1.288 mae = 0.837\n",
            "epoch132 train time: 12.850s test time: 1.244  loss = 0.803 val_mse = 1.203 mse = 1.294 mae = 0.843\n",
            "epoch133 train time: 12.851s test time: 1.247  loss = 0.803 val_mse = 1.195 mse = 1.288 mae = 0.837\n",
            "epoch134 train time: 12.844s test time: 1.246  loss = 0.794 val_mse = 1.195 mse = 1.296 mae = 0.847\n",
            "epoch135 train time: 12.875s test time: 1.243  loss = 0.798 val_mse = 1.193 mse = 1.291 mae = 0.839\n",
            "epoch136 train time: 12.884s test time: 1.254  loss = 0.801 val_mse = 1.217 mse = 1.306 mae = 0.832\n",
            "epoch137 train time: 12.909s test time: 1.255  loss = 0.796 val_mse = 1.193 mse = 1.286 mae = 0.835\n",
            "epoch138 train time: 12.899s test time: 1.247  loss = 0.799 val_mse = 1.228 mse = 1.313 mae = 0.833\n",
            "epoch139 train time: 12.888s test time: 1.249  loss = 0.798 val_mse = 1.190 mse = 1.291 mae = 0.838\n",
            "epoch140 train time: 12.898s test time: 1.249  loss = 0.800 val_mse = 1.194 mse = 1.293 mae = 0.847\n",
            "epoch141 train time: 12.896s test time: 1.248  loss = 0.798 val_mse = 1.215 mse = 1.313 mae = 0.828\n",
            "epoch142 train time: 12.911s test time: 1.246  loss = 0.792 val_mse = 1.199 mse = 1.298 mae = 0.842\n",
            "epoch143 train time: 12.894s test time: 1.249  loss = 0.799 val_mse = 1.204 mse = 1.301 mae = 0.828\n",
            "epoch144 train time: 12.908s test time: 1.250  loss = 0.791 val_mse = 1.226 mse = 1.315 mae = 0.829\n",
            "epoch145 train time: 12.878s test time: 1.253  loss = 0.796 val_mse = 1.176 mse = 1.283 mae = 0.841\n",
            "epoch146 train time: 12.876s test time: 1.261  loss = 0.795 val_mse = 1.234 mse = 1.318 mae = 0.829\n",
            "epoch147 train time: 12.838s test time: 1.248  loss = 0.794 val_mse = 1.220 mse = 1.313 mae = 0.827\n",
            "epoch148 train time: 12.856s test time: 1.243  loss = 0.794 val_mse = 1.216 mse = 1.308 mae = 0.830\n",
            "epoch149 train time: 12.849s test time: 1.249  loss = 0.797 val_mse = 1.188 mse = 1.291 mae = 0.835\n",
            "epoch150 train time: 12.877s test time: 1.249  loss = 0.792 val_mse = 1.187 mse = 1.293 mae = 0.841\n",
            "epoch151 train time: 12.884s test time: 1.248  loss = 0.794 val_mse = 1.207 mse = 1.308 mae = 0.831\n",
            "epoch152 train time: 12.871s test time: 1.249  loss = 0.798 val_mse = 1.234 mse = 1.315 mae = 0.829\n",
            "epoch153 train time: 12.878s test time: 1.250  loss = 0.801 val_mse = 1.190 mse = 1.293 mae = 0.838\n",
            "epoch154 train time: 12.880s test time: 1.251  loss = 0.794 val_mse = 1.218 mse = 1.313 mae = 0.830\n",
            "epoch155 train time: 12.882s test time: 1.248  loss = 0.791 val_mse = 1.183 mse = 1.297 mae = 0.839\n",
            "epoch156 train time: 12.883s test time: 1.248  loss = 0.798 val_mse = 1.195 mse = 1.291 mae = 0.847\n",
            "epoch157 train time: 12.890s test time: 1.247  loss = 0.793 val_mse = 1.177 mse = 1.292 mae = 0.838\n",
            "epoch158 train time: 12.863s test time: 1.242  loss = 0.788 val_mse = 1.189 mse = 1.291 mae = 0.841\n",
            "epoch159 train time: 12.857s test time: 1.254  loss = 0.793 val_mse = 1.182 mse = 1.289 mae = 0.838\n",
            "epoch160 train time: 12.893s test time: 1.251  loss = 0.791 val_mse = 1.193 mse = 1.296 mae = 0.842\n",
            "epoch161 train time: 12.897s test time: 1.250  loss = 0.792 val_mse = 1.217 mse = 1.309 mae = 0.826\n",
            "epoch162 train time: 12.879s test time: 1.248  loss = 0.792 val_mse = 1.208 mse = 1.303 mae = 0.837\n",
            "epoch163 train time: 12.883s test time: 1.245  loss = 0.792 val_mse = 1.183 mse = 1.285 mae = 0.840\n",
            "epoch164 train time: 12.853s test time: 1.250  loss = 0.790 val_mse = 1.224 mse = 1.312 mae = 0.828\n",
            "epoch165 train time: 12.855s test time: 1.255  loss = 0.786 val_mse = 1.189 mse = 1.294 mae = 0.833\n",
            "epoch166 train time: 12.870s test time: 1.255  loss = 0.793 val_mse = 1.192 mse = 1.290 mae = 0.840\n",
            "epoch167 train time: 12.877s test time: 1.244  loss = 0.789 val_mse = 1.189 mse = 1.292 mae = 0.831\n",
            "epoch168 train time: 12.883s test time: 1.253  loss = 0.786 val_mse = 1.205 mse = 1.298 mae = 0.834\n",
            "epoch169 train time: 12.870s test time: 1.254  loss = 0.790 val_mse = 1.189 mse = 1.288 mae = 0.837\n",
            "epoch170 train time: 12.877s test time: 1.248  loss = 0.796 val_mse = 1.204 mse = 1.292 mae = 0.836\n",
            "epoch171 train time: 12.891s test time: 1.255  loss = 0.791 val_mse = 1.177 mse = 1.287 mae = 0.837\n",
            "epoch172 train time: 12.875s test time: 1.249  loss = 0.787 val_mse = 1.196 mse = 1.294 mae = 0.833\n",
            "epoch173 train time: 12.875s test time: 1.245  loss = 0.786 val_mse = 1.179 mse = 1.290 mae = 0.835\n",
            "epoch174 train time: 12.839s test time: 1.250  loss = 0.787 val_mse = 1.203 mse = 1.293 mae = 0.834\n",
            "epoch175 train time: 12.895s test time: 1.250  loss = 0.787 val_mse = 1.180 mse = 1.291 mae = 0.834\n",
            "epoch176 train time: 12.930s test time: 1.243  loss = 0.787 val_mse = 1.205 mse = 1.297 mae = 0.836\n",
            "epoch177 train time: 12.897s test time: 1.247  loss = 0.787 val_mse = 1.183 mse = 1.295 mae = 0.838\n",
            "epoch178 train time: 12.875s test time: 1.244  loss = 0.788 val_mse = 1.199 mse = 1.288 mae = 0.831\n",
            "epoch179 train time: 12.844s test time: 1.246  loss = 0.785 val_mse = 1.179 mse = 1.295 mae = 0.837\n",
            "epoch180 train time: 12.895s test time: 1.245  loss = 0.789 val_mse = 1.201 mse = 1.299 mae = 0.837\n",
            "epoch181 train time: 12.899s test time: 1.248  loss = 0.790 val_mse = 1.179 mse = 1.295 mae = 0.835\n",
            "epoch182 train time: 12.898s test time: 1.245  loss = 0.786 val_mse = 1.201 mse = 1.295 mae = 0.836\n",
            "epoch183 train time: 12.892s test time: 1.263  loss = 0.784 val_mse = 1.176 mse = 1.293 mae = 0.835\n",
            "epoch184 train time: 12.879s test time: 1.246  loss = 0.785 val_mse = 1.194 mse = 1.295 mae = 0.836\n",
            "epoch185 train time: 12.884s test time: 1.255  loss = 0.787 val_mse = 1.182 mse = 1.291 mae = 0.837\n",
            "epoch186 train time: 12.902s test time: 1.252  loss = 0.785 val_mse = 1.199 mse = 1.293 mae = 0.834\n",
            "epoch187 train time: 12.892s test time: 1.251  loss = 0.784 val_mse = 1.176 mse = 1.297 mae = 0.837\n",
            "epoch188 train time: 12.899s test time: 1.245  loss = 0.786 val_mse = 1.203 mse = 1.293 mae = 0.839\n",
            "epoch189 train time: 12.903s test time: 1.256  loss = 0.788 val_mse = 1.180 mse = 1.287 mae = 0.835\n",
            "epoch190 train time: 12.908s test time: 1.252  loss = 0.787 val_mse = 1.193 mse = 1.286 mae = 0.834\n",
            "epoch191 train time: 12.914s test time: 1.251  loss = 0.787 val_mse = 1.173 mse = 1.283 mae = 0.835\n",
            "epoch192 train time: 12.882s test time: 1.248  loss = 0.786 val_mse = 1.199 mse = 1.289 mae = 0.834\n",
            "epoch193 train time: 12.841s test time: 1.246  loss = 0.785 val_mse = 1.174 mse = 1.285 mae = 0.836\n",
            "epoch194 train time: 12.885s test time: 1.260  loss = 0.789 val_mse = 1.196 mse = 1.297 mae = 0.836\n",
            "epoch195 train time: 12.895s test time: 1.254  loss = 0.785 val_mse = 1.176 mse = 1.286 mae = 0.835\n",
            "epoch196 train time: 12.904s test time: 1.253  loss = 0.787 val_mse = 1.198 mse = 1.289 mae = 0.837\n",
            "epoch197 train time: 12.905s test time: 1.247  loss = 0.787 val_mse = 1.166 mse = 1.288 mae = 0.836\n",
            "epoch198 train time: 12.875s test time: 1.248  loss = 0.788 val_mse = 1.191 mse = 1.292 mae = 0.836\n",
            "epoch199 train time: 12.882s test time: 1.249  loss = 0.789 val_mse = 1.176 mse = 1.284 mae = 0.837\n",
            "epoch200 train time: 12.881s test time: 1.247  loss = 0.789 val_mse = 1.198 mse = 1.292 mae = 0.835\n",
            "epoch201 train time: 12.896s test time: 1.246  loss = 0.785 val_mse = 1.179 mse = 1.291 mae = 0.834\n",
            "epoch202 train time: 12.890s test time: 1.250  loss = 0.784 val_mse = 1.197 mse = 1.295 mae = 0.835\n",
            "epoch203 train time: 12.899s test time: 1.247  loss = 0.786 val_mse = 1.181 mse = 1.291 mae = 0.834\n",
            "epoch204 train time: 12.877s test time: 1.252  loss = 0.786 val_mse = 1.195 mse = 1.298 mae = 0.833\n",
            "epoch205 train time: 12.895s test time: 1.247  loss = 0.784 val_mse = 1.184 mse = 1.287 mae = 0.834\n",
            "epoch206 train time: 12.901s test time: 1.255  loss = 0.788 val_mse = 1.198 mse = 1.294 mae = 0.832\n",
            "epoch207 train time: 12.876s test time: 1.251  loss = 0.785 val_mse = 1.177 mse = 1.285 mae = 0.834\n",
            "epoch208 train time: 12.873s test time: 1.248  loss = 0.784 val_mse = 1.190 mse = 1.292 mae = 0.833\n",
            "epoch209 train time: 12.909s test time: 1.250  loss = 0.785 val_mse = 1.184 mse = 1.292 mae = 0.836\n",
            "epoch210 train time: 12.896s test time: 1.248  loss = 0.787 val_mse = 1.192 mse = 1.291 mae = 0.830\n",
            "epoch211 train time: 12.894s test time: 1.248  loss = 0.784 val_mse = 1.179 mse = 1.285 mae = 0.834\n",
            "epoch212 train time: 12.890s test time: 1.248  loss = 0.786 val_mse = 1.198 mse = 1.290 mae = 0.830\n",
            "epoch213 train time: 12.895s test time: 1.248  loss = 0.790 val_mse = 1.176 mse = 1.287 mae = 0.835\n",
            "epoch214 train time: 12.893s test time: 1.252  loss = 0.787 val_mse = 1.195 mse = 1.292 mae = 0.835\n",
            "epoch215 train time: 12.899s test time: 1.251  loss = 0.781 val_mse = 1.178 mse = 1.286 mae = 0.834\n",
            "epoch216 train time: 12.889s test time: 1.251  loss = 0.781 val_mse = 1.187 mse = 1.290 mae = 0.834\n",
            "epoch217 train time: 12.862s test time: 1.247  loss = 0.784 val_mse = 1.178 mse = 1.283 mae = 0.832\n",
            "epoch218 train time: 12.850s test time: 1.250  loss = 0.783 val_mse = 1.195 mse = 1.297 mae = 0.833\n",
            "epoch219 train time: 12.903s test time: 1.247  loss = 0.786 val_mse = 1.177 mse = 1.289 mae = 0.833\n",
            "epoch220 train time: 12.902s test time: 1.250  loss = 0.782 val_mse = 1.183 mse = 1.291 mae = 0.834\n",
            "epoch221 train time: 12.880s test time: 1.253  loss = 0.782 val_mse = 1.176 mse = 1.281 mae = 0.830\n",
            "epoch222 train time: 12.888s test time: 1.251  loss = 0.783 val_mse = 1.203 mse = 1.298 mae = 0.830\n",
            "epoch223 train time: 12.869s test time: 1.248  loss = 0.784 val_mse = 1.184 mse = 1.293 mae = 0.832\n",
            "epoch224 train time: 12.879s test time: 1.252  loss = 0.784 val_mse = 1.191 mse = 1.290 mae = 0.835\n",
            "epoch225 train time: 12.878s test time: 1.251  loss = 0.788 val_mse = 1.177 mse = 1.283 mae = 0.833\n",
            "epoch226 train time: 12.900s test time: 1.246  loss = 0.786 val_mse = 1.185 mse = 1.291 mae = 0.838\n",
            "epoch227 train time: 12.901s test time: 1.254  loss = 0.788 val_mse = 1.186 mse = 1.290 mae = 0.830\n",
            "epoch228 train time: 12.892s test time: 1.246  loss = 0.784 val_mse = 1.185 mse = 1.289 mae = 0.834\n",
            "epoch229 train time: 12.899s test time: 1.246  loss = 0.785 val_mse = 1.183 mse = 1.292 mae = 0.832\n",
            "epoch230 train time: 12.896s test time: 1.247  loss = 0.782 val_mse = 1.185 mse = 1.293 mae = 0.834\n",
            "epoch231 train time: 12.872s test time: 1.246  loss = 0.787 val_mse = 1.179 mse = 1.283 mae = 0.832\n",
            "epoch232 train time: 12.872s test time: 1.248  loss = 0.784 val_mse = 1.183 mse = 1.294 mae = 0.838\n",
            "epoch233 train time: 12.884s test time: 1.253  loss = 0.783 val_mse = 1.183 mse = 1.291 mae = 0.828\n",
            "epoch234 train time: 12.898s test time: 1.248  loss = 0.786 val_mse = 1.189 mse = 1.295 mae = 0.836\n",
            "epoch235 train time: 12.912s test time: 1.244  loss = 0.785 val_mse = 1.186 mse = 1.287 mae = 0.828\n",
            "epoch236 train time: 12.905s test time: 1.256  loss = 0.783 val_mse = 1.183 mse = 1.285 mae = 0.832\n",
            "epoch237 train time: 12.889s test time: 1.251  loss = 0.784 val_mse = 1.184 mse = 1.289 mae = 0.832\n",
            "epoch238 train time: 12.871s test time: 1.247  loss = 0.788 val_mse = 1.193 mse = 1.291 mae = 0.831\n",
            "epoch239 train time: 12.878s test time: 1.252  loss = 0.783 val_mse = 1.184 mse = 1.290 mae = 0.829\n",
            "MAE 0.8492282073119388\n",
            "MSE 1.3360866416687927\n"
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    print(sys.path[0])\n",
        "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
        "    word_latent_dim = 300\n",
        "    latent_dim_list = [15, 25, 50, 100, 150, 200, 300]\n",
        "    latent_dim = 15\n",
        "    max_doc_length = 300\n",
        "    num_filters = 50\n",
        "    window_size = 3\n",
        "    v_dim = 50\n",
        "    learning_rate = 0.001\n",
        "    lambda_1 = 0.01 #normally we set from [0.05, 0.01, 0.005, 0.001]\n",
        "    drop_out = 0.8\n",
        "    batch_size = 200\n",
        "    epochs = 240\n",
        "    avg_mae_list = []\n",
        "    avg_mse_list = []\n",
        "    \n",
        "    # loading data\n",
        "    firTime = time()\n",
        "    dataSet = Dataset(max_doc_length, sys.path[0], \"dataPreprocessingWordDict.out\")\n",
        "    word_dict, user_reviews, item_reviews, train, valRatings, testRatings = dataSet.word_id_dict, dataSet.userReview_dict, dataSet.itemReview_dict, dataSet.trainMtrx, dataSet.valRatings, dataSet.testRatings\n",
        "    secTime = time()\n",
        "\n",
        "    num_users, num_items = train.shape\n",
        "    print(\"load data: %.3fs\" % (secTime - firTime))\n",
        "    print(num_users, num_items)\n",
        "    print(latent_dim)\n",
        "\n",
        "    #load word embeddings\n",
        "    word_embedding_mtrx = ini_word_embed(len(word_dict), word_latent_dim)\n",
        "    # word_embedding_mtrx = word2vec_word_embed(len(word_dict), word_latent_dim,\n",
        "    #                                           \"Directory of pretrained WordEmbedding.out\",\n",
        "    #                                           word_dict)\n",
        "\n",
        "    print( \"shape\", word_embedding_mtrx.shape)\n",
        "\n",
        "    # get train instances\n",
        "    user_input, item_input, rateings = get_train_instance(train)\n",
        "    print (len(user_input), len(item_input), len(rateings))\n",
        "\n",
        "    # get test/val instances\n",
        "    user_vals, item_vals, user_input_val, item_input_val, rating_input_val = get_test_list(200, valRatings, user_reviews, item_reviews)\n",
        "    user_tests, item_tests, user_input_test, item_input_test, rating_input_test = get_test_list(200, testRatings, user_reviews, item_reviews)\n",
        "\n",
        "    # #train & eval model\n",
        "    # train_model()\n",
        "\n",
        "    # latent_dim = 25\n",
        "    # #train & eval model\n",
        "    # train_model()\n",
        "\n",
        "    # latent_dim = 50\n",
        "    # #train & eval model\n",
        "    # train_model()\n",
        "\n",
        "    # latent_dim = 100\n",
        "    # #train & eval model\n",
        "    # train_model()\n",
        "\n",
        "    # latent_dim = 150\n",
        "    # #train & eval model\n",
        "    # train_model()\n",
        "\n",
        "    # latent_dim = 200\n",
        "    # #train & eval model\n",
        "    # train_model()\n",
        "\n",
        "    latent_dim = 300\n",
        "    #train & eval model\n",
        "    train_model()"
      ],
      "id": "ckPxBumcDdkb"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R1nVse_jDcMn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 450
        },
        "outputId": "7e672739-ded0-4372-d468-43a59b50ed6e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "avg mae over latent dimensions = [15, 25, 50, 100, 150, 200, 300]: [0.8336581640065943, 0.8367746955439975, 0.8384994930729953, 0.8393451425843893, 0.8401304164530726, 0.8422882341843104, 0.8492282073119388]\n",
            "avg mse over latent dimensions = [15, 25, 50, 100, 150, 200, 300]: [1.3068985484525304, 1.30281279577794, 1.304984137262511, 1.3077363295802589, 1.3097868493808178, 1.3139178403260174, 1.3360866416687927]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 750x350 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "minimum avg mae over latent dimensions = 0.8336581640065943 at latent dimension = 15 and mse = 1.3068985484525304\n",
            "minimum avg mse over latent dimensions = 1.30281279577794 at laten dimension = 25 and mae = 0.8367746955439975\n"
          ]
        }
      ],
      "source": [
        "avg_mae_list = [0.8336581640065943,0.8367746955439975,0.8384994930729953,0.8393451425843893,0.8401304164530726,0.8422882341843104,0.8492282073119388]\n",
        "avg_mse_list = [1.3068985484525304,1.30281279577794,1.304984137262511,1.3077363295802589,1.3097868493808178,1.3139178403260174,1.3360866416687927]\n",
        "latent_dim_list = [15, 25, 50, 100, 150, 200, 300]\n",
        "print(f'avg mae over latent dimensions = [15, 25, 50, 100, 150, 200, 300]: {avg_mae_list}')\n",
        "print(f'avg mse over latent dimensions = [15, 25, 50, 100, 150, 200, 300]: {avg_mse_list}')\n",
        "    \n",
        "plt.rcParams.update({'figure.figsize':(7.5,3.5), 'figure.dpi':100})\n",
        "fig, axes = plt.subplots(1, 2)\n",
        "axes[0].plot(latent_dim_list, avg_mae_list, 'm--o')\n",
        "axes[0].set_title('Avg MAE over varying latent dimensions')\n",
        "axes[0].set_xlabel('Latent Dimension')\n",
        "axes[0].set_ylabel('MAE')\n",
        "\n",
        "axes[1].plot(latent_dim_list, avg_mse_list, 'g--o')\n",
        "axes[1].set_title('Avg MSE over varying latent dimensions')\n",
        "axes[1].set_xlabel('Latentdimension')\n",
        "axes[1].set_ylabel('MSE')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "index = 0\n",
        "min = 100\n",
        "min_index = 0\n",
        "for i in avg_mae_list:\n",
        "  if i<min:\n",
        "    min = i\n",
        "    min_index = index\n",
        "  index+=1\n",
        "print(f'minimum avg mae over latent dimensions = {min} at latent dimension = {latent_dim_list[min_index]} and mse = {avg_mse_list[min_index]}')\n",
        "\n",
        "index = 0\n",
        "min = 100\n",
        "min_index = 0\n",
        "for i in avg_mse_list:\n",
        "  if i<min:\n",
        "    min = i\n",
        "    min_index = index\n",
        "  index+=1\n",
        "print(f'minimum avg mse over latent dimensions = {min} at laten dimension = {latent_dim_list[min_index]} and mae = {avg_mae_list[min_index]}')"
      ],
      "id": "R1nVse_jDcMn"
    },
    {
      "cell_type": "markdown",
      "source": [
        "latent dimensions = 15"
      ],
      "metadata": {
        "id": "2njsZzcQWhMI"
      },
      "id": "2njsZzcQWhMI"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jMa5CMF6Dfb5"
      },
      "source": [
        "Varying dropout rates"
      ],
      "id": "jMa5CMF6Dfb5"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hNFwT-mADevc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d6a7f282-a775-4f97-d693-bad5a6b7912e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "wordId_dict finished\n",
            "load reviews finished\n",
            "load data: 1.448s\n",
            "506 2581\n",
            "25\n",
            "shape (19930, 300)\n",
            "18880 18880 18880\n",
            "epoch0 train time: 4.964s test time: 0.528  loss = 42.522 val_mse = 1.636 mse = 1.561 mae = 0.906\n",
            "epoch1 train time: 4.623s test time: 0.530  loss = 32.014 val_mse = 1.618 mse = 1.551 mae = 0.903\n",
            "epoch2 train time: 4.646s test time: 0.538  loss = 17.242 val_mse = 1.591 mse = 1.532 mae = 0.894\n",
            "epoch3 train time: 4.670s test time: 0.536  loss = 8.598 val_mse = 1.563 mse = 1.512 mae = 0.885\n",
            "epoch4 train time: 4.696s test time: 0.540  loss = 4.204 val_mse = 1.534 mse = 1.491 mae = 0.875\n",
            "epoch5 train time: 4.714s test time: 0.543  loss = 2.300 val_mse = 1.505 mse = 1.470 mae = 0.865\n",
            "epoch6 train time: 4.731s test time: 0.548  loss = 1.649 val_mse = 1.473 mse = 1.448 mae = 0.858\n",
            "epoch7 train time: 4.757s test time: 0.545  loss = 1.471 val_mse = 1.434 mse = 1.419 mae = 0.852\n",
            "epoch8 train time: 4.787s test time: 0.545  loss = 1.380 val_mse = 1.399 mse = 1.395 mae = 0.849\n",
            "epoch9 train time: 4.794s test time: 0.546  loss = 1.306 val_mse = 1.370 mse = 1.376 mae = 0.847\n",
            "epoch10 train time: 4.814s test time: 0.544  loss = 1.241 val_mse = 1.348 mse = 1.364 mae = 0.845\n",
            "epoch11 train time: 4.830s test time: 0.548  loss = 1.187 val_mse = 1.333 mse = 1.356 mae = 0.843\n",
            "epoch12 train time: 4.816s test time: 0.549  loss = 1.139 val_mse = 1.319 mse = 1.349 mae = 0.840\n",
            "epoch13 train time: 4.804s test time: 0.553  loss = 1.096 val_mse = 1.305 mse = 1.341 mae = 0.838\n",
            "epoch14 train time: 4.806s test time: 0.542  loss = 1.058 val_mse = 1.290 mse = 1.333 mae = 0.837\n",
            "epoch15 train time: 4.812s test time: 0.550  loss = 1.024 val_mse = 1.279 mse = 1.327 mae = 0.837\n",
            "epoch16 train time: 4.813s test time: 0.546  loss = 0.996 val_mse = 1.262 mse = 1.319 mae = 0.837\n",
            "epoch17 train time: 4.828s test time: 0.550  loss = 0.973 val_mse = 1.250 mse = 1.310 mae = 0.840\n",
            "epoch18 train time: 4.868s test time: 0.552  loss = 0.955 val_mse = 1.238 mse = 1.307 mae = 0.844\n",
            "epoch19 train time: 4.884s test time: 0.560  loss = 0.940 val_mse = 1.236 mse = 1.302 mae = 0.844\n",
            "epoch20 train time: 4.893s test time: 0.557  loss = 0.924 val_mse = 1.230 mse = 1.306 mae = 0.848\n",
            "epoch21 train time: 4.879s test time: 0.555  loss = 0.921 val_mse = 1.221 mse = 1.292 mae = 0.850\n",
            "epoch22 train time: 4.866s test time: 0.554  loss = 0.918 val_mse = 1.234 mse = 1.298 mae = 0.840\n",
            "epoch23 train time: 4.857s test time: 0.575  loss = 0.909 val_mse = 1.218 mse = 1.295 mae = 0.843\n",
            "epoch24 train time: 4.848s test time: 0.551  loss = 0.900 val_mse = 1.210 mse = 1.297 mae = 0.855\n",
            "epoch25 train time: 4.852s test time: 0.555  loss = 0.894 val_mse = 1.222 mse = 1.300 mae = 0.849\n",
            "epoch26 train time: 4.850s test time: 0.556  loss = 0.888 val_mse = 1.249 mse = 1.318 mae = 0.837\n",
            "epoch27 train time: 4.840s test time: 0.558  loss = 0.882 val_mse = 1.222 mse = 1.313 mae = 0.846\n",
            "epoch28 train time: 4.852s test time: 0.548  loss = 0.878 val_mse = 1.257 mse = 1.329 mae = 0.833\n",
            "epoch29 train time: 4.870s test time: 0.547  loss = 0.872 val_mse = 1.248 mse = 1.328 mae = 0.839\n",
            "epoch30 train time: 4.840s test time: 0.557  loss = 0.867 val_mse = 1.244 mse = 1.318 mae = 0.842\n",
            "epoch31 train time: 4.857s test time: 0.548  loss = 0.874 val_mse = 1.242 mse = 1.329 mae = 0.843\n",
            "epoch32 train time: 4.848s test time: 0.553  loss = 0.861 val_mse = 1.232 mse = 1.315 mae = 0.850\n",
            "epoch33 train time: 4.855s test time: 0.548  loss = 0.862 val_mse = 1.213 mse = 1.311 mae = 0.848\n",
            "epoch34 train time: 4.863s test time: 0.555  loss = 0.857 val_mse = 1.221 mse = 1.309 mae = 0.852\n",
            "epoch35 train time: 4.849s test time: 0.558  loss = 0.859 val_mse = 1.221 mse = 1.313 mae = 0.849\n",
            "epoch36 train time: 4.850s test time: 0.564  loss = 0.859 val_mse = 1.253 mse = 1.316 mae = 0.837\n",
            "epoch37 train time: 4.861s test time: 0.549  loss = 0.851 val_mse = 1.219 mse = 1.314 mae = 0.846\n",
            "epoch38 train time: 4.878s test time: 0.552  loss = 0.852 val_mse = 1.234 mse = 1.314 mae = 0.847\n",
            "epoch39 train time: 4.871s test time: 0.553  loss = 0.851 val_mse = 1.249 mse = 1.325 mae = 0.838\n",
            "epoch40 train time: 4.863s test time: 0.557  loss = 0.844 val_mse = 1.268 mse = 1.342 mae = 0.837\n",
            "epoch41 train time: 4.856s test time: 0.557  loss = 0.845 val_mse = 1.231 mse = 1.321 mae = 0.839\n",
            "epoch42 train time: 4.859s test time: 0.574  loss = 0.843 val_mse = 1.254 mse = 1.333 mae = 0.837\n",
            "epoch43 train time: 4.872s test time: 0.558  loss = 0.840 val_mse = 1.242 mse = 1.328 mae = 0.841\n",
            "epoch44 train time: 4.881s test time: 0.554  loss = 0.840 val_mse = 1.257 mse = 1.330 mae = 0.838\n",
            "epoch45 train time: 4.876s test time: 0.552  loss = 0.843 val_mse = 1.241 mse = 1.319 mae = 0.838\n",
            "epoch46 train time: 4.885s test time: 0.550  loss = 0.837 val_mse = 1.244 mse = 1.327 mae = 0.841\n",
            "epoch47 train time: 4.880s test time: 0.561  loss = 0.840 val_mse = 1.205 mse = 1.309 mae = 0.849\n",
            "epoch48 train time: 4.869s test time: 0.560  loss = 0.835 val_mse = 1.231 mse = 1.309 mae = 0.843\n",
            "epoch49 train time: 4.852s test time: 0.553  loss = 0.837 val_mse = 1.209 mse = 1.296 mae = 0.847\n",
            "epoch50 train time: 4.868s test time: 0.563  loss = 0.834 val_mse = 1.223 mse = 1.315 mae = 0.844\n",
            "epoch51 train time: 4.881s test time: 0.568  loss = 0.839 val_mse = 1.205 mse = 1.305 mae = 0.845\n",
            "epoch52 train time: 4.885s test time: 0.558  loss = 0.828 val_mse = 1.235 mse = 1.314 mae = 0.842\n",
            "epoch53 train time: 4.886s test time: 0.548  loss = 0.834 val_mse = 1.207 mse = 1.302 mae = 0.852\n",
            "epoch54 train time: 4.877s test time: 0.554  loss = 0.829 val_mse = 1.213 mse = 1.307 mae = 0.846\n",
            "epoch55 train time: 4.891s test time: 0.552  loss = 0.828 val_mse = 1.209 mse = 1.296 mae = 0.846\n",
            "epoch56 train time: 4.890s test time: 0.561  loss = 0.824 val_mse = 1.221 mse = 1.310 mae = 0.842\n",
            "epoch57 train time: 4.902s test time: 0.574  loss = 0.827 val_mse = 1.211 mse = 1.300 mae = 0.850\n",
            "epoch58 train time: 4.884s test time: 0.555  loss = 0.825 val_mse = 1.216 mse = 1.311 mae = 0.846\n",
            "epoch59 train time: 4.879s test time: 0.553  loss = 0.829 val_mse = 1.207 mse = 1.303 mae = 0.847\n",
            "epoch60 train time: 4.869s test time: 0.569  loss = 0.823 val_mse = 1.221 mse = 1.305 mae = 0.839\n",
            "epoch61 train time: 4.901s test time: 0.551  loss = 0.823 val_mse = 1.210 mse = 1.300 mae = 0.846\n",
            "epoch62 train time: 4.874s test time: 0.552  loss = 0.819 val_mse = 1.213 mse = 1.308 mae = 0.841\n",
            "epoch63 train time: 4.874s test time: 0.557  loss = 0.821 val_mse = 1.196 mse = 1.293 mae = 0.847\n",
            "epoch64 train time: 4.903s test time: 0.552  loss = 0.819 val_mse = 1.216 mse = 1.309 mae = 0.842\n",
            "epoch65 train time: 4.880s test time: 0.558  loss = 0.824 val_mse = 1.232 mse = 1.318 mae = 0.835\n",
            "epoch66 train time: 4.905s test time: 0.564  loss = 0.817 val_mse = 1.209 mse = 1.299 mae = 0.838\n",
            "epoch67 train time: 4.887s test time: 0.552  loss = 0.818 val_mse = 1.196 mse = 1.305 mae = 0.849\n",
            "epoch68 train time: 4.883s test time: 0.553  loss = 0.819 val_mse = 1.217 mse = 1.307 mae = 0.841\n",
            "epoch69 train time: 4.867s test time: 0.551  loss = 0.819 val_mse = 1.224 mse = 1.305 mae = 0.831\n",
            "epoch70 train time: 4.855s test time: 0.554  loss = 0.814 val_mse = 1.212 mse = 1.299 mae = 0.846\n",
            "epoch71 train time: 4.859s test time: 0.552  loss = 0.818 val_mse = 1.224 mse = 1.311 mae = 0.831\n",
            "epoch72 train time: 4.865s test time: 0.551  loss = 0.816 val_mse = 1.240 mse = 1.326 mae = 0.832\n",
            "epoch73 train time: 4.856s test time: 0.569  loss = 0.813 val_mse = 1.206 mse = 1.292 mae = 0.837\n",
            "epoch74 train time: 4.847s test time: 0.545  loss = 0.812 val_mse = 1.224 mse = 1.311 mae = 0.838\n",
            "epoch75 train time: 4.868s test time: 0.554  loss = 0.811 val_mse = 1.201 mse = 1.299 mae = 0.838\n",
            "epoch76 train time: 4.863s test time: 0.549  loss = 0.812 val_mse = 1.209 mse = 1.298 mae = 0.838\n",
            "epoch77 train time: 4.860s test time: 0.559  loss = 0.811 val_mse = 1.207 mse = 1.299 mae = 0.841\n",
            "epoch78 train time: 4.859s test time: 0.554  loss = 0.810 val_mse = 1.211 mse = 1.302 mae = 0.844\n",
            "epoch79 train time: 4.874s test time: 0.555  loss = 0.808 val_mse = 1.202 mse = 1.290 mae = 0.836\n",
            "epoch80 train time: 4.859s test time: 0.549  loss = 0.806 val_mse = 1.216 mse = 1.302 mae = 0.838\n",
            "epoch81 train time: 4.866s test time: 0.560  loss = 0.809 val_mse = 1.192 mse = 1.297 mae = 0.838\n",
            "epoch82 train time: 4.891s test time: 0.550  loss = 0.804 val_mse = 1.202 mse = 1.291 mae = 0.841\n",
            "epoch83 train time: 4.882s test time: 0.552  loss = 0.807 val_mse = 1.195 mse = 1.286 mae = 0.838\n",
            "epoch84 train time: 4.888s test time: 0.557  loss = 0.805 val_mse = 1.205 mse = 1.305 mae = 0.842\n",
            "epoch85 train time: 4.887s test time: 0.562  loss = 0.807 val_mse = 1.201 mse = 1.290 mae = 0.837\n",
            "epoch86 train time: 4.865s test time: 0.558  loss = 0.805 val_mse = 1.210 mse = 1.302 mae = 0.840\n",
            "epoch87 train time: 4.865s test time: 0.552  loss = 0.807 val_mse = 1.194 mse = 1.295 mae = 0.841\n",
            "epoch88 train time: 4.872s test time: 0.563  loss = 0.804 val_mse = 1.206 mse = 1.300 mae = 0.838\n",
            "epoch89 train time: 4.898s test time: 0.562  loss = 0.803 val_mse = 1.195 mse = 1.293 mae = 0.837\n",
            "epoch90 train time: 4.880s test time: 0.549  loss = 0.799 val_mse = 1.205 mse = 1.296 mae = 0.841\n",
            "epoch91 train time: 4.863s test time: 0.554  loss = 0.803 val_mse = 1.200 mse = 1.291 mae = 0.836\n",
            "epoch92 train time: 4.868s test time: 0.552  loss = 0.800 val_mse = 1.211 mse = 1.299 mae = 0.839\n",
            "epoch93 train time: 4.887s test time: 0.575  loss = 0.803 val_mse = 1.191 mse = 1.285 mae = 0.837\n",
            "epoch94 train time: 4.869s test time: 0.552  loss = 0.798 val_mse = 1.208 mse = 1.300 mae = 0.837\n",
            "epoch95 train time: 4.856s test time: 0.549  loss = 0.801 val_mse = 1.214 mse = 1.302 mae = 0.829\n",
            "epoch96 train time: 4.870s test time: 0.549  loss = 0.799 val_mse = 1.214 mse = 1.306 mae = 0.833\n",
            "epoch97 train time: 4.861s test time: 0.555  loss = 0.802 val_mse = 1.202 mse = 1.294 mae = 0.835\n",
            "epoch98 train time: 4.862s test time: 0.552  loss = 0.800 val_mse = 1.225 mse = 1.309 mae = 0.830\n",
            "epoch99 train time: 4.882s test time: 0.561  loss = 0.796 val_mse = 1.207 mse = 1.299 mae = 0.831\n",
            "epoch100 train time: 4.877s test time: 0.571  loss = 0.799 val_mse = 1.223 mse = 1.304 mae = 0.832\n",
            "epoch101 train time: 4.901s test time: 0.553  loss = 0.798 val_mse = 1.207 mse = 1.296 mae = 0.828\n",
            "epoch102 train time: 4.860s test time: 0.561  loss = 0.792 val_mse = 1.224 mse = 1.311 mae = 0.831\n",
            "epoch103 train time: 4.883s test time: 0.549  loss = 0.796 val_mse = 1.206 mse = 1.301 mae = 0.833\n",
            "epoch104 train time: 4.869s test time: 0.557  loss = 0.795 val_mse = 1.231 mse = 1.315 mae = 0.830\n",
            "epoch105 train time: 4.865s test time: 0.561  loss = 0.797 val_mse = 1.216 mse = 1.306 mae = 0.831\n",
            "epoch106 train time: 4.897s test time: 0.560  loss = 0.794 val_mse = 1.211 mse = 1.305 mae = 0.833\n",
            "epoch107 train time: 4.890s test time: 0.553  loss = 0.797 val_mse = 1.202 mse = 1.295 mae = 0.833\n",
            "epoch108 train time: 4.845s test time: 0.550  loss = 0.790 val_mse = 1.213 mse = 1.305 mae = 0.834\n",
            "epoch109 train time: 4.853s test time: 0.556  loss = 0.791 val_mse = 1.201 mse = 1.300 mae = 0.831\n",
            "epoch110 train time: 4.855s test time: 0.554  loss = 0.791 val_mse = 1.219 mse = 1.309 mae = 0.830\n",
            "epoch111 train time: 4.864s test time: 0.557  loss = 0.791 val_mse = 1.206 mse = 1.300 mae = 0.831\n",
            "epoch112 train time: 4.873s test time: 0.561  loss = 0.786 val_mse = 1.207 mse = 1.304 mae = 0.837\n",
            "epoch113 train time: 4.887s test time: 0.553  loss = 0.790 val_mse = 1.200 mse = 1.298 mae = 0.832\n",
            "epoch114 train time: 4.880s test time: 0.550  loss = 0.789 val_mse = 1.198 mse = 1.300 mae = 0.836\n",
            "epoch115 train time: 4.859s test time: 0.558  loss = 0.787 val_mse = 1.208 mse = 1.308 mae = 0.833\n",
            "epoch116 train time: 4.869s test time: 0.557  loss = 0.786 val_mse = 1.214 mse = 1.304 mae = 0.833\n",
            "epoch117 train time: 4.879s test time: 0.569  loss = 0.794 val_mse = 1.202 mse = 1.297 mae = 0.829\n",
            "epoch118 train time: 4.868s test time: 0.552  loss = 0.786 val_mse = 1.208 mse = 1.306 mae = 0.834\n",
            "epoch119 train time: 4.867s test time: 0.551  loss = 0.789 val_mse = 1.194 mse = 1.295 mae = 0.829\n",
            "epoch120 train time: 4.859s test time: 0.554  loss = 0.784 val_mse = 1.196 mse = 1.298 mae = 0.834\n",
            "epoch121 train time: 4.871s test time: 0.552  loss = 0.787 val_mse = 1.203 mse = 1.303 mae = 0.830\n",
            "epoch122 train time: 4.889s test time: 0.557  loss = 0.786 val_mse = 1.205 mse = 1.300 mae = 0.833\n",
            "epoch123 train time: 4.875s test time: 0.551  loss = 0.787 val_mse = 1.188 mse = 1.293 mae = 0.832\n",
            "epoch124 train time: 4.867s test time: 0.555  loss = 0.782 val_mse = 1.210 mse = 1.306 mae = 0.832\n",
            "epoch125 train time: 4.856s test time: 0.560  loss = 0.789 val_mse = 1.199 mse = 1.299 mae = 0.830\n",
            "epoch126 train time: 4.849s test time: 0.549  loss = 0.787 val_mse = 1.206 mse = 1.301 mae = 0.832\n",
            "epoch127 train time: 4.870s test time: 0.561  loss = 0.787 val_mse = 1.198 mse = 1.297 mae = 0.832\n",
            "epoch128 train time: 4.876s test time: 0.554  loss = 0.782 val_mse = 1.200 mse = 1.299 mae = 0.832\n",
            "epoch129 train time: 4.874s test time: 0.550  loss = 0.787 val_mse = 1.201 mse = 1.296 mae = 0.831\n",
            "epoch130 train time: 4.864s test time: 0.553  loss = 0.783 val_mse = 1.199 mse = 1.303 mae = 0.834\n",
            "epoch131 train time: 4.867s test time: 0.555  loss = 0.783 val_mse = 1.200 mse = 1.300 mae = 0.830\n",
            "epoch132 train time: 4.878s test time: 0.562  loss = 0.784 val_mse = 1.206 mse = 1.300 mae = 0.832\n",
            "epoch133 train time: 4.873s test time: 0.558  loss = 0.785 val_mse = 1.190 mse = 1.293 mae = 0.835\n",
            "epoch134 train time: 4.861s test time: 0.553  loss = 0.782 val_mse = 1.204 mse = 1.304 mae = 0.830\n",
            "epoch135 train time: 4.863s test time: 0.548  loss = 0.783 val_mse = 1.201 mse = 1.297 mae = 0.831\n",
            "epoch136 train time: 4.865s test time: 0.552  loss = 0.787 val_mse = 1.201 mse = 1.300 mae = 0.829\n",
            "epoch137 train time: 4.872s test time: 0.551  loss = 0.784 val_mse = 1.197 mse = 1.300 mae = 0.829\n",
            "epoch138 train time: 4.878s test time: 0.563  loss = 0.779 val_mse = 1.199 mse = 1.304 mae = 0.832\n",
            "epoch139 train time: 4.862s test time: 0.560  loss = 0.781 val_mse = 1.191 mse = 1.290 mae = 0.830\n",
            "epoch140 train time: 4.863s test time: 0.554  loss = 0.780 val_mse = 1.199 mse = 1.301 mae = 0.832\n",
            "epoch141 train time: 4.866s test time: 0.548  loss = 0.783 val_mse = 1.189 mse = 1.300 mae = 0.832\n",
            "epoch142 train time: 4.874s test time: 0.554  loss = 0.782 val_mse = 1.197 mse = 1.296 mae = 0.834\n",
            "epoch143 train time: 4.849s test time: 0.547  loss = 0.783 val_mse = 1.191 mse = 1.294 mae = 0.830\n",
            "epoch144 train time: 4.863s test time: 0.553  loss = 0.778 val_mse = 1.196 mse = 1.301 mae = 0.830\n",
            "epoch145 train time: 4.870s test time: 0.558  loss = 0.782 val_mse = 1.188 mse = 1.299 mae = 0.832\n",
            "epoch146 train time: 4.877s test time: 0.550  loss = 0.782 val_mse = 1.196 mse = 1.297 mae = 0.830\n",
            "epoch147 train time: 4.856s test time: 0.557  loss = 0.778 val_mse = 1.189 mse = 1.288 mae = 0.828\n",
            "epoch148 train time: 4.864s test time: 0.549  loss = 0.780 val_mse = 1.198 mse = 1.298 mae = 0.831\n",
            "epoch149 train time: 4.863s test time: 0.553  loss = 0.781 val_mse = 1.196 mse = 1.297 mae = 0.829\n",
            "epoch150 train time: 4.867s test time: 0.550  loss = 0.777 val_mse = 1.197 mse = 1.292 mae = 0.829\n",
            "epoch151 train time: 4.841s test time: 0.556  loss = 0.781 val_mse = 1.189 mse = 1.292 mae = 0.829\n",
            "epoch152 train time: 4.852s test time: 0.551  loss = 0.777 val_mse = 1.194 mse = 1.296 mae = 0.829\n",
            "epoch153 train time: 4.857s test time: 0.548  loss = 0.776 val_mse = 1.191 mse = 1.288 mae = 0.828\n",
            "epoch154 train time: 4.853s test time: 0.549  loss = 0.779 val_mse = 1.195 mse = 1.296 mae = 0.833\n",
            "epoch155 train time: 4.868s test time: 0.555  loss = 0.780 val_mse = 1.188 mse = 1.294 mae = 0.829\n",
            "epoch156 train time: 4.872s test time: 0.553  loss = 0.775 val_mse = 1.199 mse = 1.301 mae = 0.831\n",
            "epoch157 train time: 4.837s test time: 0.543  loss = 0.779 val_mse = 1.194 mse = 1.289 mae = 0.826\n",
            "epoch158 train time: 4.836s test time: 0.558  loss = 0.777 val_mse = 1.198 mse = 1.297 mae = 0.828\n",
            "epoch159 train time: 4.864s test time: 0.545  loss = 0.777 val_mse = 1.191 mse = 1.294 mae = 0.829\n",
            "epoch160 train time: 4.852s test time: 0.549  loss = 0.778 val_mse = 1.194 mse = 1.292 mae = 0.827\n",
            "epoch161 train time: 4.848s test time: 0.549  loss = 0.776 val_mse = 1.192 mse = 1.291 mae = 0.826\n",
            "epoch162 train time: 4.851s test time: 0.560  loss = 0.779 val_mse = 1.194 mse = 1.293 mae = 0.830\n",
            "epoch163 train time: 4.862s test time: 0.547  loss = 0.779 val_mse = 1.187 mse = 1.292 mae = 0.828\n",
            "epoch164 train time: 4.862s test time: 0.557  loss = 0.775 val_mse = 1.197 mse = 1.292 mae = 0.828\n",
            "epoch165 train time: 4.856s test time: 0.551  loss = 0.779 val_mse = 1.189 mse = 1.291 mae = 0.829\n",
            "epoch166 train time: 4.850s test time: 0.549  loss = 0.778 val_mse = 1.193 mse = 1.293 mae = 0.828\n",
            "epoch167 train time: 4.847s test time: 0.545  loss = 0.777 val_mse = 1.191 mse = 1.291 mae = 0.828\n",
            "epoch168 train time: 4.857s test time: 0.561  loss = 0.778 val_mse = 1.194 mse = 1.291 mae = 0.830\n",
            "epoch169 train time: 4.853s test time: 0.554  loss = 0.777 val_mse = 1.188 mse = 1.289 mae = 0.828\n",
            "epoch170 train time: 4.860s test time: 0.555  loss = 0.775 val_mse = 1.200 mse = 1.295 mae = 0.829\n",
            "epoch171 train time: 4.857s test time: 0.563  loss = 0.780 val_mse = 1.191 mse = 1.293 mae = 0.831\n",
            "epoch172 train time: 4.854s test time: 0.557  loss = 0.779 val_mse = 1.195 mse = 1.293 mae = 0.828\n",
            "epoch173 train time: 4.845s test time: 0.559  loss = 0.776 val_mse = 1.198 mse = 1.293 mae = 0.828\n",
            "epoch174 train time: 4.849s test time: 0.551  loss = 0.779 val_mse = 1.202 mse = 1.302 mae = 0.830\n",
            "epoch175 train time: 4.840s test time: 0.561  loss = 0.776 val_mse = 1.184 mse = 1.289 mae = 0.830\n",
            "epoch176 train time: 4.864s test time: 0.548  loss = 0.775 val_mse = 1.194 mse = 1.292 mae = 0.827\n",
            "epoch177 train time: 4.867s test time: 0.552  loss = 0.775 val_mse = 1.188 mse = 1.292 mae = 0.831\n",
            "epoch178 train time: 4.867s test time: 0.556  loss = 0.775 val_mse = 1.194 mse = 1.298 mae = 0.829\n",
            "epoch179 train time: 4.867s test time: 0.564  loss = 0.773 val_mse = 1.183 mse = 1.289 mae = 0.829\n",
            "MAE 0.8381531545991346\n",
            "MSE 1.3133382432145548\n",
            "epoch0 train time: 5.350s test time: 0.560  loss = 42.825 val_mse = 1.644 mse = 1.568 mae = 0.909\n",
            "epoch1 train time: 4.936s test time: 0.563  loss = 32.084 val_mse = 1.620 mse = 1.554 mae = 0.904\n",
            "epoch2 train time: 4.958s test time: 0.567  loss = 17.315 val_mse = 1.589 mse = 1.533 mae = 0.894\n",
            "epoch3 train time: 4.937s test time: 0.554  loss = 8.654 val_mse = 1.561 mse = 1.512 mae = 0.884\n",
            "epoch4 train time: 4.954s test time: 0.552  loss = 4.239 val_mse = 1.533 mse = 1.492 mae = 0.874\n",
            "epoch5 train time: 4.936s test time: 0.551  loss = 2.316 val_mse = 1.504 mse = 1.472 mae = 0.865\n",
            "epoch6 train time: 4.916s test time: 0.560  loss = 1.658 val_mse = 1.475 mse = 1.451 mae = 0.857\n",
            "epoch7 train time: 4.909s test time: 0.555  loss = 1.486 val_mse = 1.443 mse = 1.429 mae = 0.852\n",
            "epoch8 train time: 4.917s test time: 0.548  loss = 1.406 val_mse = 1.412 mse = 1.408 mae = 0.847\n",
            "epoch9 train time: 4.931s test time: 0.560  loss = 1.340 val_mse = 1.386 mse = 1.391 mae = 0.845\n",
            "epoch10 train time: 4.950s test time: 0.551  loss = 1.282 val_mse = 1.367 mse = 1.379 mae = 0.842\n",
            "epoch11 train time: 4.910s test time: 0.558  loss = 1.229 val_mse = 1.352 mse = 1.372 mae = 0.840\n",
            "epoch12 train time: 4.919s test time: 0.560  loss = 1.181 val_mse = 1.337 mse = 1.364 mae = 0.838\n",
            "epoch13 train time: 4.909s test time: 0.551  loss = 1.138 val_mse = 1.323 mse = 1.357 mae = 0.836\n",
            "epoch14 train time: 4.916s test time: 0.551  loss = 1.098 val_mse = 1.309 mse = 1.349 mae = 0.836\n",
            "epoch15 train time: 4.914s test time: 0.555  loss = 1.063 val_mse = 1.295 mse = 1.341 mae = 0.834\n",
            "epoch16 train time: 4.925s test time: 0.551  loss = 1.033 val_mse = 1.270 mse = 1.323 mae = 0.836\n",
            "epoch17 train time: 4.917s test time: 0.560  loss = 1.007 val_mse = 1.262 mse = 1.322 mae = 0.838\n",
            "epoch18 train time: 4.917s test time: 0.550  loss = 0.989 val_mse = 1.243 mse = 1.308 mae = 0.846\n",
            "epoch19 train time: 4.916s test time: 0.552  loss = 0.971 val_mse = 1.256 mse = 1.317 mae = 0.843\n",
            "epoch20 train time: 4.906s test time: 0.552  loss = 0.958 val_mse = 1.230 mse = 1.302 mae = 0.849\n",
            "epoch21 train time: 4.919s test time: 0.549  loss = 0.944 val_mse = 1.228 mse = 1.309 mae = 0.843\n",
            "epoch22 train time: 4.911s test time: 0.561  loss = 0.946 val_mse = 1.213 mse = 1.290 mae = 0.850\n",
            "epoch23 train time: 4.905s test time: 0.549  loss = 0.935 val_mse = 1.223 mse = 1.294 mae = 0.844\n",
            "epoch24 train time: 4.909s test time: 0.558  loss = 0.922 val_mse = 1.219 mse = 1.303 mae = 0.854\n",
            "epoch25 train time: 4.901s test time: 0.552  loss = 0.915 val_mse = 1.228 mse = 1.313 mae = 0.855\n",
            "epoch26 train time: 4.916s test time: 0.568  loss = 0.912 val_mse = 1.241 mse = 1.316 mae = 0.844\n",
            "epoch27 train time: 4.916s test time: 0.549  loss = 0.905 val_mse = 1.252 mse = 1.321 mae = 0.837\n",
            "epoch28 train time: 4.935s test time: 0.546  loss = 0.901 val_mse = 1.266 mse = 1.343 mae = 0.841\n",
            "epoch29 train time: 4.917s test time: 0.554  loss = 0.896 val_mse = 1.251 mse = 1.332 mae = 0.837\n",
            "epoch30 train time: 4.928s test time: 0.552  loss = 0.893 val_mse = 1.274 mse = 1.343 mae = 0.838\n",
            "epoch31 train time: 4.919s test time: 0.559  loss = 0.887 val_mse = 1.236 mse = 1.320 mae = 0.847\n",
            "epoch32 train time: 4.936s test time: 0.550  loss = 0.897 val_mse = 1.252 mse = 1.334 mae = 0.841\n",
            "epoch33 train time: 4.927s test time: 0.549  loss = 0.877 val_mse = 1.266 mse = 1.331 mae = 0.837\n",
            "epoch34 train time: 4.938s test time: 0.554  loss = 0.882 val_mse = 1.217 mse = 1.306 mae = 0.846\n",
            "epoch35 train time: 4.952s test time: 0.557  loss = 0.882 val_mse = 1.210 mse = 1.300 mae = 0.852\n",
            "epoch36 train time: 4.941s test time: 0.558  loss = 0.880 val_mse = 1.238 mse = 1.320 mae = 0.839\n",
            "epoch37 train time: 4.946s test time: 0.561  loss = 0.864 val_mse = 1.232 mse = 1.303 mae = 0.846\n",
            "epoch38 train time: 4.929s test time: 0.552  loss = 0.880 val_mse = 1.215 mse = 1.303 mae = 0.854\n",
            "epoch39 train time: 4.935s test time: 0.553  loss = 0.863 val_mse = 1.260 mse = 1.327 mae = 0.838\n",
            "epoch40 train time: 4.926s test time: 0.557  loss = 0.859 val_mse = 1.235 mse = 1.320 mae = 0.848\n",
            "epoch41 train time: 4.915s test time: 0.551  loss = 0.862 val_mse = 1.225 mse = 1.306 mae = 0.844\n",
            "epoch42 train time: 4.900s test time: 0.562  loss = 0.863 val_mse = 1.249 mse = 1.332 mae = 0.837\n",
            "epoch43 train time: 4.914s test time: 0.557  loss = 0.852 val_mse = 1.205 mse = 1.304 mae = 0.854\n",
            "epoch44 train time: 4.950s test time: 0.557  loss = 0.857 val_mse = 1.222 mse = 1.315 mae = 0.837\n",
            "epoch45 train time: 4.943s test time: 0.551  loss = 0.845 val_mse = 1.249 mse = 1.324 mae = 0.837\n",
            "epoch46 train time: 4.919s test time: 0.550  loss = 0.852 val_mse = 1.262 mse = 1.332 mae = 0.832\n",
            "epoch47 train time: 4.931s test time: 0.548  loss = 0.846 val_mse = 1.251 mse = 1.323 mae = 0.834\n",
            "epoch48 train time: 4.932s test time: 0.553  loss = 0.846 val_mse = 1.261 mse = 1.344 mae = 0.832\n",
            "epoch49 train time: 4.944s test time: 0.550  loss = 0.842 val_mse = 1.252 mse = 1.320 mae = 0.837\n",
            "epoch50 train time: 4.951s test time: 0.552  loss = 0.849 val_mse = 1.259 mse = 1.332 mae = 0.834\n",
            "epoch51 train time: 4.952s test time: 0.551  loss = 0.837 val_mse = 1.250 mse = 1.329 mae = 0.839\n",
            "epoch52 train time: 4.945s test time: 0.550  loss = 0.838 val_mse = 1.260 mse = 1.347 mae = 0.837\n",
            "epoch53 train time: 4.944s test time: 0.558  loss = 0.837 val_mse = 1.216 mse = 1.296 mae = 0.847\n",
            "epoch54 train time: 4.935s test time: 0.550  loss = 0.844 val_mse = 1.247 mse = 1.332 mae = 0.835\n",
            "epoch55 train time: 4.926s test time: 0.552  loss = 0.836 val_mse = 1.210 mse = 1.295 mae = 0.843\n",
            "epoch56 train time: 4.910s test time: 0.549  loss = 0.831 val_mse = 1.196 mse = 1.301 mae = 0.848\n",
            "epoch57 train time: 4.919s test time: 0.553  loss = 0.824 val_mse = 1.210 mse = 1.294 mae = 0.840\n",
            "epoch58 train time: 4.923s test time: 0.550  loss = 0.828 val_mse = 1.195 mse = 1.300 mae = 0.848\n",
            "epoch59 train time: 4.949s test time: 0.552  loss = 0.828 val_mse = 1.203 mse = 1.290 mae = 0.838\n",
            "epoch60 train time: 4.950s test time: 0.568  loss = 0.824 val_mse = 1.199 mse = 1.304 mae = 0.849\n",
            "epoch61 train time: 4.938s test time: 0.553  loss = 0.819 val_mse = 1.211 mse = 1.291 mae = 0.841\n",
            "epoch62 train time: 4.946s test time: 0.565  loss = 0.824 val_mse = 1.197 mse = 1.295 mae = 0.846\n",
            "epoch63 train time: 4.957s test time: 0.559  loss = 0.818 val_mse = 1.206 mse = 1.297 mae = 0.842\n",
            "epoch64 train time: 4.929s test time: 0.553  loss = 0.819 val_mse = 1.201 mse = 1.300 mae = 0.848\n",
            "epoch65 train time: 4.933s test time: 0.552  loss = 0.818 val_mse = 1.207 mse = 1.289 mae = 0.840\n",
            "epoch66 train time: 4.946s test time: 0.554  loss = 0.815 val_mse = 1.198 mse = 1.301 mae = 0.848\n",
            "epoch67 train time: 4.926s test time: 0.553  loss = 0.813 val_mse = 1.210 mse = 1.297 mae = 0.839\n",
            "epoch68 train time: 4.924s test time: 0.554  loss = 0.815 val_mse = 1.194 mse = 1.297 mae = 0.847\n",
            "epoch69 train time: 4.933s test time: 0.562  loss = 0.810 val_mse = 1.201 mse = 1.287 mae = 0.835\n",
            "epoch70 train time: 4.916s test time: 0.553  loss = 0.809 val_mse = 1.192 mse = 1.303 mae = 0.847\n",
            "epoch71 train time: 4.932s test time: 0.546  loss = 0.811 val_mse = 1.211 mse = 1.293 mae = 0.839\n",
            "epoch72 train time: 4.923s test time: 0.551  loss = 0.815 val_mse = 1.227 mse = 1.309 mae = 0.834\n",
            "epoch73 train time: 4.949s test time: 0.559  loss = 0.805 val_mse = 1.226 mse = 1.309 mae = 0.832\n",
            "epoch74 train time: 4.917s test time: 0.551  loss = 0.806 val_mse = 1.217 mse = 1.316 mae = 0.838\n",
            "epoch75 train time: 4.918s test time: 0.561  loss = 0.803 val_mse = 1.222 mse = 1.310 mae = 0.834\n",
            "epoch76 train time: 4.925s test time: 0.565  loss = 0.807 val_mse = 1.208 mse = 1.302 mae = 0.836\n",
            "epoch77 train time: 4.925s test time: 0.548  loss = 0.800 val_mse = 1.225 mse = 1.307 mae = 0.830\n",
            "epoch78 train time: 4.920s test time: 0.561  loss = 0.802 val_mse = 1.228 mse = 1.315 mae = 0.833\n",
            "epoch79 train time: 4.947s test time: 0.552  loss = 0.804 val_mse = 1.220 mse = 1.297 mae = 0.831\n",
            "epoch80 train time: 4.945s test time: 0.563  loss = 0.805 val_mse = 1.222 mse = 1.312 mae = 0.834\n",
            "epoch81 train time: 4.929s test time: 0.556  loss = 0.798 val_mse = 1.213 mse = 1.297 mae = 0.830\n",
            "epoch82 train time: 4.934s test time: 0.553  loss = 0.801 val_mse = 1.212 mse = 1.307 mae = 0.837\n",
            "epoch83 train time: 4.918s test time: 0.556  loss = 0.803 val_mse = 1.206 mse = 1.299 mae = 0.834\n",
            "epoch84 train time: 4.915s test time: 0.550  loss = 0.800 val_mse = 1.211 mse = 1.305 mae = 0.834\n",
            "epoch85 train time: 4.925s test time: 0.566  loss = 0.804 val_mse = 1.205 mse = 1.299 mae = 0.831\n",
            "epoch86 train time: 4.916s test time: 0.563  loss = 0.799 val_mse = 1.217 mse = 1.315 mae = 0.835\n",
            "epoch87 train time: 4.920s test time: 0.555  loss = 0.797 val_mse = 1.216 mse = 1.307 mae = 0.831\n",
            "epoch88 train time: 4.939s test time: 0.564  loss = 0.796 val_mse = 1.209 mse = 1.310 mae = 0.834\n",
            "epoch89 train time: 4.932s test time: 0.560  loss = 0.796 val_mse = 1.214 mse = 1.302 mae = 0.832\n",
            "epoch90 train time: 4.931s test time: 0.544  loss = 0.797 val_mse = 1.221 mse = 1.314 mae = 0.832\n",
            "epoch91 train time: 4.924s test time: 0.558  loss = 0.798 val_mse = 1.186 mse = 1.288 mae = 0.841\n",
            "epoch92 train time: 4.923s test time: 0.545  loss = 0.796 val_mse = 1.214 mse = 1.308 mae = 0.831\n",
            "epoch93 train time: 4.915s test time: 0.550  loss = 0.793 val_mse = 1.192 mse = 1.286 mae = 0.839\n",
            "epoch94 train time: 4.923s test time: 0.554  loss = 0.792 val_mse = 1.185 mse = 1.294 mae = 0.846\n",
            "epoch95 train time: 4.923s test time: 0.549  loss = 0.792 val_mse = 1.217 mse = 1.304 mae = 0.827\n",
            "epoch96 train time: 4.921s test time: 0.562  loss = 0.794 val_mse = 1.228 mse = 1.315 mae = 0.831\n",
            "epoch97 train time: 4.904s test time: 0.558  loss = 0.792 val_mse = 1.191 mse = 1.289 mae = 0.837\n",
            "epoch98 train time: 4.892s test time: 0.554  loss = 0.791 val_mse = 1.199 mse = 1.294 mae = 0.837\n",
            "epoch99 train time: 4.915s test time: 0.550  loss = 0.790 val_mse = 1.186 mse = 1.290 mae = 0.835\n",
            "epoch100 train time: 4.913s test time: 0.548  loss = 0.786 val_mse = 1.191 mse = 1.297 mae = 0.838\n",
            "epoch101 train time: 4.945s test time: 0.559  loss = 0.789 val_mse = 1.186 mse = 1.282 mae = 0.833\n",
            "epoch102 train time: 4.916s test time: 0.565  loss = 0.791 val_mse = 1.198 mse = 1.299 mae = 0.837\n",
            "epoch103 train time: 4.921s test time: 0.558  loss = 0.791 val_mse = 1.187 mse = 1.285 mae = 0.836\n",
            "epoch104 train time: 4.936s test time: 0.552  loss = 0.791 val_mse = 1.196 mse = 1.296 mae = 0.835\n",
            "epoch105 train time: 4.926s test time: 0.550  loss = 0.788 val_mse = 1.185 mse = 1.286 mae = 0.837\n",
            "epoch106 train time: 4.930s test time: 0.551  loss = 0.790 val_mse = 1.194 mse = 1.294 mae = 0.838\n",
            "epoch107 train time: 4.943s test time: 0.548  loss = 0.790 val_mse = 1.189 mse = 1.292 mae = 0.837\n",
            "epoch108 train time: 4.921s test time: 0.551  loss = 0.790 val_mse = 1.197 mse = 1.299 mae = 0.834\n",
            "epoch109 train time: 4.923s test time: 0.561  loss = 0.789 val_mse = 1.192 mse = 1.290 mae = 0.839\n",
            "epoch110 train time: 4.922s test time: 0.554  loss = 0.792 val_mse = 1.193 mse = 1.295 mae = 0.835\n",
            "epoch111 train time: 4.931s test time: 0.560  loss = 0.789 val_mse = 1.182 mse = 1.286 mae = 0.832\n",
            "epoch112 train time: 4.947s test time: 0.556  loss = 0.786 val_mse = 1.190 mse = 1.291 mae = 0.837\n",
            "epoch113 train time: 4.928s test time: 0.554  loss = 0.786 val_mse = 1.187 mse = 1.288 mae = 0.834\n",
            "epoch114 train time: 4.922s test time: 0.551  loss = 0.785 val_mse = 1.188 mse = 1.298 mae = 0.838\n",
            "epoch115 train time: 4.948s test time: 0.562  loss = 0.786 val_mse = 1.179 mse = 1.285 mae = 0.836\n",
            "epoch116 train time: 4.940s test time: 0.557  loss = 0.785 val_mse = 1.197 mse = 1.292 mae = 0.832\n",
            "epoch117 train time: 4.944s test time: 0.568  loss = 0.787 val_mse = 1.185 mse = 1.286 mae = 0.832\n",
            "epoch118 train time: 4.928s test time: 0.556  loss = 0.788 val_mse = 1.188 mse = 1.293 mae = 0.838\n",
            "epoch119 train time: 4.942s test time: 0.554  loss = 0.783 val_mse = 1.187 mse = 1.290 mae = 0.834\n",
            "epoch120 train time: 4.939s test time: 0.548  loss = 0.783 val_mse = 1.193 mse = 1.295 mae = 0.834\n",
            "epoch121 train time: 4.926s test time: 0.558  loss = 0.784 val_mse = 1.180 mse = 1.285 mae = 0.835\n",
            "epoch122 train time: 4.914s test time: 0.558  loss = 0.783 val_mse = 1.187 mse = 1.295 mae = 0.839\n",
            "epoch123 train time: 4.911s test time: 0.551  loss = 0.785 val_mse = 1.179 mse = 1.283 mae = 0.833\n",
            "epoch124 train time: 4.903s test time: 0.557  loss = 0.786 val_mse = 1.184 mse = 1.298 mae = 0.837\n",
            "epoch125 train time: 4.921s test time: 0.553  loss = 0.782 val_mse = 1.186 mse = 1.289 mae = 0.832\n",
            "epoch126 train time: 4.939s test time: 0.554  loss = 0.784 val_mse = 1.188 mse = 1.291 mae = 0.835\n",
            "epoch127 train time: 4.940s test time: 0.549  loss = 0.780 val_mse = 1.176 mse = 1.280 mae = 0.835\n",
            "epoch128 train time: 5.083s test time: 0.672  loss = 0.780 val_mse = 1.191 mse = 1.299 mae = 0.833\n",
            "epoch129 train time: 5.689s test time: 0.703  loss = 0.781 val_mse = 1.186 mse = 1.287 mae = 0.830\n",
            "epoch130 train time: 5.467s test time: 0.656  loss = 0.779 val_mse = 1.188 mse = 1.294 mae = 0.835\n",
            "epoch131 train time: 5.444s test time: 0.703  loss = 0.781 val_mse = 1.185 mse = 1.288 mae = 0.836\n",
            "epoch132 train time: 5.429s test time: 0.577  loss = 0.779 val_mse = 1.185 mse = 1.288 mae = 0.835\n",
            "epoch133 train time: 4.947s test time: 0.559  loss = 0.778 val_mse = 1.182 mse = 1.289 mae = 0.833\n",
            "epoch134 train time: 4.956s test time: 0.556  loss = 0.779 val_mse = 1.189 mse = 1.298 mae = 0.836\n",
            "epoch135 train time: 4.909s test time: 0.556  loss = 0.779 val_mse = 1.180 mse = 1.287 mae = 0.833\n",
            "epoch136 train time: 4.911s test time: 0.556  loss = 0.777 val_mse = 1.186 mse = 1.291 mae = 0.834\n",
            "epoch137 train time: 4.903s test time: 0.551  loss = 0.780 val_mse = 1.182 mse = 1.286 mae = 0.832\n",
            "epoch138 train time: 4.904s test time: 0.547  loss = 0.780 val_mse = 1.187 mse = 1.293 mae = 0.832\n",
            "epoch139 train time: 4.940s test time: 0.550  loss = 0.779 val_mse = 1.187 mse = 1.290 mae = 0.830\n",
            "epoch140 train time: 4.951s test time: 0.573  loss = 0.780 val_mse = 1.193 mse = 1.297 mae = 0.833\n",
            "epoch141 train time: 4.942s test time: 0.551  loss = 0.778 val_mse = 1.184 mse = 1.284 mae = 0.829\n",
            "epoch142 train time: 4.944s test time: 0.565  loss = 0.778 val_mse = 1.187 mse = 1.294 mae = 0.834\n",
            "epoch143 train time: 4.955s test time: 0.566  loss = 0.783 val_mse = 1.191 mse = 1.285 mae = 0.830\n",
            "epoch144 train time: 4.952s test time: 0.548  loss = 0.781 val_mse = 1.193 mse = 1.297 mae = 0.835\n",
            "epoch145 train time: 4.935s test time: 0.556  loss = 0.780 val_mse = 1.181 mse = 1.286 mae = 0.829\n",
            "epoch146 train time: 4.939s test time: 0.549  loss = 0.778 val_mse = 1.185 mse = 1.300 mae = 0.835\n",
            "epoch147 train time: 5.072s test time: 0.550  loss = 0.780 val_mse = 1.178 mse = 1.282 mae = 0.830\n",
            "epoch148 train time: 4.945s test time: 0.552  loss = 0.785 val_mse = 1.187 mse = 1.293 mae = 0.835\n",
            "epoch149 train time: 4.952s test time: 0.555  loss = 0.782 val_mse = 1.185 mse = 1.295 mae = 0.831\n",
            "epoch150 train time: 4.940s test time: 0.551  loss = 0.778 val_mse = 1.194 mse = 1.289 mae = 0.830\n",
            "epoch151 train time: 4.943s test time: 0.553  loss = 0.782 val_mse = 1.187 mse = 1.289 mae = 0.829\n",
            "epoch152 train time: 4.932s test time: 0.554  loss = 0.782 val_mse = 1.193 mse = 1.297 mae = 0.830\n",
            "epoch153 train time: 4.915s test time: 0.565  loss = 0.781 val_mse = 1.183 mse = 1.281 mae = 0.832\n",
            "epoch154 train time: 4.930s test time: 0.559  loss = 0.782 val_mse = 1.188 mse = 1.294 mae = 0.832\n",
            "epoch155 train time: 4.940s test time: 0.553  loss = 0.778 val_mse = 1.183 mse = 1.289 mae = 0.831\n",
            "epoch156 train time: 4.935s test time: 0.555  loss = 0.779 val_mse = 1.183 mse = 1.289 mae = 0.831\n",
            "epoch157 train time: 4.933s test time: 0.548  loss = 0.777 val_mse = 1.180 mse = 1.287 mae = 0.830\n",
            "epoch158 train time: 4.919s test time: 0.553  loss = 0.778 val_mse = 1.194 mse = 1.294 mae = 0.830\n",
            "epoch159 train time: 4.909s test time: 0.561  loss = 0.778 val_mse = 1.180 mse = 1.288 mae = 0.830\n",
            "epoch160 train time: 4.927s test time: 0.556  loss = 0.778 val_mse = 1.190 mse = 1.292 mae = 0.832\n",
            "epoch161 train time: 4.936s test time: 0.549  loss = 0.777 val_mse = 1.178 mse = 1.286 mae = 0.832\n",
            "epoch162 train time: 4.900s test time: 0.550  loss = 0.778 val_mse = 1.185 mse = 1.292 mae = 0.831\n",
            "epoch163 train time: 4.920s test time: 0.551  loss = 0.777 val_mse = 1.183 mse = 1.288 mae = 0.830\n",
            "epoch164 train time: 4.936s test time: 0.556  loss = 0.780 val_mse = 1.191 mse = 1.291 mae = 0.832\n",
            "epoch165 train time: 5.085s test time: 0.555  loss = 0.780 val_mse = 1.183 mse = 1.283 mae = 0.830\n",
            "epoch166 train time: 4.918s test time: 0.552  loss = 0.777 val_mse = 1.195 mse = 1.294 mae = 0.833\n",
            "epoch167 train time: 4.926s test time: 0.550  loss = 0.780 val_mse = 1.184 mse = 1.282 mae = 0.828\n",
            "epoch168 train time: 4.925s test time: 0.551  loss = 0.778 val_mse = 1.193 mse = 1.294 mae = 0.832\n",
            "epoch169 train time: 4.918s test time: 0.550  loss = 0.778 val_mse = 1.183 mse = 1.285 mae = 0.831\n",
            "epoch170 train time: 4.935s test time: 0.551  loss = 0.777 val_mse = 1.186 mse = 1.290 mae = 0.828\n",
            "epoch171 train time: 4.913s test time: 0.556  loss = 0.776 val_mse = 1.185 mse = 1.287 mae = 0.830\n",
            "epoch172 train time: 4.904s test time: 0.552  loss = 0.782 val_mse = 1.194 mse = 1.293 mae = 0.831\n",
            "epoch173 train time: 4.915s test time: 0.555  loss = 0.779 val_mse = 1.180 mse = 1.288 mae = 0.829\n",
            "epoch174 train time: 4.927s test time: 0.547  loss = 0.778 val_mse = 1.187 mse = 1.287 mae = 0.832\n",
            "epoch175 train time: 4.906s test time: 0.558  loss = 0.778 val_mse = 1.175 mse = 1.281 mae = 0.828\n",
            "epoch176 train time: 4.915s test time: 0.550  loss = 0.777 val_mse = 1.191 mse = 1.295 mae = 0.830\n",
            "epoch177 train time: 4.947s test time: 0.550  loss = 0.776 val_mse = 1.182 mse = 1.284 mae = 0.829\n",
            "epoch178 train time: 4.919s test time: 0.550  loss = 0.780 val_mse = 1.191 mse = 1.289 mae = 0.832\n",
            "epoch179 train time: 4.922s test time: 0.557  loss = 0.780 val_mse = 1.189 mse = 1.287 mae = 0.828\n",
            "MAE 0.8383916606721673\n",
            "MSE 1.3119380319514957\n",
            "epoch0 train time: 5.307s test time: 0.572  loss = 42.037 val_mse = 1.619 mse = 1.549 mae = 0.901\n",
            "epoch1 train time: 5.066s test time: 0.569  loss = 31.911 val_mse = 1.595 mse = 1.533 mae = 0.895\n",
            "epoch2 train time: 4.960s test time: 0.551  loss = 17.185 val_mse = 1.564 mse = 1.511 mae = 0.885\n",
            "epoch3 train time: 4.944s test time: 0.559  loss = 8.572 val_mse = 1.535 mse = 1.490 mae = 0.876\n",
            "epoch4 train time: 4.940s test time: 0.553  loss = 4.198 val_mse = 1.508 mse = 1.471 mae = 0.867\n",
            "epoch5 train time: 4.937s test time: 0.555  loss = 2.302 val_mse = 1.480 mse = 1.452 mae = 0.860\n",
            "epoch6 train time: 4.948s test time: 0.558  loss = 1.658 val_mse = 1.452 mse = 1.432 mae = 0.854\n",
            "epoch7 train time: 4.946s test time: 0.559  loss = 1.490 val_mse = 1.423 mse = 1.413 mae = 0.850\n",
            "epoch8 train time: 4.935s test time: 0.556  loss = 1.411 val_mse = 1.396 mse = 1.395 mae = 0.847\n",
            "epoch9 train time: 4.932s test time: 0.555  loss = 1.342 val_mse = 1.378 mse = 1.385 mae = 0.845\n",
            "epoch10 train time: 4.922s test time: 0.543  loss = 1.280 val_mse = 1.369 mse = 1.383 mae = 0.842\n",
            "epoch11 train time: 4.921s test time: 0.568  loss = 1.224 val_mse = 1.349 mse = 1.370 mae = 0.839\n",
            "epoch12 train time: 4.918s test time: 0.551  loss = 1.174 val_mse = 1.337 mse = 1.365 mae = 0.838\n",
            "epoch13 train time: 4.916s test time: 0.550  loss = 1.130 val_mse = 1.305 mse = 1.343 mae = 0.837\n",
            "epoch14 train time: 4.914s test time: 0.555  loss = 1.091 val_mse = 1.291 mse = 1.333 mae = 0.838\n",
            "epoch15 train time: 4.924s test time: 0.557  loss = 1.055 val_mse = 1.275 mse = 1.325 mae = 0.838\n",
            "epoch16 train time: 4.920s test time: 0.551  loss = 1.026 val_mse = 1.259 mse = 1.314 mae = 0.838\n",
            "epoch17 train time: 4.924s test time: 0.556  loss = 1.001 val_mse = 1.249 mse = 1.311 mae = 0.842\n",
            "epoch18 train time: 4.917s test time: 0.559  loss = 0.983 val_mse = 1.235 mse = 1.303 mae = 0.849\n",
            "epoch19 train time: 4.912s test time: 0.554  loss = 0.967 val_mse = 1.243 mse = 1.310 mae = 0.851\n",
            "epoch20 train time: 4.923s test time: 0.561  loss = 0.952 val_mse = 1.248 mse = 1.311 mae = 0.842\n",
            "epoch21 train time: 4.915s test time: 0.550  loss = 0.942 val_mse = 1.228 mse = 1.303 mae = 0.842\n",
            "epoch22 train time: 4.913s test time: 0.559  loss = 0.935 val_mse = 1.232 mse = 1.307 mae = 0.840\n",
            "epoch23 train time: 4.901s test time: 0.552  loss = 0.925 val_mse = 1.214 mse = 1.292 mae = 0.852\n",
            "epoch24 train time: 4.926s test time: 0.552  loss = 0.917 val_mse = 1.234 mse = 1.312 mae = 0.849\n",
            "epoch25 train time: 4.902s test time: 0.551  loss = 0.910 val_mse = 1.221 mse = 1.294 mae = 0.844\n",
            "epoch26 train time: 4.907s test time: 0.548  loss = 0.906 val_mse = 1.236 mse = 1.315 mae = 0.842\n",
            "epoch27 train time: 4.915s test time: 0.559  loss = 0.899 val_mse = 1.215 mse = 1.300 mae = 0.846\n",
            "epoch28 train time: 4.885s test time: 0.546  loss = 0.902 val_mse = 1.201 mse = 1.301 mae = 0.858\n",
            "epoch29 train time: 4.899s test time: 0.552  loss = 0.893 val_mse = 1.216 mse = 1.295 mae = 0.850\n",
            "epoch30 train time: 4.888s test time: 0.555  loss = 0.886 val_mse = 1.201 mse = 1.296 mae = 0.857\n",
            "epoch31 train time: 4.895s test time: 0.550  loss = 0.882 val_mse = 1.233 mse = 1.307 mae = 0.839\n",
            "epoch32 train time: 4.921s test time: 0.556  loss = 0.880 val_mse = 1.224 mse = 1.311 mae = 0.838\n",
            "epoch33 train time: 4.921s test time: 0.576  loss = 0.876 val_mse = 1.216 mse = 1.296 mae = 0.852\n",
            "epoch34 train time: 4.919s test time: 0.554  loss = 0.871 val_mse = 1.212 mse = 1.299 mae = 0.848\n",
            "epoch35 train time: 4.950s test time: 0.552  loss = 0.865 val_mse = 1.242 mse = 1.324 mae = 0.838\n",
            "epoch36 train time: 4.928s test time: 0.553  loss = 0.863 val_mse = 1.243 mse = 1.312 mae = 0.840\n",
            "epoch37 train time: 4.925s test time: 0.569  loss = 0.861 val_mse = 1.211 mse = 1.295 mae = 0.851\n",
            "epoch38 train time: 4.921s test time: 0.554  loss = 0.857 val_mse = 1.206 mse = 1.300 mae = 0.849\n",
            "epoch39 train time: 4.923s test time: 0.558  loss = 0.853 val_mse = 1.248 mse = 1.322 mae = 0.836\n",
            "epoch40 train time: 4.936s test time: 0.559  loss = 0.853 val_mse = 1.218 mse = 1.307 mae = 0.845\n",
            "epoch41 train time: 4.917s test time: 0.554  loss = 0.856 val_mse = 1.229 mse = 1.304 mae = 0.832\n",
            "epoch42 train time: 4.890s test time: 0.554  loss = 0.846 val_mse = 1.252 mse = 1.338 mae = 0.836\n",
            "epoch43 train time: 4.909s test time: 0.549  loss = 0.848 val_mse = 1.208 mse = 1.298 mae = 0.847\n",
            "epoch44 train time: 4.902s test time: 0.563  loss = 0.848 val_mse = 1.212 mse = 1.297 mae = 0.847\n",
            "epoch45 train time: 4.909s test time: 0.549  loss = 0.843 val_mse = 1.236 mse = 1.316 mae = 0.835\n",
            "epoch46 train time: 4.916s test time: 0.557  loss = 0.837 val_mse = 1.216 mse = 1.315 mae = 0.850\n",
            "epoch47 train time: 4.933s test time: 0.559  loss = 0.845 val_mse = 1.202 mse = 1.298 mae = 0.855\n",
            "epoch48 train time: 4.910s test time: 0.547  loss = 0.839 val_mse = 1.244 mse = 1.328 mae = 0.837\n",
            "epoch49 train time: 4.923s test time: 0.552  loss = 0.835 val_mse = 1.210 mse = 1.305 mae = 0.845\n",
            "epoch50 train time: 4.927s test time: 0.549  loss = 0.840 val_mse = 1.217 mse = 1.302 mae = 0.843\n",
            "epoch51 train time: 4.918s test time: 0.550  loss = 0.835 val_mse = 1.244 mse = 1.329 mae = 0.834\n",
            "epoch52 train time: 4.937s test time: 0.558  loss = 0.832 val_mse = 1.219 mse = 1.311 mae = 0.841\n",
            "epoch53 train time: 4.941s test time: 0.555  loss = 0.835 val_mse = 1.196 mse = 1.297 mae = 0.850\n",
            "epoch54 train time: 4.936s test time: 0.560  loss = 0.834 val_mse = 1.209 mse = 1.295 mae = 0.839\n",
            "epoch55 train time: 4.911s test time: 0.557  loss = 0.830 val_mse = 1.199 mse = 1.292 mae = 0.843\n",
            "epoch56 train time: 4.918s test time: 0.552  loss = 0.831 val_mse = 1.204 mse = 1.303 mae = 0.851\n",
            "epoch57 train time: 4.926s test time: 0.555  loss = 0.832 val_mse = 1.243 mse = 1.323 mae = 0.833\n",
            "epoch58 train time: 4.905s test time: 0.554  loss = 0.831 val_mse = 1.217 mse = 1.304 mae = 0.839\n",
            "epoch59 train time: 4.931s test time: 0.554  loss = 0.827 val_mse = 1.201 mse = 1.294 mae = 0.848\n",
            "epoch60 train time: 4.948s test time: 0.566  loss = 0.826 val_mse = 1.206 mse = 1.292 mae = 0.840\n",
            "epoch61 train time: 4.920s test time: 0.564  loss = 0.832 val_mse = 1.229 mse = 1.313 mae = 0.834\n",
            "epoch62 train time: 4.942s test time: 0.560  loss = 0.826 val_mse = 1.205 mse = 1.303 mae = 0.842\n",
            "epoch63 train time: 4.940s test time: 0.554  loss = 0.821 val_mse = 1.198 mse = 1.298 mae = 0.852\n",
            "epoch64 train time: 4.931s test time: 0.555  loss = 0.825 val_mse = 1.205 mse = 1.293 mae = 0.840\n",
            "epoch65 train time: 4.913s test time: 0.569  loss = 0.826 val_mse = 1.226 mse = 1.310 mae = 0.832\n",
            "epoch66 train time: 4.918s test time: 0.560  loss = 0.819 val_mse = 1.209 mse = 1.297 mae = 0.839\n",
            "epoch67 train time: 4.939s test time: 0.560  loss = 0.819 val_mse = 1.193 mse = 1.292 mae = 0.849\n",
            "epoch68 train time: 4.924s test time: 0.554  loss = 0.818 val_mse = 1.202 mse = 1.301 mae = 0.845\n",
            "epoch69 train time: 4.926s test time: 0.559  loss = 0.821 val_mse = 1.188 mse = 1.290 mae = 0.846\n",
            "epoch70 train time: 4.926s test time: 0.562  loss = 0.816 val_mse = 1.207 mse = 1.304 mae = 0.840\n",
            "epoch71 train time: 4.930s test time: 0.552  loss = 0.812 val_mse = 1.198 mse = 1.290 mae = 0.842\n",
            "epoch72 train time: 4.935s test time: 0.562  loss = 0.815 val_mse = 1.206 mse = 1.293 mae = 0.839\n",
            "epoch73 train time: 4.931s test time: 0.554  loss = 0.811 val_mse = 1.217 mse = 1.304 mae = 0.832\n",
            "epoch74 train time: 4.921s test time: 0.556  loss = 0.810 val_mse = 1.203 mse = 1.296 mae = 0.838\n",
            "epoch75 train time: 4.909s test time: 0.553  loss = 0.810 val_mse = 1.185 mse = 1.287 mae = 0.847\n",
            "epoch76 train time: 4.942s test time: 0.555  loss = 0.812 val_mse = 1.204 mse = 1.299 mae = 0.842\n",
            "epoch77 train time: 4.917s test time: 0.563  loss = 0.814 val_mse = 1.217 mse = 1.304 mae = 0.832\n",
            "epoch78 train time: 4.921s test time: 0.553  loss = 0.805 val_mse = 1.201 mse = 1.297 mae = 0.837\n",
            "epoch79 train time: 4.919s test time: 0.547  loss = 0.807 val_mse = 1.196 mse = 1.290 mae = 0.843\n",
            "epoch80 train time: 4.931s test time: 0.554  loss = 0.807 val_mse = 1.199 mse = 1.294 mae = 0.840\n",
            "epoch81 train time: 4.916s test time: 0.552  loss = 0.804 val_mse = 1.208 mse = 1.302 mae = 0.832\n",
            "epoch82 train time: 4.913s test time: 0.556  loss = 0.805 val_mse = 1.202 mse = 1.299 mae = 0.840\n",
            "epoch83 train time: 4.921s test time: 0.554  loss = 0.808 val_mse = 1.189 mse = 1.288 mae = 0.846\n",
            "epoch84 train time: 4.918s test time: 0.551  loss = 0.804 val_mse = 1.223 mse = 1.314 mae = 0.831\n",
            "epoch85 train time: 4.924s test time: 0.559  loss = 0.802 val_mse = 1.224 mse = 1.313 mae = 0.834\n",
            "epoch86 train time: 4.920s test time: 0.548  loss = 0.803 val_mse = 1.222 mse = 1.310 mae = 0.835\n",
            "epoch87 train time: 4.936s test time: 0.560  loss = 0.806 val_mse = 1.208 mse = 1.302 mae = 0.835\n",
            "epoch88 train time: 4.914s test time: 0.560  loss = 0.802 val_mse = 1.203 mse = 1.300 mae = 0.839\n",
            "epoch89 train time: 4.916s test time: 0.565  loss = 0.799 val_mse = 1.207 mse = 1.300 mae = 0.831\n",
            "epoch90 train time: 4.938s test time: 0.564  loss = 0.799 val_mse = 1.220 mse = 1.305 mae = 0.831\n",
            "epoch91 train time: 4.922s test time: 0.563  loss = 0.803 val_mse = 1.190 mse = 1.291 mae = 0.837\n",
            "epoch92 train time: 4.936s test time: 0.552  loss = 0.798 val_mse = 1.198 mse = 1.299 mae = 0.839\n",
            "epoch93 train time: 4.910s test time: 0.552  loss = 0.798 val_mse = 1.193 mse = 1.289 mae = 0.836\n",
            "epoch94 train time: 4.910s test time: 0.551  loss = 0.798 val_mse = 1.193 mse = 1.291 mae = 0.834\n",
            "epoch95 train time: 4.925s test time: 0.565  loss = 0.800 val_mse = 1.192 mse = 1.292 mae = 0.834\n",
            "epoch96 train time: 4.922s test time: 0.557  loss = 0.798 val_mse = 1.197 mse = 1.294 mae = 0.839\n",
            "epoch97 train time: 4.915s test time: 0.563  loss = 0.797 val_mse = 1.188 mse = 1.285 mae = 0.838\n",
            "epoch98 train time: 4.976s test time: 0.570  loss = 0.794 val_mse = 1.193 mse = 1.298 mae = 0.837\n",
            "epoch99 train time: 4.945s test time: 0.556  loss = 0.791 val_mse = 1.195 mse = 1.293 mae = 0.837\n",
            "epoch100 train time: 4.935s test time: 0.552  loss = 0.794 val_mse = 1.195 mse = 1.297 mae = 0.842\n",
            "epoch101 train time: 4.966s test time: 0.554  loss = 0.798 val_mse = 1.185 mse = 1.291 mae = 0.841\n",
            "epoch102 train time: 4.945s test time: 0.552  loss = 0.794 val_mse = 1.197 mse = 1.302 mae = 0.839\n",
            "epoch103 train time: 4.957s test time: 0.557  loss = 0.794 val_mse = 1.186 mse = 1.285 mae = 0.838\n",
            "epoch104 train time: 4.930s test time: 0.550  loss = 0.794 val_mse = 1.196 mse = 1.293 mae = 0.835\n",
            "epoch105 train time: 4.924s test time: 0.551  loss = 0.795 val_mse = 1.188 mse = 1.289 mae = 0.838\n",
            "epoch106 train time: 4.926s test time: 0.551  loss = 0.792 val_mse = 1.193 mse = 1.295 mae = 0.836\n",
            "epoch107 train time: 4.925s test time: 0.552  loss = 0.790 val_mse = 1.191 mse = 1.286 mae = 0.837\n",
            "epoch108 train time: 4.917s test time: 0.554  loss = 0.790 val_mse = 1.193 mse = 1.294 mae = 0.837\n",
            "epoch109 train time: 4.916s test time: 0.554  loss = 0.790 val_mse = 1.183 mse = 1.286 mae = 0.837\n",
            "epoch110 train time: 4.923s test time: 0.554  loss = 0.789 val_mse = 1.197 mse = 1.300 mae = 0.840\n",
            "epoch111 train time: 4.927s test time: 0.557  loss = 0.790 val_mse = 1.185 mse = 1.286 mae = 0.839\n",
            "epoch112 train time: 4.924s test time: 0.554  loss = 0.788 val_mse = 1.195 mse = 1.293 mae = 0.835\n",
            "epoch113 train time: 4.940s test time: 0.559  loss = 0.788 val_mse = 1.181 mse = 1.290 mae = 0.839\n",
            "epoch114 train time: 4.921s test time: 0.558  loss = 0.789 val_mse = 1.188 mse = 1.292 mae = 0.837\n",
            "epoch115 train time: 4.910s test time: 0.549  loss = 0.789 val_mse = 1.195 mse = 1.293 mae = 0.832\n",
            "epoch116 train time: 4.902s test time: 0.562  loss = 0.785 val_mse = 1.191 mse = 1.296 mae = 0.835\n",
            "epoch117 train time: 4.896s test time: 0.551  loss = 0.785 val_mse = 1.184 mse = 1.287 mae = 0.839\n",
            "epoch118 train time: 4.902s test time: 0.555  loss = 0.785 val_mse = 1.187 mse = 1.297 mae = 0.838\n",
            "epoch119 train time: 4.915s test time: 0.552  loss = 0.784 val_mse = 1.181 mse = 1.288 mae = 0.838\n",
            "epoch120 train time: 4.916s test time: 0.550  loss = 0.784 val_mse = 1.195 mse = 1.300 mae = 0.836\n",
            "epoch121 train time: 4.922s test time: 0.557  loss = 0.783 val_mse = 1.181 mse = 1.287 mae = 0.838\n",
            "epoch122 train time: 4.918s test time: 0.552  loss = 0.783 val_mse = 1.187 mse = 1.290 mae = 0.836\n",
            "epoch123 train time: 4.924s test time: 0.555  loss = 0.785 val_mse = 1.188 mse = 1.292 mae = 0.836\n",
            "epoch124 train time: 4.923s test time: 0.553  loss = 0.784 val_mse = 1.198 mse = 1.298 mae = 0.834\n",
            "epoch125 train time: 4.935s test time: 0.550  loss = 0.781 val_mse = 1.188 mse = 1.290 mae = 0.837\n",
            "epoch126 train time: 4.936s test time: 0.576  loss = 0.779 val_mse = 1.188 mse = 1.291 mae = 0.831\n",
            "epoch127 train time: 6.078s test time: 0.757  loss = 0.781 val_mse = 1.178 mse = 1.286 mae = 0.836\n",
            "epoch128 train time: 5.676s test time: 0.676  loss = 0.783 val_mse = 1.182 mse = 1.295 mae = 0.836\n",
            "epoch129 train time: 5.384s test time: 0.553  loss = 0.779 val_mse = 1.184 mse = 1.285 mae = 0.832\n",
            "epoch130 train time: 4.900s test time: 0.547  loss = 0.778 val_mse = 1.189 mse = 1.294 mae = 0.833\n",
            "epoch131 train time: 4.909s test time: 0.553  loss = 0.782 val_mse = 1.180 mse = 1.291 mae = 0.834\n",
            "epoch132 train time: 4.931s test time: 0.557  loss = 0.777 val_mse = 1.187 mse = 1.290 mae = 0.832\n",
            "epoch133 train time: 4.900s test time: 0.555  loss = 0.781 val_mse = 1.183 mse = 1.282 mae = 0.832\n",
            "epoch134 train time: 4.913s test time: 0.558  loss = 0.778 val_mse = 1.189 mse = 1.303 mae = 0.836\n",
            "epoch135 train time: 4.905s test time: 0.558  loss = 0.778 val_mse = 1.189 mse = 1.289 mae = 0.833\n",
            "epoch136 train time: 4.915s test time: 0.555  loss = 0.780 val_mse = 1.187 mse = 1.296 mae = 0.834\n",
            "epoch137 train time: 4.900s test time: 0.555  loss = 0.779 val_mse = 1.179 mse = 1.286 mae = 0.830\n",
            "epoch138 train time: 4.912s test time: 0.554  loss = 0.778 val_mse = 1.188 mse = 1.289 mae = 0.832\n",
            "epoch139 train time: 4.907s test time: 0.549  loss = 0.782 val_mse = 1.177 mse = 1.281 mae = 0.832\n",
            "epoch140 train time: 4.913s test time: 0.554  loss = 0.778 val_mse = 1.189 mse = 1.293 mae = 0.832\n",
            "epoch141 train time: 4.926s test time: 0.547  loss = 0.777 val_mse = 1.185 mse = 1.285 mae = 0.829\n",
            "epoch142 train time: 4.920s test time: 0.550  loss = 0.779 val_mse = 1.184 mse = 1.289 mae = 0.831\n",
            "epoch143 train time: 4.910s test time: 0.548  loss = 0.779 val_mse = 1.188 mse = 1.287 mae = 0.828\n",
            "epoch144 train time: 4.914s test time: 0.550  loss = 0.779 val_mse = 1.194 mse = 1.298 mae = 0.831\n",
            "epoch145 train time: 4.912s test time: 0.552  loss = 0.779 val_mse = 1.180 mse = 1.289 mae = 0.833\n",
            "epoch146 train time: 4.913s test time: 0.552  loss = 0.778 val_mse = 1.189 mse = 1.293 mae = 0.831\n",
            "epoch147 train time: 4.915s test time: 0.559  loss = 0.780 val_mse = 1.189 mse = 1.291 mae = 0.833\n",
            "epoch148 train time: 4.901s test time: 0.557  loss = 0.778 val_mse = 1.190 mse = 1.292 mae = 0.828\n",
            "epoch149 train time: 4.905s test time: 0.550  loss = 0.780 val_mse = 1.184 mse = 1.285 mae = 0.833\n",
            "epoch150 train time: 4.910s test time: 0.558  loss = 0.780 val_mse = 1.195 mse = 1.297 mae = 0.830\n",
            "epoch151 train time: 4.909s test time: 0.554  loss = 0.779 val_mse = 1.194 mse = 1.292 mae = 0.832\n",
            "epoch152 train time: 4.899s test time: 0.551  loss = 0.779 val_mse = 1.188 mse = 1.292 mae = 0.832\n",
            "epoch153 train time: 4.920s test time: 0.550  loss = 0.776 val_mse = 1.180 mse = 1.287 mae = 0.829\n",
            "epoch154 train time: 4.918s test time: 0.550  loss = 0.776 val_mse = 1.192 mse = 1.287 mae = 0.833\n",
            "epoch155 train time: 4.915s test time: 0.560  loss = 0.783 val_mse = 1.189 mse = 1.291 mae = 0.827\n",
            "epoch156 train time: 4.920s test time: 0.551  loss = 0.778 val_mse = 1.190 mse = 1.292 mae = 0.832\n",
            "epoch157 train time: 4.927s test time: 0.557  loss = 0.779 val_mse = 1.191 mse = 1.291 mae = 0.828\n",
            "epoch158 train time: 4.921s test time: 0.557  loss = 0.776 val_mse = 1.188 mse = 1.289 mae = 0.829\n",
            "epoch159 train time: 4.925s test time: 0.561  loss = 0.775 val_mse = 1.180 mse = 1.289 mae = 0.830\n",
            "epoch160 train time: 4.913s test time: 0.557  loss = 0.775 val_mse = 1.181 mse = 1.287 mae = 0.829\n",
            "epoch161 train time: 4.925s test time: 0.549  loss = 0.775 val_mse = 1.181 mse = 1.287 mae = 0.832\n",
            "epoch162 train time: 4.929s test time: 0.552  loss = 0.775 val_mse = 1.190 mse = 1.292 mae = 0.829\n",
            "epoch163 train time: 4.944s test time: 0.568  loss = 0.775 val_mse = 1.180 mse = 1.286 mae = 0.830\n",
            "epoch164 train time: 4.912s test time: 0.555  loss = 0.775 val_mse = 1.184 mse = 1.292 mae = 0.831\n",
            "epoch165 train time: 4.935s test time: 0.555  loss = 0.775 val_mse = 1.183 mse = 1.291 mae = 0.830\n",
            "epoch166 train time: 4.927s test time: 0.555  loss = 0.775 val_mse = 1.187 mse = 1.292 mae = 0.829\n",
            "epoch167 train time: 4.937s test time: 0.555  loss = 0.776 val_mse = 1.179 mse = 1.283 mae = 0.829\n",
            "epoch168 train time: 4.933s test time: 0.561  loss = 0.774 val_mse = 1.189 mse = 1.290 mae = 0.832\n",
            "epoch169 train time: 4.937s test time: 0.552  loss = 0.778 val_mse = 1.185 mse = 1.281 mae = 0.828\n",
            "epoch170 train time: 4.927s test time: 0.559  loss = 0.775 val_mse = 1.186 mse = 1.283 mae = 0.828\n",
            "epoch171 train time: 4.931s test time: 0.550  loss = 0.774 val_mse = 1.182 mse = 1.291 mae = 0.832\n",
            "epoch172 train time: 4.943s test time: 0.554  loss = 0.774 val_mse = 1.185 mse = 1.289 mae = 0.828\n",
            "epoch173 train time: 4.913s test time: 0.555  loss = 0.775 val_mse = 1.186 mse = 1.288 mae = 0.830\n",
            "epoch174 train time: 4.928s test time: 0.546  loss = 0.774 val_mse = 1.187 mse = 1.290 mae = 0.827\n",
            "epoch175 train time: 4.914s test time: 0.553  loss = 0.773 val_mse = 1.187 mse = 1.286 mae = 0.828\n",
            "epoch176 train time: 4.939s test time: 0.551  loss = 0.777 val_mse = 1.187 mse = 1.288 mae = 0.828\n",
            "epoch177 train time: 4.909s test time: 0.552  loss = 0.776 val_mse = 1.181 mse = 1.287 mae = 0.829\n",
            "epoch178 train time: 4.930s test time: 0.560  loss = 0.777 val_mse = 1.191 mse = 1.292 mae = 0.830\n",
            "epoch179 train time: 4.935s test time: 0.550  loss = 0.775 val_mse = 1.182 mse = 1.287 mae = 0.830\n",
            "MAE 0.838968696967958\n",
            "MSE 1.307635431930038\n",
            "epoch0 train time: 5.309s test time: 0.546  loss = 43.016 val_mse = 1.664 mse = 1.592 mae = 0.919\n",
            "epoch1 train time: 4.919s test time: 0.556  loss = 32.035 val_mse = 1.637 mse = 1.573 mae = 0.912\n",
            "epoch2 train time: 4.910s test time: 0.552  loss = 17.289 val_mse = 1.606 mse = 1.550 mae = 0.902\n",
            "epoch3 train time: 4.923s test time: 0.562  loss = 8.648 val_mse = 1.576 mse = 1.527 mae = 0.891\n",
            "epoch4 train time: 4.946s test time: 0.559  loss = 4.253 val_mse = 1.546 mse = 1.505 mae = 0.879\n",
            "epoch5 train time: 4.944s test time: 0.553  loss = 2.357 val_mse = 1.515 mse = 1.483 mae = 0.869\n",
            "epoch6 train time: 4.929s test time: 0.575  loss = 1.719 val_mse = 1.484 mse = 1.461 mae = 0.860\n",
            "epoch7 train time: 4.923s test time: 0.553  loss = 1.553 val_mse = 1.453 mse = 1.439 mae = 0.853\n",
            "epoch8 train time: 4.939s test time: 0.550  loss = 1.472 val_mse = 1.424 mse = 1.419 mae = 0.848\n",
            "epoch9 train time: 4.923s test time: 0.553  loss = 1.401 val_mse = 1.398 mse = 1.402 mae = 0.845\n",
            "epoch10 train time: 4.912s test time: 0.549  loss = 1.335 val_mse = 1.379 mse = 1.391 mae = 0.842\n",
            "epoch11 train time: 4.918s test time: 0.553  loss = 1.275 val_mse = 1.366 mse = 1.385 mae = 0.840\n",
            "epoch12 train time: 4.898s test time: 0.547  loss = 1.222 val_mse = 1.351 mse = 1.376 mae = 0.838\n",
            "epoch13 train time: 4.896s test time: 0.557  loss = 1.172 val_mse = 1.336 mse = 1.368 mae = 0.836\n",
            "epoch14 train time: 5.112s test time: 0.672  loss = 1.129 val_mse = 1.321 mse = 1.359 mae = 0.836\n",
            "epoch15 train time: 5.546s test time: 0.681  loss = 1.093 val_mse = 1.309 mse = 1.354 mae = 0.836\n",
            "epoch16 train time: 5.348s test time: 0.661  loss = 1.062 val_mse = 1.284 mse = 1.337 mae = 0.836\n",
            "epoch17 train time: 5.430s test time: 0.671  loss = 1.035 val_mse = 1.273 mse = 1.331 mae = 0.837\n",
            "epoch18 train time: 5.410s test time: 0.690  loss = 1.013 val_mse = 1.250 mse = 1.317 mae = 0.841\n",
            "epoch19 train time: 5.484s test time: 0.668  loss = 0.993 val_mse = 1.243 mse = 1.310 mae = 0.850\n",
            "epoch20 train time: 5.481s test time: 0.657  loss = 0.984 val_mse = 1.253 mse = 1.319 mae = 0.841\n",
            "epoch21 train time: 5.212s test time: 0.557  loss = 0.969 val_mse = 1.249 mse = 1.315 mae = 0.841\n",
            "epoch22 train time: 4.948s test time: 0.557  loss = 0.963 val_mse = 1.253 mse = 1.316 mae = 0.839\n",
            "epoch23 train time: 4.946s test time: 0.559  loss = 0.956 val_mse = 1.252 mse = 1.319 mae = 0.839\n",
            "epoch24 train time: 4.918s test time: 0.548  loss = 0.949 val_mse = 1.228 mse = 1.302 mae = 0.848\n",
            "epoch25 train time: 4.900s test time: 0.551  loss = 0.944 val_mse = 1.223 mse = 1.307 mae = 0.842\n",
            "epoch26 train time: 4.912s test time: 0.551  loss = 0.926 val_mse = 1.245 mse = 1.314 mae = 0.841\n",
            "epoch27 train time: 4.888s test time: 0.558  loss = 0.925 val_mse = 1.226 mse = 1.307 mae = 0.844\n",
            "epoch28 train time: 4.905s test time: 0.545  loss = 0.923 val_mse = 1.224 mse = 1.310 mae = 0.845\n",
            "epoch29 train time: 4.887s test time: 0.550  loss = 0.911 val_mse = 1.250 mse = 1.334 mae = 0.838\n",
            "epoch30 train time: 4.903s test time: 0.555  loss = 0.905 val_mse = 1.269 mse = 1.334 mae = 0.839\n",
            "epoch31 train time: 4.922s test time: 0.546  loss = 0.903 val_mse = 1.226 mse = 1.304 mae = 0.846\n",
            "epoch32 train time: 4.925s test time: 0.554  loss = 0.898 val_mse = 1.214 mse = 1.296 mae = 0.857\n",
            "epoch33 train time: 4.915s test time: 0.559  loss = 0.892 val_mse = 1.230 mse = 1.314 mae = 0.836\n",
            "epoch34 train time: 4.914s test time: 0.552  loss = 0.889 val_mse = 1.232 mse = 1.317 mae = 0.845\n",
            "epoch35 train time: 4.912s test time: 0.556  loss = 0.883 val_mse = 1.221 mse = 1.309 mae = 0.849\n",
            "epoch36 train time: 4.910s test time: 0.548  loss = 0.879 val_mse = 1.221 mse = 1.304 mae = 0.845\n",
            "epoch37 train time: 4.913s test time: 0.552  loss = 0.879 val_mse = 1.204 mse = 1.297 mae = 0.850\n",
            "epoch38 train time: 4.929s test time: 0.552  loss = 0.875 val_mse = 1.209 mse = 1.296 mae = 0.849\n",
            "epoch39 train time: 5.072s test time: 0.550  loss = 0.874 val_mse = 1.232 mse = 1.324 mae = 0.841\n",
            "epoch40 train time: 4.915s test time: 0.551  loss = 0.875 val_mse = 1.232 mse = 1.307 mae = 0.848\n",
            "epoch41 train time: 4.925s test time: 0.552  loss = 0.864 val_mse = 1.205 mse = 1.306 mae = 0.853\n",
            "epoch42 train time: 4.907s test time: 0.555  loss = 0.863 val_mse = 1.226 mse = 1.305 mae = 0.845\n",
            "epoch43 train time: 4.922s test time: 0.550  loss = 0.866 val_mse = 1.200 mse = 1.297 mae = 0.858\n",
            "epoch44 train time: 4.923s test time: 0.552  loss = 0.862 val_mse = 1.257 mse = 1.316 mae = 0.834\n",
            "epoch45 train time: 4.940s test time: 0.559  loss = 0.860 val_mse = 1.207 mse = 1.302 mae = 0.839\n",
            "epoch46 train time: 4.930s test time: 0.549  loss = 0.861 val_mse = 1.215 mse = 1.304 mae = 0.848\n",
            "epoch47 train time: 4.940s test time: 0.554  loss = 0.852 val_mse = 1.239 mse = 1.322 mae = 0.836\n",
            "epoch48 train time: 4.929s test time: 0.547  loss = 0.855 val_mse = 1.232 mse = 1.302 mae = 0.843\n",
            "epoch49 train time: 4.907s test time: 0.550  loss = 0.847 val_mse = 1.219 mse = 1.300 mae = 0.842\n",
            "epoch50 train time: 4.924s test time: 0.554  loss = 0.847 val_mse = 1.225 mse = 1.308 mae = 0.845\n",
            "epoch51 train time: 4.924s test time: 0.545  loss = 0.841 val_mse = 1.211 mse = 1.303 mae = 0.844\n",
            "epoch52 train time: 4.910s test time: 0.563  loss = 0.845 val_mse = 1.233 mse = 1.301 mae = 0.844\n",
            "epoch53 train time: 4.915s test time: 0.551  loss = 0.836 val_mse = 1.205 mse = 1.305 mae = 0.852\n",
            "epoch54 train time: 4.890s test time: 0.547  loss = 0.845 val_mse = 1.231 mse = 1.314 mae = 0.852\n",
            "epoch55 train time: 4.924s test time: 0.553  loss = 0.838 val_mse = 1.207 mse = 1.300 mae = 0.850\n",
            "epoch56 train time: 4.922s test time: 0.551  loss = 0.839 val_mse = 1.219 mse = 1.302 mae = 0.844\n",
            "epoch57 train time: 4.898s test time: 0.555  loss = 0.837 val_mse = 1.200 mse = 1.298 mae = 0.848\n",
            "epoch58 train time: 5.075s test time: 0.556  loss = 0.835 val_mse = 1.222 mse = 1.303 mae = 0.845\n",
            "epoch59 train time: 4.908s test time: 0.553  loss = 0.835 val_mse = 1.235 mse = 1.319 mae = 0.835\n",
            "epoch60 train time: 4.912s test time: 0.549  loss = 0.832 val_mse = 1.220 mse = 1.302 mae = 0.841\n",
            "epoch61 train time: 4.895s test time: 0.550  loss = 0.828 val_mse = 1.202 mse = 1.298 mae = 0.847\n",
            "epoch62 train time: 4.919s test time: 0.552  loss = 0.829 val_mse = 1.225 mse = 1.308 mae = 0.839\n",
            "epoch63 train time: 4.910s test time: 0.558  loss = 0.823 val_mse = 1.213 mse = 1.305 mae = 0.835\n",
            "epoch64 train time: 4.902s test time: 0.546  loss = 0.826 val_mse = 1.222 mse = 1.309 mae = 0.845\n",
            "epoch65 train time: 4.907s test time: 0.556  loss = 0.826 val_mse = 1.192 mse = 1.295 mae = 0.847\n",
            "epoch66 train time: 4.908s test time: 0.557  loss = 0.831 val_mse = 1.211 mse = 1.297 mae = 0.844\n",
            "epoch67 train time: 4.909s test time: 0.550  loss = 0.824 val_mse = 1.191 mse = 1.299 mae = 0.847\n",
            "epoch68 train time: 4.904s test time: 0.559  loss = 0.823 val_mse = 1.216 mse = 1.305 mae = 0.844\n",
            "epoch69 train time: 4.890s test time: 0.551  loss = 0.820 val_mse = 1.199 mse = 1.295 mae = 0.844\n",
            "epoch70 train time: 4.893s test time: 0.551  loss = 0.820 val_mse = 1.209 mse = 1.302 mae = 0.843\n",
            "epoch71 train time: 4.924s test time: 0.550  loss = 0.816 val_mse = 1.183 mse = 1.284 mae = 0.842\n",
            "epoch72 train time: 4.906s test time: 0.558  loss = 0.816 val_mse = 1.214 mse = 1.300 mae = 0.843\n",
            "epoch73 train time: 4.920s test time: 0.554  loss = 0.812 val_mse = 1.191 mse = 1.299 mae = 0.845\n",
            "epoch74 train time: 4.905s test time: 0.547  loss = 0.811 val_mse = 1.208 mse = 1.301 mae = 0.843\n",
            "epoch75 train time: 4.894s test time: 0.553  loss = 0.812 val_mse = 1.181 mse = 1.290 mae = 0.842\n",
            "epoch76 train time: 4.908s test time: 0.550  loss = 0.815 val_mse = 1.208 mse = 1.302 mae = 0.847\n",
            "epoch77 train time: 4.899s test time: 0.551  loss = 0.810 val_mse = 1.183 mse = 1.292 mae = 0.843\n",
            "epoch78 train time: 4.890s test time: 0.550  loss = 0.811 val_mse = 1.208 mse = 1.295 mae = 0.840\n",
            "epoch79 train time: 4.898s test time: 0.557  loss = 0.805 val_mse = 1.194 mse = 1.288 mae = 0.839\n",
            "epoch80 train time: 4.931s test time: 0.553  loss = 0.807 val_mse = 1.208 mse = 1.304 mae = 0.847\n",
            "epoch81 train time: 4.910s test time: 0.549  loss = 0.807 val_mse = 1.192 mse = 1.297 mae = 0.840\n",
            "epoch82 train time: 4.918s test time: 0.554  loss = 0.807 val_mse = 1.210 mse = 1.294 mae = 0.836\n",
            "epoch83 train time: 4.912s test time: 0.549  loss = 0.803 val_mse = 1.188 mse = 1.290 mae = 0.841\n",
            "epoch84 train time: 4.916s test time: 0.554  loss = 0.805 val_mse = 1.210 mse = 1.298 mae = 0.840\n",
            "epoch85 train time: 4.916s test time: 0.561  loss = 0.804 val_mse = 1.186 mse = 1.294 mae = 0.839\n",
            "epoch86 train time: 4.920s test time: 0.550  loss = 0.803 val_mse = 1.210 mse = 1.306 mae = 0.841\n",
            "epoch87 train time: 4.930s test time: 0.557  loss = 0.803 val_mse = 1.188 mse = 1.296 mae = 0.843\n",
            "epoch88 train time: 4.920s test time: 0.551  loss = 0.802 val_mse = 1.197 mse = 1.294 mae = 0.841\n",
            "epoch89 train time: 4.920s test time: 0.555  loss = 0.800 val_mse = 1.190 mse = 1.290 mae = 0.841\n",
            "epoch90 train time: 4.905s test time: 0.555  loss = 0.805 val_mse = 1.204 mse = 1.302 mae = 0.841\n",
            "epoch91 train time: 4.894s test time: 0.547  loss = 0.797 val_mse = 1.184 mse = 1.289 mae = 0.837\n",
            "epoch92 train time: 4.903s test time: 0.551  loss = 0.800 val_mse = 1.206 mse = 1.293 mae = 0.835\n",
            "epoch93 train time: 4.905s test time: 0.549  loss = 0.798 val_mse = 1.202 mse = 1.304 mae = 0.831\n",
            "epoch94 train time: 4.927s test time: 0.552  loss = 0.797 val_mse = 1.215 mse = 1.301 mae = 0.835\n",
            "epoch95 train time: 4.923s test time: 0.555  loss = 0.794 val_mse = 1.183 mse = 1.287 mae = 0.832\n",
            "epoch96 train time: 4.923s test time: 0.559  loss = 0.797 val_mse = 1.221 mse = 1.313 mae = 0.835\n",
            "epoch97 train time: 4.922s test time: 0.558  loss = 0.792 val_mse = 1.201 mse = 1.301 mae = 0.832\n",
            "epoch98 train time: 4.922s test time: 0.551  loss = 0.796 val_mse = 1.212 mse = 1.299 mae = 0.834\n",
            "epoch99 train time: 4.906s test time: 0.557  loss = 0.791 val_mse = 1.197 mse = 1.299 mae = 0.829\n",
            "epoch100 train time: 4.908s test time: 0.559  loss = 0.796 val_mse = 1.214 mse = 1.307 mae = 0.835\n",
            "epoch101 train time: 4.900s test time: 0.550  loss = 0.794 val_mse = 1.204 mse = 1.298 mae = 0.826\n",
            "epoch102 train time: 4.923s test time: 0.559  loss = 0.793 val_mse = 1.216 mse = 1.302 mae = 0.832\n",
            "epoch103 train time: 4.912s test time: 0.564  loss = 0.792 val_mse = 1.202 mse = 1.304 mae = 0.831\n",
            "epoch104 train time: 4.916s test time: 0.567  loss = 0.795 val_mse = 1.212 mse = 1.296 mae = 0.832\n",
            "epoch105 train time: 4.922s test time: 0.560  loss = 0.793 val_mse = 1.207 mse = 1.308 mae = 0.831\n",
            "epoch106 train time: 4.922s test time: 0.552  loss = 0.794 val_mse = 1.197 mse = 1.295 mae = 0.837\n",
            "epoch107 train time: 4.925s test time: 0.571  loss = 0.789 val_mse = 1.186 mse = 1.292 mae = 0.830\n",
            "epoch108 train time: 4.920s test time: 0.549  loss = 0.791 val_mse = 1.206 mse = 1.305 mae = 0.839\n",
            "epoch109 train time: 4.926s test time: 0.548  loss = 0.788 val_mse = 1.201 mse = 1.296 mae = 0.827\n",
            "epoch110 train time: 4.905s test time: 0.559  loss = 0.790 val_mse = 1.207 mse = 1.297 mae = 0.833\n",
            "epoch111 train time: 4.915s test time: 0.548  loss = 0.790 val_mse = 1.204 mse = 1.300 mae = 0.829\n",
            "epoch112 train time: 4.913s test time: 0.553  loss = 0.786 val_mse = 1.199 mse = 1.291 mae = 0.833\n",
            "epoch113 train time: 4.896s test time: 0.549  loss = 0.786 val_mse = 1.198 mse = 1.293 mae = 0.828\n",
            "epoch114 train time: 4.904s test time: 0.554  loss = 0.795 val_mse = 1.199 mse = 1.296 mae = 0.836\n",
            "epoch115 train time: 4.893s test time: 0.552  loss = 0.787 val_mse = 1.194 mse = 1.300 mae = 0.827\n",
            "epoch116 train time: 4.883s test time: 0.547  loss = 0.788 val_mse = 1.206 mse = 1.299 mae = 0.835\n",
            "epoch117 train time: 4.883s test time: 0.547  loss = 0.786 val_mse = 1.189 mse = 1.294 mae = 0.829\n",
            "epoch118 train time: 4.896s test time: 0.556  loss = 0.786 val_mse = 1.195 mse = 1.289 mae = 0.832\n",
            "epoch119 train time: 4.910s test time: 0.565  loss = 0.787 val_mse = 1.195 mse = 1.297 mae = 0.828\n",
            "epoch120 train time: 4.901s test time: 0.553  loss = 0.786 val_mse = 1.205 mse = 1.299 mae = 0.831\n",
            "epoch121 train time: 4.908s test time: 0.557  loss = 0.781 val_mse = 1.201 mse = 1.300 mae = 0.827\n",
            "epoch122 train time: 4.915s test time: 0.553  loss = 0.783 val_mse = 1.192 mse = 1.293 mae = 0.835\n",
            "epoch123 train time: 4.900s test time: 0.547  loss = 0.783 val_mse = 1.185 mse = 1.289 mae = 0.827\n",
            "epoch124 train time: 4.897s test time: 0.550  loss = 0.783 val_mse = 1.199 mse = 1.289 mae = 0.832\n",
            "epoch125 train time: 4.919s test time: 0.549  loss = 0.781 val_mse = 1.186 mse = 1.292 mae = 0.829\n",
            "epoch126 train time: 4.905s test time: 0.553  loss = 0.781 val_mse = 1.197 mse = 1.295 mae = 0.833\n",
            "epoch127 train time: 4.903s test time: 0.550  loss = 0.780 val_mse = 1.199 mse = 1.298 mae = 0.828\n",
            "epoch128 train time: 4.902s test time: 0.549  loss = 0.783 val_mse = 1.193 mse = 1.289 mae = 0.833\n",
            "epoch129 train time: 4.895s test time: 0.552  loss = 0.781 val_mse = 1.192 mse = 1.290 mae = 0.828\n",
            "epoch130 train time: 4.895s test time: 0.550  loss = 0.780 val_mse = 1.190 mse = 1.289 mae = 0.832\n",
            "epoch131 train time: 4.892s test time: 0.552  loss = 0.776 val_mse = 1.183 mse = 1.289 mae = 0.830\n",
            "epoch132 train time: 4.894s test time: 0.553  loss = 0.779 val_mse = 1.192 mse = 1.290 mae = 0.833\n",
            "epoch133 train time: 4.900s test time: 0.549  loss = 0.779 val_mse = 1.188 mse = 1.298 mae = 0.827\n",
            "epoch134 train time: 4.919s test time: 0.553  loss = 0.777 val_mse = 1.191 mse = 1.294 mae = 0.836\n",
            "epoch135 train time: 4.918s test time: 0.551  loss = 0.776 val_mse = 1.185 mse = 1.290 mae = 0.829\n",
            "epoch136 train time: 4.915s test time: 0.552  loss = 0.773 val_mse = 1.192 mse = 1.294 mae = 0.836\n",
            "epoch137 train time: 4.907s test time: 0.554  loss = 0.779 val_mse = 1.178 mse = 1.291 mae = 0.829\n",
            "epoch138 train time: 4.908s test time: 0.549  loss = 0.774 val_mse = 1.191 mse = 1.293 mae = 0.832\n",
            "epoch139 train time: 4.915s test time: 0.560  loss = 0.770 val_mse = 1.183 mse = 1.286 mae = 0.828\n",
            "epoch140 train time: 4.890s test time: 0.551  loss = 0.774 val_mse = 1.188 mse = 1.290 mae = 0.833\n",
            "epoch141 train time: 4.901s test time: 0.548  loss = 0.770 val_mse = 1.188 mse = 1.296 mae = 0.829\n",
            "epoch142 train time: 4.900s test time: 0.552  loss = 0.772 val_mse = 1.192 mse = 1.293 mae = 0.835\n",
            "epoch143 train time: 4.894s test time: 0.551  loss = 0.776 val_mse = 1.193 mse = 1.295 mae = 0.826\n",
            "epoch144 train time: 4.893s test time: 0.554  loss = 0.771 val_mse = 1.190 mse = 1.289 mae = 0.833\n",
            "epoch145 train time: 4.894s test time: 0.545  loss = 0.770 val_mse = 1.183 mse = 1.292 mae = 0.828\n",
            "epoch146 train time: 4.911s test time: 0.549  loss = 0.771 val_mse = 1.194 mse = 1.294 mae = 0.832\n",
            "epoch147 train time: 4.901s test time: 0.546  loss = 0.770 val_mse = 1.188 mse = 1.295 mae = 0.827\n",
            "epoch148 train time: 4.904s test time: 0.553  loss = 0.772 val_mse = 1.189 mse = 1.294 mae = 0.835\n",
            "epoch149 train time: 4.903s test time: 0.548  loss = 0.771 val_mse = 1.181 mse = 1.287 mae = 0.827\n",
            "epoch150 train time: 4.907s test time: 0.561  loss = 0.772 val_mse = 1.187 mse = 1.291 mae = 0.833\n",
            "epoch151 train time: 4.908s test time: 0.557  loss = 0.775 val_mse = 1.190 mse = 1.299 mae = 0.829\n",
            "epoch152 train time: 4.912s test time: 0.547  loss = 0.772 val_mse = 1.191 mse = 1.290 mae = 0.834\n",
            "epoch153 train time: 4.922s test time: 0.548  loss = 0.771 val_mse = 1.192 mse = 1.297 mae = 0.831\n",
            "epoch154 train time: 4.902s test time: 0.546  loss = 0.775 val_mse = 1.192 mse = 1.292 mae = 0.833\n",
            "epoch155 train time: 4.901s test time: 0.558  loss = 0.774 val_mse = 1.190 mse = 1.289 mae = 0.825\n",
            "epoch156 train time: 4.906s test time: 0.552  loss = 0.772 val_mse = 1.204 mse = 1.291 mae = 0.831\n",
            "epoch157 train time: 4.896s test time: 0.553  loss = 0.773 val_mse = 1.183 mse = 1.290 mae = 0.826\n",
            "epoch158 train time: 4.892s test time: 0.555  loss = 0.769 val_mse = 1.186 mse = 1.294 mae = 0.833\n",
            "epoch159 train time: 4.890s test time: 0.545  loss = 0.769 val_mse = 1.183 mse = 1.290 mae = 0.829\n",
            "epoch160 train time: 4.883s test time: 0.548  loss = 0.773 val_mse = 1.191 mse = 1.289 mae = 0.831\n",
            "epoch161 train time: 4.912s test time: 0.548  loss = 0.771 val_mse = 1.182 mse = 1.292 mae = 0.828\n",
            "epoch162 train time: 4.910s test time: 0.550  loss = 0.769 val_mse = 1.194 mse = 1.289 mae = 0.830\n",
            "epoch163 train time: 4.918s test time: 0.560  loss = 0.770 val_mse = 1.184 mse = 1.290 mae = 0.826\n",
            "epoch164 train time: 4.914s test time: 0.550  loss = 0.770 val_mse = 1.193 mse = 1.296 mae = 0.834\n",
            "epoch165 train time: 4.920s test time: 0.554  loss = 0.771 val_mse = 1.188 mse = 1.292 mae = 0.826\n",
            "epoch166 train time: 4.900s test time: 0.557  loss = 0.770 val_mse = 1.191 mse = 1.286 mae = 0.833\n",
            "epoch167 train time: 4.912s test time: 0.552  loss = 0.771 val_mse = 1.186 mse = 1.291 mae = 0.828\n",
            "epoch168 train time: 4.917s test time: 0.564  loss = 0.769 val_mse = 1.191 mse = 1.292 mae = 0.832\n",
            "epoch169 train time: 4.905s test time: 0.551  loss = 0.769 val_mse = 1.180 mse = 1.284 mae = 0.827\n",
            "epoch170 train time: 4.902s test time: 0.552  loss = 0.768 val_mse = 1.195 mse = 1.292 mae = 0.830\n",
            "epoch171 train time: 4.891s test time: 0.546  loss = 0.770 val_mse = 1.183 mse = 1.293 mae = 0.829\n",
            "epoch172 train time: 4.901s test time: 0.550  loss = 0.770 val_mse = 1.194 mse = 1.294 mae = 0.832\n",
            "epoch173 train time: 4.899s test time: 0.550  loss = 0.770 val_mse = 1.184 mse = 1.293 mae = 0.828\n",
            "epoch174 train time: 4.893s test time: 0.559  loss = 0.769 val_mse = 1.182 mse = 1.284 mae = 0.828\n",
            "epoch175 train time: 4.892s test time: 0.555  loss = 0.768 val_mse = 1.184 mse = 1.293 mae = 0.829\n",
            "epoch176 train time: 4.916s test time: 0.550  loss = 0.772 val_mse = 1.188 mse = 1.287 mae = 0.830\n",
            "epoch177 train time: 4.908s test time: 0.551  loss = 0.768 val_mse = 1.187 mse = 1.293 mae = 0.828\n",
            "epoch178 train time: 4.912s test time: 0.548  loss = 0.772 val_mse = 1.186 mse = 1.290 mae = 0.834\n",
            "epoch179 train time: 4.920s test time: 0.560  loss = 0.771 val_mse = 1.184 mse = 1.291 mae = 0.825\n",
            "MAE 0.8388697302821002\n",
            "MSE 1.3123322760781038\n",
            "epoch0 train time: 5.291s test time: 0.549  loss = 42.417 val_mse = 1.643 mse = 1.569 mae = 0.910\n",
            "epoch1 train time: 4.890s test time: 0.554  loss = 31.855 val_mse = 1.611 mse = 1.549 mae = 0.902\n",
            "epoch2 train time: 4.900s test time: 0.564  loss = 17.149 val_mse = 1.578 mse = 1.526 mae = 0.891\n",
            "epoch3 train time: 4.883s test time: 0.553  loss = 8.553 val_mse = 1.549 mse = 1.504 mae = 0.880\n",
            "epoch4 train time: 4.897s test time: 0.553  loss = 4.189 val_mse = 1.520 mse = 1.484 mae = 0.870\n",
            "epoch5 train time: 4.875s test time: 0.559  loss = 2.301 val_mse = 1.491 mse = 1.464 mae = 0.861\n",
            "epoch6 train time: 4.868s test time: 0.549  loss = 1.658 val_mse = 1.460 mse = 1.442 mae = 0.854\n",
            "epoch7 train time: 4.859s test time: 0.552  loss = 1.484 val_mse = 1.425 mse = 1.417 mae = 0.849\n",
            "epoch8 train time: 4.871s test time: 0.557  loss = 1.398 val_mse = 1.390 mse = 1.393 mae = 0.845\n",
            "epoch9 train time: 4.867s test time: 0.548  loss = 1.327 val_mse = 1.364 mse = 1.376 mae = 0.843\n",
            "epoch10 train time: 4.871s test time: 0.556  loss = 1.264 val_mse = 1.346 mse = 1.366 mae = 0.842\n",
            "epoch11 train time: 4.867s test time: 0.546  loss = 1.209 val_mse = 1.330 mse = 1.358 mae = 0.840\n",
            "epoch12 train time: 4.875s test time: 0.549  loss = 1.159 val_mse = 1.316 mse = 1.349 mae = 0.838\n",
            "epoch13 train time: 4.862s test time: 0.549  loss = 1.114 val_mse = 1.304 mse = 1.344 mae = 0.838\n",
            "epoch14 train time: 4.877s test time: 0.550  loss = 1.075 val_mse = 1.285 mse = 1.332 mae = 0.837\n",
            "epoch15 train time: 4.866s test time: 0.555  loss = 1.041 val_mse = 1.276 mse = 1.326 mae = 0.835\n",
            "epoch16 train time: 4.876s test time: 0.560  loss = 1.012 val_mse = 1.257 mse = 1.318 mae = 0.840\n",
            "epoch17 train time: 4.872s test time: 0.548  loss = 0.991 val_mse = 1.257 mse = 1.315 mae = 0.839\n",
            "epoch18 train time: 4.852s test time: 0.551  loss = 0.972 val_mse = 1.271 mse = 1.332 mae = 0.836\n",
            "epoch19 train time: 4.875s test time: 0.552  loss = 0.959 val_mse = 1.230 mse = 1.304 mae = 0.845\n",
            "epoch20 train time: 4.841s test time: 0.546  loss = 0.946 val_mse = 1.236 mse = 1.310 mae = 0.841\n",
            "epoch21 train time: 4.856s test time: 0.555  loss = 0.944 val_mse = 1.234 mse = 1.304 mae = 0.840\n",
            "epoch22 train time: 4.839s test time: 0.542  loss = 0.931 val_mse = 1.223 mse = 1.289 mae = 0.847\n",
            "epoch23 train time: 4.855s test time: 0.559  loss = 0.922 val_mse = 1.209 mse = 1.290 mae = 0.849\n",
            "epoch24 train time: 4.859s test time: 0.550  loss = 0.911 val_mse = 1.214 mse = 1.295 mae = 0.852\n",
            "epoch25 train time: 4.879s test time: 0.556  loss = 0.915 val_mse = 1.217 mse = 1.301 mae = 0.854\n",
            "epoch26 train time: 4.868s test time: 0.549  loss = 0.903 val_mse = 1.231 mse = 1.300 mae = 0.838\n",
            "epoch27 train time: 4.873s test time: 0.554  loss = 0.898 val_mse = 1.224 mse = 1.312 mae = 0.849\n",
            "epoch28 train time: 4.877s test time: 0.550  loss = 0.893 val_mse = 1.230 mse = 1.302 mae = 0.838\n",
            "epoch29 train time: 4.861s test time: 0.551  loss = 0.892 val_mse = 1.219 mse = 1.316 mae = 0.852\n",
            "epoch30 train time: 4.868s test time: 0.556  loss = 0.892 val_mse = 1.232 mse = 1.302 mae = 0.838\n",
            "epoch31 train time: 4.861s test time: 0.548  loss = 0.881 val_mse = 1.212 mse = 1.313 mae = 0.846\n",
            "epoch32 train time: 4.862s test time: 0.558  loss = 0.881 val_mse = 1.237 mse = 1.313 mae = 0.834\n",
            "epoch33 train time: 4.847s test time: 0.546  loss = 0.881 val_mse = 1.211 mse = 1.314 mae = 0.851\n",
            "epoch34 train time: 4.854s test time: 0.555  loss = 0.878 val_mse = 1.236 mse = 1.303 mae = 0.836\n",
            "epoch35 train time: 4.858s test time: 0.549  loss = 0.873 val_mse = 1.218 mse = 1.306 mae = 0.843\n",
            "epoch36 train time: 4.869s test time: 0.550  loss = 0.875 val_mse = 1.232 mse = 1.309 mae = 0.838\n",
            "epoch37 train time: 4.845s test time: 0.544  loss = 0.868 val_mse = 1.259 mse = 1.333 mae = 0.835\n",
            "epoch38 train time: 4.862s test time: 0.556  loss = 0.865 val_mse = 1.211 mse = 1.296 mae = 0.843\n",
            "epoch39 train time: 4.862s test time: 0.547  loss = 0.865 val_mse = 1.208 mse = 1.308 mae = 0.849\n",
            "epoch40 train time: 4.861s test time: 0.556  loss = 0.860 val_mse = 1.238 mse = 1.314 mae = 0.834\n",
            "epoch41 train time: 4.880s test time: 0.558  loss = 0.858 val_mse = 1.222 mse = 1.306 mae = 0.843\n",
            "epoch42 train time: 4.860s test time: 0.546  loss = 0.859 val_mse = 1.242 mse = 1.326 mae = 0.843\n",
            "epoch43 train time: 4.860s test time: 0.553  loss = 0.862 val_mse = 1.211 mse = 1.301 mae = 0.846\n",
            "epoch44 train time: 4.854s test time: 0.544  loss = 0.850 val_mse = 1.245 mse = 1.318 mae = 0.838\n",
            "epoch45 train time: 4.870s test time: 0.557  loss = 0.856 val_mse = 1.219 mse = 1.302 mae = 0.843\n",
            "epoch46 train time: 4.870s test time: 0.554  loss = 0.852 val_mse = 1.207 mse = 1.295 mae = 0.854\n",
            "epoch47 train time: 4.873s test time: 0.556  loss = 0.854 val_mse = 1.209 mse = 1.302 mae = 0.840\n",
            "epoch48 train time: 4.852s test time: 0.551  loss = 0.844 val_mse = 1.207 mse = 1.294 mae = 0.846\n",
            "epoch49 train time: 4.870s test time: 0.546  loss = 0.846 val_mse = 1.235 mse = 1.319 mae = 0.845\n",
            "epoch50 train time: 4.867s test time: 0.546  loss = 0.845 val_mse = 1.210 mse = 1.299 mae = 0.846\n",
            "epoch51 train time: 4.858s test time: 0.561  loss = 0.840 val_mse = 1.226 mse = 1.308 mae = 0.844\n",
            "epoch52 train time: 4.881s test time: 0.560  loss = 0.837 val_mse = 1.213 mse = 1.300 mae = 0.847\n",
            "epoch53 train time: 4.864s test time: 0.544  loss = 0.844 val_mse = 1.220 mse = 1.304 mae = 0.844\n",
            "epoch54 train time: 4.870s test time: 0.551  loss = 0.835 val_mse = 1.205 mse = 1.291 mae = 0.848\n",
            "epoch55 train time: 4.869s test time: 0.549  loss = 0.837 val_mse = 1.217 mse = 1.308 mae = 0.846\n",
            "epoch56 train time: 4.871s test time: 0.551  loss = 0.838 val_mse = 1.203 mse = 1.300 mae = 0.854\n",
            "epoch57 train time: 4.856s test time: 0.554  loss = 0.834 val_mse = 1.211 mse = 1.306 mae = 0.845\n",
            "epoch58 train time: 4.867s test time: 0.550  loss = 0.827 val_mse = 1.196 mse = 1.288 mae = 0.845\n",
            "epoch59 train time: 4.877s test time: 0.548  loss = 0.830 val_mse = 1.217 mse = 1.304 mae = 0.844\n",
            "epoch60 train time: 4.855s test time: 0.552  loss = 0.826 val_mse = 1.228 mse = 1.316 mae = 0.831\n",
            "epoch61 train time: 4.879s test time: 0.555  loss = 0.822 val_mse = 1.205 mse = 1.308 mae = 0.845\n",
            "epoch62 train time: 4.888s test time: 0.554  loss = 0.823 val_mse = 1.203 mse = 1.291 mae = 0.848\n",
            "epoch63 train time: 4.879s test time: 0.567  loss = 0.831 val_mse = 1.212 mse = 1.296 mae = 0.842\n",
            "epoch64 train time: 4.881s test time: 0.554  loss = 0.822 val_mse = 1.185 mse = 1.293 mae = 0.849\n",
            "epoch65 train time: 4.882s test time: 0.546  loss = 0.825 val_mse = 1.212 mse = 1.301 mae = 0.840\n",
            "epoch66 train time: 4.869s test time: 0.554  loss = 0.822 val_mse = 1.195 mse = 1.279 mae = 0.841\n",
            "epoch67 train time: 4.869s test time: 0.551  loss = 0.817 val_mse = 1.207 mse = 1.294 mae = 0.842\n",
            "epoch68 train time: 4.874s test time: 0.553  loss = 0.820 val_mse = 1.189 mse = 1.291 mae = 0.846\n",
            "epoch69 train time: 4.874s test time: 0.558  loss = 0.817 val_mse = 1.211 mse = 1.306 mae = 0.845\n",
            "epoch70 train time: 4.858s test time: 0.548  loss = 0.815 val_mse = 1.181 mse = 1.283 mae = 0.847\n",
            "epoch71 train time: 4.866s test time: 0.556  loss = 0.817 val_mse = 1.208 mse = 1.299 mae = 0.843\n",
            "epoch72 train time: 4.889s test time: 0.559  loss = 0.816 val_mse = 1.228 mse = 1.311 mae = 0.830\n",
            "epoch73 train time: 4.866s test time: 0.553  loss = 0.812 val_mse = 1.204 mse = 1.296 mae = 0.839\n",
            "epoch74 train time: 4.868s test time: 0.554  loss = 0.812 val_mse = 1.219 mse = 1.297 mae = 0.829\n",
            "epoch75 train time: 4.860s test time: 0.550  loss = 0.814 val_mse = 1.209 mse = 1.298 mae = 0.843\n",
            "epoch76 train time: 4.861s test time: 0.552  loss = 0.809 val_mse = 1.191 mse = 1.298 mae = 0.847\n",
            "epoch77 train time: 4.848s test time: 0.549  loss = 0.811 val_mse = 1.204 mse = 1.301 mae = 0.840\n",
            "epoch78 train time: 4.856s test time: 0.558  loss = 0.811 val_mse = 1.193 mse = 1.284 mae = 0.840\n",
            "epoch79 train time: 4.861s test time: 0.548  loss = 0.808 val_mse = 1.200 mse = 1.302 mae = 0.844\n",
            "epoch80 train time: 4.866s test time: 0.552  loss = 0.813 val_mse = 1.210 mse = 1.292 mae = 0.830\n",
            "epoch81 train time: 4.840s test time: 0.545  loss = 0.809 val_mse = 1.227 mse = 1.319 mae = 0.837\n",
            "epoch82 train time: 4.860s test time: 0.548  loss = 0.804 val_mse = 1.206 mse = 1.296 mae = 0.832\n",
            "epoch83 train time: 4.859s test time: 0.556  loss = 0.806 val_mse = 1.226 mse = 1.315 mae = 0.836\n",
            "epoch84 train time: 4.861s test time: 0.555  loss = 0.808 val_mse = 1.205 mse = 1.291 mae = 0.828\n",
            "epoch85 train time: 4.864s test time: 0.550  loss = 0.799 val_mse = 1.199 mse = 1.301 mae = 0.847\n",
            "epoch86 train time: 4.855s test time: 0.548  loss = 0.807 val_mse = 1.186 mse = 1.287 mae = 0.841\n",
            "epoch87 train time: 4.864s test time: 0.550  loss = 0.805 val_mse = 1.198 mse = 1.298 mae = 0.841\n",
            "epoch88 train time: 4.857s test time: 0.550  loss = 0.802 val_mse = 1.196 mse = 1.291 mae = 0.841\n",
            "epoch89 train time: 4.851s test time: 0.554  loss = 0.801 val_mse = 1.191 mse = 1.297 mae = 0.841\n",
            "epoch90 train time: 4.859s test time: 0.551  loss = 0.798 val_mse = 1.192 mse = 1.287 mae = 0.836\n",
            "epoch91 train time: 4.874s test time: 0.553  loss = 0.803 val_mse = 1.194 mse = 1.295 mae = 0.841\n",
            "epoch92 train time: 4.864s test time: 0.545  loss = 0.800 val_mse = 1.189 mse = 1.281 mae = 0.834\n",
            "epoch93 train time: 4.859s test time: 0.560  loss = 0.796 val_mse = 1.192 mse = 1.306 mae = 0.845\n",
            "epoch94 train time: 4.844s test time: 0.551  loss = 0.802 val_mse = 1.191 mse = 1.280 mae = 0.841\n",
            "epoch95 train time: 4.881s test time: 0.554  loss = 0.798 val_mse = 1.190 mse = 1.291 mae = 0.839\n",
            "epoch96 train time: 4.860s test time: 0.550  loss = 0.798 val_mse = 1.196 mse = 1.294 mae = 0.835\n",
            "epoch97 train time: 4.853s test time: 0.549  loss = 0.796 val_mse = 1.201 mse = 1.298 mae = 0.842\n",
            "epoch98 train time: 4.886s test time: 0.548  loss = 0.795 val_mse = 1.186 mse = 1.279 mae = 0.833\n",
            "epoch99 train time: 4.862s test time: 0.550  loss = 0.794 val_mse = 1.193 mse = 1.296 mae = 0.840\n",
            "epoch100 train time: 4.844s test time: 0.547  loss = 0.797 val_mse = 1.191 mse = 1.282 mae = 0.835\n",
            "epoch101 train time: 4.856s test time: 0.543  loss = 0.793 val_mse = 1.188 mse = 1.294 mae = 0.842\n",
            "epoch102 train time: 4.863s test time: 0.556  loss = 0.795 val_mse = 1.190 mse = 1.283 mae = 0.831\n",
            "epoch103 train time: 4.847s test time: 0.547  loss = 0.791 val_mse = 1.195 mse = 1.299 mae = 0.838\n",
            "epoch104 train time: 4.849s test time: 0.558  loss = 0.799 val_mse = 1.184 mse = 1.282 mae = 0.836\n",
            "epoch105 train time: 4.873s test time: 0.548  loss = 0.797 val_mse = 1.197 mse = 1.301 mae = 0.839\n",
            "epoch106 train time: 4.856s test time: 0.561  loss = 0.793 val_mse = 1.182 mse = 1.282 mae = 0.836\n",
            "epoch107 train time: 4.871s test time: 0.560  loss = 0.794 val_mse = 1.186 mse = 1.296 mae = 0.839\n",
            "epoch108 train time: 4.848s test time: 0.544  loss = 0.794 val_mse = 1.191 mse = 1.288 mae = 0.832\n",
            "epoch109 train time: 4.873s test time: 0.551  loss = 0.791 val_mse = 1.188 mse = 1.296 mae = 0.837\n",
            "epoch110 train time: 4.890s test time: 0.561  loss = 0.788 val_mse = 1.189 mse = 1.288 mae = 0.831\n",
            "epoch111 train time: 4.874s test time: 0.559  loss = 0.796 val_mse = 1.198 mse = 1.295 mae = 0.836\n",
            "epoch112 train time: 4.870s test time: 0.564  loss = 0.792 val_mse = 1.192 mse = 1.288 mae = 0.830\n",
            "epoch113 train time: 4.859s test time: 0.550  loss = 0.793 val_mse = 1.193 mse = 1.296 mae = 0.836\n",
            "epoch114 train time: 4.874s test time: 0.546  loss = 0.788 val_mse = 1.185 mse = 1.288 mae = 0.834\n",
            "epoch115 train time: 4.883s test time: 0.554  loss = 0.791 val_mse = 1.194 mse = 1.300 mae = 0.838\n",
            "epoch116 train time: 4.864s test time: 0.553  loss = 0.793 val_mse = 1.185 mse = 1.286 mae = 0.837\n",
            "epoch117 train time: 4.886s test time: 0.557  loss = 0.794 val_mse = 1.199 mse = 1.300 mae = 0.838\n",
            "epoch118 train time: 4.855s test time: 0.552  loss = 0.795 val_mse = 1.186 mse = 1.279 mae = 0.828\n",
            "epoch119 train time: 4.841s test time: 0.549  loss = 0.788 val_mse = 1.196 mse = 1.295 mae = 0.836\n",
            "epoch120 train time: 4.860s test time: 0.548  loss = 0.788 val_mse = 1.188 mse = 1.280 mae = 0.831\n",
            "epoch121 train time: 4.854s test time: 0.550  loss = 0.788 val_mse = 1.189 mse = 1.293 mae = 0.836\n",
            "epoch122 train time: 4.858s test time: 0.557  loss = 0.790 val_mse = 1.185 mse = 1.280 mae = 0.834\n",
            "epoch123 train time: 4.863s test time: 0.549  loss = 0.787 val_mse = 1.190 mse = 1.296 mae = 0.835\n",
            "epoch124 train time: 4.855s test time: 0.555  loss = 0.789 val_mse = 1.180 mse = 1.279 mae = 0.832\n",
            "epoch125 train time: 4.871s test time: 0.546  loss = 0.786 val_mse = 1.192 mse = 1.295 mae = 0.833\n",
            "epoch126 train time: 4.856s test time: 0.557  loss = 0.785 val_mse = 1.186 mse = 1.288 mae = 0.833\n",
            "epoch127 train time: 4.866s test time: 0.556  loss = 0.787 val_mse = 1.194 mse = 1.296 mae = 0.837\n",
            "epoch128 train time: 4.866s test time: 0.549  loss = 0.786 val_mse = 1.182 mse = 1.278 mae = 0.831\n",
            "epoch129 train time: 4.840s test time: 0.555  loss = 0.785 val_mse = 1.184 mse = 1.290 mae = 0.836\n",
            "epoch130 train time: 4.866s test time: 0.546  loss = 0.790 val_mse = 1.189 mse = 1.290 mae = 0.832\n",
            "epoch131 train time: 4.872s test time: 0.560  loss = 0.785 val_mse = 1.193 mse = 1.301 mae = 0.838\n",
            "epoch132 train time: 4.865s test time: 0.547  loss = 0.784 val_mse = 1.181 mse = 1.278 mae = 0.830\n",
            "epoch133 train time: 4.860s test time: 0.558  loss = 0.784 val_mse = 1.196 mse = 1.293 mae = 0.834\n",
            "epoch134 train time: 4.867s test time: 0.549  loss = 0.785 val_mse = 1.189 mse = 1.281 mae = 0.832\n",
            "epoch135 train time: 4.842s test time: 0.560  loss = 0.784 val_mse = 1.197 mse = 1.299 mae = 0.835\n",
            "epoch136 train time: 4.854s test time: 0.559  loss = 0.784 val_mse = 1.188 mse = 1.283 mae = 0.834\n",
            "epoch137 train time: 4.861s test time: 0.554  loss = 0.784 val_mse = 1.187 mse = 1.293 mae = 0.833\n",
            "epoch138 train time: 4.863s test time: 0.552  loss = 0.783 val_mse = 1.184 mse = 1.281 mae = 0.829\n",
            "epoch139 train time: 4.868s test time: 0.567  loss = 0.782 val_mse = 1.187 mse = 1.293 mae = 0.834\n",
            "epoch140 train time: 4.854s test time: 0.548  loss = 0.783 val_mse = 1.184 mse = 1.284 mae = 0.832\n",
            "epoch141 train time: 4.853s test time: 0.545  loss = 0.781 val_mse = 1.188 mse = 1.297 mae = 0.836\n",
            "epoch142 train time: 4.863s test time: 0.559  loss = 0.782 val_mse = 1.178 mse = 1.284 mae = 0.837\n",
            "epoch143 train time: 4.856s test time: 0.544  loss = 0.782 val_mse = 1.188 mse = 1.293 mae = 0.832\n",
            "epoch144 train time: 4.852s test time: 0.558  loss = 0.779 val_mse = 1.183 mse = 1.281 mae = 0.833\n",
            "epoch145 train time: 4.868s test time: 0.558  loss = 0.783 val_mse = 1.190 mse = 1.299 mae = 0.839\n",
            "epoch146 train time: 4.865s test time: 0.550  loss = 0.781 val_mse = 1.187 mse = 1.279 mae = 0.829\n",
            "epoch147 train time: 4.891s test time: 0.549  loss = 0.780 val_mse = 1.184 mse = 1.296 mae = 0.837\n",
            "epoch148 train time: 4.866s test time: 0.549  loss = 0.782 val_mse = 1.184 mse = 1.280 mae = 0.830\n",
            "epoch149 train time: 4.872s test time: 0.561  loss = 0.780 val_mse = 1.192 mse = 1.288 mae = 0.834\n",
            "epoch150 train time: 4.878s test time: 0.551  loss = 0.780 val_mse = 1.179 mse = 1.275 mae = 0.827\n",
            "epoch151 train time: 4.875s test time: 0.569  loss = 0.777 val_mse = 1.189 mse = 1.296 mae = 0.834\n",
            "epoch152 train time: 4.865s test time: 0.561  loss = 0.779 val_mse = 1.180 mse = 1.280 mae = 0.829\n",
            "epoch153 train time: 4.871s test time: 0.557  loss = 0.775 val_mse = 1.195 mse = 1.295 mae = 0.833\n",
            "epoch154 train time: 4.899s test time: 0.548  loss = 0.777 val_mse = 1.179 mse = 1.284 mae = 0.833\n",
            "epoch155 train time: 4.870s test time: 0.558  loss = 0.775 val_mse = 1.189 mse = 1.290 mae = 0.833\n",
            "epoch156 train time: 4.857s test time: 0.549  loss = 0.777 val_mse = 1.182 mse = 1.280 mae = 0.827\n",
            "epoch157 train time: 4.844s test time: 0.548  loss = 0.772 val_mse = 1.190 mse = 1.297 mae = 0.833\n",
            "epoch158 train time: 4.870s test time: 0.558  loss = 0.775 val_mse = 1.186 mse = 1.282 mae = 0.831\n",
            "epoch159 train time: 4.868s test time: 0.551  loss = 0.775 val_mse = 1.189 mse = 1.289 mae = 0.832\n",
            "epoch160 train time: 4.863s test time: 0.554  loss = 0.777 val_mse = 1.186 mse = 1.288 mae = 0.828\n",
            "epoch161 train time: 4.853s test time: 0.549  loss = 0.773 val_mse = 1.191 mse = 1.298 mae = 0.831\n",
            "epoch162 train time: 4.858s test time: 0.556  loss = 0.773 val_mse = 1.188 mse = 1.280 mae = 0.827\n",
            "epoch163 train time: 4.838s test time: 0.548  loss = 0.771 val_mse = 1.190 mse = 1.295 mae = 0.833\n",
            "epoch164 train time: 4.872s test time: 0.555  loss = 0.772 val_mse = 1.183 mse = 1.285 mae = 0.829\n",
            "epoch165 train time: 4.849s test time: 0.547  loss = 0.772 val_mse = 1.188 mse = 1.292 mae = 0.833\n",
            "epoch166 train time: 4.863s test time: 0.556  loss = 0.774 val_mse = 1.182 mse = 1.284 mae = 0.830\n",
            "epoch167 train time: 4.856s test time: 0.554  loss = 0.771 val_mse = 1.184 mse = 1.292 mae = 0.831\n",
            "epoch168 train time: 4.849s test time: 0.556  loss = 0.773 val_mse = 1.182 mse = 1.283 mae = 0.830\n",
            "epoch169 train time: 4.870s test time: 0.551  loss = 0.774 val_mse = 1.188 mse = 1.293 mae = 0.832\n",
            "epoch170 train time: 4.863s test time: 0.546  loss = 0.772 val_mse = 1.184 mse = 1.285 mae = 0.828\n",
            "epoch171 train time: 4.846s test time: 0.547  loss = 0.769 val_mse = 1.181 mse = 1.291 mae = 0.835\n",
            "epoch172 train time: 4.864s test time: 0.554  loss = 0.772 val_mse = 1.186 mse = 1.287 mae = 0.828\n",
            "epoch173 train time: 4.866s test time: 0.551  loss = 0.770 val_mse = 1.194 mse = 1.293 mae = 0.830\n",
            "epoch174 train time: 4.859s test time: 0.548  loss = 0.778 val_mse = 1.189 mse = 1.286 mae = 0.829\n",
            "epoch175 train time: 4.845s test time: 0.553  loss = 0.778 val_mse = 1.184 mse = 1.294 mae = 0.833\n",
            "epoch176 train time: 4.860s test time: 0.550  loss = 0.772 val_mse = 1.183 mse = 1.284 mae = 0.826\n",
            "epoch177 train time: 4.860s test time: 0.547  loss = 0.769 val_mse = 1.186 mse = 1.296 mae = 0.836\n",
            "epoch178 train time: 4.850s test time: 0.556  loss = 0.772 val_mse = 1.181 mse = 1.284 mae = 0.828\n",
            "epoch179 train time: 4.861s test time: 0.552  loss = 0.769 val_mse = 1.192 mse = 1.295 mae = 0.832\n",
            "MAE 0.8393858781233233\n",
            "MSE 1.3068397265143166\n",
            "epoch0 train time: 5.323s test time: 0.549  loss = 41.972 val_mse = 1.608 mse = 1.533 mae = 0.893\n",
            "epoch1 train time: 4.933s test time: 0.556  loss = 31.769 val_mse = 1.577 mse = 1.516 mae = 0.887\n",
            "epoch2 train time: 4.931s test time: 0.557  loss = 17.093 val_mse = 1.543 mse = 1.493 mae = 0.878\n",
            "epoch3 train time: 4.936s test time: 0.550  loss = 8.500 val_mse = 1.513 mse = 1.472 mae = 0.869\n",
            "epoch4 train time: 4.938s test time: 0.561  loss = 4.121 val_mse = 1.483 mse = 1.451 mae = 0.862\n",
            "epoch5 train time: 4.926s test time: 0.553  loss = 2.219 val_mse = 1.454 mse = 1.432 mae = 0.856\n",
            "epoch6 train time: 4.907s test time: 0.551  loss = 1.574 val_mse = 1.426 mse = 1.413 mae = 0.852\n",
            "epoch7 train time: 4.925s test time: 0.566  loss = 1.407 val_mse = 1.397 mse = 1.394 mae = 0.849\n",
            "epoch8 train time: 4.906s test time: 0.552  loss = 1.330 val_mse = 1.371 mse = 1.378 mae = 0.847\n",
            "epoch9 train time: 4.900s test time: 0.554  loss = 1.264 val_mse = 1.349 mse = 1.366 mae = 0.846\n",
            "epoch10 train time: 4.889s test time: 0.560  loss = 1.205 val_mse = 1.321 mse = 1.348 mae = 0.845\n",
            "epoch11 train time: 4.895s test time: 0.563  loss = 1.154 val_mse = 1.305 mse = 1.339 mae = 0.843\n",
            "epoch12 train time: 4.887s test time: 0.564  loss = 1.109 val_mse = 1.291 mse = 1.332 mae = 0.841\n",
            "epoch13 train time: 4.877s test time: 0.551  loss = 1.071 val_mse = 1.277 mse = 1.324 mae = 0.840\n",
            "epoch14 train time: 4.917s test time: 0.554  loss = 1.037 val_mse = 1.265 mse = 1.319 mae = 0.839\n",
            "epoch15 train time: 4.896s test time: 0.549  loss = 1.008 val_mse = 1.255 mse = 1.314 mae = 0.839\n",
            "epoch16 train time: 4.902s test time: 0.555  loss = 0.983 val_mse = 1.241 mse = 1.308 mae = 0.840\n",
            "epoch17 train time: 4.903s test time: 0.549  loss = 0.965 val_mse = 1.245 mse = 1.311 mae = 0.839\n",
            "epoch18 train time: 4.897s test time: 0.552  loss = 0.946 val_mse = 1.254 mse = 1.321 mae = 0.837\n",
            "epoch19 train time: 4.909s test time: 0.554  loss = 0.938 val_mse = 1.234 mse = 1.305 mae = 0.847\n",
            "epoch20 train time: 4.910s test time: 0.551  loss = 0.925 val_mse = 1.224 mse = 1.306 mae = 0.849\n",
            "epoch21 train time: 4.907s test time: 0.563  loss = 0.921 val_mse = 1.217 mse = 1.300 mae = 0.855\n",
            "epoch22 train time: 4.888s test time: 0.548  loss = 0.915 val_mse = 1.217 mse = 1.302 mae = 0.854\n",
            "epoch23 train time: 4.905s test time: 0.568  loss = 0.910 val_mse = 1.231 mse = 1.300 mae = 0.846\n",
            "epoch24 train time: 4.893s test time: 0.550  loss = 0.900 val_mse = 1.243 mse = 1.326 mae = 0.843\n",
            "epoch25 train time: 4.895s test time: 0.554  loss = 0.893 val_mse = 1.209 mse = 1.296 mae = 0.847\n",
            "epoch26 train time: 4.888s test time: 0.558  loss = 0.895 val_mse = 1.238 mse = 1.313 mae = 0.839\n",
            "epoch27 train time: 4.894s test time: 0.547  loss = 0.884 val_mse = 1.219 mse = 1.301 mae = 0.849\n",
            "epoch28 train time: 4.889s test time: 0.551  loss = 0.880 val_mse = 1.204 mse = 1.300 mae = 0.851\n",
            "epoch29 train time: 4.905s test time: 0.557  loss = 0.881 val_mse = 1.222 mse = 1.311 mae = 0.853\n",
            "epoch30 train time: 4.894s test time: 0.556  loss = 0.875 val_mse = 1.215 mse = 1.301 mae = 0.856\n",
            "epoch31 train time: 4.899s test time: 0.551  loss = 0.870 val_mse = 1.209 mse = 1.298 mae = 0.852\n",
            "epoch32 train time: 4.896s test time: 0.548  loss = 0.870 val_mse = 1.230 mse = 1.321 mae = 0.840\n",
            "epoch33 train time: 4.889s test time: 0.552  loss = 0.870 val_mse = 1.216 mse = 1.302 mae = 0.848\n",
            "epoch34 train time: 4.906s test time: 0.552  loss = 0.864 val_mse = 1.212 mse = 1.313 mae = 0.860\n",
            "epoch35 train time: 4.905s test time: 0.554  loss = 0.866 val_mse = 1.239 mse = 1.310 mae = 0.839\n",
            "epoch36 train time: 4.907s test time: 0.554  loss = 0.860 val_mse = 1.243 mse = 1.325 mae = 0.835\n",
            "epoch37 train time: 4.892s test time: 0.549  loss = 0.855 val_mse = 1.234 mse = 1.310 mae = 0.837\n",
            "epoch38 train time: 4.901s test time: 0.566  loss = 0.856 val_mse = 1.244 mse = 1.331 mae = 0.840\n",
            "epoch39 train time: 4.889s test time: 0.550  loss = 0.854 val_mse = 1.231 mse = 1.308 mae = 0.835\n",
            "epoch40 train time: 4.898s test time: 0.551  loss = 0.851 val_mse = 1.248 mse = 1.331 mae = 0.838\n",
            "epoch41 train time: 4.919s test time: 0.557  loss = 0.855 val_mse = 1.243 mse = 1.315 mae = 0.837\n",
            "epoch42 train time: 4.910s test time: 0.550  loss = 0.850 val_mse = 1.243 mse = 1.330 mae = 0.834\n",
            "epoch43 train time: 4.912s test time: 0.552  loss = 0.843 val_mse = 1.242 mse = 1.325 mae = 0.840\n",
            "epoch44 train time: 4.914s test time: 0.550  loss = 0.846 val_mse = 1.244 mse = 1.325 mae = 0.836\n",
            "epoch45 train time: 4.909s test time: 0.551  loss = 0.849 val_mse = 1.243 mse = 1.318 mae = 0.836\n",
            "epoch46 train time: 4.920s test time: 0.553  loss = 0.842 val_mse = 1.247 mse = 1.327 mae = 0.836\n",
            "epoch47 train time: 4.901s test time: 0.556  loss = 0.842 val_mse = 1.238 mse = 1.317 mae = 0.837\n",
            "epoch48 train time: 4.913s test time: 0.558  loss = 0.839 val_mse = 1.234 mse = 1.325 mae = 0.835\n",
            "epoch49 train time: 4.905s test time: 0.551  loss = 0.836 val_mse = 1.247 mse = 1.326 mae = 0.835\n",
            "epoch50 train time: 4.918s test time: 0.559  loss = 0.837 val_mse = 1.241 mse = 1.332 mae = 0.838\n",
            "epoch51 train time: 4.923s test time: 0.547  loss = 0.834 val_mse = 1.246 mse = 1.319 mae = 0.837\n",
            "epoch52 train time: 4.926s test time: 0.554  loss = 0.835 val_mse = 1.235 mse = 1.320 mae = 0.836\n",
            "epoch53 train time: 4.903s test time: 0.557  loss = 0.832 val_mse = 1.232 mse = 1.310 mae = 0.833\n",
            "epoch54 train time: 4.895s test time: 0.551  loss = 0.831 val_mse = 1.239 mse = 1.326 mae = 0.836\n",
            "epoch55 train time: 4.905s test time: 0.560  loss = 0.829 val_mse = 1.248 mse = 1.326 mae = 0.839\n",
            "epoch56 train time: 4.915s test time: 0.548  loss = 0.831 val_mse = 1.242 mse = 1.328 mae = 0.832\n",
            "epoch57 train time: 4.906s test time: 0.553  loss = 0.827 val_mse = 1.235 mse = 1.314 mae = 0.834\n",
            "epoch58 train time: 4.904s test time: 0.556  loss = 0.822 val_mse = 1.226 mse = 1.321 mae = 0.836\n",
            "epoch59 train time: 4.915s test time: 0.549  loss = 0.822 val_mse = 1.241 mse = 1.322 mae = 0.837\n",
            "epoch60 train time: 4.897s test time: 0.556  loss = 0.823 val_mse = 1.223 mse = 1.315 mae = 0.832\n",
            "epoch61 train time: 4.893s test time: 0.563  loss = 0.817 val_mse = 1.225 mse = 1.311 mae = 0.835\n",
            "epoch62 train time: 4.901s test time: 0.554  loss = 0.817 val_mse = 1.231 mse = 1.313 mae = 0.830\n",
            "epoch63 train time: 4.904s test time: 0.561  loss = 0.819 val_mse = 1.241 mse = 1.315 mae = 0.834\n",
            "epoch64 train time: 4.923s test time: 0.551  loss = 0.816 val_mse = 1.221 mse = 1.313 mae = 0.835\n",
            "epoch65 train time: 4.897s test time: 0.557  loss = 0.816 val_mse = 1.232 mse = 1.310 mae = 0.832\n",
            "epoch66 train time: 4.912s test time: 0.548  loss = 0.812 val_mse = 1.227 mse = 1.313 mae = 0.834\n",
            "epoch67 train time: 4.899s test time: 0.552  loss = 0.811 val_mse = 1.229 mse = 1.312 mae = 0.835\n",
            "epoch68 train time: 4.907s test time: 0.548  loss = 0.809 val_mse = 1.221 mse = 1.317 mae = 0.834\n",
            "epoch69 train time: 4.900s test time: 0.552  loss = 0.813 val_mse = 1.236 mse = 1.306 mae = 0.833\n",
            "epoch70 train time: 4.906s test time: 0.561  loss = 0.811 val_mse = 1.220 mse = 1.309 mae = 0.832\n",
            "epoch71 train time: 4.908s test time: 0.553  loss = 0.808 val_mse = 1.230 mse = 1.311 mae = 0.834\n",
            "epoch72 train time: 4.910s test time: 0.553  loss = 0.807 val_mse = 1.210 mse = 1.307 mae = 0.834\n",
            "epoch73 train time: 4.913s test time: 0.549  loss = 0.807 val_mse = 1.220 mse = 1.299 mae = 0.836\n",
            "epoch74 train time: 4.905s test time: 0.566  loss = 0.805 val_mse = 1.217 mse = 1.315 mae = 0.832\n",
            "epoch75 train time: 4.919s test time: 0.565  loss = 0.806 val_mse = 1.228 mse = 1.313 mae = 0.835\n",
            "epoch76 train time: 4.914s test time: 0.554  loss = 0.807 val_mse = 1.208 mse = 1.304 mae = 0.832\n",
            "epoch77 train time: 4.914s test time: 0.560  loss = 0.802 val_mse = 1.219 mse = 1.302 mae = 0.838\n",
            "epoch78 train time: 4.898s test time: 0.552  loss = 0.812 val_mse = 1.207 mse = 1.301 mae = 0.833\n",
            "epoch79 train time: 4.904s test time: 0.549  loss = 0.805 val_mse = 1.223 mse = 1.301 mae = 0.834\n",
            "epoch80 train time: 4.896s test time: 0.553  loss = 0.807 val_mse = 1.216 mse = 1.303 mae = 0.833\n",
            "epoch81 train time: 4.895s test time: 0.547  loss = 0.804 val_mse = 1.223 mse = 1.305 mae = 0.832\n",
            "epoch82 train time: 4.899s test time: 0.570  loss = 0.803 val_mse = 1.209 mse = 1.304 mae = 0.833\n",
            "epoch83 train time: 4.904s test time: 0.561  loss = 0.800 val_mse = 1.217 mse = 1.303 mae = 0.832\n",
            "epoch84 train time: 4.906s test time: 0.561  loss = 0.800 val_mse = 1.220 mse = 1.305 mae = 0.830\n",
            "epoch85 train time: 4.905s test time: 0.550  loss = 0.806 val_mse = 1.217 mse = 1.312 mae = 0.838\n",
            "epoch86 train time: 4.908s test time: 0.551  loss = 0.803 val_mse = 1.205 mse = 1.309 mae = 0.833\n",
            "epoch87 train time: 4.915s test time: 0.556  loss = 0.798 val_mse = 1.209 mse = 1.297 mae = 0.832\n",
            "epoch88 train time: 4.903s test time: 0.544  loss = 0.799 val_mse = 1.213 mse = 1.299 mae = 0.831\n",
            "epoch89 train time: 4.917s test time: 0.562  loss = 0.800 val_mse = 1.212 mse = 1.299 mae = 0.832\n",
            "epoch90 train time: 4.900s test time: 0.552  loss = 0.799 val_mse = 1.216 mse = 1.311 mae = 0.833\n",
            "epoch91 train time: 4.906s test time: 0.564  loss = 0.800 val_mse = 1.214 mse = 1.306 mae = 0.833\n",
            "epoch92 train time: 4.937s test time: 0.556  loss = 0.800 val_mse = 1.208 mse = 1.308 mae = 0.831\n",
            "epoch93 train time: 4.915s test time: 0.566  loss = 0.795 val_mse = 1.210 mse = 1.298 mae = 0.833\n",
            "epoch94 train time: 4.907s test time: 0.565  loss = 0.795 val_mse = 1.212 mse = 1.305 mae = 0.830\n",
            "epoch95 train time: 4.900s test time: 0.554  loss = 0.795 val_mse = 1.213 mse = 1.304 mae = 0.831\n",
            "epoch96 train time: 4.900s test time: 0.557  loss = 0.796 val_mse = 1.206 mse = 1.301 mae = 0.833\n",
            "epoch97 train time: 4.894s test time: 0.550  loss = 0.793 val_mse = 1.210 mse = 1.299 mae = 0.835\n",
            "epoch98 train time: 4.895s test time: 0.547  loss = 0.794 val_mse = 1.204 mse = 1.309 mae = 0.831\n",
            "epoch99 train time: 4.895s test time: 0.556  loss = 0.789 val_mse = 1.207 mse = 1.307 mae = 0.836\n",
            "epoch100 train time: 4.895s test time: 0.558  loss = 0.792 val_mse = 1.200 mse = 1.301 mae = 0.832\n",
            "epoch101 train time: 4.900s test time: 0.553  loss = 0.790 val_mse = 1.208 mse = 1.294 mae = 0.834\n",
            "epoch102 train time: 4.912s test time: 0.554  loss = 0.793 val_mse = 1.201 mse = 1.300 mae = 0.832\n",
            "epoch103 train time: 4.912s test time: 0.556  loss = 0.791 val_mse = 1.208 mse = 1.300 mae = 0.832\n",
            "epoch104 train time: 4.903s test time: 0.555  loss = 0.792 val_mse = 1.205 mse = 1.300 mae = 0.828\n",
            "epoch105 train time: 4.891s test time: 0.551  loss = 0.788 val_mse = 1.207 mse = 1.296 mae = 0.833\n",
            "epoch106 train time: 4.900s test time: 0.561  loss = 0.790 val_mse = 1.202 mse = 1.304 mae = 0.831\n",
            "epoch107 train time: 4.896s test time: 0.550  loss = 0.787 val_mse = 1.210 mse = 1.299 mae = 0.833\n",
            "epoch108 train time: 4.891s test time: 0.563  loss = 0.790 val_mse = 1.203 mse = 1.296 mae = 0.829\n",
            "epoch109 train time: 4.890s test time: 0.550  loss = 0.785 val_mse = 1.199 mse = 1.297 mae = 0.834\n",
            "epoch110 train time: 4.899s test time: 0.557  loss = 0.786 val_mse = 1.206 mse = 1.303 mae = 0.832\n",
            "epoch111 train time: 4.893s test time: 0.560  loss = 0.792 val_mse = 1.209 mse = 1.297 mae = 0.832\n",
            "epoch112 train time: 4.912s test time: 0.550  loss = 0.786 val_mse = 1.204 mse = 1.304 mae = 0.829\n",
            "epoch113 train time: 4.905s test time: 0.552  loss = 0.783 val_mse = 1.201 mse = 1.292 mae = 0.832\n",
            "epoch114 train time: 4.911s test time: 0.553  loss = 0.786 val_mse = 1.198 mse = 1.298 mae = 0.830\n",
            "epoch115 train time: 4.905s test time: 0.559  loss = 0.787 val_mse = 1.212 mse = 1.302 mae = 0.831\n",
            "epoch116 train time: 4.901s test time: 0.550  loss = 0.786 val_mse = 1.204 mse = 1.298 mae = 0.831\n",
            "epoch117 train time: 4.894s test time: 0.552  loss = 0.781 val_mse = 1.200 mse = 1.300 mae = 0.831\n",
            "epoch118 train time: 4.887s test time: 0.561  loss = 0.781 val_mse = 1.208 mse = 1.307 mae = 0.828\n",
            "epoch119 train time: 4.896s test time: 0.563  loss = 0.782 val_mse = 1.203 mse = 1.296 mae = 0.831\n",
            "epoch120 train time: 4.910s test time: 0.552  loss = 0.783 val_mse = 1.200 mse = 1.298 mae = 0.829\n",
            "epoch121 train time: 4.911s test time: 0.563  loss = 0.783 val_mse = 1.211 mse = 1.301 mae = 0.829\n",
            "epoch122 train time: 4.901s test time: 0.557  loss = 0.780 val_mse = 1.189 mse = 1.297 mae = 0.833\n",
            "epoch123 train time: 4.901s test time: 0.557  loss = 0.781 val_mse = 1.205 mse = 1.300 mae = 0.828\n",
            "epoch124 train time: 4.897s test time: 0.549  loss = 0.781 val_mse = 1.197 mse = 1.301 mae = 0.829\n",
            "epoch125 train time: 4.916s test time: 0.559  loss = 0.778 val_mse = 1.193 mse = 1.289 mae = 0.833\n",
            "epoch126 train time: 4.886s test time: 0.555  loss = 0.780 val_mse = 1.191 mse = 1.299 mae = 0.831\n",
            "epoch127 train time: 4.894s test time: 0.552  loss = 0.781 val_mse = 1.198 mse = 1.291 mae = 0.830\n",
            "epoch128 train time: 4.889s test time: 0.569  loss = 0.780 val_mse = 1.187 mse = 1.296 mae = 0.832\n",
            "epoch129 train time: 4.900s test time: 0.550  loss = 0.777 val_mse = 1.199 mse = 1.289 mae = 0.830\n",
            "epoch130 train time: 4.922s test time: 0.557  loss = 0.783 val_mse = 1.181 mse = 1.288 mae = 0.831\n",
            "epoch131 train time: 4.907s test time: 0.552  loss = 0.778 val_mse = 1.193 mse = 1.297 mae = 0.835\n",
            "epoch132 train time: 4.919s test time: 0.547  loss = 0.781 val_mse = 1.184 mse = 1.294 mae = 0.835\n",
            "epoch133 train time: 4.890s test time: 0.553  loss = 0.781 val_mse = 1.189 mse = 1.294 mae = 0.834\n",
            "epoch134 train time: 4.898s test time: 0.552  loss = 0.782 val_mse = 1.181 mse = 1.292 mae = 0.834\n",
            "epoch135 train time: 4.911s test time: 0.550  loss = 0.776 val_mse = 1.189 mse = 1.291 mae = 0.834\n",
            "epoch136 train time: 4.903s test time: 0.554  loss = 0.781 val_mse = 1.187 mse = 1.293 mae = 0.836\n",
            "epoch137 train time: 4.901s test time: 0.558  loss = 0.781 val_mse = 1.193 mse = 1.292 mae = 0.830\n",
            "epoch138 train time: 4.895s test time: 0.556  loss = 0.781 val_mse = 1.186 mse = 1.294 mae = 0.836\n",
            "epoch139 train time: 4.896s test time: 0.558  loss = 0.779 val_mse = 1.195 mse = 1.294 mae = 0.833\n",
            "epoch140 train time: 4.925s test time: 0.559  loss = 0.782 val_mse = 1.182 mse = 1.288 mae = 0.828\n",
            "epoch141 train time: 4.907s test time: 0.547  loss = 0.776 val_mse = 1.190 mse = 1.292 mae = 0.833\n",
            "epoch142 train time: 4.907s test time: 0.550  loss = 0.781 val_mse = 1.184 mse = 1.297 mae = 0.832\n",
            "epoch143 train time: 4.890s test time: 0.554  loss = 0.778 val_mse = 1.192 mse = 1.291 mae = 0.829\n",
            "epoch144 train time: 4.893s test time: 0.548  loss = 0.779 val_mse = 1.187 mse = 1.293 mae = 0.835\n",
            "epoch145 train time: 4.895s test time: 0.549  loss = 0.778 val_mse = 1.193 mse = 1.289 mae = 0.826\n",
            "epoch146 train time: 4.902s test time: 0.550  loss = 0.781 val_mse = 1.190 mse = 1.293 mae = 0.832\n",
            "epoch147 train time: 4.932s test time: 0.552  loss = 0.777 val_mse = 1.191 mse = 1.293 mae = 0.832\n",
            "epoch148 train time: 4.895s test time: 0.550  loss = 0.779 val_mse = 1.179 mse = 1.284 mae = 0.831\n",
            "epoch149 train time: 4.904s test time: 0.555  loss = 0.776 val_mse = 1.186 mse = 1.287 mae = 0.830\n",
            "epoch150 train time: 4.897s test time: 0.557  loss = 0.777 val_mse = 1.185 mse = 1.296 mae = 0.835\n",
            "epoch151 train time: 4.902s test time: 0.551  loss = 0.778 val_mse = 1.187 mse = 1.286 mae = 0.830\n",
            "epoch152 train time: 4.897s test time: 0.553  loss = 0.779 val_mse = 1.182 mse = 1.296 mae = 0.832\n",
            "epoch153 train time: 4.897s test time: 0.551  loss = 0.777 val_mse = 1.183 mse = 1.288 mae = 0.832\n",
            "epoch154 train time: 4.908s test time: 0.550  loss = 0.780 val_mse = 1.180 mse = 1.291 mae = 0.834\n",
            "epoch155 train time: 4.893s test time: 0.555  loss = 0.775 val_mse = 1.187 mse = 1.289 mae = 0.833\n",
            "epoch156 train time: 4.896s test time: 0.563  loss = 0.781 val_mse = 1.187 mse = 1.293 mae = 0.831\n",
            "epoch157 train time: 4.899s test time: 0.571  loss = 0.778 val_mse = 1.192 mse = 1.294 mae = 0.832\n",
            "epoch158 train time: 4.898s test time: 0.554  loss = 0.781 val_mse = 1.182 mse = 1.293 mae = 0.831\n",
            "epoch159 train time: 4.902s test time: 0.549  loss = 0.778 val_mse = 1.186 mse = 1.286 mae = 0.831\n",
            "epoch160 train time: 4.915s test time: 0.555  loss = 0.779 val_mse = 1.187 mse = 1.289 mae = 0.831\n",
            "epoch161 train time: 4.922s test time: 0.550  loss = 0.776 val_mse = 1.185 mse = 1.289 mae = 0.833\n",
            "epoch162 train time: 4.882s test time: 0.557  loss = 0.781 val_mse = 1.193 mse = 1.297 mae = 0.831\n",
            "epoch163 train time: 4.891s test time: 0.554  loss = 0.779 val_mse = 1.190 mse = 1.286 mae = 0.828\n",
            "epoch164 train time: 4.892s test time: 0.554  loss = 0.779 val_mse = 1.180 mse = 1.287 mae = 0.832\n",
            "epoch165 train time: 4.899s test time: 0.552  loss = 0.775 val_mse = 1.191 mse = 1.295 mae = 0.830\n",
            "epoch166 train time: 4.892s test time: 0.551  loss = 0.779 val_mse = 1.185 mse = 1.290 mae = 0.832\n",
            "epoch167 train time: 4.910s test time: 0.551  loss = 0.779 val_mse = 1.191 mse = 1.288 mae = 0.831\n",
            "epoch168 train time: 4.902s test time: 0.552  loss = 0.776 val_mse = 1.183 mse = 1.289 mae = 0.834\n",
            "epoch169 train time: 4.891s test time: 0.557  loss = 0.778 val_mse = 1.182 mse = 1.282 mae = 0.831\n",
            "epoch170 train time: 4.882s test time: 0.545  loss = 0.779 val_mse = 1.183 mse = 1.287 mae = 0.832\n",
            "epoch171 train time: 4.895s test time: 0.552  loss = 0.776 val_mse = 1.198 mse = 1.291 mae = 0.832\n",
            "epoch172 train time: 4.879s test time: 0.549  loss = 0.781 val_mse = 1.183 mse = 1.291 mae = 0.830\n",
            "epoch173 train time: 4.908s test time: 0.555  loss = 0.777 val_mse = 1.191 mse = 1.289 mae = 0.830\n",
            "epoch174 train time: 4.898s test time: 0.551  loss = 0.776 val_mse = 1.187 mse = 1.291 mae = 0.831\n",
            "epoch175 train time: 4.890s test time: 0.547  loss = 0.774 val_mse = 1.194 mse = 1.287 mae = 0.829\n",
            "epoch176 train time: 4.914s test time: 0.558  loss = 0.776 val_mse = 1.182 mse = 1.290 mae = 0.829\n",
            "epoch177 train time: 4.909s test time: 0.556  loss = 0.775 val_mse = 1.198 mse = 1.292 mae = 0.832\n",
            "epoch178 train time: 4.915s test time: 0.553  loss = 0.778 val_mse = 1.184 mse = 1.296 mae = 0.832\n",
            "epoch179 train time: 4.918s test time: 0.545  loss = 0.775 val_mse = 1.188 mse = 1.288 mae = 0.830\n",
            "MAE 0.8363583598807807\n",
            "MSE 1.3114134781678832\n",
            "epoch0 train time: 5.296s test time: 0.553  loss = 43.093 val_mse = 1.629 mse = 1.566 mae = 0.908\n",
            "epoch1 train time: 4.854s test time: 0.543  loss = 32.113 val_mse = 1.593 mse = 1.540 mae = 0.898\n",
            "epoch2 train time: 4.867s test time: 0.557  loss = 17.340 val_mse = 1.559 mse = 1.514 mae = 0.886\n",
            "epoch3 train time: 4.914s test time: 0.567  loss = 8.684 val_mse = 1.530 mse = 1.493 mae = 0.875\n",
            "epoch4 train time: 4.886s test time: 0.559  loss = 4.278 val_mse = 1.502 mse = 1.473 mae = 0.866\n",
            "epoch5 train time: 4.892s test time: 0.556  loss = 2.370 val_mse = 1.474 mse = 1.453 mae = 0.859\n",
            "epoch6 train time: 4.884s test time: 0.565  loss = 1.723 val_mse = 1.445 mse = 1.433 mae = 0.853\n",
            "epoch7 train time: 4.866s test time: 0.549  loss = 1.550 val_mse = 1.422 mse = 1.418 mae = 0.850\n",
            "epoch8 train time: 4.846s test time: 0.551  loss = 1.463 val_mse = 1.393 mse = 1.398 mae = 0.846\n",
            "epoch9 train time: 4.846s test time: 0.555  loss = 1.390 val_mse = 1.372 mse = 1.385 mae = 0.843\n",
            "epoch10 train time: 4.856s test time: 0.548  loss = 1.325 val_mse = 1.358 mse = 1.377 mae = 0.842\n",
            "epoch11 train time: 4.840s test time: 0.550  loss = 1.268 val_mse = 1.347 mse = 1.374 mae = 0.839\n",
            "epoch12 train time: 4.858s test time: 0.556  loss = 1.217 val_mse = 1.330 mse = 1.363 mae = 0.838\n",
            "epoch13 train time: 4.848s test time: 0.552  loss = 1.170 val_mse = 1.321 mse = 1.359 mae = 0.837\n",
            "epoch14 train time: 4.848s test time: 0.548  loss = 1.127 val_mse = 1.300 mse = 1.345 mae = 0.838\n",
            "epoch15 train time: 4.842s test time: 0.555  loss = 1.088 val_mse = 1.307 mse = 1.354 mae = 0.835\n",
            "epoch16 train time: 4.858s test time: 0.550  loss = 1.057 val_mse = 1.253 mse = 1.317 mae = 0.843\n",
            "epoch17 train time: 4.866s test time: 0.558  loss = 1.029 val_mse = 1.245 mse = 1.309 mae = 0.845\n",
            "epoch18 train time: 4.862s test time: 0.543  loss = 1.005 val_mse = 1.276 mse = 1.341 mae = 0.838\n",
            "epoch19 train time: 4.871s test time: 0.559  loss = 0.988 val_mse = 1.245 mse = 1.312 mae = 0.838\n",
            "epoch20 train time: 4.854s test time: 0.553  loss = 0.974 val_mse = 1.220 mse = 1.298 mae = 0.849\n",
            "epoch21 train time: 4.855s test time: 0.547  loss = 0.973 val_mse = 1.231 mse = 1.309 mae = 0.846\n",
            "epoch22 train time: 4.854s test time: 0.570  loss = 0.951 val_mse = 1.230 mse = 1.299 mae = 0.842\n",
            "epoch23 train time: 4.873s test time: 0.558  loss = 0.946 val_mse = 1.238 mse = 1.319 mae = 0.840\n",
            "epoch24 train time: 4.874s test time: 0.552  loss = 0.933 val_mse = 1.218 mse = 1.298 mae = 0.850\n",
            "epoch25 train time: 4.851s test time: 0.558  loss = 0.930 val_mse = 1.212 mse = 1.304 mae = 0.860\n",
            "epoch26 train time: 4.850s test time: 0.552  loss = 0.921 val_mse = 1.228 mse = 1.304 mae = 0.846\n",
            "epoch27 train time: 4.855s test time: 0.562  loss = 0.913 val_mse = 1.233 mse = 1.313 mae = 0.850\n",
            "epoch28 train time: 4.858s test time: 0.558  loss = 0.905 val_mse = 1.231 mse = 1.317 mae = 0.844\n",
            "epoch29 train time: 4.840s test time: 0.554  loss = 0.904 val_mse = 1.216 mse = 1.312 mae = 0.849\n",
            "epoch30 train time: 4.856s test time: 0.562  loss = 0.901 val_mse = 1.223 mse = 1.311 mae = 0.846\n",
            "epoch31 train time: 4.847s test time: 0.555  loss = 0.893 val_mse = 1.218 mse = 1.296 mae = 0.843\n",
            "epoch32 train time: 4.863s test time: 0.562  loss = 0.887 val_mse = 1.226 mse = 1.297 mae = 0.839\n",
            "epoch33 train time: 4.842s test time: 0.550  loss = 0.883 val_mse = 1.212 mse = 1.312 mae = 0.846\n",
            "epoch34 train time: 4.847s test time: 0.545  loss = 0.880 val_mse = 1.227 mse = 1.313 mae = 0.843\n",
            "epoch35 train time: 4.855s test time: 0.552  loss = 0.886 val_mse = 1.212 mse = 1.307 mae = 0.850\n",
            "epoch36 train time: 4.847s test time: 0.548  loss = 0.877 val_mse = 1.220 mse = 1.304 mae = 0.840\n",
            "epoch37 train time: 4.854s test time: 0.557  loss = 0.881 val_mse = 1.215 mse = 1.314 mae = 0.856\n",
            "epoch38 train time: 4.860s test time: 0.554  loss = 0.869 val_mse = 1.208 mse = 1.300 mae = 0.859\n",
            "epoch39 train time: 4.867s test time: 0.566  loss = 0.872 val_mse = 1.246 mse = 1.324 mae = 0.839\n",
            "epoch40 train time: 4.859s test time: 0.550  loss = 0.861 val_mse = 1.202 mse = 1.296 mae = 0.840\n",
            "epoch41 train time: 4.856s test time: 0.551  loss = 0.865 val_mse = 1.214 mse = 1.310 mae = 0.856\n",
            "epoch42 train time: 4.856s test time: 0.548  loss = 0.858 val_mse = 1.200 mse = 1.299 mae = 0.850\n",
            "epoch43 train time: 4.852s test time: 0.558  loss = 0.857 val_mse = 1.210 mse = 1.313 mae = 0.857\n",
            "epoch44 train time: 4.857s test time: 0.549  loss = 0.858 val_mse = 1.206 mse = 1.295 mae = 0.846\n",
            "epoch45 train time: 4.861s test time: 0.558  loss = 0.856 val_mse = 1.215 mse = 1.309 mae = 0.856\n",
            "epoch46 train time: 4.834s test time: 0.550  loss = 0.854 val_mse = 1.207 mse = 1.286 mae = 0.843\n",
            "epoch47 train time: 4.874s test time: 0.549  loss = 0.845 val_mse = 1.207 mse = 1.300 mae = 0.855\n",
            "epoch48 train time: 4.868s test time: 0.552  loss = 0.846 val_mse = 1.201 mse = 1.298 mae = 0.851\n",
            "epoch49 train time: 4.859s test time: 0.550  loss = 0.846 val_mse = 1.200 mse = 1.305 mae = 0.853\n",
            "epoch50 train time: 4.850s test time: 0.559  loss = 0.841 val_mse = 1.211 mse = 1.299 mae = 0.847\n",
            "epoch51 train time: 4.864s test time: 0.547  loss = 0.840 val_mse = 1.207 mse = 1.298 mae = 0.852\n",
            "epoch52 train time: 4.869s test time: 0.552  loss = 0.841 val_mse = 1.209 mse = 1.298 mae = 0.840\n",
            "epoch53 train time: 4.861s test time: 0.545  loss = 0.835 val_mse = 1.219 mse = 1.314 mae = 0.856\n",
            "epoch54 train time: 4.851s test time: 0.551  loss = 0.846 val_mse = 1.224 mse = 1.309 mae = 0.850\n",
            "epoch55 train time: 4.843s test time: 0.555  loss = 0.850 val_mse = 1.209 mse = 1.294 mae = 0.851\n",
            "epoch56 train time: 4.864s test time: 0.549  loss = 0.836 val_mse = 1.208 mse = 1.302 mae = 0.843\n",
            "epoch57 train time: 4.858s test time: 0.552  loss = 0.831 val_mse = 1.213 mse = 1.303 mae = 0.853\n",
            "epoch58 train time: 4.866s test time: 0.563  loss = 0.838 val_mse = 1.210 mse = 1.299 mae = 0.846\n",
            "epoch59 train time: 4.859s test time: 0.548  loss = 0.834 val_mse = 1.214 mse = 1.298 mae = 0.846\n",
            "epoch60 train time: 4.862s test time: 0.546  loss = 0.832 val_mse = 1.198 mse = 1.295 mae = 0.843\n",
            "epoch61 train time: 4.865s test time: 0.551  loss = 0.826 val_mse = 1.205 mse = 1.301 mae = 0.850\n",
            "epoch62 train time: 4.853s test time: 0.544  loss = 0.829 val_mse = 1.235 mse = 1.324 mae = 0.835\n",
            "epoch63 train time: 5.279s test time: 0.703  loss = 0.825 val_mse = 1.219 mse = 1.305 mae = 0.840\n",
            "epoch64 train time: 5.745s test time: 0.768  loss = 0.825 val_mse = 1.194 mse = 1.294 mae = 0.846\n",
            "epoch65 train time: 5.470s test time: 0.680  loss = 0.817 val_mse = 1.206 mse = 1.298 mae = 0.841\n",
            "epoch66 train time: 5.449s test time: 0.655  loss = 0.822 val_mse = 1.228 mse = 1.321 mae = 0.839\n",
            "epoch67 train time: 5.394s test time: 0.671  loss = 0.822 val_mse = 1.201 mse = 1.303 mae = 0.843\n",
            "epoch68 train time: 5.457s test time: 0.717  loss = 0.821 val_mse = 1.228 mse = 1.322 mae = 0.837\n",
            "epoch69 train time: 5.555s test time: 0.661  loss = 0.816 val_mse = 1.205 mse = 1.301 mae = 0.837\n",
            "epoch70 train time: 5.279s test time: 0.554  loss = 0.816 val_mse = 1.193 mse = 1.299 mae = 0.855\n",
            "epoch71 train time: 4.845s test time: 0.562  loss = 0.824 val_mse = 1.239 mse = 1.322 mae = 0.831\n",
            "epoch72 train time: 4.877s test time: 0.553  loss = 0.817 val_mse = 1.222 mse = 1.313 mae = 0.833\n",
            "epoch73 train time: 4.868s test time: 0.550  loss = 0.811 val_mse = 1.225 mse = 1.315 mae = 0.835\n",
            "epoch74 train time: 4.855s test time: 0.552  loss = 0.822 val_mse = 1.227 mse = 1.317 mae = 0.836\n",
            "epoch75 train time: 4.863s test time: 0.555  loss = 0.819 val_mse = 1.223 mse = 1.303 mae = 0.831\n",
            "epoch76 train time: 4.861s test time: 0.551  loss = 0.812 val_mse = 1.222 mse = 1.311 mae = 0.833\n",
            "epoch77 train time: 4.855s test time: 0.549  loss = 0.809 val_mse = 1.225 mse = 1.320 mae = 0.835\n",
            "epoch78 train time: 4.850s test time: 0.566  loss = 0.813 val_mse = 1.209 mse = 1.304 mae = 0.833\n",
            "epoch79 train time: 4.849s test time: 0.547  loss = 0.808 val_mse = 1.220 mse = 1.300 mae = 0.833\n",
            "epoch80 train time: 4.871s test time: 0.547  loss = 0.813 val_mse = 1.224 mse = 1.316 mae = 0.832\n",
            "epoch81 train time: 4.857s test time: 0.554  loss = 0.805 val_mse = 1.220 mse = 1.318 mae = 0.833\n",
            "epoch82 train time: 4.855s test time: 0.551  loss = 0.810 val_mse = 1.221 mse = 1.307 mae = 0.834\n",
            "epoch83 train time: 4.856s test time: 0.546  loss = 0.806 val_mse = 1.215 mse = 1.310 mae = 0.832\n",
            "epoch84 train time: 4.846s test time: 0.552  loss = 0.805 val_mse = 1.214 mse = 1.303 mae = 0.831\n",
            "epoch85 train time: 4.855s test time: 0.552  loss = 0.802 val_mse = 1.213 mse = 1.302 mae = 0.834\n",
            "epoch86 train time: 4.995s test time: 0.548  loss = 0.808 val_mse = 1.208 mse = 1.298 mae = 0.835\n",
            "epoch87 train time: 4.851s test time: 0.563  loss = 0.805 val_mse = 1.208 mse = 1.304 mae = 0.830\n",
            "epoch88 train time: 4.848s test time: 0.548  loss = 0.803 val_mse = 1.215 mse = 1.308 mae = 0.833\n",
            "epoch89 train time: 4.864s test time: 0.551  loss = 0.802 val_mse = 1.205 mse = 1.300 mae = 0.830\n",
            "epoch90 train time: 4.862s test time: 0.549  loss = 0.803 val_mse = 1.215 mse = 1.305 mae = 0.833\n",
            "epoch91 train time: 4.868s test time: 0.550  loss = 0.803 val_mse = 1.210 mse = 1.306 mae = 0.835\n",
            "epoch92 train time: 4.855s test time: 0.551  loss = 0.805 val_mse = 1.209 mse = 1.298 mae = 0.835\n",
            "epoch93 train time: 4.873s test time: 0.555  loss = 0.802 val_mse = 1.199 mse = 1.295 mae = 0.832\n",
            "epoch94 train time: 4.860s test time: 0.554  loss = 0.805 val_mse = 1.209 mse = 1.306 mae = 0.836\n",
            "epoch95 train time: 4.872s test time: 0.554  loss = 0.803 val_mse = 1.208 mse = 1.302 mae = 0.831\n",
            "epoch96 train time: 4.865s test time: 0.545  loss = 0.800 val_mse = 1.201 mse = 1.301 mae = 0.834\n",
            "epoch97 train time: 4.854s test time: 0.551  loss = 0.803 val_mse = 1.199 mse = 1.295 mae = 0.839\n",
            "epoch98 train time: 4.859s test time: 0.551  loss = 0.801 val_mse = 1.202 mse = 1.295 mae = 0.835\n",
            "epoch99 train time: 4.839s test time: 0.549  loss = 0.798 val_mse = 1.194 mse = 1.299 mae = 0.840\n",
            "epoch100 train time: 4.867s test time: 0.558  loss = 0.804 val_mse = 1.189 mse = 1.289 mae = 0.837\n",
            "epoch101 train time: 4.862s test time: 0.543  loss = 0.803 val_mse = 1.197 mse = 1.296 mae = 0.840\n",
            "epoch102 train time: 4.855s test time: 0.550  loss = 0.808 val_mse = 1.193 mse = 1.293 mae = 0.840\n",
            "epoch103 train time: 4.849s test time: 0.557  loss = 0.804 val_mse = 1.187 mse = 1.287 mae = 0.840\n",
            "epoch104 train time: 4.862s test time: 0.552  loss = 0.798 val_mse = 1.206 mse = 1.301 mae = 0.831\n",
            "epoch105 train time: 4.998s test time: 0.547  loss = 0.798 val_mse = 1.215 mse = 1.300 mae = 0.832\n",
            "epoch106 train time: 4.858s test time: 0.552  loss = 0.804 val_mse = 1.205 mse = 1.295 mae = 0.833\n",
            "epoch107 train time: 4.861s test time: 0.561  loss = 0.800 val_mse = 1.214 mse = 1.304 mae = 0.827\n",
            "epoch108 train time: 4.857s test time: 0.547  loss = 0.799 val_mse = 1.196 mse = 1.295 mae = 0.837\n",
            "epoch109 train time: 4.848s test time: 0.556  loss = 0.803 val_mse = 1.203 mse = 1.297 mae = 0.830\n",
            "epoch110 train time: 4.855s test time: 0.557  loss = 0.799 val_mse = 1.203 mse = 1.298 mae = 0.835\n",
            "epoch111 train time: 4.887s test time: 0.558  loss = 0.799 val_mse = 1.207 mse = 1.304 mae = 0.830\n",
            "epoch112 train time: 4.863s test time: 0.549  loss = 0.798 val_mse = 1.208 mse = 1.306 mae = 0.829\n",
            "epoch113 train time: 4.852s test time: 0.558  loss = 0.797 val_mse = 1.199 mse = 1.291 mae = 0.843\n",
            "epoch114 train time: 4.850s test time: 0.552  loss = 0.802 val_mse = 1.203 mse = 1.302 mae = 0.832\n",
            "epoch115 train time: 4.856s test time: 0.557  loss = 0.795 val_mse = 1.219 mse = 1.307 mae = 0.827\n",
            "epoch116 train time: 4.858s test time: 0.546  loss = 0.799 val_mse = 1.207 mse = 1.297 mae = 0.830\n",
            "epoch117 train time: 4.867s test time: 0.565  loss = 0.800 val_mse = 1.206 mse = 1.295 mae = 0.832\n",
            "epoch118 train time: 4.867s test time: 0.555  loss = 0.798 val_mse = 1.196 mse = 1.299 mae = 0.833\n",
            "epoch119 train time: 4.850s test time: 0.551  loss = 0.795 val_mse = 1.188 mse = 1.290 mae = 0.844\n",
            "epoch120 train time: 4.851s test time: 0.556  loss = 0.799 val_mse = 1.212 mse = 1.300 mae = 0.829\n",
            "epoch121 train time: 4.853s test time: 0.549  loss = 0.798 val_mse = 1.184 mse = 1.285 mae = 0.835\n",
            "epoch122 train time: 4.851s test time: 0.552  loss = 0.799 val_mse = 1.198 mse = 1.293 mae = 0.829\n",
            "epoch123 train time: 4.848s test time: 0.545  loss = 0.796 val_mse = 1.197 mse = 1.283 mae = 0.835\n",
            "epoch124 train time: 4.980s test time: 0.551  loss = 0.800 val_mse = 1.180 mse = 1.284 mae = 0.839\n",
            "epoch125 train time: 4.856s test time: 0.548  loss = 0.793 val_mse = 1.192 mse = 1.288 mae = 0.836\n",
            "epoch126 train time: 4.850s test time: 0.557  loss = 0.803 val_mse = 1.198 mse = 1.290 mae = 0.838\n",
            "epoch127 train time: 4.855s test time: 0.548  loss = 0.798 val_mse = 1.196 mse = 1.287 mae = 0.834\n",
            "epoch128 train time: 4.853s test time: 0.559  loss = 0.793 val_mse = 1.178 mse = 1.283 mae = 0.834\n",
            "epoch129 train time: 4.866s test time: 0.548  loss = 0.793 val_mse = 1.195 mse = 1.296 mae = 0.832\n",
            "epoch130 train time: 4.858s test time: 0.553  loss = 0.793 val_mse = 1.193 mse = 1.283 mae = 0.832\n",
            "epoch131 train time: 4.852s test time: 0.550  loss = 0.794 val_mse = 1.194 mse = 1.293 mae = 0.833\n",
            "epoch132 train time: 4.867s test time: 0.561  loss = 0.795 val_mse = 1.191 mse = 1.291 mae = 0.838\n",
            "epoch133 train time: 4.852s test time: 0.564  loss = 0.792 val_mse = 1.192 mse = 1.290 mae = 0.833\n",
            "epoch134 train time: 4.852s test time: 0.550  loss = 0.792 val_mse = 1.187 mse = 1.288 mae = 0.837\n",
            "epoch135 train time: 4.853s test time: 0.552  loss = 0.794 val_mse = 1.186 mse = 1.283 mae = 0.830\n",
            "epoch136 train time: 4.849s test time: 0.548  loss = 0.793 val_mse = 1.191 mse = 1.284 mae = 0.836\n",
            "epoch137 train time: 4.856s test time: 0.554  loss = 0.794 val_mse = 1.189 mse = 1.284 mae = 0.833\n",
            "epoch138 train time: 4.863s test time: 0.552  loss = 0.796 val_mse = 1.201 mse = 1.293 mae = 0.829\n",
            "epoch139 train time: 4.860s test time: 0.559  loss = 0.789 val_mse = 1.201 mse = 1.294 mae = 0.830\n",
            "epoch140 train time: 4.851s test time: 0.555  loss = 0.793 val_mse = 1.199 mse = 1.293 mae = 0.832\n",
            "epoch141 train time: 4.850s test time: 0.550  loss = 0.789 val_mse = 1.193 mse = 1.292 mae = 0.829\n",
            "epoch142 train time: 4.847s test time: 0.549  loss = 0.791 val_mse = 1.203 mse = 1.297 mae = 0.832\n",
            "epoch143 train time: 4.858s test time: 0.564  loss = 0.792 val_mse = 1.208 mse = 1.295 mae = 0.826\n",
            "epoch144 train time: 4.847s test time: 0.554  loss = 0.793 val_mse = 1.192 mse = 1.290 mae = 0.833\n",
            "epoch145 train time: 4.857s test time: 0.548  loss = 0.791 val_mse = 1.195 mse = 1.296 mae = 0.832\n",
            "epoch146 train time: 4.852s test time: 0.550  loss = 0.792 val_mse = 1.197 mse = 1.289 mae = 0.831\n",
            "epoch147 train time: 4.854s test time: 0.549  loss = 0.788 val_mse = 1.191 mse = 1.286 mae = 0.831\n",
            "epoch148 train time: 4.846s test time: 0.550  loss = 0.793 val_mse = 1.199 mse = 1.295 mae = 0.830\n",
            "epoch149 train time: 4.869s test time: 0.558  loss = 0.787 val_mse = 1.195 mse = 1.288 mae = 0.829\n",
            "epoch150 train time: 4.854s test time: 0.560  loss = 0.787 val_mse = 1.195 mse = 1.293 mae = 0.829\n",
            "epoch151 train time: 4.856s test time: 0.551  loss = 0.787 val_mse = 1.190 mse = 1.291 mae = 0.833\n",
            "epoch152 train time: 4.839s test time: 0.555  loss = 0.790 val_mse = 1.195 mse = 1.289 mae = 0.832\n",
            "epoch153 train time: 4.854s test time: 0.550  loss = 0.790 val_mse = 1.188 mse = 1.289 mae = 0.830\n",
            "epoch154 train time: 4.866s test time: 0.557  loss = 0.788 val_mse = 1.201 mse = 1.301 mae = 0.830\n",
            "epoch155 train time: 4.865s test time: 0.546  loss = 0.787 val_mse = 1.187 mse = 1.284 mae = 0.830\n",
            "epoch156 train time: 4.847s test time: 0.561  loss = 0.794 val_mse = 1.201 mse = 1.301 mae = 0.829\n",
            "epoch157 train time: 4.845s test time: 0.550  loss = 0.786 val_mse = 1.201 mse = 1.296 mae = 0.827\n",
            "epoch158 train time: 4.863s test time: 0.549  loss = 0.787 val_mse = 1.203 mse = 1.298 mae = 0.830\n",
            "epoch159 train time: 4.859s test time: 0.556  loss = 0.787 val_mse = 1.191 mse = 1.286 mae = 0.826\n",
            "epoch160 train time: 4.865s test time: 0.548  loss = 0.787 val_mse = 1.198 mse = 1.293 mae = 0.830\n",
            "epoch161 train time: 4.851s test time: 0.553  loss = 0.788 val_mse = 1.185 mse = 1.284 mae = 0.829\n",
            "epoch162 train time: 4.851s test time: 0.553  loss = 0.785 val_mse = 1.190 mse = 1.290 mae = 0.829\n",
            "epoch163 train time: 4.842s test time: 0.560  loss = 0.782 val_mse = 1.195 mse = 1.290 mae = 0.828\n",
            "epoch164 train time: 4.850s test time: 0.548  loss = 0.789 val_mse = 1.194 mse = 1.292 mae = 0.830\n",
            "epoch165 train time: 4.865s test time: 0.558  loss = 0.786 val_mse = 1.187 mse = 1.294 mae = 0.831\n",
            "epoch166 train time: 4.862s test time: 0.551  loss = 0.788 val_mse = 1.193 mse = 1.289 mae = 0.830\n",
            "epoch167 train time: 4.866s test time: 0.559  loss = 0.786 val_mse = 1.187 mse = 1.290 mae = 0.832\n",
            "epoch168 train time: 4.859s test time: 0.545  loss = 0.789 val_mse = 1.200 mse = 1.295 mae = 0.829\n",
            "epoch169 train time: 4.860s test time: 0.555  loss = 0.784 val_mse = 1.196 mse = 1.298 mae = 0.829\n",
            "epoch170 train time: 4.850s test time: 0.558  loss = 0.788 val_mse = 1.191 mse = 1.295 mae = 0.832\n",
            "epoch171 train time: 4.848s test time: 0.556  loss = 0.785 val_mse = 1.186 mse = 1.289 mae = 0.829\n",
            "epoch172 train time: 4.846s test time: 0.551  loss = 0.787 val_mse = 1.197 mse = 1.299 mae = 0.830\n",
            "epoch173 train time: 4.854s test time: 0.546  loss = 0.786 val_mse = 1.191 mse = 1.293 mae = 0.829\n",
            "epoch174 train time: 4.856s test time: 0.553  loss = 0.785 val_mse = 1.192 mse = 1.290 mae = 0.830\n",
            "epoch175 train time: 4.848s test time: 0.551  loss = 0.783 val_mse = 1.192 mse = 1.294 mae = 0.827\n",
            "epoch176 train time: 4.854s test time: 0.560  loss = 0.787 val_mse = 1.189 mse = 1.295 mae = 0.832\n",
            "epoch177 train time: 4.856s test time: 0.552  loss = 0.783 val_mse = 1.197 mse = 1.297 mae = 0.829\n",
            "epoch178 train time: 4.855s test time: 0.552  loss = 0.786 val_mse = 1.199 mse = 1.293 mae = 0.828\n",
            "epoch179 train time: 4.851s test time: 0.542  loss = 0.781 val_mse = 1.185 mse = 1.286 mae = 0.827\n",
            "MAE 0.8387940185962007\n",
            "MSE 1.3108765653804264\n",
            "epoch0 train time: 5.337s test time: 0.556  loss = 42.113 val_mse = 1.621 mse = 1.549 mae = 0.900\n",
            "epoch1 train time: 4.875s test time: 0.556  loss = 32.047 val_mse = 1.590 mse = 1.528 mae = 0.893\n",
            "epoch2 train time: 5.796s test time: 0.942  loss = 17.241 val_mse = 1.557 mse = 1.504 mae = 0.883\n",
            "epoch3 train time: 5.734s test time: 0.685  loss = 8.568 val_mse = 1.528 mse = 1.484 mae = 0.873\n",
            "epoch4 train time: 5.524s test time: 0.619  loss = 4.158 val_mse = 1.500 mse = 1.465 mae = 0.865\n",
            "epoch5 train time: 4.873s test time: 0.559  loss = 2.248 val_mse = 1.474 mse = 1.447 mae = 0.859\n",
            "epoch6 train time: 4.878s test time: 0.548  loss = 1.602 val_mse = 1.447 mse = 1.429 mae = 0.854\n",
            "epoch7 train time: 4.860s test time: 0.552  loss = 1.435 val_mse = 1.420 mse = 1.410 mae = 0.850\n",
            "epoch8 train time: 4.857s test time: 0.546  loss = 1.360 val_mse = 1.392 mse = 1.392 mae = 0.848\n",
            "epoch9 train time: 4.854s test time: 0.555  loss = 1.297 val_mse = 1.362 mse = 1.372 mae = 0.846\n",
            "epoch10 train time: 4.859s test time: 0.559  loss = 1.242 val_mse = 1.342 mse = 1.360 mae = 0.844\n",
            "epoch11 train time: 4.868s test time: 0.554  loss = 1.192 val_mse = 1.328 mse = 1.354 mae = 0.842\n",
            "epoch12 train time: 4.865s test time: 0.560  loss = 1.148 val_mse = 1.317 mse = 1.349 mae = 0.839\n",
            "epoch13 train time: 4.839s test time: 0.553  loss = 1.107 val_mse = 1.302 mse = 1.341 mae = 0.838\n",
            "epoch14 train time: 4.863s test time: 0.559  loss = 1.070 val_mse = 1.290 mse = 1.335 mae = 0.836\n",
            "epoch15 train time: 4.858s test time: 0.553  loss = 1.037 val_mse = 1.277 mse = 1.327 mae = 0.836\n",
            "epoch16 train time: 4.867s test time: 0.549  loss = 1.010 val_mse = 1.261 mse = 1.319 mae = 0.840\n",
            "epoch17 train time: 4.861s test time: 0.554  loss = 0.985 val_mse = 1.257 mse = 1.318 mae = 0.837\n",
            "epoch18 train time: 4.861s test time: 0.551  loss = 0.968 val_mse = 1.248 mse = 1.310 mae = 0.843\n",
            "epoch19 train time: 4.855s test time: 0.548  loss = 0.950 val_mse = 1.234 mse = 1.303 mae = 0.840\n",
            "epoch20 train time: 4.853s test time: 0.556  loss = 0.941 val_mse = 1.248 mse = 1.313 mae = 0.842\n",
            "epoch21 train time: 4.856s test time: 0.555  loss = 0.931 val_mse = 1.235 mse = 1.310 mae = 0.841\n",
            "epoch22 train time: 4.871s test time: 0.556  loss = 0.925 val_mse = 1.213 mse = 1.297 mae = 0.850\n",
            "epoch23 train time: 4.852s test time: 0.551  loss = 0.919 val_mse = 1.237 mse = 1.314 mae = 0.843\n",
            "epoch24 train time: 4.845s test time: 0.551  loss = 0.907 val_mse = 1.219 mse = 1.299 mae = 0.844\n",
            "epoch25 train time: 4.874s test time: 0.551  loss = 0.904 val_mse = 1.215 mse = 1.304 mae = 0.855\n",
            "epoch26 train time: 4.853s test time: 0.557  loss = 0.900 val_mse = 1.238 mse = 1.309 mae = 0.839\n",
            "epoch27 train time: 4.870s test time: 0.557  loss = 0.899 val_mse = 1.221 mse = 1.307 mae = 0.844\n",
            "epoch28 train time: 4.848s test time: 0.549  loss = 0.888 val_mse = 1.227 mse = 1.308 mae = 0.841\n",
            "epoch29 train time: 4.856s test time: 0.551  loss = 0.888 val_mse = 1.259 mse = 1.336 mae = 0.836\n",
            "epoch30 train time: 4.875s test time: 0.558  loss = 0.886 val_mse = 1.251 mse = 1.315 mae = 0.835\n",
            "epoch31 train time: 4.867s test time: 0.554  loss = 0.875 val_mse = 1.237 mse = 1.324 mae = 0.842\n",
            "epoch32 train time: 4.860s test time: 0.551  loss = 0.877 val_mse = 1.209 mse = 1.296 mae = 0.850\n",
            "epoch33 train time: 4.871s test time: 0.556  loss = 0.872 val_mse = 1.238 mse = 1.320 mae = 0.842\n",
            "epoch34 train time: 4.868s test time: 0.555  loss = 0.863 val_mse = 1.217 mse = 1.301 mae = 0.849\n",
            "epoch35 train time: 4.862s test time: 0.558  loss = 0.871 val_mse = 1.202 mse = 1.295 mae = 0.848\n",
            "epoch36 train time: 4.877s test time: 0.558  loss = 0.859 val_mse = 1.224 mse = 1.311 mae = 0.838\n",
            "epoch37 train time: 4.861s test time: 0.552  loss = 0.856 val_mse = 1.263 mse = 1.342 mae = 0.839\n",
            "epoch38 train time: 4.868s test time: 0.550  loss = 0.859 val_mse = 1.248 mse = 1.317 mae = 0.835\n",
            "epoch39 train time: 4.849s test time: 0.556  loss = 0.854 val_mse = 1.249 mse = 1.333 mae = 0.836\n",
            "epoch40 train time: 4.874s test time: 0.549  loss = 0.852 val_mse = 1.249 mse = 1.332 mae = 0.839\n",
            "epoch41 train time: 4.879s test time: 0.558  loss = 0.854 val_mse = 1.257 mse = 1.320 mae = 0.838\n",
            "epoch42 train time: 4.875s test time: 0.574  loss = 0.849 val_mse = 1.250 mse = 1.327 mae = 0.835\n",
            "epoch43 train time: 4.869s test time: 0.552  loss = 0.843 val_mse = 1.245 mse = 1.325 mae = 0.836\n",
            "epoch44 train time: 4.865s test time: 0.564  loss = 0.847 val_mse = 1.243 mse = 1.325 mae = 0.837\n",
            "epoch45 train time: 4.866s test time: 0.553  loss = 0.841 val_mse = 1.248 mse = 1.324 mae = 0.839\n",
            "epoch46 train time: 4.858s test time: 0.551  loss = 0.845 val_mse = 1.246 mse = 1.324 mae = 0.833\n",
            "epoch47 train time: 4.848s test time: 0.553  loss = 0.832 val_mse = 1.245 mse = 1.334 mae = 0.839\n",
            "epoch48 train time: 4.861s test time: 0.567  loss = 0.839 val_mse = 1.244 mse = 1.329 mae = 0.836\n",
            "epoch49 train time: 4.867s test time: 0.555  loss = 0.833 val_mse = 1.244 mse = 1.331 mae = 0.838\n",
            "epoch50 train time: 4.868s test time: 0.554  loss = 0.835 val_mse = 1.241 mse = 1.320 mae = 0.837\n",
            "epoch51 train time: 4.877s test time: 0.556  loss = 0.829 val_mse = 1.253 mse = 1.336 mae = 0.837\n",
            "epoch52 train time: 4.871s test time: 0.553  loss = 0.830 val_mse = 1.197 mse = 1.299 mae = 0.849\n",
            "epoch53 train time: 4.867s test time: 0.557  loss = 0.825 val_mse = 1.215 mse = 1.304 mae = 0.841\n",
            "epoch54 train time: 4.868s test time: 0.568  loss = 0.827 val_mse = 1.217 mse = 1.310 mae = 0.841\n",
            "epoch55 train time: 4.852s test time: 0.550  loss = 0.828 val_mse = 1.206 mse = 1.298 mae = 0.841\n",
            "epoch56 train time: 4.870s test time: 0.556  loss = 0.824 val_mse = 1.199 mse = 1.295 mae = 0.844\n",
            "epoch57 train time: 4.861s test time: 0.546  loss = 0.820 val_mse = 1.210 mse = 1.303 mae = 0.846\n",
            "epoch58 train time: 4.870s test time: 0.551  loss = 0.824 val_mse = 1.204 mse = 1.297 mae = 0.844\n",
            "epoch59 train time: 4.882s test time: 0.551  loss = 0.819 val_mse = 1.204 mse = 1.305 mae = 0.842\n",
            "epoch60 train time: 4.869s test time: 0.553  loss = 0.821 val_mse = 1.197 mse = 1.291 mae = 0.844\n",
            "epoch61 train time: 4.862s test time: 0.552  loss = 0.817 val_mse = 1.204 mse = 1.303 mae = 0.844\n",
            "epoch62 train time: 4.879s test time: 0.551  loss = 0.820 val_mse = 1.198 mse = 1.296 mae = 0.845\n",
            "epoch63 train time: 4.863s test time: 0.554  loss = 0.815 val_mse = 1.205 mse = 1.306 mae = 0.846\n",
            "epoch64 train time: 4.874s test time: 0.566  loss = 0.821 val_mse = 1.205 mse = 1.291 mae = 0.844\n",
            "epoch65 train time: 4.863s test time: 0.559  loss = 0.817 val_mse = 1.202 mse = 1.298 mae = 0.844\n",
            "epoch66 train time: 4.857s test time: 0.551  loss = 0.814 val_mse = 1.196 mse = 1.294 mae = 0.842\n",
            "epoch67 train time: 4.860s test time: 0.553  loss = 0.812 val_mse = 1.207 mse = 1.298 mae = 0.844\n",
            "epoch68 train time: 4.857s test time: 0.548  loss = 0.819 val_mse = 1.200 mse = 1.293 mae = 0.842\n",
            "epoch69 train time: 4.867s test time: 0.564  loss = 0.810 val_mse = 1.204 mse = 1.302 mae = 0.841\n",
            "epoch70 train time: 4.861s test time: 0.557  loss = 0.814 val_mse = 1.192 mse = 1.290 mae = 0.842\n",
            "epoch71 train time: 4.859s test time: 0.556  loss = 0.808 val_mse = 1.241 mse = 1.328 mae = 0.833\n",
            "epoch72 train time: 4.862s test time: 0.551  loss = 0.811 val_mse = 1.206 mse = 1.300 mae = 0.843\n",
            "epoch73 train time: 4.864s test time: 0.558  loss = 0.812 val_mse = 1.200 mse = 1.301 mae = 0.841\n",
            "epoch74 train time: 4.858s test time: 0.549  loss = 0.808 val_mse = 1.198 mse = 1.301 mae = 0.842\n",
            "epoch75 train time: 4.862s test time: 0.555  loss = 0.808 val_mse = 1.197 mse = 1.295 mae = 0.840\n",
            "epoch76 train time: 4.859s test time: 0.567  loss = 0.810 val_mse = 1.197 mse = 1.290 mae = 0.845\n",
            "epoch77 train time: 4.856s test time: 0.554  loss = 0.806 val_mse = 1.201 mse = 1.295 mae = 0.842\n",
            "epoch78 train time: 4.862s test time: 0.561  loss = 0.804 val_mse = 1.194 mse = 1.293 mae = 0.839\n",
            "epoch79 train time: 4.860s test time: 0.547  loss = 0.808 val_mse = 1.205 mse = 1.299 mae = 0.841\n",
            "epoch80 train time: 4.863s test time: 0.559  loss = 0.804 val_mse = 1.196 mse = 1.292 mae = 0.840\n",
            "epoch81 train time: 4.865s test time: 0.549  loss = 0.802 val_mse = 1.197 mse = 1.291 mae = 0.838\n",
            "epoch82 train time: 4.872s test time: 0.554  loss = 0.803 val_mse = 1.190 mse = 1.288 mae = 0.840\n",
            "epoch83 train time: 4.864s test time: 0.552  loss = 0.799 val_mse = 1.196 mse = 1.294 mae = 0.841\n",
            "epoch84 train time: 4.880s test time: 0.551  loss = 0.799 val_mse = 1.191 mse = 1.292 mae = 0.842\n",
            "epoch85 train time: 4.853s test time: 0.542  loss = 0.801 val_mse = 1.210 mse = 1.307 mae = 0.832\n",
            "epoch86 train time: 4.878s test time: 0.548  loss = 0.797 val_mse = 1.209 mse = 1.297 mae = 0.836\n",
            "epoch87 train time: 4.881s test time: 0.559  loss = 0.796 val_mse = 1.216 mse = 1.304 mae = 0.834\n",
            "epoch88 train time: 4.860s test time: 0.549  loss = 0.797 val_mse = 1.199 mse = 1.293 mae = 0.832\n",
            "epoch89 train time: 4.853s test time: 0.554  loss = 0.795 val_mse = 1.212 mse = 1.311 mae = 0.833\n",
            "epoch90 train time: 4.872s test time: 0.547  loss = 0.795 val_mse = 1.205 mse = 1.296 mae = 0.833\n",
            "epoch91 train time: 4.869s test time: 0.556  loss = 0.794 val_mse = 1.199 mse = 1.301 mae = 0.834\n",
            "epoch92 train time: 4.876s test time: 0.548  loss = 0.796 val_mse = 1.205 mse = 1.296 mae = 0.830\n",
            "epoch93 train time: 4.855s test time: 0.564  loss = 0.792 val_mse = 1.206 mse = 1.300 mae = 0.830\n",
            "epoch94 train time: 4.866s test time: 0.553  loss = 0.796 val_mse = 1.192 mse = 1.293 mae = 0.834\n",
            "epoch95 train time: 4.864s test time: 0.554  loss = 0.793 val_mse = 1.208 mse = 1.308 mae = 0.832\n",
            "epoch96 train time: 4.879s test time: 0.556  loss = 0.792 val_mse = 1.203 mse = 1.298 mae = 0.833\n",
            "epoch97 train time: 4.868s test time: 0.554  loss = 0.791 val_mse = 1.207 mse = 1.307 mae = 0.834\n",
            "epoch98 train time: 4.858s test time: 0.555  loss = 0.793 val_mse = 1.199 mse = 1.296 mae = 0.831\n",
            "epoch99 train time: 4.858s test time: 0.566  loss = 0.790 val_mse = 1.206 mse = 1.304 mae = 0.831\n",
            "epoch100 train time: 4.861s test time: 0.563  loss = 0.791 val_mse = 1.194 mse = 1.296 mae = 0.832\n",
            "epoch101 train time: 4.859s test time: 0.547  loss = 0.791 val_mse = 1.204 mse = 1.305 mae = 0.833\n",
            "epoch102 train time: 4.879s test time: 0.556  loss = 0.790 val_mse = 1.192 mse = 1.287 mae = 0.832\n",
            "epoch103 train time: 4.858s test time: 0.547  loss = 0.788 val_mse = 1.204 mse = 1.302 mae = 0.831\n",
            "epoch104 train time: 4.860s test time: 0.559  loss = 0.792 val_mse = 1.204 mse = 1.303 mae = 0.834\n",
            "epoch105 train time: 4.866s test time: 0.556  loss = 0.788 val_mse = 1.205 mse = 1.310 mae = 0.834\n",
            "epoch106 train time: 4.872s test time: 0.562  loss = 0.787 val_mse = 1.191 mse = 1.292 mae = 0.833\n",
            "epoch107 train time: 4.884s test time: 0.554  loss = 0.786 val_mse = 1.207 mse = 1.306 mae = 0.831\n",
            "epoch108 train time: 4.879s test time: 0.549  loss = 0.784 val_mse = 1.192 mse = 1.298 mae = 0.834\n",
            "epoch109 train time: 4.869s test time: 0.559  loss = 0.782 val_mse = 1.200 mse = 1.304 mae = 0.830\n",
            "epoch110 train time: 4.870s test time: 0.547  loss = 0.781 val_mse = 1.204 mse = 1.303 mae = 0.831\n",
            "epoch111 train time: 4.853s test time: 0.551  loss = 0.782 val_mse = 1.200 mse = 1.300 mae = 0.833\n",
            "epoch112 train time: 4.863s test time: 0.559  loss = 0.780 val_mse = 1.198 mse = 1.296 mae = 0.831\n",
            "epoch113 train time: 4.858s test time: 0.552  loss = 0.781 val_mse = 1.204 mse = 1.305 mae = 0.830\n",
            "epoch114 train time: 4.880s test time: 0.549  loss = 0.785 val_mse = 1.199 mse = 1.292 mae = 0.831\n",
            "epoch115 train time: 4.868s test time: 0.554  loss = 0.780 val_mse = 1.199 mse = 1.302 mae = 0.831\n",
            "epoch116 train time: 4.881s test time: 0.553  loss = 0.780 val_mse = 1.194 mse = 1.296 mae = 0.830\n",
            "epoch117 train time: 4.860s test time: 0.550  loss = 0.778 val_mse = 1.178 mse = 1.295 mae = 0.842\n",
            "epoch118 train time: 4.861s test time: 0.550  loss = 0.784 val_mse = 1.193 mse = 1.292 mae = 0.829\n",
            "epoch119 train time: 4.871s test time: 0.549  loss = 0.780 val_mse = 1.206 mse = 1.308 mae = 0.833\n",
            "epoch120 train time: 4.846s test time: 0.559  loss = 0.783 val_mse = 1.193 mse = 1.294 mae = 0.832\n",
            "epoch121 train time: 4.869s test time: 0.550  loss = 0.780 val_mse = 1.201 mse = 1.301 mae = 0.831\n",
            "epoch122 train time: 4.872s test time: 0.558  loss = 0.783 val_mse = 1.185 mse = 1.294 mae = 0.836\n",
            "epoch123 train time: 4.860s test time: 0.564  loss = 0.780 val_mse = 1.199 mse = 1.305 mae = 0.833\n",
            "epoch124 train time: 4.878s test time: 0.558  loss = 0.780 val_mse = 1.191 mse = 1.290 mae = 0.830\n",
            "epoch125 train time: 4.864s test time: 0.552  loss = 0.781 val_mse = 1.195 mse = 1.297 mae = 0.831\n",
            "epoch126 train time: 4.867s test time: 0.549  loss = 0.781 val_mse = 1.195 mse = 1.293 mae = 0.828\n",
            "epoch127 train time: 4.854s test time: 0.565  loss = 0.777 val_mse = 1.203 mse = 1.305 mae = 0.831\n",
            "epoch128 train time: 4.862s test time: 0.571  loss = 0.781 val_mse = 1.191 mse = 1.296 mae = 0.831\n",
            "epoch129 train time: 4.870s test time: 0.558  loss = 0.779 val_mse = 1.193 mse = 1.300 mae = 0.831\n",
            "epoch130 train time: 4.868s test time: 0.563  loss = 0.779 val_mse = 1.186 mse = 1.289 mae = 0.832\n",
            "epoch131 train time: 4.855s test time: 0.556  loss = 0.782 val_mse = 1.204 mse = 1.306 mae = 0.830\n",
            "epoch132 train time: 4.855s test time: 0.548  loss = 0.782 val_mse = 1.194 mse = 1.287 mae = 0.828\n",
            "epoch133 train time: 4.861s test time: 0.558  loss = 0.780 val_mse = 1.201 mse = 1.298 mae = 0.830\n",
            "epoch134 train time: 4.867s test time: 0.549  loss = 0.781 val_mse = 1.195 mse = 1.294 mae = 0.829\n",
            "epoch135 train time: 4.870s test time: 0.559  loss = 0.778 val_mse = 1.197 mse = 1.302 mae = 0.831\n",
            "epoch136 train time: 4.863s test time: 0.556  loss = 0.781 val_mse = 1.188 mse = 1.288 mae = 0.832\n",
            "epoch137 train time: 4.866s test time: 0.548  loss = 0.778 val_mse = 1.200 mse = 1.301 mae = 0.829\n",
            "epoch138 train time: 4.857s test time: 0.549  loss = 0.778 val_mse = 1.188 mse = 1.295 mae = 0.832\n",
            "epoch139 train time: 4.871s test time: 0.555  loss = 0.779 val_mse = 1.191 mse = 1.295 mae = 0.831\n",
            "epoch140 train time: 4.856s test time: 0.557  loss = 0.778 val_mse = 1.190 mse = 1.292 mae = 0.831\n",
            "epoch141 train time: 4.884s test time: 0.544  loss = 0.781 val_mse = 1.194 mse = 1.305 mae = 0.832\n",
            "epoch142 train time: 4.856s test time: 0.552  loss = 0.779 val_mse = 1.185 mse = 1.286 mae = 0.831\n",
            "epoch143 train time: 4.854s test time: 0.562  loss = 0.778 val_mse = 1.199 mse = 1.299 mae = 0.831\n",
            "epoch144 train time: 4.869s test time: 0.557  loss = 0.780 val_mse = 1.179 mse = 1.286 mae = 0.834\n",
            "epoch145 train time: 4.868s test time: 0.553  loss = 0.780 val_mse = 1.193 mse = 1.298 mae = 0.829\n",
            "epoch146 train time: 4.860s test time: 0.545  loss = 0.779 val_mse = 1.191 mse = 1.290 mae = 0.829\n",
            "epoch147 train time: 4.857s test time: 0.551  loss = 0.778 val_mse = 1.196 mse = 1.299 mae = 0.830\n",
            "epoch148 train time: 4.870s test time: 0.554  loss = 0.782 val_mse = 1.183 mse = 1.295 mae = 0.829\n",
            "epoch149 train time: 4.881s test time: 0.565  loss = 0.777 val_mse = 1.194 mse = 1.296 mae = 0.830\n",
            "epoch150 train time: 4.884s test time: 0.553  loss = 0.781 val_mse = 1.189 mse = 1.289 mae = 0.830\n",
            "epoch151 train time: 4.887s test time: 0.550  loss = 0.778 val_mse = 1.192 mse = 1.299 mae = 0.832\n",
            "epoch152 train time: 4.872s test time: 0.550  loss = 0.781 val_mse = 1.186 mse = 1.291 mae = 0.828\n",
            "epoch153 train time: 4.866s test time: 0.553  loss = 0.776 val_mse = 1.197 mse = 1.294 mae = 0.829\n",
            "epoch154 train time: 4.862s test time: 0.559  loss = 0.779 val_mse = 1.183 mse = 1.286 mae = 0.831\n",
            "epoch155 train time: 4.854s test time: 0.561  loss = 0.777 val_mse = 1.197 mse = 1.302 mae = 0.828\n",
            "epoch156 train time: 4.861s test time: 0.561  loss = 0.781 val_mse = 1.187 mse = 1.289 mae = 0.830\n",
            "epoch157 train time: 4.877s test time: 0.553  loss = 0.779 val_mse = 1.197 mse = 1.297 mae = 0.828\n",
            "epoch158 train time: 4.877s test time: 0.545  loss = 0.778 val_mse = 1.186 mse = 1.290 mae = 0.830\n",
            "epoch159 train time: 4.847s test time: 0.553  loss = 0.778 val_mse = 1.192 mse = 1.297 mae = 0.830\n",
            "epoch160 train time: 4.863s test time: 0.550  loss = 0.780 val_mse = 1.193 mse = 1.289 mae = 0.828\n",
            "epoch161 train time: 4.915s test time: 0.553  loss = 0.780 val_mse = 1.195 mse = 1.299 mae = 0.827\n",
            "epoch162 train time: 4.894s test time: 0.553  loss = 0.778 val_mse = 1.187 mse = 1.287 mae = 0.829\n",
            "epoch163 train time: 4.881s test time: 0.546  loss = 0.777 val_mse = 1.193 mse = 1.297 mae = 0.830\n",
            "epoch164 train time: 4.867s test time: 0.559  loss = 0.779 val_mse = 1.186 mse = 1.289 mae = 0.830\n",
            "epoch165 train time: 4.880s test time: 0.560  loss = 0.778 val_mse = 1.194 mse = 1.300 mae = 0.827\n",
            "epoch166 train time: 4.885s test time: 0.552  loss = 0.781 val_mse = 1.193 mse = 1.286 mae = 0.832\n",
            "epoch167 train time: 4.877s test time: 0.566  loss = 0.784 val_mse = 1.199 mse = 1.301 mae = 0.830\n",
            "epoch168 train time: 4.855s test time: 0.549  loss = 0.780 val_mse = 1.195 mse = 1.293 mae = 0.830\n",
            "epoch169 train time: 4.856s test time: 0.559  loss = 0.780 val_mse = 1.200 mse = 1.298 mae = 0.827\n",
            "epoch170 train time: 4.849s test time: 0.550  loss = 0.781 val_mse = 1.185 mse = 1.286 mae = 0.828\n",
            "epoch171 train time: 4.871s test time: 0.555  loss = 0.778 val_mse = 1.188 mse = 1.296 mae = 0.829\n",
            "epoch172 train time: 4.859s test time: 0.549  loss = 0.779 val_mse = 1.195 mse = 1.286 mae = 0.827\n",
            "epoch173 train time: 4.872s test time: 0.560  loss = 0.778 val_mse = 1.192 mse = 1.299 mae = 0.830\n",
            "epoch174 train time: 4.862s test time: 0.551  loss = 0.778 val_mse = 1.189 mse = 1.295 mae = 0.831\n",
            "epoch175 train time: 4.871s test time: 0.560  loss = 0.780 val_mse = 1.196 mse = 1.296 mae = 0.828\n",
            "epoch176 train time: 4.869s test time: 0.551  loss = 0.777 val_mse = 1.185 mse = 1.289 mae = 0.828\n",
            "epoch177 train time: 4.850s test time: 0.552  loss = 0.779 val_mse = 1.199 mse = 1.297 mae = 0.829\n",
            "epoch178 train time: 4.855s test time: 0.555  loss = 0.779 val_mse = 1.187 mse = 1.288 mae = 0.829\n",
            "epoch179 train time: 4.846s test time: 0.552  loss = 0.775 val_mse = 1.195 mse = 1.301 mae = 0.829\n",
            "MAE 0.8371213764721958\n",
            "MSE 1.3116436500942947\n"
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    print(sys.path[0])\n",
        "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
        "    word_latent_dim = 300\n",
        "    latent_dim = 25\n",
        "    max_doc_length = 300\n",
        "    num_filters = 50\n",
        "    window_size = 3\n",
        "    v_dim = 50\n",
        "    learning_rate = 0.001\n",
        "    lambda_1 = 0.01 #normally we set from [0.05, 0.01, 0.005, 0.001]\n",
        "    drop_out_list = [0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
        "    drop_out = 0.2\n",
        "    batch_size = 200\n",
        "    epochs = 180\n",
        "    avg_mae_list = []\n",
        "    avg_mse_list = []\n",
        "    \n",
        "    # loading data\n",
        "    firTime = time()\n",
        "    dataSet = Dataset(max_doc_length, sys.path[0], \"dataPreprocessingWordDict.out\")\n",
        "    word_dict, user_reviews, item_reviews, train, valRatings, testRatings = dataSet.word_id_dict, dataSet.userReview_dict, dataSet.itemReview_dict, dataSet.trainMtrx, dataSet.valRatings, dataSet.testRatings\n",
        "    secTime = time()\n",
        "\n",
        "    num_users, num_items = train.shape\n",
        "    print(\"load data: %.3fs\" % (secTime - firTime))\n",
        "    print(num_users, num_items)\n",
        "    print(latent_dim)\n",
        "\n",
        "    #load word embeddings\n",
        "    word_embedding_mtrx = ini_word_embed(len(word_dict), word_latent_dim)\n",
        "    # word_embedding_mtrx = word2vec_word_embed(len(word_dict), word_latent_dim,\n",
        "    #                                           \"Directory of pretrained WordEmbedding.out\",\n",
        "    #                                           word_dict)\n",
        "\n",
        "    print( \"shape\", word_embedding_mtrx.shape)\n",
        "\n",
        "    # get train instances\n",
        "    user_input, item_input, rateings = get_train_instance(train)\n",
        "    print (len(user_input), len(item_input), len(rateings))\n",
        "\n",
        "    # get test/val instances\n",
        "    user_vals, item_vals, user_input_val, item_input_val, rating_input_val = get_test_list(200, valRatings, user_reviews, item_reviews)\n",
        "    user_tests, item_tests, user_input_test, item_input_test, rating_input_test = get_test_list(200, testRatings, user_reviews, item_reviews)\n",
        "\n",
        "    #train & eval model\n",
        "    train_model()\n",
        "\n",
        "    drop_out = 0.3\n",
        "    #train & eval model\n",
        "    train_model()\n",
        "\n",
        "    drop_out = 0.4\n",
        "    #train & eval model\n",
        "    train_model()\n",
        "    \n",
        "    drop_out = 0.5\n",
        "    #train & eval model\n",
        "    train_model()\n",
        "\n",
        "    drop_out = 0.6\n",
        "    #train & eval model\n",
        "    train_model()\n",
        "\n",
        "    drop_out = 0.7\n",
        "    #train & eval model\n",
        "    train_model()\n",
        "\n",
        "    drop_out = 0.8\n",
        "    #train & eval model\n",
        "    train_model()\n",
        "\n",
        "    drop_out = 0.9\n",
        "    #train & eval model\n",
        "    train_model()"
      ],
      "id": "hNFwT-mADevc"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tQqGFVnxDesx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 426
        },
        "outputId": "cb7756cd-a29d-43ee-817e-b466e4155b35"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "avg mae over drop out rate = [0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]: [0.8381531545991346, 0.8383916606721673, 0.838968696967958, 0.8388697302821002, 0.8393858781233233, 0.8363583598807807, 0.8387940185962007, 0.8371213764721958]\n",
            "avg mse over drop out rate = [0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]: [1.3133382432145548, 1.3119380319514957, 1.307635431930038, 1.3123322760781038, 1.3068397265143166, 1.3114134781678832, 1.3108765653804264, 1.3116436500942947]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 750x350 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "minimum avg mae over drop out rates = 0.8363583598807807 at drop out rate = 0.7 and mse = 1.3114134781678832\n",
            "minimum avg mse over drop out rates = 1.3068397265143166 at drop out rate = 0.6 and mae = 0.8393858781233233\n"
          ]
        }
      ],
      "source": [
        "avg_mae_list = [0.8381531545991346,0.8383916606721673,0.838968696967958,0.8388697302821002,0.8393858781233233,0.8363583598807807,0.8387940185962007,0.8371213764721958]\n",
        "avg_mse_list = [1.3133382432145548,1.3119380319514957,1.307635431930038,1.3123322760781038,1.3068397265143166,1.3114134781678832,1.3108765653804264,1.3116436500942947]\n",
        "drop_out_list = [0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
        "print(f'avg mae over drop out rate = [0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]: {avg_mae_list}')\n",
        "print(f'avg mse over drop out rate = [0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]: {avg_mse_list}')\n",
        "    \n",
        "plt.rcParams.update({'figure.figsize':(7.5,3.5), 'figure.dpi':100})\n",
        "fig, axes = plt.subplots(1, 2)\n",
        "axes[0].plot(drop_out_list, avg_mae_list, 'm--o')\n",
        "axes[0].set_title('Avg MAE over varying drop out rate')\n",
        "axes[0].set_xlabel('Drop out')\n",
        "axes[0].set_ylabel('MAE')\n",
        "\n",
        "axes[1].plot(drop_out_list, avg_mse_list, 'g--o')\n",
        "axes[1].set_title('Avg MSE over varying drop out rate')\n",
        "axes[1].set_xlabel('Drop out')\n",
        "axes[1].set_ylabel('MSE')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "index = 0\n",
        "min = 100\n",
        "min_index = 0\n",
        "for i in avg_mae_list:\n",
        "  if i<min:\n",
        "    min = i\n",
        "    min_index = index\n",
        "  index+=1\n",
        "print(f'minimum avg mae over drop out rates = {min} at drop out rate = {drop_out_list[min_index]} and mse = {avg_mse_list[min_index]}')\n",
        "\n",
        "index = 0\n",
        "min = 100\n",
        "min_index = 0\n",
        "for i in avg_mse_list:\n",
        "  if i<min:\n",
        "    min = i\n",
        "    min_index = index\n",
        "  index+=1\n",
        "print(f'minimum avg mse over drop out rates = {min} at drop out rate = {drop_out_list[min_index]} and mae = {avg_mae_list[min_index]}')"
      ],
      "id": "tQqGFVnxDesx"
    },
    {
      "cell_type": "markdown",
      "source": [
        "drop out rate = 0.6"
      ],
      "metadata": {
        "id": "0nmS9zIK4jgR"
      },
      "id": "0nmS9zIK4jgR"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ajUClIHDghr"
      },
      "source": [
        "Varying sliding window"
      ],
      "id": "-ajUClIHDghr"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "War3DkA-DeqH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b3eae630-c53f-4892-eed8-28a426a6db57"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "wordId_dict finished\n",
            "load reviews finished\n",
            "load data: 2.773s\n",
            "506 2581\n",
            "15\n",
            "shape (19930, 300)\n",
            "18880 18880 18880\n",
            "epoch0 train time: 29.008s test time: 0.593  loss = 30.842 val_mse = 1.586 mse = 1.522 mae = 0.885\n",
            "epoch1 train time: 4.504s test time: 0.484  loss = 21.264 val_mse = 1.572 mse = 1.512 mae = 0.881\n",
            "epoch2 train time: 4.503s test time: 0.483  loss = 11.531 val_mse = 1.550 mse = 1.496 mae = 0.876\n",
            "epoch3 train time: 4.514s test time: 0.490  loss = 5.857 val_mse = 1.529 mse = 1.480 mae = 0.871\n",
            "epoch4 train time: 4.528s test time: 0.490  loss = 2.996 val_mse = 1.507 mse = 1.464 mae = 0.867\n",
            "epoch5 train time: 4.545s test time: 0.497  loss = 1.774 val_mse = 1.484 mse = 1.448 mae = 0.863\n",
            "epoch6 train time: 4.542s test time: 0.491  loss = 1.373 val_mse = 1.461 mse = 1.433 mae = 0.860\n",
            "epoch7 train time: 4.549s test time: 0.492  loss = 1.279 val_mse = 1.439 mse = 1.417 mae = 0.858\n",
            "epoch8 train time: 4.539s test time: 0.489  loss = 1.238 val_mse = 1.416 mse = 1.403 mae = 0.856\n",
            "epoch9 train time: 4.521s test time: 0.488  loss = 1.200 val_mse = 1.393 mse = 1.389 mae = 0.854\n",
            "epoch10 train time: 4.508s test time: 0.548  loss = 1.164 val_mse = 1.371 mse = 1.375 mae = 0.853\n",
            "epoch11 train time: 4.637s test time: 0.493  loss = 1.131 val_mse = 1.351 mse = 1.364 mae = 0.851\n",
            "epoch12 train time: 4.509s test time: 0.495  loss = 1.100 val_mse = 1.328 mse = 1.351 mae = 0.850\n",
            "epoch13 train time: 4.507s test time: 0.492  loss = 1.071 val_mse = 1.308 mse = 1.339 mae = 0.849\n",
            "epoch14 train time: 4.486s test time: 0.493  loss = 1.045 val_mse = 1.297 mse = 1.334 mae = 0.845\n",
            "epoch15 train time: 4.484s test time: 0.489  loss = 1.019 val_mse = 1.286 mse = 1.329 mae = 0.843\n",
            "epoch16 train time: 4.493s test time: 0.491  loss = 0.997 val_mse = 1.276 mse = 1.325 mae = 0.840\n",
            "epoch17 train time: 4.517s test time: 0.490  loss = 0.976 val_mse = 1.265 mse = 1.319 mae = 0.837\n",
            "epoch18 train time: 4.491s test time: 0.493  loss = 0.958 val_mse = 1.253 mse = 1.312 mae = 0.837\n",
            "epoch19 train time: 4.500s test time: 0.489  loss = 0.943 val_mse = 1.243 mse = 1.308 mae = 0.837\n",
            "epoch20 train time: 4.508s test time: 0.489  loss = 0.928 val_mse = 1.238 mse = 1.308 mae = 0.839\n",
            "epoch21 train time: 4.496s test time: 0.493  loss = 0.918 val_mse = 1.232 mse = 1.304 mae = 0.838\n",
            "epoch22 train time: 4.489s test time: 0.493  loss = 0.908 val_mse = 1.229 mse = 1.303 mae = 0.842\n",
            "epoch23 train time: 4.511s test time: 0.494  loss = 0.898 val_mse = 1.219 mse = 1.300 mae = 0.839\n",
            "epoch24 train time: 4.481s test time: 0.486  loss = 0.893 val_mse = 1.216 mse = 1.300 mae = 0.842\n",
            "epoch25 train time: 4.486s test time: 0.493  loss = 0.885 val_mse = 1.214 mse = 1.300 mae = 0.841\n",
            "epoch26 train time: 4.482s test time: 0.492  loss = 0.886 val_mse = 1.215 mse = 1.297 mae = 0.848\n",
            "epoch27 train time: 4.489s test time: 0.482  loss = 0.884 val_mse = 1.204 mse = 1.300 mae = 0.850\n",
            "epoch28 train time: 4.489s test time: 0.493  loss = 0.878 val_mse = 1.202 mse = 1.298 mae = 0.848\n",
            "epoch29 train time: 4.508s test time: 0.488  loss = 0.871 val_mse = 1.231 mse = 1.315 mae = 0.837\n",
            "epoch30 train time: 4.483s test time: 0.491  loss = 0.870 val_mse = 1.216 mse = 1.296 mae = 0.842\n",
            "epoch31 train time: 4.676s test time: 0.500  loss = 0.864 val_mse = 1.202 mse = 1.300 mae = 0.854\n",
            "epoch32 train time: 4.483s test time: 0.494  loss = 0.863 val_mse = 1.207 mse = 1.297 mae = 0.847\n",
            "epoch33 train time: 4.473s test time: 0.493  loss = 0.855 val_mse = 1.206 mse = 1.300 mae = 0.847\n",
            "epoch34 train time: 4.481s test time: 0.492  loss = 0.848 val_mse = 1.220 mse = 1.307 mae = 0.841\n",
            "epoch35 train time: 4.491s test time: 0.492  loss = 0.855 val_mse = 1.209 mse = 1.302 mae = 0.846\n",
            "epoch36 train time: 4.478s test time: 0.490  loss = 0.847 val_mse = 1.215 mse = 1.301 mae = 0.841\n",
            "epoch37 train time: 4.491s test time: 0.483  loss = 0.847 val_mse = 1.239 mse = 1.328 mae = 0.835\n",
            "epoch38 train time: 4.479s test time: 0.495  loss = 0.845 val_mse = 1.225 mse = 1.308 mae = 0.838\n",
            "epoch39 train time: 4.492s test time: 0.489  loss = 0.839 val_mse = 1.202 mse = 1.312 mae = 0.848\n",
            "epoch40 train time: 4.497s test time: 0.497  loss = 0.839 val_mse = 1.221 mse = 1.305 mae = 0.842\n",
            "epoch41 train time: 4.503s test time: 0.491  loss = 0.842 val_mse = 1.200 mse = 1.302 mae = 0.847\n",
            "epoch42 train time: 4.489s test time: 0.497  loss = 0.836 val_mse = 1.218 mse = 1.297 mae = 0.839\n",
            "epoch43 train time: 4.482s test time: 0.494  loss = 0.837 val_mse = 1.202 mse = 1.304 mae = 0.844\n",
            "epoch44 train time: 4.470s test time: 0.497  loss = 0.829 val_mse = 1.220 mse = 1.311 mae = 0.839\n",
            "epoch45 train time: 4.482s test time: 0.484  loss = 0.833 val_mse = 1.199 mse = 1.306 mae = 0.847\n",
            "epoch46 train time: 4.488s test time: 0.500  loss = 0.826 val_mse = 1.214 mse = 1.305 mae = 0.842\n",
            "epoch47 train time: 4.489s test time: 0.492  loss = 0.828 val_mse = 1.209 mse = 1.306 mae = 0.848\n",
            "epoch48 train time: 4.475s test time: 0.491  loss = 0.830 val_mse = 1.224 mse = 1.302 mae = 0.838\n",
            "epoch49 train time: 4.496s test time: 0.493  loss = 0.825 val_mse = 1.202 mse = 1.303 mae = 0.844\n",
            "epoch50 train time: 4.487s test time: 0.492  loss = 0.827 val_mse = 1.228 mse = 1.309 mae = 0.839\n",
            "epoch51 train time: 4.477s test time: 0.487  loss = 0.832 val_mse = 1.208 mse = 1.302 mae = 0.842\n",
            "epoch52 train time: 4.675s test time: 0.492  loss = 0.827 val_mse = 1.229 mse = 1.314 mae = 0.837\n",
            "epoch53 train time: 4.498s test time: 0.497  loss = 0.824 val_mse = 1.207 mse = 1.300 mae = 0.846\n",
            "epoch54 train time: 4.474s test time: 0.493  loss = 0.823 val_mse = 1.217 mse = 1.303 mae = 0.837\n",
            "epoch55 train time: 4.493s test time: 0.494  loss = 0.822 val_mse = 1.198 mse = 1.303 mae = 0.843\n",
            "epoch56 train time: 4.476s test time: 0.486  loss = 0.821 val_mse = 1.222 mse = 1.304 mae = 0.837\n",
            "epoch57 train time: 4.480s test time: 0.480  loss = 0.817 val_mse = 1.200 mse = 1.305 mae = 0.842\n",
            "epoch58 train time: 4.483s test time: 0.492  loss = 0.814 val_mse = 1.221 mse = 1.304 mae = 0.838\n",
            "epoch59 train time: 4.497s test time: 0.494  loss = 0.817 val_mse = 1.206 mse = 1.307 mae = 0.849\n",
            "epoch60 train time: 4.477s test time: 0.491  loss = 0.819 val_mse = 1.220 mse = 1.302 mae = 0.837\n",
            "epoch61 train time: 4.486s test time: 0.492  loss = 0.816 val_mse = 1.235 mse = 1.322 mae = 0.830\n",
            "epoch62 train time: 4.489s test time: 0.480  loss = 0.815 val_mse = 1.211 mse = 1.296 mae = 0.839\n",
            "epoch63 train time: 4.490s test time: 0.498  loss = 0.814 val_mse = 1.196 mse = 1.296 mae = 0.843\n",
            "epoch64 train time: 4.491s test time: 0.491  loss = 0.808 val_mse = 1.215 mse = 1.313 mae = 0.838\n",
            "epoch65 train time: 4.496s test time: 0.490  loss = 0.810 val_mse = 1.207 mse = 1.303 mae = 0.842\n",
            "epoch66 train time: 4.488s test time: 0.492  loss = 0.812 val_mse = 1.228 mse = 1.307 mae = 0.836\n",
            "epoch67 train time: 4.479s test time: 0.484  loss = 0.809 val_mse = 1.198 mse = 1.306 mae = 0.838\n",
            "epoch68 train time: 4.470s test time: 0.496  loss = 0.807 val_mse = 1.204 mse = 1.290 mae = 0.837\n",
            "epoch69 train time: 4.478s test time: 0.488  loss = 0.807 val_mse = 1.196 mse = 1.294 mae = 0.839\n",
            "epoch70 train time: 4.490s test time: 0.486  loss = 0.802 val_mse = 1.211 mse = 1.304 mae = 0.838\n",
            "epoch71 train time: 4.493s test time: 0.493  loss = 0.806 val_mse = 1.196 mse = 1.294 mae = 0.838\n",
            "epoch72 train time: 4.480s test time: 0.490  loss = 0.803 val_mse = 1.215 mse = 1.305 mae = 0.839\n",
            "epoch73 train time: 4.491s test time: 0.502  loss = 0.808 val_mse = 1.215 mse = 1.310 mae = 0.833\n",
            "epoch74 train time: 4.499s test time: 0.496  loss = 0.803 val_mse = 1.226 mse = 1.306 mae = 0.833\n",
            "epoch75 train time: 4.494s test time: 0.492  loss = 0.804 val_mse = 1.207 mse = 1.306 mae = 0.839\n",
            "epoch76 train time: 4.489s test time: 0.491  loss = 0.801 val_mse = 1.218 mse = 1.309 mae = 0.833\n",
            "epoch77 train time: 4.490s test time: 0.492  loss = 0.805 val_mse = 1.209 mse = 1.307 mae = 0.837\n",
            "epoch78 train time: 4.494s test time: 0.481  loss = 0.803 val_mse = 1.216 mse = 1.311 mae = 0.836\n",
            "epoch79 train time: 4.481s test time: 0.488  loss = 0.805 val_mse = 1.204 mse = 1.304 mae = 0.835\n",
            "epoch80 train time: 4.491s test time: 0.491  loss = 0.801 val_mse = 1.223 mse = 1.309 mae = 0.833\n",
            "epoch81 train time: 4.490s test time: 0.490  loss = 0.804 val_mse = 1.207 mse = 1.304 mae = 0.836\n",
            "epoch82 train time: 4.500s test time: 0.492  loss = 0.798 val_mse = 1.213 mse = 1.305 mae = 0.831\n",
            "epoch83 train time: 4.488s test time: 0.491  loss = 0.800 val_mse = 1.213 mse = 1.302 mae = 0.837\n",
            "epoch84 train time: 4.479s test time: 0.501  loss = 0.803 val_mse = 1.215 mse = 1.303 mae = 0.832\n",
            "epoch85 train time: 4.479s test time: 0.489  loss = 0.799 val_mse = 1.210 mse = 1.311 mae = 0.833\n",
            "epoch86 train time: 4.480s test time: 0.491  loss = 0.798 val_mse = 1.221 mse = 1.311 mae = 0.833\n",
            "epoch87 train time: 4.479s test time: 0.494  loss = 0.799 val_mse = 1.203 mse = 1.303 mae = 0.837\n",
            "epoch88 train time: 4.487s test time: 0.495  loss = 0.799 val_mse = 1.211 mse = 1.303 mae = 0.830\n",
            "epoch89 train time: 4.476s test time: 0.492  loss = 0.799 val_mse = 1.206 mse = 1.300 mae = 0.832\n",
            "epoch90 train time: 4.492s test time: 0.486  loss = 0.802 val_mse = 1.219 mse = 1.306 mae = 0.829\n",
            "epoch91 train time: 4.488s test time: 0.480  loss = 0.799 val_mse = 1.207 mse = 1.308 mae = 0.837\n",
            "epoch92 train time: 4.481s test time: 0.491  loss = 0.799 val_mse = 1.216 mse = 1.301 mae = 0.832\n",
            "epoch93 train time: 4.479s test time: 0.484  loss = 0.801 val_mse = 1.197 mse = 1.296 mae = 0.834\n",
            "epoch94 train time: 4.502s test time: 0.492  loss = 0.796 val_mse = 1.213 mse = 1.302 mae = 0.830\n",
            "epoch95 train time: 4.501s test time: 0.489  loss = 0.796 val_mse = 1.197 mse = 1.302 mae = 0.833\n",
            "epoch96 train time: 4.478s test time: 0.495  loss = 0.791 val_mse = 1.219 mse = 1.309 mae = 0.832\n",
            "epoch97 train time: 4.480s test time: 0.492  loss = 0.795 val_mse = 1.197 mse = 1.295 mae = 0.833\n",
            "epoch98 train time: 4.488s test time: 0.485  loss = 0.794 val_mse = 1.211 mse = 1.306 mae = 0.834\n",
            "epoch99 train time: 4.492s test time: 0.495  loss = 0.799 val_mse = 1.202 mse = 1.298 mae = 0.834\n",
            "epoch100 train time: 4.474s test time: 0.481  loss = 0.793 val_mse = 1.206 mse = 1.295 mae = 0.831\n",
            "epoch101 train time: 4.483s test time: 0.489  loss = 0.795 val_mse = 1.201 mse = 1.302 mae = 0.833\n",
            "epoch102 train time: 4.480s test time: 0.487  loss = 0.792 val_mse = 1.210 mse = 1.299 mae = 0.828\n",
            "epoch103 train time: 4.476s test time: 0.491  loss = 0.789 val_mse = 1.204 mse = 1.298 mae = 0.834\n",
            "epoch104 train time: 4.476s test time: 0.492  loss = 0.792 val_mse = 1.202 mse = 1.302 mae = 0.834\n",
            "epoch105 train time: 4.481s test time: 0.501  loss = 0.793 val_mse = 1.197 mse = 1.299 mae = 0.832\n",
            "epoch106 train time: 4.494s test time: 0.492  loss = 0.788 val_mse = 1.211 mse = 1.310 mae = 0.832\n",
            "epoch107 train time: 4.489s test time: 0.494  loss = 0.788 val_mse = 1.191 mse = 1.291 mae = 0.833\n",
            "epoch108 train time: 4.475s test time: 0.494  loss = 0.786 val_mse = 1.212 mse = 1.301 mae = 0.829\n",
            "epoch109 train time: 4.477s test time: 0.498  loss = 0.789 val_mse = 1.194 mse = 1.301 mae = 0.834\n",
            "epoch110 train time: 4.474s test time: 0.492  loss = 0.787 val_mse = 1.202 mse = 1.295 mae = 0.832\n",
            "epoch111 train time: 4.495s test time: 0.494  loss = 0.788 val_mse = 1.203 mse = 1.301 mae = 0.831\n",
            "epoch112 train time: 4.493s test time: 0.489  loss = 0.786 val_mse = 1.205 mse = 1.297 mae = 0.830\n",
            "epoch113 train time: 4.486s test time: 0.483  loss = 0.788 val_mse = 1.193 mse = 1.300 mae = 0.837\n",
            "epoch114 train time: 4.498s test time: 0.492  loss = 0.786 val_mse = 1.197 mse = 1.295 mae = 0.830\n",
            "epoch115 train time: 4.493s test time: 0.487  loss = 0.787 val_mse = 1.193 mse = 1.296 mae = 0.829\n",
            "epoch116 train time: 4.499s test time: 0.489  loss = 0.784 val_mse = 1.202 mse = 1.298 mae = 0.829\n",
            "epoch117 train time: 4.489s test time: 0.492  loss = 0.785 val_mse = 1.195 mse = 1.295 mae = 0.830\n",
            "epoch118 train time: 4.495s test time: 0.492  loss = 0.784 val_mse = 1.202 mse = 1.299 mae = 0.830\n",
            "epoch119 train time: 4.479s test time: 0.493  loss = 0.786 val_mse = 1.185 mse = 1.293 mae = 0.832\n",
            "epoch120 train time: 4.478s test time: 0.491  loss = 0.784 val_mse = 1.200 mse = 1.295 mae = 0.830\n",
            "epoch121 train time: 4.491s test time: 0.495  loss = 0.785 val_mse = 1.197 mse = 1.297 mae = 0.830\n",
            "epoch122 train time: 4.495s test time: 0.492  loss = 0.779 val_mse = 1.204 mse = 1.303 mae = 0.829\n",
            "epoch123 train time: 4.494s test time: 0.484  loss = 0.782 val_mse = 1.192 mse = 1.292 mae = 0.832\n",
            "epoch124 train time: 4.475s test time: 0.491  loss = 0.786 val_mse = 1.203 mse = 1.290 mae = 0.832\n",
            "epoch125 train time: 4.481s test time: 0.489  loss = 0.784 val_mse = 1.184 mse = 1.295 mae = 0.834\n",
            "epoch126 train time: 4.490s test time: 0.485  loss = 0.779 val_mse = 1.202 mse = 1.298 mae = 0.831\n",
            "epoch127 train time: 4.473s test time: 0.493  loss = 0.779 val_mse = 1.193 mse = 1.292 mae = 0.832\n",
            "epoch128 train time: 4.485s test time: 0.493  loss = 0.778 val_mse = 1.197 mse = 1.294 mae = 0.829\n",
            "epoch129 train time: 4.502s test time: 0.499  loss = 0.779 val_mse = 1.189 mse = 1.297 mae = 0.834\n",
            "epoch130 train time: 4.480s test time: 0.492  loss = 0.776 val_mse = 1.198 mse = 1.291 mae = 0.832\n",
            "epoch131 train time: 4.494s test time: 0.491  loss = 0.776 val_mse = 1.185 mse = 1.291 mae = 0.832\n",
            "epoch132 train time: 4.475s test time: 0.493  loss = 0.776 val_mse = 1.205 mse = 1.297 mae = 0.828\n",
            "epoch133 train time: 4.484s test time: 0.491  loss = 0.778 val_mse = 1.189 mse = 1.292 mae = 0.833\n",
            "epoch134 train time: 4.503s test time: 0.492  loss = 0.774 val_mse = 1.188 mse = 1.289 mae = 0.831\n",
            "epoch135 train time: 4.491s test time: 0.491  loss = 0.775 val_mse = 1.194 mse = 1.296 mae = 0.829\n",
            "epoch136 train time: 4.478s test time: 0.490  loss = 0.773 val_mse = 1.197 mse = 1.296 mae = 0.831\n",
            "epoch137 train time: 4.477s test time: 0.492  loss = 0.774 val_mse = 1.186 mse = 1.291 mae = 0.833\n",
            "epoch138 train time: 4.492s test time: 0.493  loss = 0.772 val_mse = 1.193 mse = 1.298 mae = 0.831\n",
            "epoch139 train time: 4.480s test time: 0.492  loss = 0.772 val_mse = 1.185 mse = 1.294 mae = 0.833\n",
            "epoch140 train time: 4.477s test time: 0.489  loss = 0.770 val_mse = 1.194 mse = 1.296 mae = 0.830\n",
            "epoch141 train time: 4.487s test time: 0.488  loss = 0.773 val_mse = 1.186 mse = 1.294 mae = 0.832\n",
            "epoch142 train time: 4.487s test time: 0.495  loss = 0.770 val_mse = 1.193 mse = 1.290 mae = 0.831\n",
            "epoch143 train time: 4.479s test time: 0.493  loss = 0.771 val_mse = 1.182 mse = 1.294 mae = 0.833\n",
            "epoch144 train time: 4.475s test time: 0.492  loss = 0.772 val_mse = 1.190 mse = 1.296 mae = 0.829\n",
            "epoch145 train time: 4.482s test time: 0.492  loss = 0.771 val_mse = 1.192 mse = 1.292 mae = 0.833\n",
            "epoch146 train time: 4.487s test time: 0.481  loss = 0.770 val_mse = 1.192 mse = 1.289 mae = 0.831\n",
            "epoch147 train time: 4.497s test time: 0.489  loss = 0.772 val_mse = 1.185 mse = 1.297 mae = 0.830\n",
            "epoch148 train time: 4.490s test time: 0.492  loss = 0.771 val_mse = 1.190 mse = 1.292 mae = 0.833\n",
            "epoch149 train time: 4.483s test time: 0.491  loss = 0.773 val_mse = 1.181 mse = 1.293 mae = 0.832\n",
            "epoch150 train time: 4.481s test time: 0.491  loss = 0.769 val_mse = 1.193 mse = 1.296 mae = 0.830\n",
            "epoch151 train time: 4.488s test time: 0.488  loss = 0.772 val_mse = 1.185 mse = 1.294 mae = 0.831\n",
            "epoch152 train time: 4.492s test time: 0.502  loss = 0.770 val_mse = 1.189 mse = 1.293 mae = 0.830\n",
            "epoch153 train time: 4.486s test time: 0.496  loss = 0.771 val_mse = 1.186 mse = 1.288 mae = 0.830\n",
            "epoch154 train time: 4.492s test time: 0.483  loss = 0.767 val_mse = 1.184 mse = 1.293 mae = 0.831\n",
            "epoch155 train time: 4.480s test time: 0.479  loss = 0.769 val_mse = 1.188 mse = 1.291 mae = 0.830\n",
            "epoch156 train time: 4.498s test time: 0.482  loss = 0.771 val_mse = 1.193 mse = 1.287 mae = 0.828\n",
            "epoch157 train time: 4.477s test time: 0.494  loss = 0.771 val_mse = 1.181 mse = 1.295 mae = 0.832\n",
            "epoch158 train time: 4.483s test time: 0.488  loss = 0.772 val_mse = 1.196 mse = 1.294 mae = 0.828\n",
            "epoch159 train time: 4.478s test time: 0.492  loss = 0.768 val_mse = 1.183 mse = 1.287 mae = 0.831\n",
            "epoch160 train time: 4.483s test time: 0.493  loss = 0.769 val_mse = 1.191 mse = 1.286 mae = 0.826\n",
            "epoch161 train time: 4.476s test time: 0.492  loss = 0.767 val_mse = 1.179 mse = 1.289 mae = 0.829\n",
            "epoch162 train time: 4.484s test time: 0.490  loss = 0.767 val_mse = 1.192 mse = 1.295 mae = 0.830\n",
            "epoch163 train time: 4.500s test time: 0.488  loss = 0.769 val_mse = 1.180 mse = 1.290 mae = 0.832\n",
            "epoch164 train time: 4.508s test time: 0.496  loss = 0.769 val_mse = 1.183 mse = 1.282 mae = 0.830\n",
            "epoch165 train time: 4.484s test time: 0.502  loss = 0.766 val_mse = 1.184 mse = 1.295 mae = 0.831\n",
            "epoch166 train time: 4.477s test time: 0.484  loss = 0.769 val_mse = 1.191 mse = 1.291 mae = 0.829\n",
            "epoch167 train time: 4.483s test time: 0.485  loss = 0.771 val_mse = 1.183 mse = 1.288 mae = 0.831\n",
            "epoch168 train time: 4.494s test time: 0.489  loss = 0.770 val_mse = 1.189 mse = 1.291 mae = 0.831\n",
            "epoch169 train time: 4.482s test time: 0.485  loss = 0.772 val_mse = 1.182 mse = 1.293 mae = 0.833\n",
            "epoch170 train time: 4.503s test time: 0.490  loss = 0.768 val_mse = 1.190 mse = 1.286 mae = 0.828\n",
            "epoch171 train time: 4.489s test time: 0.490  loss = 0.769 val_mse = 1.182 mse = 1.291 mae = 0.831\n",
            "epoch172 train time: 4.500s test time: 0.491  loss = 0.770 val_mse = 1.185 mse = 1.290 mae = 0.830\n",
            "epoch173 train time: 4.484s test time: 0.491  loss = 0.769 val_mse = 1.179 mse = 1.288 mae = 0.831\n",
            "epoch174 train time: 4.499s test time: 0.491  loss = 0.768 val_mse = 1.184 mse = 1.286 mae = 0.831\n",
            "epoch175 train time: 4.490s test time: 0.507  loss = 0.771 val_mse = 1.190 mse = 1.293 mae = 0.829\n",
            "epoch176 train time: 4.577s test time: 0.488  loss = 0.772 val_mse = 1.188 mse = 1.285 mae = 0.829\n",
            "epoch177 train time: 4.474s test time: 0.495  loss = 0.773 val_mse = 1.184 mse = 1.292 mae = 0.830\n",
            "epoch178 train time: 4.495s test time: 0.496  loss = 0.769 val_mse = 1.191 mse = 1.291 mae = 0.829\n",
            "epoch179 train time: 4.472s test time: 0.492  loss = 0.771 val_mse = 1.186 mse = 1.290 mae = 0.829\n",
            "MAE 0.8371292235590698\n",
            "MSE 1.3096931666826137\n",
            "epoch0 train time: 5.383s test time: 0.545  loss = 42.247 val_mse = 1.587 mse = 1.520 mae = 0.883\n",
            "epoch1 train time: 4.872s test time: 0.540  loss = 30.313 val_mse = 1.572 mse = 1.510 mae = 0.880\n",
            "epoch2 train time: 4.890s test time: 0.559  loss = 16.138 val_mse = 1.550 mse = 1.495 mae = 0.876\n",
            "epoch3 train time: 4.931s test time: 0.558  loss = 7.875 val_mse = 1.529 mse = 1.479 mae = 0.871\n",
            "epoch4 train time: 4.937s test time: 0.548  loss = 3.717 val_mse = 1.507 mse = 1.464 mae = 0.867\n",
            "epoch5 train time: 4.913s test time: 0.550  loss = 1.955 val_mse = 1.486 mse = 1.450 mae = 0.864\n",
            "epoch6 train time: 4.890s test time: 0.548  loss = 1.394 val_mse = 1.465 mse = 1.436 mae = 0.861\n",
            "epoch7 train time: 4.872s test time: 0.544  loss = 1.278 val_mse = 1.444 mse = 1.423 mae = 0.858\n",
            "epoch8 train time: 4.870s test time: 0.548  loss = 1.236 val_mse = 1.424 mse = 1.409 mae = 0.856\n",
            "epoch9 train time: 4.865s test time: 0.550  loss = 1.199 val_mse = 1.403 mse = 1.396 mae = 0.855\n",
            "epoch10 train time: 4.855s test time: 0.548  loss = 1.164 val_mse = 1.382 mse = 1.383 mae = 0.853\n",
            "epoch11 train time: 4.861s test time: 0.559  loss = 1.133 val_mse = 1.362 mse = 1.371 mae = 0.852\n",
            "epoch12 train time: 4.875s test time: 0.552  loss = 1.104 val_mse = 1.338 mse = 1.357 mae = 0.852\n",
            "epoch13 train time: 4.864s test time: 0.539  loss = 1.077 val_mse = 1.318 mse = 1.346 mae = 0.850\n",
            "epoch14 train time: 4.865s test time: 0.550  loss = 1.051 val_mse = 1.306 mse = 1.340 mae = 0.847\n",
            "epoch15 train time: 4.859s test time: 0.544  loss = 1.026 val_mse = 1.295 mse = 1.334 mae = 0.844\n",
            "epoch16 train time: 4.885s test time: 0.550  loss = 1.003 val_mse = 1.285 mse = 1.330 mae = 0.841\n",
            "epoch17 train time: 4.877s test time: 0.547  loss = 0.981 val_mse = 1.271 mse = 1.322 mae = 0.839\n",
            "epoch18 train time: 4.882s test time: 0.548  loss = 0.962 val_mse = 1.260 mse = 1.316 mae = 0.838\n",
            "epoch19 train time: 4.870s test time: 0.547  loss = 0.944 val_mse = 1.250 mse = 1.312 mae = 0.838\n",
            "epoch20 train time: 4.873s test time: 0.552  loss = 0.929 val_mse = 1.241 mse = 1.308 mae = 0.838\n",
            "epoch21 train time: 4.872s test time: 0.554  loss = 0.915 val_mse = 1.244 mse = 1.310 mae = 0.836\n",
            "epoch22 train time: 4.867s test time: 0.551  loss = 0.905 val_mse = 1.242 mse = 1.315 mae = 0.834\n",
            "epoch23 train time: 4.874s test time: 0.547  loss = 0.894 val_mse = 1.220 mse = 1.302 mae = 0.838\n",
            "epoch24 train time: 4.867s test time: 0.542  loss = 0.888 val_mse = 1.217 mse = 1.296 mae = 0.842\n",
            "epoch25 train time: 4.870s test time: 0.564  loss = 0.882 val_mse = 1.218 mse = 1.300 mae = 0.841\n",
            "epoch26 train time: 4.879s test time: 0.553  loss = 0.879 val_mse = 1.214 mse = 1.296 mae = 0.844\n",
            "epoch27 train time: 4.877s test time: 0.553  loss = 0.878 val_mse = 1.231 mse = 1.310 mae = 0.836\n",
            "epoch28 train time: 4.875s test time: 0.548  loss = 0.873 val_mse = 1.206 mse = 1.293 mae = 0.842\n",
            "epoch29 train time: 4.874s test time: 0.558  loss = 0.866 val_mse = 1.204 mse = 1.297 mae = 0.848\n",
            "epoch30 train time: 4.986s test time: 0.546  loss = 0.863 val_mse = 1.207 mse = 1.300 mae = 0.843\n",
            "epoch31 train time: 4.893s test time: 0.548  loss = 0.856 val_mse = 1.209 mse = 1.303 mae = 0.844\n",
            "epoch32 train time: 4.854s test time: 0.544  loss = 0.860 val_mse = 1.217 mse = 1.303 mae = 0.843\n",
            "epoch33 train time: 4.876s test time: 0.548  loss = 0.854 val_mse = 1.187 mse = 1.289 mae = 0.846\n",
            "epoch34 train time: 4.868s test time: 0.555  loss = 0.854 val_mse = 1.209 mse = 1.305 mae = 0.845\n",
            "epoch35 train time: 4.865s test time: 0.540  loss = 0.852 val_mse = 1.227 mse = 1.311 mae = 0.832\n",
            "epoch36 train time: 4.878s test time: 0.553  loss = 0.846 val_mse = 1.217 mse = 1.311 mae = 0.843\n",
            "epoch37 train time: 4.875s test time: 0.545  loss = 0.847 val_mse = 1.225 mse = 1.309 mae = 0.842\n",
            "epoch38 train time: 4.859s test time: 0.558  loss = 0.843 val_mse = 1.235 mse = 1.326 mae = 0.839\n",
            "epoch39 train time: 4.861s test time: 0.557  loss = 0.844 val_mse = 1.244 mse = 1.309 mae = 0.831\n",
            "epoch40 train time: 4.859s test time: 0.550  loss = 0.850 val_mse = 1.245 mse = 1.322 mae = 0.832\n",
            "epoch41 train time: 4.868s test time: 0.548  loss = 0.840 val_mse = 1.241 mse = 1.315 mae = 0.831\n",
            "epoch42 train time: 4.864s test time: 0.544  loss = 0.835 val_mse = 1.239 mse = 1.323 mae = 0.835\n",
            "epoch43 train time: 4.876s test time: 0.544  loss = 0.833 val_mse = 1.248 mse = 1.321 mae = 0.832\n",
            "epoch44 train time: 4.865s test time: 0.549  loss = 0.835 val_mse = 1.233 mse = 1.315 mae = 0.836\n",
            "epoch45 train time: 4.874s test time: 0.551  loss = 0.834 val_mse = 1.234 mse = 1.311 mae = 0.831\n",
            "epoch46 train time: 4.872s test time: 0.541  loss = 0.830 val_mse = 1.234 mse = 1.328 mae = 0.840\n",
            "epoch47 train time: 4.874s test time: 0.548  loss = 0.832 val_mse = 1.240 mse = 1.313 mae = 0.831\n",
            "epoch48 train time: 4.876s test time: 0.550  loss = 0.829 val_mse = 1.226 mse = 1.322 mae = 0.835\n",
            "epoch49 train time: 4.887s test time: 0.552  loss = 0.824 val_mse = 1.232 mse = 1.311 mae = 0.832\n",
            "epoch50 train time: 4.885s test time: 0.548  loss = 0.827 val_mse = 1.238 mse = 1.331 mae = 0.838\n",
            "epoch51 train time: 4.871s test time: 0.551  loss = 0.825 val_mse = 1.243 mse = 1.315 mae = 0.827\n",
            "epoch52 train time: 4.888s test time: 0.555  loss = 0.820 val_mse = 1.242 mse = 1.344 mae = 0.839\n",
            "epoch53 train time: 4.875s test time: 0.548  loss = 0.831 val_mse = 1.249 mse = 1.318 mae = 0.830\n",
            "epoch54 train time: 4.876s test time: 0.556  loss = 0.824 val_mse = 1.227 mse = 1.319 mae = 0.835\n",
            "epoch55 train time: 4.873s test time: 0.543  loss = 0.820 val_mse = 1.237 mse = 1.310 mae = 0.833\n",
            "epoch56 train time: 4.873s test time: 0.552  loss = 0.824 val_mse = 1.228 mse = 1.327 mae = 0.839\n",
            "epoch57 train time: 4.866s test time: 0.547  loss = 0.821 val_mse = 1.230 mse = 1.306 mae = 0.832\n",
            "epoch58 train time: 4.875s test time: 0.549  loss = 0.820 val_mse = 1.219 mse = 1.318 mae = 0.835\n",
            "epoch59 train time: 4.885s test time: 0.550  loss = 0.821 val_mse = 1.236 mse = 1.308 mae = 0.833\n",
            "epoch60 train time: 4.873s test time: 0.553  loss = 0.822 val_mse = 1.215 mse = 1.311 mae = 0.837\n",
            "epoch61 train time: 4.872s test time: 0.550  loss = 0.818 val_mse = 1.231 mse = 1.311 mae = 0.831\n",
            "epoch62 train time: 4.884s test time: 0.545  loss = 0.818 val_mse = 1.218 mse = 1.318 mae = 0.837\n",
            "epoch63 train time: 4.873s test time: 0.555  loss = 0.819 val_mse = 1.233 mse = 1.307 mae = 0.830\n",
            "epoch64 train time: 4.872s test time: 0.548  loss = 0.816 val_mse = 1.220 mse = 1.317 mae = 0.839\n",
            "epoch65 train time: 4.862s test time: 0.556  loss = 0.818 val_mse = 1.220 mse = 1.308 mae = 0.832\n",
            "epoch66 train time: 4.879s test time: 0.548  loss = 0.810 val_mse = 1.222 mse = 1.313 mae = 0.835\n",
            "epoch67 train time: 4.872s test time: 0.557  loss = 0.819 val_mse = 1.228 mse = 1.307 mae = 0.832\n",
            "epoch68 train time: 4.878s test time: 0.548  loss = 0.814 val_mse = 1.226 mse = 1.316 mae = 0.832\n",
            "epoch69 train time: 4.864s test time: 0.545  loss = 0.811 val_mse = 1.223 mse = 1.307 mae = 0.832\n",
            "epoch70 train time: 4.876s test time: 0.553  loss = 0.808 val_mse = 1.225 mse = 1.328 mae = 0.836\n",
            "epoch71 train time: 4.876s test time: 0.551  loss = 0.807 val_mse = 1.222 mse = 1.304 mae = 0.829\n",
            "epoch72 train time: 4.871s test time: 0.544  loss = 0.804 val_mse = 1.210 mse = 1.305 mae = 0.836\n",
            "epoch73 train time: 4.881s test time: 0.546  loss = 0.809 val_mse = 1.211 mse = 1.298 mae = 0.835\n",
            "epoch74 train time: 4.865s test time: 0.546  loss = 0.803 val_mse = 1.215 mse = 1.319 mae = 0.840\n",
            "epoch75 train time: 4.870s test time: 0.537  loss = 0.806 val_mse = 1.223 mse = 1.309 mae = 0.829\n",
            "epoch76 train time: 4.867s test time: 0.557  loss = 0.802 val_mse = 1.219 mse = 1.315 mae = 0.834\n",
            "epoch77 train time: 4.863s test time: 0.547  loss = 0.805 val_mse = 1.223 mse = 1.298 mae = 0.831\n",
            "epoch78 train time: 4.868s test time: 0.551  loss = 0.802 val_mse = 1.209 mse = 1.316 mae = 0.836\n",
            "epoch79 train time: 4.855s test time: 0.548  loss = 0.801 val_mse = 1.216 mse = 1.301 mae = 0.835\n",
            "epoch80 train time: 4.859s test time: 0.547  loss = 0.807 val_mse = 1.208 mse = 1.316 mae = 0.839\n",
            "epoch81 train time: 4.871s test time: 0.551  loss = 0.801 val_mse = 1.219 mse = 1.302 mae = 0.831\n",
            "epoch82 train time: 4.857s test time: 0.541  loss = 0.799 val_mse = 1.203 mse = 1.309 mae = 0.836\n",
            "epoch83 train time: 4.872s test time: 0.554  loss = 0.804 val_mse = 1.216 mse = 1.304 mae = 0.835\n",
            "epoch84 train time: 4.877s test time: 0.547  loss = 0.802 val_mse = 1.207 mse = 1.314 mae = 0.838\n",
            "epoch85 train time: 4.865s test time: 0.557  loss = 0.802 val_mse = 1.220 mse = 1.306 mae = 0.830\n",
            "epoch86 train time: 4.858s test time: 0.551  loss = 0.797 val_mse = 1.214 mse = 1.317 mae = 0.837\n",
            "epoch87 train time: 4.863s test time: 0.551  loss = 0.801 val_mse = 1.217 mse = 1.297 mae = 0.830\n",
            "epoch88 train time: 4.867s test time: 0.551  loss = 0.795 val_mse = 1.207 mse = 1.312 mae = 0.839\n",
            "epoch89 train time: 4.881s test time: 0.550  loss = 0.796 val_mse = 1.211 mse = 1.297 mae = 0.829\n",
            "epoch90 train time: 4.857s test time: 0.542  loss = 0.794 val_mse = 1.204 mse = 1.307 mae = 0.836\n",
            "epoch91 train time: 4.865s test time: 0.552  loss = 0.796 val_mse = 1.212 mse = 1.301 mae = 0.832\n",
            "epoch92 train time: 4.872s test time: 0.562  loss = 0.794 val_mse = 1.206 mse = 1.314 mae = 0.837\n",
            "epoch93 train time: 4.877s test time: 0.539  loss = 0.795 val_mse = 1.218 mse = 1.308 mae = 0.832\n",
            "epoch94 train time: 4.871s test time: 0.557  loss = 0.794 val_mse = 1.204 mse = 1.316 mae = 0.841\n",
            "epoch95 train time: 4.860s test time: 0.540  loss = 0.795 val_mse = 1.213 mse = 1.301 mae = 0.829\n",
            "epoch96 train time: 4.866s test time: 0.563  loss = 0.791 val_mse = 1.200 mse = 1.311 mae = 0.836\n",
            "epoch97 train time: 4.873s test time: 0.545  loss = 0.794 val_mse = 1.207 mse = 1.295 mae = 0.831\n",
            "epoch98 train time: 4.862s test time: 0.547  loss = 0.792 val_mse = 1.205 mse = 1.304 mae = 0.834\n",
            "epoch99 train time: 4.871s test time: 0.548  loss = 0.797 val_mse = 1.212 mse = 1.293 mae = 0.828\n",
            "epoch100 train time: 4.876s test time: 0.549  loss = 0.794 val_mse = 1.196 mse = 1.305 mae = 0.833\n",
            "epoch101 train time: 4.864s test time: 0.552  loss = 0.793 val_mse = 1.208 mse = 1.293 mae = 0.831\n",
            "epoch102 train time: 4.866s test time: 0.549  loss = 0.792 val_mse = 1.201 mse = 1.305 mae = 0.835\n",
            "epoch103 train time: 4.874s test time: 0.543  loss = 0.794 val_mse = 1.213 mse = 1.297 mae = 0.832\n",
            "epoch104 train time: 4.876s test time: 0.544  loss = 0.792 val_mse = 1.199 mse = 1.313 mae = 0.834\n",
            "epoch105 train time: 4.876s test time: 0.549  loss = 0.788 val_mse = 1.209 mse = 1.291 mae = 0.825\n",
            "epoch106 train time: 4.862s test time: 0.546  loss = 0.792 val_mse = 1.201 mse = 1.302 mae = 0.834\n",
            "epoch107 train time: 4.873s test time: 0.553  loss = 0.792 val_mse = 1.212 mse = 1.302 mae = 0.827\n",
            "epoch108 train time: 4.864s test time: 0.550  loss = 0.791 val_mse = 1.203 mse = 1.307 mae = 0.837\n",
            "epoch109 train time: 4.883s test time: 0.549  loss = 0.791 val_mse = 1.198 mse = 1.284 mae = 0.831\n",
            "epoch110 train time: 4.865s test time: 0.543  loss = 0.790 val_mse = 1.197 mse = 1.301 mae = 0.833\n",
            "epoch111 train time: 4.881s test time: 0.546  loss = 0.788 val_mse = 1.208 mse = 1.293 mae = 0.829\n",
            "epoch112 train time: 4.862s test time: 0.547  loss = 0.790 val_mse = 1.200 mse = 1.302 mae = 0.835\n",
            "epoch113 train time: 4.864s test time: 0.540  loss = 0.790 val_mse = 1.202 mse = 1.292 mae = 0.831\n",
            "epoch114 train time: 4.869s test time: 0.554  loss = 0.788 val_mse = 1.199 mse = 1.306 mae = 0.833\n",
            "epoch115 train time: 4.869s test time: 0.548  loss = 0.790 val_mse = 1.206 mse = 1.298 mae = 0.828\n",
            "epoch116 train time: 4.873s test time: 0.552  loss = 0.787 val_mse = 1.194 mse = 1.302 mae = 0.835\n",
            "epoch117 train time: 4.871s test time: 0.547  loss = 0.788 val_mse = 1.197 mse = 1.289 mae = 0.830\n",
            "epoch118 train time: 4.870s test time: 0.558  loss = 0.790 val_mse = 1.196 mse = 1.303 mae = 0.834\n",
            "epoch119 train time: 4.861s test time: 0.546  loss = 0.789 val_mse = 1.198 mse = 1.298 mae = 0.832\n",
            "epoch120 train time: 4.883s test time: 0.551  loss = 0.787 val_mse = 1.200 mse = 1.302 mae = 0.833\n",
            "epoch121 train time: 4.867s test time: 0.539  loss = 0.790 val_mse = 1.200 mse = 1.290 mae = 0.828\n",
            "epoch122 train time: 4.866s test time: 0.551  loss = 0.786 val_mse = 1.192 mse = 1.303 mae = 0.834\n",
            "epoch123 train time: 4.870s test time: 0.548  loss = 0.785 val_mse = 1.196 mse = 1.288 mae = 0.828\n",
            "epoch124 train time: 4.863s test time: 0.552  loss = 0.789 val_mse = 1.192 mse = 1.301 mae = 0.835\n",
            "epoch125 train time: 4.852s test time: 0.550  loss = 0.788 val_mse = 1.195 mse = 1.289 mae = 0.827\n",
            "epoch126 train time: 4.866s test time: 0.544  loss = 0.784 val_mse = 1.194 mse = 1.301 mae = 0.833\n",
            "epoch127 train time: 4.869s test time: 0.545  loss = 0.787 val_mse = 1.200 mse = 1.290 mae = 0.828\n",
            "epoch128 train time: 4.864s test time: 0.544  loss = 0.785 val_mse = 1.195 mse = 1.302 mae = 0.832\n",
            "epoch129 train time: 4.869s test time: 0.551  loss = 0.788 val_mse = 1.201 mse = 1.289 mae = 0.830\n",
            "epoch130 train time: 4.865s test time: 0.545  loss = 0.789 val_mse = 1.198 mse = 1.303 mae = 0.832\n",
            "epoch131 train time: 4.865s test time: 0.547  loss = 0.788 val_mse = 1.198 mse = 1.288 mae = 0.828\n",
            "epoch132 train time: 4.886s test time: 0.548  loss = 0.783 val_mse = 1.190 mse = 1.295 mae = 0.832\n",
            "epoch133 train time: 4.868s test time: 0.557  loss = 0.785 val_mse = 1.202 mse = 1.292 mae = 0.830\n",
            "epoch134 train time: 4.863s test time: 0.550  loss = 0.784 val_mse = 1.192 mse = 1.303 mae = 0.831\n",
            "epoch135 train time: 4.871s test time: 0.544  loss = 0.784 val_mse = 1.195 mse = 1.288 mae = 0.826\n",
            "epoch136 train time: 4.870s test time: 0.547  loss = 0.782 val_mse = 1.193 mse = 1.295 mae = 0.831\n",
            "epoch137 train time: 4.872s test time: 0.541  loss = 0.781 val_mse = 1.193 mse = 1.282 mae = 0.829\n",
            "epoch138 train time: 4.864s test time: 0.552  loss = 0.784 val_mse = 1.184 mse = 1.296 mae = 0.832\n",
            "epoch139 train time: 4.861s test time: 0.546  loss = 0.784 val_mse = 1.196 mse = 1.287 mae = 0.827\n",
            "epoch140 train time: 4.864s test time: 0.552  loss = 0.780 val_mse = 1.192 mse = 1.293 mae = 0.831\n",
            "epoch141 train time: 4.861s test time: 0.551  loss = 0.779 val_mse = 1.197 mse = 1.288 mae = 0.828\n",
            "epoch142 train time: 4.869s test time: 0.548  loss = 0.783 val_mse = 1.191 mse = 1.299 mae = 0.832\n",
            "epoch143 train time: 4.877s test time: 0.549  loss = 0.782 val_mse = 1.201 mse = 1.292 mae = 0.829\n",
            "epoch144 train time: 4.875s test time: 0.551  loss = 0.781 val_mse = 1.194 mse = 1.303 mae = 0.831\n",
            "epoch145 train time: 4.866s test time: 0.548  loss = 0.779 val_mse = 1.191 mse = 1.285 mae = 0.829\n",
            "epoch146 train time: 4.859s test time: 0.545  loss = 0.781 val_mse = 1.191 mse = 1.297 mae = 0.832\n",
            "epoch147 train time: 4.888s test time: 0.552  loss = 0.782 val_mse = 1.195 mse = 1.287 mae = 0.826\n",
            "epoch148 train time: 4.864s test time: 0.545  loss = 0.778 val_mse = 1.192 mse = 1.303 mae = 0.830\n",
            "epoch149 train time: 4.861s test time: 0.549  loss = 0.779 val_mse = 1.194 mse = 1.293 mae = 0.829\n",
            "epoch150 train time: 4.866s test time: 0.553  loss = 0.777 val_mse = 1.186 mse = 1.296 mae = 0.830\n",
            "epoch151 train time: 4.873s test time: 0.549  loss = 0.779 val_mse = 1.189 mse = 1.282 mae = 0.826\n",
            "epoch152 train time: 4.879s test time: 0.543  loss = 0.778 val_mse = 1.183 mse = 1.292 mae = 0.830\n",
            "epoch153 train time: 4.893s test time: 0.549  loss = 0.778 val_mse = 1.198 mse = 1.295 mae = 0.828\n",
            "epoch154 train time: 4.868s test time: 0.544  loss = 0.777 val_mse = 1.191 mse = 1.297 mae = 0.831\n",
            "epoch155 train time: 4.872s test time: 0.550  loss = 0.780 val_mse = 1.187 mse = 1.282 mae = 0.828\n",
            "epoch156 train time: 4.856s test time: 0.549  loss = 0.776 val_mse = 1.190 mse = 1.299 mae = 0.830\n",
            "epoch157 train time: 4.871s test time: 0.555  loss = 0.779 val_mse = 1.195 mse = 1.283 mae = 0.825\n",
            "epoch158 train time: 4.880s test time: 0.552  loss = 0.779 val_mse = 1.183 mse = 1.300 mae = 0.832\n",
            "epoch159 train time: 4.871s test time: 0.548  loss = 0.776 val_mse = 1.194 mse = 1.291 mae = 0.825\n",
            "epoch160 train time: 4.871s test time: 0.553  loss = 0.776 val_mse = 1.190 mse = 1.291 mae = 0.830\n",
            "epoch161 train time: 4.860s test time: 0.546  loss = 0.777 val_mse = 1.183 mse = 1.284 mae = 0.829\n",
            "epoch162 train time: 4.867s test time: 0.548  loss = 0.776 val_mse = 1.193 mse = 1.296 mae = 0.827\n",
            "epoch163 train time: 4.882s test time: 0.546  loss = 0.781 val_mse = 1.194 mse = 1.283 mae = 0.825\n",
            "epoch164 train time: 4.861s test time: 0.547  loss = 0.777 val_mse = 1.188 mse = 1.294 mae = 0.830\n",
            "epoch165 train time: 4.881s test time: 0.549  loss = 0.776 val_mse = 1.187 mse = 1.287 mae = 0.829\n",
            "epoch166 train time: 4.859s test time: 0.557  loss = 0.776 val_mse = 1.188 mse = 1.298 mae = 0.831\n",
            "epoch167 train time: 4.873s test time: 0.545  loss = 0.779 val_mse = 1.193 mse = 1.285 mae = 0.828\n",
            "epoch168 train time: 4.875s test time: 0.557  loss = 0.778 val_mse = 1.183 mse = 1.295 mae = 0.831\n",
            "epoch169 train time: 4.880s test time: 0.555  loss = 0.776 val_mse = 1.191 mse = 1.282 mae = 0.825\n",
            "epoch170 train time: 4.876s test time: 0.544  loss = 0.777 val_mse = 1.184 mse = 1.295 mae = 0.830\n",
            "epoch171 train time: 4.874s test time: 0.549  loss = 0.777 val_mse = 1.190 mse = 1.283 mae = 0.825\n",
            "epoch172 train time: 4.862s test time: 0.549  loss = 0.777 val_mse = 1.191 mse = 1.294 mae = 0.829\n",
            "epoch173 train time: 4.866s test time: 0.551  loss = 0.775 val_mse = 1.186 mse = 1.286 mae = 0.828\n",
            "epoch174 train time: 4.861s test time: 0.542  loss = 0.778 val_mse = 1.178 mse = 1.295 mae = 0.831\n",
            "epoch175 train time: 4.883s test time: 0.559  loss = 0.775 val_mse = 1.186 mse = 1.281 mae = 0.826\n",
            "epoch176 train time: 4.875s test time: 0.547  loss = 0.776 val_mse = 1.187 mse = 1.295 mae = 0.833\n",
            "epoch177 train time: 4.870s test time: 0.544  loss = 0.778 val_mse = 1.195 mse = 1.284 mae = 0.824\n",
            "epoch178 train time: 4.869s test time: 0.547  loss = 0.774 val_mse = 1.188 mse = 1.292 mae = 0.829\n",
            "epoch179 train time: 4.863s test time: 0.551  loss = 0.777 val_mse = 1.187 mse = 1.284 mae = 0.828\n",
            "MAE 0.8353306033873166\n",
            "MSE 1.3127908019585943\n",
            "epoch0 train time: 8.777s test time: 0.899  loss = 68.605 val_mse = 1.577 mse = 1.518 mae = 0.884\n",
            "epoch1 train time: 6.549s test time: 0.777  loss = 48.599 val_mse = 1.559 mse = 1.504 mae = 0.878\n",
            "epoch2 train time: 7.853s test time: 0.874  loss = 25.411 val_mse = 1.532 mse = 1.483 mae = 0.872\n",
            "epoch3 train time: 7.351s test time: 0.867  loss = 11.908 val_mse = 1.508 mse = 1.465 mae = 0.867\n",
            "epoch4 train time: 7.115s test time: 1.039  loss = 5.143 val_mse = 1.484 mse = 1.448 mae = 0.864\n",
            "epoch5 train time: 7.128s test time: 0.921  loss = 2.295 val_mse = 1.461 mse = 1.432 mae = 0.861\n",
            "epoch6 train time: 6.739s test time: 0.747  loss = 1.405 val_mse = 1.437 mse = 1.416 mae = 0.858\n",
            "epoch7 train time: 6.519s test time: 0.747  loss = 1.242 val_mse = 1.414 mse = 1.401 mae = 0.857\n",
            "epoch8 train time: 6.510s test time: 0.749  loss = 1.201 val_mse = 1.391 mse = 1.387 mae = 0.855\n",
            "epoch9 train time: 6.514s test time: 0.747  loss = 1.166 val_mse = 1.363 mse = 1.368 mae = 0.857\n",
            "epoch10 train time: 6.524s test time: 0.748  loss = 1.134 val_mse = 1.340 mse = 1.355 mae = 0.856\n",
            "epoch11 train time: 6.544s test time: 0.748  loss = 1.103 val_mse = 1.322 mse = 1.345 mae = 0.854\n",
            "epoch12 train time: 6.556s test time: 0.754  loss = 1.075 val_mse = 1.308 mse = 1.339 mae = 0.850\n",
            "epoch13 train time: 6.550s test time: 0.748  loss = 1.049 val_mse = 1.296 mse = 1.333 mae = 0.847\n",
            "epoch14 train time: 6.554s test time: 0.748  loss = 1.024 val_mse = 1.285 mse = 1.328 mae = 0.844\n",
            "epoch15 train time: 6.549s test time: 0.750  loss = 1.001 val_mse = 1.274 mse = 1.324 mae = 0.841\n",
            "epoch16 train time: 6.539s test time: 0.749  loss = 0.979 val_mse = 1.258 mse = 1.314 mae = 0.839\n",
            "epoch17 train time: 6.517s test time: 0.747  loss = 0.959 val_mse = 1.256 mse = 1.317 mae = 0.838\n",
            "epoch18 train time: 6.628s test time: 0.753  loss = 0.943 val_mse = 1.238 mse = 1.304 mae = 0.838\n",
            "epoch19 train time: 6.529s test time: 0.748  loss = 0.928 val_mse = 1.234 mse = 1.304 mae = 0.839\n",
            "epoch20 train time: 6.539s test time: 0.749  loss = 0.916 val_mse = 1.227 mse = 1.300 mae = 0.838\n",
            "epoch21 train time: 6.538s test time: 0.743  loss = 0.905 val_mse = 1.224 mse = 1.301 mae = 0.842\n",
            "epoch22 train time: 6.543s test time: 0.747  loss = 0.895 val_mse = 1.222 mse = 1.299 mae = 0.840\n",
            "epoch23 train time: 6.525s test time: 0.747  loss = 0.887 val_mse = 1.213 mse = 1.302 mae = 0.846\n",
            "epoch24 train time: 6.510s test time: 0.745  loss = 0.883 val_mse = 1.207 mse = 1.294 mae = 0.841\n",
            "epoch25 train time: 6.523s test time: 0.745  loss = 0.877 val_mse = 1.206 mse = 1.295 mae = 0.849\n",
            "epoch26 train time: 6.521s test time: 0.757  loss = 0.873 val_mse = 1.203 mse = 1.298 mae = 0.843\n",
            "epoch27 train time: 6.507s test time: 0.746  loss = 0.870 val_mse = 1.205 mse = 1.298 mae = 0.849\n",
            "epoch28 train time: 6.518s test time: 0.749  loss = 0.869 val_mse = 1.206 mse = 1.300 mae = 0.842\n",
            "epoch29 train time: 6.519s test time: 0.749  loss = 0.860 val_mse = 1.200 mse = 1.302 mae = 0.852\n",
            "epoch30 train time: 6.514s test time: 0.746  loss = 0.865 val_mse = 1.212 mse = 1.308 mae = 0.843\n",
            "epoch31 train time: 6.524s test time: 0.745  loss = 0.862 val_mse = 1.225 mse = 1.303 mae = 0.841\n",
            "epoch32 train time: 6.640s test time: 0.755  loss = 0.859 val_mse = 1.218 mse = 1.298 mae = 0.839\n",
            "epoch33 train time: 6.523s test time: 0.751  loss = 0.854 val_mse = 1.213 mse = 1.304 mae = 0.843\n",
            "epoch34 train time: 6.532s test time: 0.749  loss = 0.850 val_mse = 1.216 mse = 1.298 mae = 0.843\n",
            "epoch35 train time: 6.541s test time: 0.753  loss = 0.845 val_mse = 1.211 mse = 1.298 mae = 0.845\n",
            "epoch36 train time: 6.520s test time: 0.749  loss = 0.847 val_mse = 1.207 mse = 1.310 mae = 0.847\n",
            "epoch37 train time: 6.517s test time: 0.750  loss = 0.842 val_mse = 1.214 mse = 1.300 mae = 0.845\n",
            "epoch38 train time: 6.529s test time: 0.749  loss = 0.844 val_mse = 1.203 mse = 1.298 mae = 0.843\n",
            "epoch39 train time: 6.518s test time: 0.744  loss = 0.835 val_mse = 1.206 mse = 1.307 mae = 0.845\n",
            "epoch40 train time: 6.513s test time: 0.749  loss = 0.835 val_mse = 1.213 mse = 1.302 mae = 0.838\n",
            "epoch41 train time: 6.510s test time: 0.750  loss = 0.830 val_mse = 1.210 mse = 1.301 mae = 0.844\n",
            "epoch42 train time: 6.526s test time: 0.744  loss = 0.830 val_mse = 1.209 mse = 1.313 mae = 0.842\n",
            "epoch43 train time: 6.511s test time: 0.753  loss = 0.827 val_mse = 1.213 mse = 1.309 mae = 0.843\n",
            "epoch44 train time: 6.523s test time: 0.743  loss = 0.828 val_mse = 1.198 mse = 1.298 mae = 0.841\n",
            "epoch45 train time: 6.525s test time: 0.746  loss = 0.825 val_mse = 1.211 mse = 1.305 mae = 0.840\n",
            "epoch46 train time: 6.672s test time: 0.751  loss = 0.831 val_mse = 1.243 mse = 1.320 mae = 0.831\n",
            "epoch47 train time: 6.528s test time: 0.750  loss = 0.828 val_mse = 1.223 mse = 1.316 mae = 0.836\n",
            "epoch48 train time: 6.534s test time: 0.753  loss = 0.821 val_mse = 1.236 mse = 1.320 mae = 0.833\n",
            "epoch49 train time: 6.545s test time: 0.746  loss = 0.824 val_mse = 1.236 mse = 1.321 mae = 0.834\n",
            "epoch50 train time: 6.576s test time: 0.749  loss = 0.818 val_mse = 1.241 mse = 1.330 mae = 0.833\n",
            "epoch51 train time: 6.547s test time: 0.749  loss = 0.822 val_mse = 1.244 mse = 1.329 mae = 0.835\n",
            "epoch52 train time: 6.535s test time: 0.752  loss = 0.831 val_mse = 1.248 mse = 1.321 mae = 0.831\n",
            "epoch53 train time: 6.554s test time: 0.755  loss = 0.825 val_mse = 1.211 mse = 1.311 mae = 0.844\n",
            "epoch54 train time: 6.552s test time: 0.746  loss = 0.819 val_mse = 1.208 mse = 1.310 mae = 0.837\n",
            "epoch55 train time: 6.534s test time: 0.756  loss = 0.814 val_mse = 1.218 mse = 1.304 mae = 0.840\n",
            "epoch56 train time: 6.531s test time: 0.742  loss = 0.819 val_mse = 1.215 mse = 1.301 mae = 0.840\n",
            "epoch57 train time: 6.524s test time: 0.752  loss = 0.824 val_mse = 1.216 mse = 1.306 mae = 0.843\n",
            "epoch58 train time: 6.529s test time: 0.744  loss = 0.812 val_mse = 1.206 mse = 1.298 mae = 0.836\n",
            "epoch59 train time: 6.515s test time: 0.747  loss = 0.814 val_mse = 1.206 mse = 1.299 mae = 0.842\n",
            "epoch60 train time: 6.537s test time: 0.744  loss = 0.812 val_mse = 1.211 mse = 1.310 mae = 0.840\n",
            "epoch61 train time: 6.511s test time: 0.748  loss = 0.818 val_mse = 1.217 mse = 1.302 mae = 0.846\n",
            "epoch62 train time: 6.515s test time: 0.751  loss = 0.824 val_mse = 1.202 mse = 1.298 mae = 0.836\n",
            "epoch63 train time: 6.515s test time: 0.745  loss = 0.813 val_mse = 1.207 mse = 1.308 mae = 0.840\n",
            "epoch64 train time: 6.522s test time: 0.751  loss = 0.809 val_mse = 1.205 mse = 1.302 mae = 0.838\n",
            "epoch65 train time: 6.519s test time: 0.755  loss = 0.812 val_mse = 1.210 mse = 1.305 mae = 0.843\n",
            "epoch66 train time: 6.514s test time: 0.756  loss = 0.809 val_mse = 1.212 mse = 1.305 mae = 0.836\n",
            "epoch67 train time: 6.521s test time: 0.744  loss = 0.805 val_mse = 1.202 mse = 1.296 mae = 0.840\n",
            "epoch68 train time: 6.507s test time: 0.746  loss = 0.804 val_mse = 1.208 mse = 1.302 mae = 0.838\n",
            "epoch69 train time: 6.520s test time: 0.747  loss = 0.808 val_mse = 1.208 mse = 1.303 mae = 0.843\n",
            "epoch70 train time: 6.523s test time: 0.755  loss = 0.808 val_mse = 1.201 mse = 1.302 mae = 0.835\n",
            "epoch71 train time: 6.514s test time: 0.749  loss = 0.801 val_mse = 1.207 mse = 1.301 mae = 0.842\n",
            "epoch72 train time: 6.516s test time: 0.755  loss = 0.805 val_mse = 1.206 mse = 1.305 mae = 0.835\n",
            "epoch73 train time: 6.516s test time: 0.745  loss = 0.804 val_mse = 1.197 mse = 1.295 mae = 0.840\n",
            "epoch74 train time: 6.511s test time: 0.750  loss = 0.804 val_mse = 1.206 mse = 1.294 mae = 0.832\n",
            "epoch75 train time: 6.513s test time: 0.751  loss = 0.801 val_mse = 1.205 mse = 1.302 mae = 0.839\n",
            "epoch76 train time: 6.524s test time: 0.747  loss = 0.799 val_mse = 1.205 mse = 1.302 mae = 0.835\n",
            "epoch77 train time: 6.521s test time: 0.752  loss = 0.799 val_mse = 1.201 mse = 1.302 mae = 0.843\n",
            "epoch78 train time: 6.516s test time: 0.747  loss = 0.799 val_mse = 1.200 mse = 1.298 mae = 0.836\n",
            "epoch79 train time: 6.511s test time: 0.750  loss = 0.798 val_mse = 1.198 mse = 1.298 mae = 0.842\n",
            "epoch80 train time: 6.525s test time: 0.756  loss = 0.798 val_mse = 1.205 mse = 1.302 mae = 0.836\n",
            "epoch81 train time: 6.517s test time: 0.749  loss = 0.799 val_mse = 1.198 mse = 1.293 mae = 0.839\n",
            "epoch82 train time: 6.513s test time: 0.752  loss = 0.798 val_mse = 1.200 mse = 1.298 mae = 0.835\n",
            "epoch83 train time: 6.529s test time: 0.748  loss = 0.800 val_mse = 1.195 mse = 1.295 mae = 0.840\n",
            "epoch84 train time: 6.520s test time: 0.747  loss = 0.794 val_mse = 1.197 mse = 1.301 mae = 0.836\n",
            "epoch85 train time: 6.523s test time: 0.744  loss = 0.798 val_mse = 1.194 mse = 1.299 mae = 0.844\n",
            "epoch86 train time: 6.544s test time: 0.757  loss = 0.796 val_mse = 1.225 mse = 1.319 mae = 0.827\n",
            "epoch87 train time: 6.520s test time: 0.747  loss = 0.793 val_mse = 1.200 mse = 1.297 mae = 0.842\n",
            "epoch88 train time: 6.529s test time: 0.750  loss = 0.793 val_mse = 1.189 mse = 1.298 mae = 0.839\n",
            "epoch89 train time: 6.544s test time: 0.746  loss = 0.792 val_mse = 1.213 mse = 1.305 mae = 0.836\n",
            "epoch90 train time: 6.533s test time: 0.749  loss = 0.797 val_mse = 1.203 mse = 1.292 mae = 0.835\n",
            "epoch91 train time: 6.512s test time: 0.744  loss = 0.795 val_mse = 1.200 mse = 1.298 mae = 0.838\n",
            "epoch92 train time: 6.521s test time: 0.751  loss = 0.791 val_mse = 1.199 mse = 1.305 mae = 0.837\n",
            "epoch93 train time: 6.513s test time: 0.747  loss = 0.794 val_mse = 1.194 mse = 1.293 mae = 0.841\n",
            "epoch94 train time: 6.515s test time: 0.745  loss = 0.792 val_mse = 1.192 mse = 1.296 mae = 0.836\n",
            "epoch95 train time: 6.534s test time: 0.746  loss = 0.791 val_mse = 1.193 mse = 1.299 mae = 0.837\n",
            "epoch96 train time: 6.537s test time: 0.748  loss = 0.785 val_mse = 1.200 mse = 1.300 mae = 0.834\n",
            "epoch97 train time: 6.517s test time: 0.746  loss = 0.792 val_mse = 1.202 mse = 1.294 mae = 0.836\n",
            "epoch98 train time: 6.507s test time: 0.744  loss = 0.789 val_mse = 1.191 mse = 1.295 mae = 0.833\n",
            "epoch99 train time: 6.519s test time: 0.751  loss = 0.786 val_mse = 1.194 mse = 1.294 mae = 0.836\n",
            "epoch100 train time: 6.519s test time: 0.743  loss = 0.789 val_mse = 1.198 mse = 1.300 mae = 0.835\n",
            "epoch101 train time: 6.527s test time: 0.749  loss = 0.785 val_mse = 1.191 mse = 1.294 mae = 0.837\n",
            "epoch102 train time: 6.520s test time: 0.744  loss = 0.784 val_mse = 1.199 mse = 1.308 mae = 0.840\n",
            "epoch103 train time: 6.516s test time: 0.748  loss = 0.790 val_mse = 1.200 mse = 1.300 mae = 0.838\n",
            "epoch104 train time: 6.515s test time: 0.750  loss = 0.787 val_mse = 1.190 mse = 1.294 mae = 0.832\n",
            "epoch105 train time: 6.523s test time: 0.746  loss = 0.785 val_mse = 1.194 mse = 1.294 mae = 0.837\n",
            "epoch106 train time: 6.520s test time: 0.749  loss = 0.783 val_mse = 1.190 mse = 1.291 mae = 0.833\n",
            "epoch107 train time: 6.516s test time: 0.745  loss = 0.784 val_mse = 1.190 mse = 1.291 mae = 0.834\n",
            "epoch108 train time: 6.508s test time: 0.748  loss = 0.788 val_mse = 1.197 mse = 1.293 mae = 0.830\n",
            "epoch109 train time: 6.514s test time: 0.749  loss = 0.783 val_mse = 1.187 mse = 1.291 mae = 0.835\n",
            "epoch110 train time: 6.520s test time: 0.750  loss = 0.780 val_mse = 1.191 mse = 1.298 mae = 0.836\n",
            "epoch111 train time: 6.513s test time: 0.748  loss = 0.785 val_mse = 1.192 mse = 1.292 mae = 0.834\n",
            "epoch112 train time: 6.511s test time: 0.744  loss = 0.782 val_mse = 1.188 mse = 1.295 mae = 0.832\n",
            "epoch113 train time: 6.508s test time: 0.745  loss = 0.782 val_mse = 1.189 mse = 1.296 mae = 0.835\n",
            "epoch114 train time: 6.509s test time: 0.745  loss = 0.783 val_mse = 1.191 mse = 1.294 mae = 0.830\n",
            "epoch115 train time: 6.524s test time: 0.742  loss = 0.780 val_mse = 1.192 mse = 1.293 mae = 0.834\n",
            "epoch116 train time: 6.500s test time: 0.743  loss = 0.780 val_mse = 1.191 mse = 1.297 mae = 0.830\n",
            "epoch117 train time: 6.512s test time: 0.747  loss = 0.784 val_mse = 1.193 mse = 1.290 mae = 0.835\n",
            "epoch118 train time: 6.515s test time: 0.755  loss = 0.780 val_mse = 1.192 mse = 1.288 mae = 0.829\n",
            "epoch119 train time: 6.518s test time: 0.744  loss = 0.779 val_mse = 1.189 mse = 1.292 mae = 0.833\n",
            "epoch120 train time: 6.530s test time: 0.746  loss = 0.781 val_mse = 1.197 mse = 1.294 mae = 0.829\n",
            "epoch121 train time: 6.519s test time: 0.745  loss = 0.780 val_mse = 1.194 mse = 1.290 mae = 0.832\n",
            "epoch122 train time: 6.523s test time: 0.753  loss = 0.779 val_mse = 1.188 mse = 1.293 mae = 0.832\n",
            "epoch123 train time: 6.532s test time: 0.749  loss = 0.781 val_mse = 1.183 mse = 1.294 mae = 0.837\n",
            "epoch124 train time: 6.532s test time: 0.750  loss = 0.781 val_mse = 1.184 mse = 1.291 mae = 0.832\n",
            "epoch125 train time: 6.511s test time: 0.745  loss = 0.779 val_mse = 1.184 mse = 1.293 mae = 0.833\n",
            "epoch126 train time: 6.545s test time: 0.752  loss = 0.781 val_mse = 1.193 mse = 1.290 mae = 0.829\n",
            "epoch127 train time: 6.564s test time: 0.757  loss = 0.780 val_mse = 1.181 mse = 1.287 mae = 0.833\n",
            "epoch128 train time: 6.554s test time: 0.757  loss = 0.781 val_mse = 1.193 mse = 1.295 mae = 0.833\n",
            "epoch129 train time: 6.538s test time: 0.751  loss = 0.779 val_mse = 1.191 mse = 1.291 mae = 0.833\n",
            "epoch130 train time: 6.545s test time: 0.749  loss = 0.782 val_mse = 1.189 mse = 1.290 mae = 0.830\n",
            "epoch131 train time: 6.552s test time: 0.750  loss = 0.780 val_mse = 1.187 mse = 1.295 mae = 0.834\n",
            "epoch132 train time: 6.528s test time: 0.749  loss = 0.781 val_mse = 1.187 mse = 1.292 mae = 0.831\n",
            "epoch133 train time: 6.547s test time: 0.752  loss = 0.778 val_mse = 1.186 mse = 1.290 mae = 0.834\n",
            "epoch134 train time: 6.527s test time: 0.744  loss = 0.779 val_mse = 1.192 mse = 1.292 mae = 0.831\n",
            "epoch135 train time: 6.542s test time: 0.745  loss = 0.779 val_mse = 1.183 mse = 1.288 mae = 0.835\n",
            "epoch136 train time: 6.521s test time: 0.747  loss = 0.780 val_mse = 1.194 mse = 1.295 mae = 0.829\n",
            "epoch137 train time: 6.532s test time: 0.746  loss = 0.778 val_mse = 1.188 mse = 1.291 mae = 0.833\n",
            "epoch138 train time: 6.533s test time: 0.754  loss = 0.777 val_mse = 1.185 mse = 1.294 mae = 0.830\n",
            "epoch139 train time: 6.536s test time: 0.746  loss = 0.777 val_mse = 1.187 mse = 1.289 mae = 0.835\n",
            "epoch140 train time: 6.549s test time: 0.745  loss = 0.780 val_mse = 1.184 mse = 1.289 mae = 0.829\n",
            "epoch141 train time: 6.542s test time: 0.747  loss = 0.779 val_mse = 1.183 mse = 1.293 mae = 0.836\n",
            "epoch142 train time: 6.530s test time: 0.752  loss = 0.781 val_mse = 1.188 mse = 1.295 mae = 0.827\n",
            "epoch143 train time: 6.536s test time: 0.746  loss = 0.778 val_mse = 1.191 mse = 1.289 mae = 0.833\n",
            "epoch144 train time: 6.539s test time: 0.747  loss = 0.777 val_mse = 1.182 mse = 1.288 mae = 0.827\n",
            "epoch145 train time: 6.543s test time: 0.746  loss = 0.774 val_mse = 1.179 mse = 1.288 mae = 0.833\n",
            "epoch146 train time: 6.541s test time: 0.749  loss = 0.777 val_mse = 1.186 mse = 1.289 mae = 0.832\n",
            "epoch147 train time: 6.528s test time: 0.751  loss = 0.778 val_mse = 1.185 mse = 1.291 mae = 0.831\n",
            "epoch148 train time: 6.542s test time: 0.751  loss = 0.776 val_mse = 1.186 mse = 1.290 mae = 0.825\n",
            "epoch149 train time: 6.531s test time: 0.745  loss = 0.775 val_mse = 1.187 mse = 1.289 mae = 0.833\n",
            "epoch150 train time: 6.536s test time: 0.751  loss = 0.781 val_mse = 1.185 mse = 1.291 mae = 0.833\n",
            "epoch151 train time: 6.563s test time: 0.750  loss = 0.777 val_mse = 1.175 mse = 1.286 mae = 0.833\n",
            "epoch152 train time: 6.565s test time: 0.752  loss = 0.775 val_mse = 1.185 mse = 1.292 mae = 0.830\n",
            "epoch153 train time: 6.565s test time: 0.746  loss = 0.777 val_mse = 1.184 mse = 1.292 mae = 0.830\n",
            "epoch154 train time: 6.555s test time: 0.750  loss = 0.776 val_mse = 1.187 mse = 1.289 mae = 0.829\n",
            "epoch155 train time: 6.550s test time: 0.754  loss = 0.775 val_mse = 1.182 mse = 1.289 mae = 0.834\n",
            "epoch156 train time: 6.565s test time: 0.752  loss = 0.776 val_mse = 1.181 mse = 1.290 mae = 0.828\n",
            "epoch157 train time: 6.574s test time: 0.753  loss = 0.776 val_mse = 1.187 mse = 1.286 mae = 0.831\n",
            "epoch158 train time: 6.533s test time: 0.755  loss = 0.777 val_mse = 1.196 mse = 1.290 mae = 0.827\n",
            "epoch159 train time: 6.549s test time: 0.748  loss = 0.779 val_mse = 1.187 mse = 1.287 mae = 0.828\n",
            "epoch160 train time: 6.530s test time: 0.750  loss = 0.774 val_mse = 1.184 mse = 1.289 mae = 0.828\n",
            "epoch161 train time: 6.521s test time: 0.743  loss = 0.778 val_mse = 1.190 mse = 1.286 mae = 0.831\n",
            "epoch162 train time: 6.524s test time: 0.743  loss = 0.775 val_mse = 1.184 mse = 1.290 mae = 0.828\n",
            "epoch163 train time: 6.533s test time: 0.749  loss = 0.774 val_mse = 1.180 mse = 1.288 mae = 0.834\n",
            "epoch164 train time: 6.520s test time: 0.743  loss = 0.775 val_mse = 1.191 mse = 1.289 mae = 0.825\n",
            "epoch165 train time: 6.519s test time: 0.747  loss = 0.778 val_mse = 1.186 mse = 1.288 mae = 0.833\n",
            "epoch166 train time: 6.509s test time: 0.744  loss = 0.777 val_mse = 1.183 mse = 1.288 mae = 0.830\n",
            "epoch167 train time: 6.526s test time: 0.753  loss = 0.774 val_mse = 1.190 mse = 1.288 mae = 0.829\n",
            "epoch168 train time: 6.515s test time: 0.753  loss = 0.773 val_mse = 1.186 mse = 1.291 mae = 0.828\n",
            "epoch169 train time: 6.515s test time: 0.748  loss = 0.774 val_mse = 1.183 mse = 1.286 mae = 0.832\n",
            "epoch170 train time: 6.529s test time: 0.743  loss = 0.773 val_mse = 1.180 mse = 1.283 mae = 0.827\n",
            "epoch171 train time: 6.524s test time: 0.744  loss = 0.773 val_mse = 1.182 mse = 1.288 mae = 0.831\n",
            "epoch172 train time: 6.526s test time: 0.751  loss = 0.776 val_mse = 1.184 mse = 1.286 mae = 0.827\n",
            "epoch173 train time: 6.534s test time: 0.744  loss = 0.774 val_mse = 1.191 mse = 1.284 mae = 0.827\n",
            "epoch174 train time: 6.544s test time: 0.750  loss = 0.775 val_mse = 1.189 mse = 1.292 mae = 0.830\n",
            "epoch175 train time: 6.539s test time: 0.753  loss = 0.775 val_mse = 1.179 mse = 1.285 mae = 0.833\n",
            "epoch176 train time: 6.534s test time: 0.749  loss = 0.772 val_mse = 1.191 mse = 1.289 mae = 0.825\n",
            "epoch177 train time: 6.557s test time: 0.752  loss = 0.775 val_mse = 1.188 mse = 1.284 mae = 0.828\n",
            "epoch178 train time: 6.510s test time: 0.749  loss = 0.776 val_mse = 1.186 mse = 1.290 mae = 0.827\n",
            "epoch179 train time: 6.541s test time: 0.753  loss = 0.774 val_mse = 1.191 mse = 1.286 mae = 0.829\n",
            "MAE 0.837623648531389\n",
            "MSE 1.3066071470052691\n",
            "epoch0 train time: 18.456s test time: 2.501  loss = 138.942 val_mse = 15.404 mse = 15.597 mae = 3.754\n",
            "epoch1 train time: 14.526s test time: 2.189  loss = 104.394 val_mse = 1.549 mse = 1.499 mae = 0.970\n",
            "epoch2 train time: 14.465s test time: 2.173  loss = 48.941 val_mse = 1.593 mse = 1.528 mae = 0.891\n",
            "epoch3 train time: 14.402s test time: 2.167  loss = 22.342 val_mse = 1.573 mse = 1.515 mae = 0.885\n",
            "epoch4 train time: 14.401s test time: 2.177  loss = 9.023 val_mse = 1.552 mse = 1.500 mae = 0.879\n",
            "epoch5 train time: 14.376s test time: 2.168  loss = 3.447 val_mse = 1.531 mse = 1.486 mae = 0.873\n",
            "epoch6 train time: 14.376s test time: 2.173  loss = 1.739 val_mse = 1.510 mse = 1.471 mae = 0.867\n",
            "epoch7 train time: 14.387s test time: 2.182  loss = 1.454 val_mse = 1.488 mse = 1.456 mae = 0.862\n",
            "epoch8 train time: 14.345s test time: 2.170  loss = 1.402 val_mse = 1.465 mse = 1.440 mae = 0.858\n",
            "epoch9 train time: 14.334s test time: 2.165  loss = 1.358 val_mse = 1.441 mse = 1.424 mae = 0.854\n",
            "epoch10 train time: 14.344s test time: 2.163  loss = 1.316 val_mse = 1.416 mse = 1.408 mae = 0.852\n",
            "epoch11 train time: 14.309s test time: 2.167  loss = 1.275 val_mse = 1.392 mse = 1.392 mae = 0.850\n",
            "epoch12 train time: 14.351s test time: 2.176  loss = 1.238 val_mse = 1.368 mse = 1.377 mae = 0.849\n",
            "epoch13 train time: 14.419s test time: 2.177  loss = 1.201 val_mse = 1.346 mse = 1.363 mae = 0.848\n",
            "epoch14 train time: 14.354s test time: 2.170  loss = 1.166 val_mse = 1.331 mse = 1.355 mae = 0.846\n",
            "epoch15 train time: 14.345s test time: 2.169  loss = 1.133 val_mse = 1.320 mse = 1.350 mae = 0.843\n",
            "epoch16 train time: 14.329s test time: 2.168  loss = 1.101 val_mse = 1.309 mse = 1.346 mae = 0.841\n",
            "epoch17 train time: 14.329s test time: 2.162  loss = 1.071 val_mse = 1.298 mse = 1.341 mae = 0.839\n",
            "epoch18 train time: 14.314s test time: 2.164  loss = 1.043 val_mse = 1.287 mse = 1.335 mae = 0.837\n",
            "epoch19 train time: 14.346s test time: 2.178  loss = 1.017 val_mse = 1.277 mse = 1.331 mae = 0.837\n",
            "epoch20 train time: 14.375s test time: 2.183  loss = 0.994 val_mse = 1.259 mse = 1.319 mae = 0.837\n",
            "epoch21 train time: 14.337s test time: 2.171  loss = 0.973 val_mse = 1.253 mse = 1.318 mae = 0.835\n",
            "epoch22 train time: 14.383s test time: 2.180  loss = 0.957 val_mse = 1.237 mse = 1.309 mae = 0.846\n",
            "epoch23 train time: 14.372s test time: 2.179  loss = 0.941 val_mse = 1.231 mse = 1.307 mae = 0.841\n",
            "epoch24 train time: 14.361s test time: 2.177  loss = 0.931 val_mse = 1.230 mse = 1.306 mae = 0.845\n",
            "epoch25 train time: 14.356s test time: 2.168  loss = 0.918 val_mse = 1.218 mse = 1.300 mae = 0.843\n",
            "epoch26 train time: 14.370s test time: 2.171  loss = 0.910 val_mse = 1.212 mse = 1.288 mae = 0.843\n",
            "epoch27 train time: 14.392s test time: 2.179  loss = 0.912 val_mse = 1.208 mse = 1.297 mae = 0.849\n",
            "epoch28 train time: 14.435s test time: 2.178  loss = 0.908 val_mse = 1.201 mse = 1.294 mae = 0.854\n",
            "epoch29 train time: 14.444s test time: 2.184  loss = 0.898 val_mse = 1.237 mse = 1.309 mae = 0.843\n",
            "epoch30 train time: 14.419s test time: 2.177  loss = 0.890 val_mse = 1.240 mse = 1.320 mae = 0.836\n",
            "epoch31 train time: 14.432s test time: 2.177  loss = 0.887 val_mse = 1.252 mse = 1.322 mae = 0.833\n",
            "epoch32 train time: 14.397s test time: 2.173  loss = 0.880 val_mse = 1.256 mse = 1.340 mae = 0.834\n",
            "epoch33 train time: 14.394s test time: 2.179  loss = 0.878 val_mse = 1.228 mse = 1.319 mae = 0.840\n",
            "epoch34 train time: 14.409s test time: 2.178  loss = 0.876 val_mse = 1.213 mse = 1.298 mae = 0.849\n",
            "epoch35 train time: 14.416s test time: 2.187  loss = 0.883 val_mse = 1.227 mse = 1.317 mae = 0.839\n",
            "epoch36 train time: 14.400s test time: 2.183  loss = 0.869 val_mse = 1.211 mse = 1.299 mae = 0.849\n",
            "epoch37 train time: 14.417s test time: 2.184  loss = 0.867 val_mse = 1.225 mse = 1.300 mae = 0.837\n",
            "epoch38 train time: 14.424s test time: 2.186  loss = 0.862 val_mse = 1.251 mse = 1.320 mae = 0.832\n",
            "epoch39 train time: 14.392s test time: 2.167  loss = 0.872 val_mse = 1.254 mse = 1.335 mae = 0.834\n",
            "epoch40 train time: 14.327s test time: 2.164  loss = 0.859 val_mse = 1.228 mse = 1.317 mae = 0.842\n",
            "epoch41 train time: 14.338s test time: 2.182  loss = 0.857 val_mse = 1.257 mse = 1.331 mae = 0.832\n",
            "epoch42 train time: 14.304s test time: 2.163  loss = 0.854 val_mse = 1.225 mse = 1.304 mae = 0.836\n",
            "epoch43 train time: 14.311s test time: 2.158  loss = 0.853 val_mse = 1.256 mse = 1.331 mae = 0.832\n",
            "epoch44 train time: 14.294s test time: 2.171  loss = 0.849 val_mse = 1.244 mse = 1.330 mae = 0.834\n",
            "epoch45 train time: 14.323s test time: 2.164  loss = 0.849 val_mse = 1.258 mse = 1.335 mae = 0.829\n",
            "epoch46 train time: 14.328s test time: 2.167  loss = 0.847 val_mse = 1.199 mse = 1.297 mae = 0.852\n",
            "epoch47 train time: 14.341s test time: 2.165  loss = 0.846 val_mse = 1.233 mse = 1.314 mae = 0.834\n",
            "epoch48 train time: 14.315s test time: 2.161  loss = 0.837 val_mse = 1.259 mse = 1.338 mae = 0.836\n",
            "epoch49 train time: 14.320s test time: 2.164  loss = 0.837 val_mse = 1.256 mse = 1.320 mae = 0.828\n",
            "epoch50 train time: 14.310s test time: 2.175  loss = 0.834 val_mse = 1.234 mse = 1.320 mae = 0.835\n",
            "epoch51 train time: 14.318s test time: 2.156  loss = 0.833 val_mse = 1.252 mse = 1.316 mae = 0.831\n",
            "epoch52 train time: 14.316s test time: 2.164  loss = 0.835 val_mse = 1.228 mse = 1.315 mae = 0.834\n",
            "epoch53 train time: 14.365s test time: 2.170  loss = 0.829 val_mse = 1.257 mse = 1.331 mae = 0.831\n",
            "epoch54 train time: 14.336s test time: 2.157  loss = 0.836 val_mse = 1.192 mse = 1.295 mae = 0.849\n",
            "epoch55 train time: 14.281s test time: 2.162  loss = 0.834 val_mse = 1.235 mse = 1.320 mae = 0.832\n",
            "epoch56 train time: 14.346s test time: 2.173  loss = 0.831 val_mse = 1.255 mse = 1.336 mae = 0.832\n",
            "epoch57 train time: 14.335s test time: 2.167  loss = 0.825 val_mse = 1.237 mse = 1.313 mae = 0.833\n",
            "epoch58 train time: 14.383s test time: 2.380  loss = 0.827 val_mse = 1.245 mse = 1.327 mae = 0.836\n",
            "epoch59 train time: 15.160s test time: 2.170  loss = 0.828 val_mse = 1.246 mse = 1.326 mae = 0.831\n",
            "epoch60 train time: 14.313s test time: 2.163  loss = 0.825 val_mse = 1.229 mse = 1.321 mae = 0.835\n",
            "epoch61 train time: 14.296s test time: 2.166  loss = 0.821 val_mse = 1.238 mse = 1.318 mae = 0.835\n",
            "epoch62 train time: 14.338s test time: 2.169  loss = 0.826 val_mse = 1.227 mse = 1.320 mae = 0.837\n",
            "epoch63 train time: 14.316s test time: 2.166  loss = 0.819 val_mse = 1.236 mse = 1.317 mae = 0.830\n",
            "epoch64 train time: 14.390s test time: 2.183  loss = 0.823 val_mse = 1.229 mse = 1.318 mae = 0.838\n",
            "epoch65 train time: 14.414s test time: 2.201  loss = 0.826 val_mse = 1.240 mse = 1.319 mae = 0.828\n",
            "epoch66 train time: 14.435s test time: 2.176  loss = 0.821 val_mse = 1.194 mse = 1.302 mae = 0.847\n",
            "epoch67 train time: 14.410s test time: 2.170  loss = 0.817 val_mse = 1.239 mse = 1.323 mae = 0.829\n",
            "epoch68 train time: 14.351s test time: 2.174  loss = 0.815 val_mse = 1.238 mse = 1.320 mae = 0.831\n",
            "epoch69 train time: 14.360s test time: 2.185  loss = 0.815 val_mse = 1.238 mse = 1.321 mae = 0.830\n",
            "epoch70 train time: 14.396s test time: 2.181  loss = 0.815 val_mse = 1.198 mse = 1.300 mae = 0.846\n",
            "epoch71 train time: 14.360s test time: 2.171  loss = 0.819 val_mse = 1.221 mse = 1.306 mae = 0.832\n",
            "epoch72 train time: 14.387s test time: 2.211  loss = 0.813 val_mse = 1.194 mse = 1.300 mae = 0.843\n",
            "epoch73 train time: 14.411s test time: 2.178  loss = 0.808 val_mse = 1.208 mse = 1.297 mae = 0.841\n",
            "epoch74 train time: 14.398s test time: 2.177  loss = 0.809 val_mse = 1.202 mse = 1.304 mae = 0.840\n",
            "epoch75 train time: 14.385s test time: 2.181  loss = 0.810 val_mse = 1.206 mse = 1.308 mae = 0.844\n",
            "epoch76 train time: 14.422s test time: 2.177  loss = 0.809 val_mse = 1.202 mse = 1.301 mae = 0.841\n",
            "epoch77 train time: 14.389s test time: 2.178  loss = 0.805 val_mse = 1.202 mse = 1.303 mae = 0.840\n",
            "epoch78 train time: 14.401s test time: 2.180  loss = 0.805 val_mse = 1.205 mse = 1.303 mae = 0.841\n",
            "epoch79 train time: 14.373s test time: 2.181  loss = 0.807 val_mse = 1.209 mse = 1.298 mae = 0.843\n",
            "epoch80 train time: 14.424s test time: 2.175  loss = 0.804 val_mse = 1.192 mse = 1.294 mae = 0.839\n",
            "epoch81 train time: 14.351s test time: 2.174  loss = 0.800 val_mse = 1.203 mse = 1.304 mae = 0.840\n",
            "epoch82 train time: 14.386s test time: 2.171  loss = 0.805 val_mse = 1.201 mse = 1.292 mae = 0.840\n",
            "epoch83 train time: 14.372s test time: 2.181  loss = 0.800 val_mse = 1.202 mse = 1.294 mae = 0.835\n",
            "epoch84 train time: 14.368s test time: 2.175  loss = 0.801 val_mse = 1.198 mse = 1.296 mae = 0.839\n",
            "epoch85 train time: 14.406s test time: 2.184  loss = 0.799 val_mse = 1.205 mse = 1.294 mae = 0.834\n",
            "epoch86 train time: 14.375s test time: 2.171  loss = 0.801 val_mse = 1.193 mse = 1.302 mae = 0.840\n",
            "epoch87 train time: 14.340s test time: 2.165  loss = 0.796 val_mse = 1.207 mse = 1.306 mae = 0.837\n",
            "epoch88 train time: 14.349s test time: 2.165  loss = 0.799 val_mse = 1.193 mse = 1.297 mae = 0.837\n",
            "epoch89 train time: 14.343s test time: 2.167  loss = 0.795 val_mse = 1.197 mse = 1.296 mae = 0.835\n",
            "epoch90 train time: 14.377s test time: 2.166  loss = 0.793 val_mse = 1.201 mse = 1.304 mae = 0.839\n",
            "epoch91 train time: 14.330s test time: 2.173  loss = 0.800 val_mse = 1.204 mse = 1.305 mae = 0.837\n",
            "epoch92 train time: 14.369s test time: 2.183  loss = 0.794 val_mse = 1.190 mse = 1.295 mae = 0.841\n",
            "epoch93 train time: 14.348s test time: 2.180  loss = 0.793 val_mse = 1.193 mse = 1.296 mae = 0.837\n",
            "epoch94 train time: 14.358s test time: 2.177  loss = 0.792 val_mse = 1.192 mse = 1.289 mae = 0.837\n",
            "epoch95 train time: 14.379s test time: 2.175  loss = 0.792 val_mse = 1.204 mse = 1.298 mae = 0.835\n",
            "epoch96 train time: 14.356s test time: 2.165  loss = 0.789 val_mse = 1.196 mse = 1.297 mae = 0.837\n",
            "epoch97 train time: 14.310s test time: 2.174  loss = 0.793 val_mse = 1.199 mse = 1.293 mae = 0.836\n",
            "epoch98 train time: 14.298s test time: 2.168  loss = 0.791 val_mse = 1.193 mse = 1.298 mae = 0.837\n",
            "epoch99 train time: 14.303s test time: 2.163  loss = 0.793 val_mse = 1.197 mse = 1.296 mae = 0.833\n",
            "epoch100 train time: 14.325s test time: 2.162  loss = 0.787 val_mse = 1.187 mse = 1.293 mae = 0.833\n",
            "epoch101 train time: 14.352s test time: 2.171  loss = 0.782 val_mse = 1.202 mse = 1.301 mae = 0.838\n",
            "epoch102 train time: 14.381s test time: 2.180  loss = 0.790 val_mse = 1.183 mse = 1.293 mae = 0.838\n",
            "epoch103 train time: 14.387s test time: 2.166  loss = 0.786 val_mse = 1.198 mse = 1.296 mae = 0.831\n",
            "epoch104 train time: 14.364s test time: 2.169  loss = 0.781 val_mse = 1.198 mse = 1.300 mae = 0.839\n",
            "epoch105 train time: 14.375s test time: 2.180  loss = 0.792 val_mse = 1.202 mse = 1.291 mae = 0.834\n",
            "epoch106 train time: 14.365s test time: 2.166  loss = 0.784 val_mse = 1.188 mse = 1.294 mae = 0.836\n",
            "epoch107 train time: 14.357s test time: 2.169  loss = 0.781 val_mse = 1.202 mse = 1.297 mae = 0.835\n",
            "epoch108 train time: 14.352s test time: 2.162  loss = 0.785 val_mse = 1.185 mse = 1.293 mae = 0.836\n",
            "epoch109 train time: 14.325s test time: 2.162  loss = 0.784 val_mse = 1.203 mse = 1.300 mae = 0.833\n",
            "epoch110 train time: 14.389s test time: 2.179  loss = 0.781 val_mse = 1.191 mse = 1.294 mae = 0.832\n",
            "epoch111 train time: 14.408s test time: 2.194  loss = 0.781 val_mse = 1.193 mse = 1.291 mae = 0.834\n",
            "epoch112 train time: 14.341s test time: 2.168  loss = 0.784 val_mse = 1.190 mse = 1.291 mae = 0.836\n",
            "epoch113 train time: 14.333s test time: 2.175  loss = 0.782 val_mse = 1.189 mse = 1.293 mae = 0.834\n",
            "epoch114 train time: 14.352s test time: 2.174  loss = 0.782 val_mse = 1.184 mse = 1.293 mae = 0.834\n",
            "epoch115 train time: 14.353s test time: 2.167  loss = 0.781 val_mse = 1.198 mse = 1.290 mae = 0.830\n",
            "epoch116 train time: 14.330s test time: 2.167  loss = 0.782 val_mse = 1.191 mse = 1.295 mae = 0.836\n",
            "epoch117 train time: 14.325s test time: 2.166  loss = 0.783 val_mse = 1.193 mse = 1.295 mae = 0.831\n",
            "epoch118 train time: 14.341s test time: 2.173  loss = 0.780 val_mse = 1.193 mse = 1.292 mae = 0.835\n",
            "epoch119 train time: 14.366s test time: 2.172  loss = 0.784 val_mse = 1.196 mse = 1.286 mae = 0.830\n",
            "epoch120 train time: 14.311s test time: 2.156  loss = 0.778 val_mse = 1.187 mse = 1.293 mae = 0.835\n",
            "epoch121 train time: 14.306s test time: 2.167  loss = 0.780 val_mse = 1.199 mse = 1.295 mae = 0.828\n",
            "epoch122 train time: 14.316s test time: 2.169  loss = 0.778 val_mse = 1.193 mse = 1.297 mae = 0.834\n",
            "epoch123 train time: 14.324s test time: 2.169  loss = 0.781 val_mse = 1.189 mse = 1.288 mae = 0.836\n",
            "epoch124 train time: 14.327s test time: 2.164  loss = 0.783 val_mse = 1.182 mse = 1.291 mae = 0.834\n",
            "epoch125 train time: 14.316s test time: 2.168  loss = 0.781 val_mse = 1.197 mse = 1.294 mae = 0.828\n",
            "epoch126 train time: 14.320s test time: 2.167  loss = 0.778 val_mse = 1.188 mse = 1.295 mae = 0.837\n",
            "epoch127 train time: 14.317s test time: 2.159  loss = 0.779 val_mse = 1.189 mse = 1.289 mae = 0.829\n",
            "epoch128 train time: 14.290s test time: 2.161  loss = 0.777 val_mse = 1.186 mse = 1.296 mae = 0.834\n",
            "epoch129 train time: 14.303s test time: 2.159  loss = 0.781 val_mse = 1.191 mse = 1.285 mae = 0.831\n",
            "epoch130 train time: 14.304s test time: 2.164  loss = 0.778 val_mse = 1.188 mse = 1.291 mae = 0.833\n",
            "epoch131 train time: 14.329s test time: 2.167  loss = 0.777 val_mse = 1.192 mse = 1.293 mae = 0.830\n",
            "epoch132 train time: 14.307s test time: 2.165  loss = 0.778 val_mse = 1.186 mse = 1.289 mae = 0.834\n",
            "epoch133 train time: 14.318s test time: 2.160  loss = 0.780 val_mse = 1.191 mse = 1.290 mae = 0.829\n",
            "epoch134 train time: 14.320s test time: 2.168  loss = 0.776 val_mse = 1.185 mse = 1.293 mae = 0.833\n",
            "epoch135 train time: 14.329s test time: 2.163  loss = 0.777 val_mse = 1.185 mse = 1.288 mae = 0.830\n",
            "epoch136 train time: 14.308s test time: 2.161  loss = 0.775 val_mse = 1.185 mse = 1.293 mae = 0.831\n",
            "epoch137 train time: 14.307s test time: 2.160  loss = 0.778 val_mse = 1.200 mse = 1.297 mae = 0.832\n",
            "epoch138 train time: 14.300s test time: 2.164  loss = 0.776 val_mse = 1.186 mse = 1.287 mae = 0.831\n",
            "epoch139 train time: 14.347s test time: 2.175  loss = 0.776 val_mse = 1.190 mse = 1.289 mae = 0.830\n",
            "epoch140 train time: 14.353s test time: 2.171  loss = 0.774 val_mse = 1.178 mse = 1.290 mae = 0.832\n",
            "epoch141 train time: 14.352s test time: 2.170  loss = 0.773 val_mse = 1.192 mse = 1.293 mae = 0.833\n",
            "epoch142 train time: 14.361s test time: 2.166  loss = 0.776 val_mse = 1.190 mse = 1.289 mae = 0.832\n",
            "epoch143 train time: 14.347s test time: 2.166  loss = 0.774 val_mse = 1.195 mse = 1.287 mae = 0.829\n",
            "epoch144 train time: 14.340s test time: 2.162  loss = 0.776 val_mse = 1.187 mse = 1.295 mae = 0.832\n",
            "epoch145 train time: 14.329s test time: 2.164  loss = 0.771 val_mse = 1.192 mse = 1.286 mae = 0.831\n",
            "epoch146 train time: 14.346s test time: 2.162  loss = 0.771 val_mse = 1.190 mse = 1.290 mae = 0.833\n",
            "epoch147 train time: 14.343s test time: 2.170  loss = 0.773 val_mse = 1.195 mse = 1.294 mae = 0.830\n",
            "epoch148 train time: 14.346s test time: 2.168  loss = 0.772 val_mse = 1.179 mse = 1.289 mae = 0.832\n",
            "epoch149 train time: 14.352s test time: 2.170  loss = 0.772 val_mse = 1.192 mse = 1.291 mae = 0.829\n",
            "epoch150 train time: 14.363s test time: 2.168  loss = 0.770 val_mse = 1.182 mse = 1.294 mae = 0.832\n",
            "epoch151 train time: 14.360s test time: 2.164  loss = 0.770 val_mse = 1.185 mse = 1.288 mae = 0.830\n",
            "epoch152 train time: 14.340s test time: 2.167  loss = 0.769 val_mse = 1.180 mse = 1.286 mae = 0.833\n",
            "epoch153 train time: 14.335s test time: 2.167  loss = 0.769 val_mse = 1.195 mse = 1.293 mae = 0.828\n",
            "epoch154 train time: 14.328s test time: 2.163  loss = 0.769 val_mse = 1.184 mse = 1.294 mae = 0.831\n",
            "epoch155 train time: 14.323s test time: 2.165  loss = 0.773 val_mse = 1.188 mse = 1.284 mae = 0.828\n",
            "epoch156 train time: 14.315s test time: 2.164  loss = 0.768 val_mse = 1.185 mse = 1.286 mae = 0.834\n",
            "epoch157 train time: 14.304s test time: 2.168  loss = 0.771 val_mse = 1.182 mse = 1.288 mae = 0.827\n",
            "epoch158 train time: 14.308s test time: 2.162  loss = 0.768 val_mse = 1.177 mse = 1.284 mae = 0.830\n",
            "epoch159 train time: 14.298s test time: 2.167  loss = 0.769 val_mse = 1.186 mse = 1.285 mae = 0.831\n",
            "epoch160 train time: 14.322s test time: 2.178  loss = 0.769 val_mse = 1.182 mse = 1.293 mae = 0.832\n",
            "epoch161 train time: 14.320s test time: 2.167  loss = 0.770 val_mse = 1.189 mse = 1.292 mae = 0.832\n",
            "epoch162 train time: 14.334s test time: 2.170  loss = 0.770 val_mse = 1.181 mse = 1.288 mae = 0.832\n",
            "epoch163 train time: 14.328s test time: 2.172  loss = 0.768 val_mse = 1.186 mse = 1.292 mae = 0.830\n",
            "epoch164 train time: 14.312s test time: 2.170  loss = 0.771 val_mse = 1.180 mse = 1.283 mae = 0.829\n",
            "epoch165 train time: 14.333s test time: 2.169  loss = 0.772 val_mse = 1.188 mse = 1.286 mae = 0.830\n",
            "epoch166 train time: 14.324s test time: 2.169  loss = 0.770 val_mse = 1.180 mse = 1.287 mae = 0.832\n",
            "epoch167 train time: 14.323s test time: 2.166  loss = 0.770 val_mse = 1.186 mse = 1.289 mae = 0.831\n",
            "epoch168 train time: 14.284s test time: 2.159  loss = 0.770 val_mse = 1.179 mse = 1.286 mae = 0.831\n",
            "epoch169 train time: 14.328s test time: 2.163  loss = 0.771 val_mse = 1.189 mse = 1.287 mae = 0.827\n",
            "epoch170 train time: 14.299s test time: 2.158  loss = 0.771 val_mse = 1.188 mse = 1.290 mae = 0.830\n",
            "epoch171 train time: 14.300s test time: 2.162  loss = 0.769 val_mse = 1.183 mse = 1.286 mae = 0.830\n",
            "epoch172 train time: 14.295s test time: 2.160  loss = 0.768 val_mse = 1.174 mse = 1.288 mae = 0.832\n",
            "epoch173 train time: 14.336s test time: 2.162  loss = 0.767 val_mse = 1.185 mse = 1.289 mae = 0.829\n",
            "epoch174 train time: 14.316s test time: 2.164  loss = 0.767 val_mse = 1.181 mse = 1.288 mae = 0.831\n",
            "epoch175 train time: 14.333s test time: 2.170  loss = 0.769 val_mse = 1.181 mse = 1.291 mae = 0.832\n",
            "epoch176 train time: 14.340s test time: 2.175  loss = 0.768 val_mse = 1.182 mse = 1.288 mae = 0.831\n",
            "epoch177 train time: 14.367s test time: 2.165  loss = 0.768 val_mse = 1.181 mse = 1.284 mae = 0.828\n",
            "epoch178 train time: 14.344s test time: 2.175  loss = 0.766 val_mse = 1.175 mse = 1.286 mae = 0.834\n",
            "epoch179 train time: 14.389s test time: 2.172  loss = 0.770 val_mse = 1.186 mse = 1.285 mae = 0.827\n",
            "MAE 0.8539131697407135\n",
            "MSE 1.3919414789888864\n"
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    print(sys.path[0])\n",
        "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
        "    word_latent_dim = 300\n",
        "    latent_dim = 15\n",
        "    max_doc_length = 300\n",
        "    num_filters = 50\n",
        "    window_size_list = [2, 3, 5, 10]\n",
        "    window_size = 2\n",
        "    v_dim = 50\n",
        "    learning_rate = 0.001\n",
        "    lambda_1 = 0.01 #normally we set from [0.05, 0.01, 0.005, 0.001]\n",
        "    drop_out = 0.6\n",
        "    batch_size = 200\n",
        "    epochs = 180\n",
        "    avg_mae_list = []\n",
        "    avg_mse_list = []\n",
        "    \n",
        "    # loading data\n",
        "    firTime = time()\n",
        "    dataSet = Dataset(max_doc_length, sys.path[0], \"dataPreprocessingWordDict.out\")\n",
        "    word_dict, user_reviews, item_reviews, train, valRatings, testRatings = dataSet.word_id_dict, dataSet.userReview_dict, dataSet.itemReview_dict, dataSet.trainMtrx, dataSet.valRatings, dataSet.testRatings\n",
        "    secTime = time()\n",
        "\n",
        "    num_users, num_items = train.shape\n",
        "    print(\"load data: %.3fs\" % (secTime - firTime))\n",
        "    print(num_users, num_items)\n",
        "    print(latent_dim)\n",
        "\n",
        "    #load word embeddings\n",
        "    word_embedding_mtrx = ini_word_embed(len(word_dict), word_latent_dim)\n",
        "    # word_embedding_mtrx = word2vec_word_embed(len(word_dict), word_latent_dim,\n",
        "    #                                           \"Directory of pretrained WordEmbedding.out\",\n",
        "    #                                           word_dict)\n",
        "\n",
        "    print( \"shape\", word_embedding_mtrx.shape)\n",
        "\n",
        "    # get train instances\n",
        "    user_input, item_input, rateings = get_train_instance(train)\n",
        "    print (len(user_input), len(item_input), len(rateings))\n",
        "\n",
        "    # get test/val instances\n",
        "    user_vals, item_vals, user_input_val, item_input_val, rating_input_val = get_test_list(200, valRatings, user_reviews, item_reviews)\n",
        "    user_tests, item_tests, user_input_test, item_input_test, rating_input_test = get_test_list(200, testRatings, user_reviews, item_reviews)\n",
        "\n",
        "    #train & eval model\n",
        "    train_model()\n",
        "    \n",
        "    window_size = 3\n",
        "    #train & eval model\n",
        "    train_model()\n",
        "\n",
        "    window_size = 5\n",
        "    #train & eval model\n",
        "    train_model()\n",
        "\n",
        "    window_size = 10\n",
        "    #train & eval model\n",
        "    train_model()"
      ],
      "id": "War3DkA-DeqH"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9K2Htd_bGwep",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "outputId": "d95736cf-baf8-4251-8bab-7cf8bc80fb31"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "avg mae over window size = [2, 3, 5, 10]: [0.8371292235590698, 0.8353306033873166, 0.837623648531389, 0.8539131697407135]\n",
            "avg mse over window size = [2, 3, 5, 10]: [1.3096931666826137, 1.3127908019585943, 1.3066071470052691, 1.3919414789888864]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 750x350 with 2 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAuAAAAFUCAYAAABlbYn3AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAAPYQAAD2EBqD+naQAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOzdd5wV1fnH8c93F1jqAoJURUAQhKgo9oiiRo0tGmOviLGbSIwxUWPsscRgb+jP3guaYO+KPRYsNBUFAekIS4fdfX5/nLkwDHd378Kys+V5v173tTszZ2aee+/cM2fOnHNGZoZzzjnnnHOueuSlHYBzzjnnnHP1iRfAnXPOOeecq0ZeAHfOOeecc64aeQHcOeecc865auQFcOecc84556qRF8Cdc84555yrRl4Ad84555xzrhp5Adw555xzzrlq5AVw55xzzjnnqpEXwJ2rhSRNlHRf2nHkStIgSSapa03e5voSxXlJ2nE459YfSW9JeivtOHIlaWCUNw2sydtcX9I+j3oBvIpJOiM6+D5KMYZMwcQk7ZJluSRNjpY/V8Y2WklaGqXZvIw098X2k3wtrer35Zxza6u2582Smku6VNLXkhZJmiNplKQbJXWKpbuknHzZJHWojvfqnCtfg7QDqIOOASYC20vqYWbfpRjLUuBo4N3E/N2AjYBl5ax7GGDAdMJ7+nsZ6ZYBv88yv6RSkbrK6gWUph1EJTwIPEb5x1xd1gQoTjuIeq7W5s2SGgLvAL2B+4GbgeZA32g7zwA/JbZ1OrAwy77nrWPsrmx7px1AJb1DyJuWpx1ISlI9j3oBvApJ6gbsDBwC3EnI8C9NMaQXgMMk/dHM4if/o4FPgbblrHtstP6kKH1ZBfBiM3uoKoJNiyQBjc1sSYoxNDWzxbmmN7NaVZA1sxLq8UWZmfkdoRTVgbz5YGBr4BgzeyS+QFJjoFGWfTxlZrOrMOZqV9l8cT3sv5mZLco1vZnVqoKsmZUSLgbrpbTPo94EpWodA/wMPA88FU0DoQZD0lxJ9yZXklQYNfe4LjZvE0n/jW41zpR0vaR9Ktm26lGgDbBXbLuNgEOBR8paSVIXYAChxvIxoJuknXPcZ84kNZP07+iW6zJJ4yWdGxWIM2m+lvRmlnXzJE2V9FRi3hBJo6PPc4akOyW1Tqw7UdJz0ef5CbAEOLWMGG+RtFBS0yzLHpU0XVJ+NH2QpOcl/RS9nwmSLsosj633VvS++kt6R9Ji4J+S7pc0O6rtSu7rFUnjE+/hvth05tb2LyUNlTQrOnaekbRhls/ukijOxZLelNQnuc0yPo/PJA1PzPsq2veWsXlHKNZ8SVnaa8e+h10kfRx9Z99LOj7LfvtKekPSEklTJP2dMvIvhaYGo6Pv4CdJt0pqFVv+R0kliXl/juIbGpuXL2mBpGsq+Ey2lfRy9N0tkfSDpHsSaVa2AZfUVeU0EUist4OklyTNj76rtyX9srx4XFa1PW/eNPr7XnKBmS01s6Ic91shSQ2ifGtC9BuaKOmfkgpiaZ6T9H0Z63+gkK/G5x0r6dPo9zFX0mOSNk6kyZovlrGPc6PPe5Msy66StFxRvi9pgKQnJf0YvZ/J0XfWJLHefQp5/aaSXpC0AHhYodnPimQ+Gq0zTNI8hYugNdqAa1V76MMlXRjlXUslvS6pR5btnRnlgUuiPHFAcptlfB7DJX2WmDci2vdvYvN2iObtm4hvYCxN5nvoo3BuWKxwrj0vy343kvRs/LcAFCTTRWkPix0DsyU9JKlzbPlvtOZ55HfRvOQ5Z6ykxyv4THpKelrhHL00+uwfk9QyliZ5Hi2v6VbXWLrekp6KjuWlkj6Jf8658gJ41ToGGB5dBT8K9JS0HYCZrSDcJjw4ymjjDiYctI9BKJgCbwC/Am4CriTU3pRbEMhiIvABcFRs3r5Ay8y+ynAUsAh4zsw+BiYQO2ElSWqb5VVYXmCSBPwX+BPwEnAOMB74FzA0lvRxYFet2W5xF6BT4n3cGa3/HnA2cG8U98tas1Dbi/AdvRqlHVVGqI8DzYD9E/E3BQ4k1DJlanYHEW75Do22+SlwGXB1lu22AV6M9jsEeJPQTKMNsE9iXx2APYBc7jTcDGxFqN27PYrxlkSaq4CLgU+AvwDfAi9H77MiIwmffSa2DQi3wUsJF20ZA4BZZja2gu31IBSIXgX+TCgk3Sepb2wfHQifTz/CZ3kDcDzhM16NQiH3VsLt+D8DTxMurl6JHQMjCXlfvA3ugCzvYWvCbf53ygpeUjvgFaBrFNsfgIeBHct5z7OA4xKvwcD8aFlm23tE+y4kfJ8XAK2ANyRtX8723Zpqe948Kfp7fJR35mKDLPlyq4pX425CvvUZIX9+Gzg/EdfjhIqZ7eIrKhSId4ynlXQh8AAhnzmH8PvdE3gnSzzZ8sVsniA0kTw8y7LDgVfM7Odo+jCgKSE//AMhr/tDFFNSg2j5TOBcQv7xYDT/iMR7zVwwPZ3DHa6/Ab8FriPkvzsS8on49k4n5NVTgPMI+dSzhCZJFRkJbJU570bHyC/Jni+XkuVCLqE14bz8BSEfHQdckym4R/toArxOOF/dQvgtDACuTW5M0iDCd1ZCOJbuItyNejd2DLxL+E53zRJv/JyzIaEpVnn5ciPC97gj4Zx4JjAM6E7IQ8uSzJePI/z2lhA154rOTR8CmxPy/D8TykvPSvptOdtek5n5qwpeQH/CwfOraFrAZOCGWJq9ozQHJNZ9HpgQmz4nSndQbF5jYGw0f2AFsQyK0m0bHXhFQJNo2RPAG9H/EwmF7OT6XwIPxaavJBQMGiTS3RftJ9vrpQpiPChKd2Fi/pOEH9ym0fRmUbqzEuluBRbE3tcuUbqjE+n2Sc6P3rcB++TwvYqQIT6VmJ9pIz8gNq9JlvXvIPw4C2Lz3orWPTWRNi86Zh5LzP9T9Jl0S7yH+7J8568Cis0fSmh73DKabg+sAJ5J7OPiaP37sn0OsXSHRuk2j6YPJNzC/E88bkLGPTxLfF2zfA/xz3DDaHvXxeZdH6XbPpFuXnyb0bxlhIw3L5b2zCjdibHPeT5wTew7nk34bRQDzWOfewnQqpzP4+Bo29tW8LkZcEk5y2+N9r17LKZvCCfB+PfZBPieUMBIPd+rDS/qQN4cfe/jonUnEioXBgPtsuzjEsrOl8dVEN9WUbq7EvP/Fc3PHJ+Fyd9pNP8vhLyqSzS9SXRcX5BI9wtCPnRBbN5bZMkXy4n1feCTxLztom0cF//ssqz7t3ic0bz7onWvKmNfHybm/Tb5nUfv4a3Y9MAozRigUWz+H6P5v4imGxHyoI+JnWeBE6J0b2X7DGLpto3S7RtNbxFNPxGPm5BPf5YlvuR7SH6GjYBpxM6DhAoQAw6LzWtKuNBauU2gITAD+IrQ1DOTdv8o3aWxeV8Dj8emP2XVxVbvxOe+ZTmfR78ozaEVfG4TKeecRziek5/Fa4QyUvycLsJFzTe5HLuZl9eAV51jCAfZmwAWvpXHgSO1qgnCG4Qf2coraYXbZHtFaTN+DUwl1BATbW8p4aqxsp4gZN4HSGoBHED5zU+2JPx4H43NfpTQJnGfLKssjeJPvv5WQVz7EQo3NyXm/5twMO8LYGbfEGpD4p9ZPqEgOMJWtds+jFCoejVe40P4AS8Edk/s5wcze7mCGDPf45PAfpKaxxYdQfiO3o2lXdmGXFKLaP8jCZlS78SmlxFOovF9lRJqRX4TfVcZxwDvm9kPFcULDItizhgJ5BNOhBBqnhoAtyXWuzmHbWe2B6tqKQYA/yMU/AdAGEGHcIIducbaaxpjZivTmdkswp2Q7rE0+xFOIh8n0q1Wg0SolWxEKFjFO9bcRSjo7B+tW0o4oWbew+aEmrerCcfeTrH39rWZlddpLbPsgCx3WXKi0OTmDOA8M3szmt0P6En4rbaJHc/NCLVOu0ry/Ds3tT5vjvKWHQgFYQgF+f8Dpkm6WbHmITG/Y818+cQKYtov+js0Mf/f0d/Mb6iIUFN9eKJG/gjCb/XHaPoQwgXvE4l8eTqhoJbMl9fIF8vxONBf0qaxeUdE2/hPZkYiX24W7f99wm996yzbvT3LvAeAHRL7OoZwIfd2DrHea6u3D8/keZl8bltCHnSXrd4n4GHCXcGKfE44z8Xz5SlR3NtIahp9T7uQW768kNgd1yj2j1kzX55GuIOZSbeYUNMcty3QDrjNYncKzOx5wkVl/O7ySFadR1oQLgiHEX6bmZr8AYR89+ty4p8f/d1HWZqP5kLS7oS7FTeb2YPRvA0Id6OfAFrEjuc2hIqfnvFmNRXxDLwKRJn4kYQMvpukHgrtuz4i1DjuCRD9sJ4GDoplmIcQrhDjmfwmhFqXeEEKoNK99qOCymuEzj2HEApjT5WzyrGEGtvvY+9jKeFKMVszlBIzey3Lq6wmHRmbAD+Z2YLE/LGx5RmPA7+MHdgDCT/o+GfWk3D7diahtj7+ah6lj8ulMBvffxPgNxCGAyNkPk/GvyOFdsrPSJpPKPDNYlUm1nL1TTLVsnfYeSDa12+jbfYi1OA9mGOsPyamM5l3ph185nNd7Vgys7nkkNGb2QzCiTOeGY4k3A7sJKk74dZnHrll9Ml4MzHH2+1vEu0zaXxiepNs86PP+XtWP6ZGEk7eTaL3MM3MPiPU3GfeWy4nq7cJv+mLgdmS/iPpxDIKRGuQ1I9wl+RRM4sXenpGf+9nzeP594RmEcljyiXUpbzZzOab2Xlm1pXQ5OkkwrF+FnBRllXeyZIvf1BBWJsQaoaT+cN0QqEnmS9vTHTBGhVO+7NmvizC7zd5HG/OmvlyWfliNpm7pUdE+xehIuZFi7WJl9RFoX33XELBcharCs3J31AxoeCa9DihYH9MtM2WhAumh7McC9msbb5cTDj3lstCM8gPWDNffpdwXO0I9AE2ILd8eUqW95UtX/4uS7qc8uXIONbMlztGv9GdCbXPHxArmEd/30tUsqwmqqwaSsgrZyv00TlTsfbf5ZG0EeE7f49w1yujB+F4vpw1j+dMp+7kMV0mHwWlauwBdCRk9EdmWX4MoZ0ohLZxpxJqeJ8ltFcbZ2ZfrMf4HiHU0HQgZE5Za/SiDOwoQi3bmCxJ2klqbmbZhrZanx4nXIkeRmg/eDjhCvelWJo8QuG7rLbqsxLTOY94YmYfSpoY7fcRQrOLJsRONFGt79uEgvc/CO3mlwLbENqHJi92s+7fzMZI+pRwIfRA9Hc54Yo7F2WNNJJru9FcvAvsGRVe+xPai35NOEEPIJxYFxJqZSpSHfFm8y6hcLUTq05WRH8HSOpNaNJS7skqOvkcKmlHwnGxD3AP8GdJO5b3W4lqWJ8mNDVJDuWZOV7+Qtn9E6r7d1gb1Ym8OcnMJgH3SHqGcHFZ3lCxayOXQuUIYDHhc3o/+ltKKBhn5EXb2pfsv/XkMVyZfPknSSOj/f6TUMjsAvw1kya6AHuVUPC8hlDgWwR0JjQ5SebLy7IV7MzsZ4Vx2Y8h5HeHEi6Ccx0BrLry5QsVOoQOAK40s3mSvo6mZ0TpcimAp5kvQ6jJ705oLrMo+p7/GFV+bQ1cWNGGzOzPCh0sDyI0MbsJOD/Kl7NdZAEr248/RbjgOjxxRyJzvFxHqPHOJueLcS+AV41jCIW/M7MsOwT4raTTolth7xBu2xwh6V3CCeLKxDqTgD6SlLi6XKPXdI6eIXRQ3JFER5KE3QgdPv7BqprojNaEW0EHk3umU55JwK8ktUjUgveOLQfC1aykjwmf2S2Ez/RZW30IoQmEJgjv2foZTvAJ4Oyok8sRwEQz+zC2fCDhNtQhZrayc4jC8GeV9QAwVFJHQu3Y87aqQ9G6ynyuPYjdBZDUhtVrN8ozknAr+0hC7cr7ZlYaHc+ZAvj7tqpzalXE3DPL/F5Z0mXmrxyhIcpQuxFqGzM+JlzYDIhemVv77wAnE9WMUk5Hn7joWPiQcAI8mnDr+EhCh7Y1RM1HHiZ0CPqVrTnU2oTob5GZvYZbW3Ulb84qKhhOIDT5qgqTCIWMnsTOAZLaE47VeL68KCqUHibpHEL8I80sPh75BEKh7YeoOWFVexy4LbpTeAThgmBEbPkWhH5EJ5jZyk6Xkvai8h4A/qPQ8fQY4HMzG73Wka8uni9nmqEhqQHhbseXOWxjJKEJ3lGEC4xMQfsdVhXAv4nuYlaFScAvsvwWysuX30gs68Xqx9SPkn6M4u3O6u9hKKESLp/c8+WvCG3Pr1AYye094DTKv1i9idAEcNcsn1XmvLKiKvJlb4KyjqJawEMIHWaeSr4IvYNbEDVfiK6unyLUlh1HuAhKDqfzMuEHFB8+qDGhYFBpUS3c6YTOOSPKSZppfvKvLO/lLsJtxDJHQ6mkFwg/pLMS8/9EqDF5MTH/ccJJajChPXryM3si2t4at2IVhtXKpfd/eR4n1HicQGgHmqyRzhQ240MoNiK07a2sRwmfwY2ETKgqx1l/nXCb9fTE/OT3UJ5MpvhX4Eszmx+bvyehzV8utSy5egHYUbGRPxR6wiePxdcIheo/JtqlnkS41fx8ZkbUFvF/hJNVF1avAW9C6CQ1wcymlReYpNaJfcGqGuvymqFcTKgtP8qyt+3/lFB4OVer9z3I7HeNIdHc6upS3ixpq6itaXL+JoSmBdlu76+NF6K/QxLzM7fhn0/Mf5wwGtXvCe11k5/XcELeeHHyd6KgzTrG+3S0/aMIhbPnbPVxu7PlyyLLCEo5eJHQFvmvhMqqqsyXPwHmACdHhe6MY8i9YuQjQsfWvwJzgczFwUjCuXM3qj5f7kS4GwCsHB3slES6TwgXwadp9aEs9yVU1iSPqZGEi9/tY/GOIgy68DfCXZJPywtMYfjQZAXzV4Q7NGXmy5JOJNwFO9NifY4yzGwmoZPqqVEFWXL9SuXLXgO+7n5DyMT/W8byDwnNH45hVeb0OGEYpEuBr2zNodruJBSIHpV0I6FW5hhWDZify+3B1ZjZ/eUtj34YvwNetbKHVPovoRa4XXQgAjSQdGwZ6Z+xsh9iMIJwpX+lwviaXxBuEx1E6EQ3IZH+CcJtn+sImctqV59m9rakOwm3mPoRbiuvINTkHEbIcMtr+14uM/tM0neEGrEC1jzRvE9oI3e/pJuIek6zFrfszGyWpJeiuOexZga11sxsRnRM/VnSfwnNeLYi3CKeTQ7Hlpl9J2k6ofYi3nnzHVYNx1aVGf21hM/ypSj2RYRMfhKwcszY6HPLDLH4UvT+ehEugv7HmifMkYQMfT4hc8bMZiqMt96LcIu6IicAZ0RNASYQ8oKTCU2RXsi2gqQtCBeK7xCada32+zGzh6I7Cr8nnPRHK4xRPZVQ+Ns92v6BOcRXn9WJvDmyF3BpdEx/SGi60Z1QIVFAKMAnHSopWzOlV8uqBTWzLyTdD5wSa1a3PeE4f9ZWdRLOeIFQMLqOUNh9OrG9CQpj9l8FdJX0bJS+G6Gfy7Bo3bUS/V7fJFwgtGDNfHkc4Xd5nUIfoiLCeS7XQm18XyskPUb4/ktYfaCCdWJmyxWGUL2ZMMzoE4Sa70GE+HPJlxdHzRd3JAxQkFnnHUKz0mZUbb58F+GzeEBSf8Jv4TjCXYh4XCsk/ZXQufZtSY8S+l+cTWjffn1iuyMJvykjapJiZiWS3idUWryVQz+BPYBbJD1JaOLXIIptjWM0I7rAvY3Q/HZZlnJNpjxzZhTXV5LuItSKtyc0Z9yIcD7NjVViyBR/ZR2m5r+EK7Km5aS5l1Az18ZWDVnzI1mG4Yut0w14jnAwzyRkUodE6+xQQUyDyG1otIlEQ13Ftj24nPS7RWn+GE3fR9nDXRmxIefK2F5zwm2lqdHn8w1h7FWVkT4zTuhd5WzzZMIV92JCZvsloVDYMdv7ruR3fUW0/2/LWL4zocPI4ug9XcOq4c0GxtK9RRhdo7x9ZYY5vLOc7+6+ir5zsg8zlU9oxzgtivV1QtOf2cDtOX4WK8fhjc1rSCgcLyM23FQivq6J95BtGMy3SAy7RbiV/BbhtzaFcAtxcLbjjJBBjo2OqemETHWNoQQJHWkNeCEx/y4q+C3E0m5NaMc7iVAIm0G4uOyfSGdEwxDGvpOsr8R6/QgnjNms6gz9OLBHZY/f+vaijuTNsX1eSshfZhAqF2ZGceyeWPeS8o4vKh4qsQGhGeL30WfzI6GNdUEZ6R+KtvtqOds8hFCwWhi9xhLuQGwWS/MWFeSLZWz799H+i0jkO9HyzQntwBcQLriGES7cDRgUS3cfsLCCfWWGOXy5jOVvkX0YwkMT6bom9x/N/0P03S8l1GjvTDifvZjjZ3FttN3zEvMzQwN2T8zPxDcw8R7W+B6iz2diYl4Xwogzi6LP9gZWDf07MJH2cMLY8ksJtf0PAZ2z7KdPtP6YxPwLo/mX5fA5dCOMEvQdIQ+YQ2j+smeW39l9ie+kwvIM4eL3fsI5dDnhnDQC+F1ljl1FG3O1gKQhhKvFjcxsatrxuPVH0kGEjmC7WmyYvvW4v1aEGvy/m1my3atzrhyeN9cPkrYiNIc43qKh6dbz/vIIBdvhZrZWzZxczeVtwGsorfmY3MaEtknfegZfL5xMqIF6t6KElZU8tiKZNp9vVfX+nKtLPG+u104m1OAPryhhZUlqnKU/yfGEEVzequr9ufR5G/Caa3jUG3gUoQPZsYRmAlXVCdLVQJKOJNwe3R8429bPLaojFB4N/ALhZLILoRPTK2ZW0SOKnavvPG+uZyQdSGgacQpwi5Xdt2ld7AhcH7VbnkMYwvYkwhCvT5a3oqudvAlKDRXd0vw9oV1SPqFjwLVmluxk4uoQSUYoFD8OnGarj0FaVfvYhtBWsB/hkdIzCO2M/27VP8a7c7WK5831j8JzINoTRsE5ztZ8gFxV7KMrYQi87Qm13nMJlSR/s1WDHrg6xAvgzjnnnHPOVSNvA+6cc84551w18gK4c84555xz1cg7Ya5HUY/mToTxR51zrrJaAD+tp8649Zbnzc65dbTOebMXwNevToQB2p1zbm1tRHiwk6s6njc759bVOuXNXgBfvxYATJ48mcLCwrRjcc7VIkVFRWy88cbgtbTrg+fNzrm1UlV5sxfAq0FhYaFn8s45V8N43uycS4t3wnTOOeecc64aeQHcOeecc865auQFcOecc84556qRF8Cdc84555yrRt4J0znn1iMrMeaNnMfyactp1LERrQa0QvlKOyznnKvXSkpLGPnjSKYtmEbHFh0Z0GUA+Xn51bZ/L4A759x6Mmv4LL47+zuWTVm2cl7BRgX0uLEHGx6yYYqROedc/TV87HDOfulsphStehzARoUbceOvb+SQzQ+plhi8CYpzzq0Hs4bPYvSho1crfAMsm7qM0YeOZtbwWSlF5pxz9dfwscM59IlDVyt8A0wtmsqhTxzK8LHDqyUOL4A751wVsxLju7O/g2wPKY7mfTfkO6zEnzDvnHPVpaS0hLNfOhvLkjln5g15aQglpSXrPRYvgDvnXBWbN3LeGjXfqzFYNnkZ80bOq76gnHOunhv548g1ar7jDGNy0WRG/jhyvcfiBXDnnKtiy6ctr9J0zjnn1t20BdOqNN268AK4c85VsUYdG1VpOuecc+uuY4uOVZpuXXgB3DnnqlirAa1o2L5h2QkEBRsX0GpAq+oLyjnn6rkBXQbQuUXnMpcLsXHhxgzoMmC9x+IFcOecq2IlC8vpwBMNAd7jhh4+HrhzzlWj/Lz8MgvXijLnG359Q7WMB+4FcOecq0JmxrjB41gxYwUNNmxAo06rNzMp2KiAvk/19XHAnXOumn0540ueHvs0ABs03mC1ZRsVbsRThz9VbeOA+4N4nHOuCi36chFzRsxBDcWWz21Ji/4t/EmYzjmXshUlKxj07CBWlK7goF4H8dRhT/Hu5Hf9SZjOOVcXNN+qOVuP3JrF4xdTuH0hAK0Htk45Kuecq9+uevcqPp/+ORs02YA7DriDBvkNGNh1YGrxeAHcOeeqWOEOhRTuUJh2GM4554Blxct44IsHALh535vp0LxDyhF5G3DnnFtnVmp8c/o3LPxiYdqhOOecSyhoUMCnp3zKrfvdylG/OCrtcAAvgDvn3DqbdOUkfrrjJ0btMYrihcVph+Occy6hZeOWnLHdGUg1ow+OF8Cdc24d/Pz6z0y8eCIAPYb2oEFzb9nnnHM1wajpo7j7s7sxs7RDWYOfKZxzbi0t+2kZY44eAwYdTupAhxPSb1fonHMOlpcsZ9Czg/hixhfMXDSTCwZckHZIq/EacOecWwulK0oZc8QYVsxcQbOtmtHz5p5ph+Sccy7yz5H/5IsZX9CmSRtO2vqktMNZQ+oFcElnSpooaamkjyRtX0H6IZLGS1oiabKk6yU1ji2/RJIlXuNiyzeQdHNsGz9KuklSy8R+ktswSUdW/SfgnKuNfrjwB+a/O5/8Fvn0fbIv+U2qb/xY55xzZRs1fRRXjrwSgFv3u5X2zdunHNGaUm2CIukIYChwGvARMAR4WVIvM5uZJf3RwNXAYOB9YDPgPsCAc2JJRwO/ik3He0V1il7nAmOATYA7onmHJnZ5IvBSbHpepd6gc65OKi0uXTniSe97e9O0Z9OUI3LOOQeh6ckJz55AcWkxv9v8dxze9/C0Q8oq7Tbg5wB3mdm9AJJOA/YnFLCvzpJ+Z+A9M3skmp4o6VFgh0S6YjObnm2HZvY18LvYrAmSLgQektTAzOKF9Xllbcc5V3/lNchjyxe2ZO6rc2nz6zZph+Occy5y5TtX8uWML2nbtC237X9bjRn1JCm1JiiSGgH9gdcy88ysNJreqYzV3gf6Z5qpSOoO7Ae8kEjXU9JPkr6X9LCkLhWE0xIoShS+AW6VNFvSx5IGq4JvUVKBpMLMC2hRwX6dc7WIla7qSa98eeHbOedqkOkLp3PNe9cAoelJu2btUo6obGnWgLcF8oEZifkzgN7ZVjCzRyS1Bd6NCsMNgDvM7J+xZB8Bg4DxQEfgYmCkpF+Y2YLkNqPtXQQMSyz6B/AGsBjYG7gNaA7cVM57Oj/an3OuDvr2rG+xEqPHjT3Ib+xtvp1zribp0LwDb57wJs+Me6bGNj3JUFpjI0rqBEwFdjazD2LzrwV2M7NksxIkDQQeA/5OKGj3AG4kNGO5vHzdtK8AACAASURBVIz9tAImAeeY2f8llhUCrwJzgd+Y2Ypy4r0MONHMNi4nTQFQEJvVApgyf/58Cgv9sdTO1WYzHpnB2GPGgmCrN7ai9cDW63V/RUVFtGzZEqClmRWt153VM1HeP9/zZudcZVVV3pxmDfhsoARIdk1tD5TV7vpy4EEzuzua/kpSM2CYpCujJiyrMbN5kr4hFNZXktSC0MFyAfDb8grfkY+AiyQVmNmybAmi+SuX1dR2R865ylk0dhHjTxkPwCYXbrLeC9/OOedy99WMr2iY35DebbM2oKiRUmsDbmbLgU+BPTPzJOVF0x+UsVpTIFnILsmsnm0FSc2BTYFpsXmFwCvAckLN99IcQu4H/FxW4ds5VzeVLCph9KGjKV1USqs9WtH1kq5ph+Sccy6yrHgZRw8/mn539OO5b55LO5ycpT0KylDgfkmfAB8ThiFsBmRGRXkAmGpm50fpRwDnSPqcVU1QLgdGmFlJtM51UbpJhKEFLyUU0h+NlmcK302BY4FMh0mAWWZWIulAQk38h8BSYC/gAuC69fQ5OOdqIDPjm9O+YfGYxTTq2Ig+j/RB+X5nyznnaorL37mcr2d+zYZNN2SHzmu0Xq6xUi2Am9njkjYELgM6AKOAX5tZpmNmF1av8b6CMOb3FUBnYBahsH1hLM1GhMJ2m2j5u8COZjYrWr4Nq4Yt/C4RUjdgIrACOBO4nlCz/h3RkIlr/26dc7XNtLumMeOhGZAPfR7rQ6P2jdIOyTnnXOSTnz7h6nfDqNW37X8bGzbbMOWIcpd2DThmdgtwSxnLBiamiwk12peWs71yn1ZpZm9RRnOVWJqXWP0BPM65eqhgowIatG5Al792odWurdIOxznnXGRZ8TIGPTuIEivhiL5HcGif5LMUa7bUC+DOOVdTtdmvDduN3s5rvp1zroa57O3LGD1rNO2ateOW/bLW49ZoqXXCdM65msjMWDZ9VV/rgo4FKM/bfTvnXE3xxfQvVj5w5/b9b6dt07YpR1R5XgB3zrmYKddP4X99/sec5+ekHYpzzrks+rbry2W7X8YJW53AIZsfknY4a8WboDjnXGT++/P5/q/fY8XG0km5jE7qnHOuujXIa8AFAy4grYdJVgWvAXfOOWD5rOWMPnw0Vmy0O7IdnU7vlHZItZ6kXSWNkPSTJJN0cAXpd5H0nqQ5kpZIGifpT4k0+ZIul/RDlGaCpIvkTz5zrs6bOG8iy4pXNRGszT97rwF3ztV7VmKMPWYsy6cup2nvpmw2bLNanbHXIM2AL4B7gOE5pF9EGBXry+j/XYA7JS0ys2FRmr8CpwMnAKOBbQnPjpgP3FSl0TvnaoylxUvZ/5H9yVMeTx32FL3a9ko7pHXiBXDnXL036YpJ/Pzqz+Q1zaPvU31p0MKzxqpgZi8CL0JuNVVm9jnweWzWREmHAAOATAF8Z+A/ZvZ8LM1RwPZVFbdzrua59K1LGTNrDO2bta+VnS6TvAmKc65emzdyHhMvnQjAZndsRrO+zdINyK0kaWtCgfvt2Oz3gT0lbRal2YpQU/5iOdspkFSYeQEt1mPYzrkq9vHUj7n2/WsBuPOAO2nTtE3KEa07r+ZxztVrLbZrQcdTOkIpdDiuQ9rhOEDSFGBDwjnqEjO7O7b4aqAQGCepBMgHLjSzh8vZ5PnAxesrXufc+rO0eCknPHsCpVbKMVscw0G9D0o7pCrhBXDnXL2W3zifXnf0wkpqb2/6OmgA0BzYEbha0ndm9mi07HDgGOBoQhvwfsANkn4ys/vL2N5VwNDYdAtgynqJ3DlXpS5+82LGzR5H+2btuWnfutPNwwvgzrl6ae4rc2m9Z2uUH9omZ/669JnZD9G/X0lqD1wCZArg/wKuNrPHYmk2IdRyZy2Am9kyYOXQCd7B1rna4cMpH3LdB9cBoenJBk02SDmiquMFcOdcvTPr2VmM/u1oWv+qNVu8sAV5Db07TA2WBxTEppsCpYk0JXifJufqnPbN2rPrJrvSuUXnOtP0JMML4M65emXJ90sYN2gcAM22aOaF7/VIUnOgR2xWN0n9gLlm9qOkq4DOZnZ8lP5M4EdgXJR+V+BcVh9ecARwoaQfCU1QtgbOIQx16JyrQ7q17sbrx7/O0uK692A0L4A75+qNkqUljD50NCXzSyjcqZDu13RPO6S6blvgzdh0ph32/cAgoCPQJbY8j9BeuxtQDEwgjPt9ZyzNH4DLgduAdsBP0fLLqjx651wqFq9YTNOGTQHIU97K/+sS1ebHeNZ00XBX8+fPn09hYWHa4ThX740/bTzT7pxGw7YN6f9Zfxpv3DjtkMpUVFREy5YtAVqaWVHa8dQlnjc7V3MtWbGE/sP6s3vX3blmr2to3qh52iGtpqryZr/36pyrF6Y/NJ1pd04DweYPb16jC9/OOVdf/ePNfzB29lieGfcMK0pWpB3OeuMFcOdcnVeyqIQJf5oAwCYXbcIGe9ednvTOOVdXfDD5A/79wb8BGHbgMFo3aZ1yROuPF8Cdc3VefrN8tnp9Kzqe0pGu/+iadjjOOecSlqxYwqD/DMIwjt/qeA7Y7IC0Q1qvvBOmc65eaL5lc3rd2SvtMJxzzmVx0ZsX8c2cb+jUohM37HND2uGsd14D7pyrs2Y8PIP5H85POwznnHPleH/y+wz9IAySNOyAut30JCP1ArikMyVNlLRU0keStq8g/RBJ4yUtkTRZ0vWSGseWXyLJEq9xiW00lnSrpDmSFkp6OnraWjxNF0nPS1osaaakf0nyOwbO1RILPl3AuMHjGLXrKBZ8uiDtcJxzzpVhzuI5tGrcihO2OoH9N9s/7XCqRaoFSklHEMaFPQ34CBgCvCypl5nNzJL+aOBqYDDwPrAZcB9ghAcxZIwGfhWbLk5s6npgf+AwYD5wCzAc+GW0n3zgeWA6sDNhrNoHgBXABWv7fp1z1WPFzysYfdhobLnR5qA2NN+mZg1j5ZxzbpUDex3ImDPH0LhB/RmdKu0a3XOAu8zsXgBJpxEKxoMJBe2knYH3zOyRaHqipEeBHRLpis1serYdSmoJnAQcbWZvRPNOBMZK2tHMPgT2BvoAvzKzGcAoSRcB10i6xMyWr8N7ds6tR2bGuBPHsfSHpTTu1pje9/VGUtphOeecSzCzlflzh+YdUo6meqXWBEVSI6A/8FpmnpmVRtM7lbHa+0D/TDMVSd2B/YAXEul6SvpJ0veSHpYUf9Jaf6BhYr/jCI8/zux3J+CrqPCd8TJQCPSt1Bt1zlWryf+ezJz/zEGNRN8n+9KwVcO0Q3LOOZeweMVidrtvN54d92zaoaQizRrwtkA+MCMxfwbQO9sKZvaIpLbAuwqXTA2AO8zsn7FkHxEecTye0HTkYmCkpF+Y2QKgA7DczOZl2W/m8qtDGXERS7MGSQVAQWxWi7LSOueq3rx35/H9374HoMeNPWjR33+CzjlXE134+oWM/HEk3//8PXtvunedfNx8eVLvhFkZkgYS2mCfAWwDHALsHzUPAcDMXjSzJ83sSzN7mVBD3go4vBpCPJ/QpjzzmlIN+3TORWY+PBNKoN3R7eh0aqe0w3HOOZfFyEkjufGjGwG468C76l3hG9KtAZ8NlADtE/PbEzo/ZnM58KCZ3R1NfyWpGTBM0pVRE5bVmNk8Sd8APaJZ04FGklolasHj+50OJEdjaR9bVparCJ1KM1rghXDnqk3P23rSfJvmtDuqnbf7ds65GmjxisUM/u9gDGNwv8Hs23PftENKRWo14FFHxk+BPTPzJOVF0x+UsVpTIFnILsmsnm0FSc2BTYFp0axPCaOZxPfbC+gS2+8HwBaS2sU2tRdQBIwp5z0tM7OizAvwsc+cq0aS6HRyJxo0T7t/uXPOuWwueP0Cvpv7HRsVbsTQfYZWvEIdlXYTlKHAyZJOkLQ5cDvQDMiMivKApKti6UcAp0s6UlI3SXsRasVHmFlJtM51knaT1FXSzsAzhEL6owBmNh/4P2CopN0l9Y/290E0AgrAK4SC9oOStpK0D3AFcKuZLVufH4hzrnLmvjKXsceNpXhhcrRR55xzNcnISSO56aObALj7wLtp2bhlyhGlJ9VqIjN7XNKGwGWEzo2jgF/HRh/pwuo13lcQxvy+AugMzCIUyi+MpdmIUNhuEy1/F9jRzGbF0vwp2u7ThE6TLxPalWfiKpF0AOGC4ANgEXA/8I91f9fOuaqydMpSxh4zlhWzV9B408Z0u6Rb2iE555wrwysTXsEwTtr6JPbpsU/a4aRKZpZ2DHWWpEJg/vz58yksLEw7HOfqlNIVpYzabRRFHxTRfJvmbP3e1uQ3zk87rCpTVFREy5YtAVpGTdpcFfG82bn0vPjti+y88c61tva7qvJmbyjpnKuVvv/r9xR9UER+y3z6Ptm3ThW+nXOurqqvnS6T0m4D7pxzlTZr+CymXB8GGNr8/s1p0r1JyhE555zLZtHyRZwy4hSmFk1NO5QaxQvgzrlaZfF3ixl34jgANj53Y9oe1DbliJxzzpXl/NfP567P7mK/R/bDmz2v4k1QnHO1yopZK8hrkkfzLZvT7Z/e6dI552qqtye+zc0f3wzAv/b6lz+fIcYL4M65WqXlTi3Z9vNtwSCvod/Ec865mmjh8oUM/u9gAE7e5mT23nTvlCOqWbwA7pyrFUqWlJDfJHS0LOhYkHI0zjnnyvO31/7G9z9/T5eWXbhu7+vSDqfG8eoj51yNt2j0Ij7s9iEzn5iZdijOOecq8OYPb3Lr/24FwgN3Cgt8uM8kL4A752q04oXFjD50NCtmrGDa/03zTjzOOVfDXf3e1QCcss0p7LXpXilHUzN5ExTnXI1lZnxz8jcsHreYRp0bsflDm3snHuecq+GeOeIZrn3vWs7Z6Zy0Q6mxvADunKuxfrr9J2Y+NhM1EH0f70ujDRulHZJzzrkKNG3YlEsGXpJ2GDWaN0FxztVIRZ8U8d2fvgOg+zXdafnL2vnYYuecqw8WLl/IsE+HUWqlaYdSK3gB3DlX46z4eQVjDhuDLTfa/rYtG/1po7RDcs45V47zXj2PU587leOeOS7tUGoFL4A752qc/Ob5tD2kLU16NKHXPb283bdzztVgb/zwBrd/cjsAg/sNTjma2sEL4M65GievYR49/t2D/p/1p2GrhmmH45xzrgwLli1g8H9Cofu0/qexZ/c9U46odvACuHOuxlj87WJKV6xqP9ighfcTd865muy8V89j0vxJbNJyE67d69q0w6k1vADunKsRls9YzqjdRjFq4CiWz1iedjjOOecq8Pr3r3PHp3cAcM9B99CioEXKEdUeXgB3zqXOSowxR49h+bTlFM8vJr95ftohOeecK0dJaQmnPX8aAGdsewZ7dNsj5YhqFy+AO+dSN/GSicx7Yx55zfLo+2Rf8pt5Adw552qy/Lx8njzsSQ7qdRDX7HVN2uHUOt7A0jmXqjkvzWHSFZMA6HVXL5pt3izliJxzzuWiX4d+PHvks2mHUSt5DbhzLjVLJy9l7LFjAeh0eifaH9U+5Yicc86Vp2hZEV9M/yLtMGq91Avgks6UNFHSUkkfSdq+gvRDJI2XtETSZEnXS2pcRtq/STJJN8TmdY3mZXsdFkuXbfmRVffOnXPjfz+e4jnFNO/fnB7X90g7HOeccxX4yyt/Ydu7tuW2/92Wdii1WqoFcElHAEOBS4FtgC+AlyW1KyP90cDVUfrNgZOAI4B/Zkm7HXAq8GVi0WSgY+J1MbAQeDGR9sREOr/P4lwV6nlzT1ru2pK+T/YlryD1+gDnnHPleHXCqwz7bBjFpcX02bBP2uHUamm3AT8HuMvM7gWQdBqwPzCYUNBO2hl4z8weiaYnSnoU2CGeSFJz4GHgZODv8WVmVgJMT6T/LfCEmS1M7G+emU3HObdeNN2sKVu/vXXaYTjnnKtA0bIiTvrvSQCctd1ZDOw6MN2AarnUqpwkNQL6A69l5plZaTS9UxmrvQ/0zzRTkdQd2A94IZHuVuB5M3uNCkjqD/QD/i/L4lslzZb0saTB8udhO7fOFn+7mJ/f+jntMJxzzlXCua+cy+SiyXRv3Z2rf5WtjtRVRpo14G2BfGBGYv4MoHe2FczsEUltgXejwnAD4A4zW9kEJWqnvQ2wXY5xnASMNbP3E/P/AbwBLAb2Bm4DmgM3lbUhSQVAQWyWj0jvXEzJkhJGHzqaRV8vYvMHN6f90d7p0jnnarqXv3uZuz67C4B7fnMPzRr5aFXrqlY1upQ0ELgAOINQyD4E2F/SRdHyjYEbgWPMbGkO22sCHE2W2m8zu9zM3jOzz83sGuBa4C8VbPJ8YH7sNSXHt+ZcvfDtH75l0ZeLaNi2Ia0Gtko7HOeccxUoWlbE70f8HoA/bP8Hduu6W8oR1Q1pFsBnAyVAsgqsPYk22jGXAw+a2d1m9pWZPUMokJ8vKY/QpKUd8JmkYknFwG7AH6Pp5NM9DgWaAg/kEO9HwEZRLXdZrgJaxl4b5bBd5+qFafdNY/r/TQdBn0f6UNCpvJ+Sc865mqB5o+acu9O5bNFuC67a86q0w6kzUiuAm9ly4FNgz8y8qBC9J/BBGas1BUoT80oyqwOvA1sQ2nRnXp8QOmT2izpgxp0E/NfMZuUQcj/gZzNbVs57WmZmRZkXsCCH7TpX5y38aiHfnvEtAF0v60rrPVunHJGrDpJ2lTRC0k/RUK4HV5B+F0nvSZoTDTU7TtKfsqTrLOmhWLqvJG27/t6Jc/VXnvI4e8ez+fzUz73pSRVKexSUocD9kj4BPgaGAM2AzKgoDwBTzez8KP0I4BxJnxNqpHsQasVHRIXrBcDX8R1IWgTMMbPk/B7AroROnCSWHUioif8QWArsRahpv64K3rNz9UpxUTGjDx1N6ZJSNvj1BmxywSZph+SqTzPC8LL3AMNzSL8IuIUwfOwiYBfgTkmLzGwYgKTWwHvAm8C+wCygJ+A9e52rQguWLSBPeSsL3fl5yUYEbl2kWgA3s8clbQhcBnQARgG/NrNMx8wurF7jfQVg0d/OhIx3BHDhWux+MKGN9itZlq0AzgSuJ9Ssf0c0ZOJa7Me5em3GQzNY8s0SCjYqoPeDvVGeDyZUX5jZi0TPV8hlECkz+xz4PDZroqRDgAHAsGjeX4HJZnZiLN0PVRKwc26lIS8N4e1Jb/PQIQ+x40Y7ph1OnZN2DThmdguhxiPbsoGJ6WLCQ3gurcT2B5Yx/wJCrXa2ZS8BL+W6D+dc2Tqd3gnyoPlWzWnUtlHa4bhaRNLWhOc/xJ/n8BvCA9ueJPTxmQrcZmZlVpD4CFXOVc6L377IPaPuQYji0uK0w6mTatUoKM652kcSnU/rTMudWqYdiqslJE2RtIzQh+dWM7s7trg7cDrwLbAPcDtwk6QTytmkj1DlXI7mLZ3HySNOBuDsHc5mly67pBxR3eQFcOdclVsxdwXfnP4NK+atSDsUVzsNALYFTgOGSDoqtiwP+MzMLoiGiR1GaB54Wjnb8xGqnMvROS+fw9QFU+m5QU+u3PPKtMOps1JvguKcq1us1Bh7/FjmPj+XJd8vYauXt0o7JFfLmFmmTfdXktoDlwCPRvOmAWMSq4wFflfO9pYBK0ew8ocaO5fdC9++wL2j7kWIew+6l6YNm6YdUp3lNeDOuSr147U/Mvf5uahAdL+me9rhuNovj9Xbb78H9Eqk2QyYVG0ROVcHxZueDNlxCL/s8suUI6rbvAbcOVdlfn7rZ364MFRe9rylJy36eV+3+kxSc8JwsRndJPUD5prZj5KuAjqb2fFR+jOBH4FxUfpdgXOBm2LbuB54X9IFwBPA9sAp0cs5t5aWlyxn6w5b06xhM67Y44q0w6nzvADunKsSy6YvY8yRY6AU2h/fno4ndUw7JJe+bQnjdWcMjf7eDwwCOhKGm83II7TX7gYUAxMIww7emUlgZv+T9Nso3T8IQxAOMbOH189bcK5+aNesHSOOGsGsxbO86Uk1kJmlHUOdJakQmD9//nwKCwvTDse59aa0uJQv9/qSeW/No2nfpvT/qD/5zfyhDeuiqKiIli1bArSMnqzrqojnzc6tUlxaTIM8r4/NVVXlzd4G3Dm3zpZNWcaSCUvIb55P36f6euHbOedqiZP+exKDnh3EvKXz0g6lXvFLHufcOmvStQnbfr4tC79cSLPezdIOxznnXA5GjB/BA188gBCn9j+VnTbeKe2Q6g2vAXfOrbV4E7aGbRrSevfWKUbjnHMuVz8v+ZlTnzsVgHN2OscL39XMC+DOubVSuryUL/b4gmn3TEs7FOecc5V09ktnM23hNHq16cXlu1+edjj1jhfAnXNrZcK5E5j31jwmnDuB5bOXpx2Oc865HI0YP4IHv3yQPOVx70H30qRhk7RDqne8AO6cq7SZT85k6s1TAej9QG8atW2UckTOOedyMXfJXE55Lgyb/+ed/uxNT1LiBXDnXKUs/mYx408aD0CXv3Wh7QFtU47IOedcrr6b+x1mRu+2vbls98vSDqfe8lFQnHM5K1lcwuhDR1OyoISWu7Wk6+Vd0w7JOedcJWzfeXvGnDmGGQtn0LhB47TDqbe8AO6cy9m3Z33Loq8W0bB9Q/o82oe8Bn4TzTnnapsNmmzABk02SDuMes3Pns65nJgZBV0KUAPR59E+FHQsSDsk55xzOfrDC3/g4S8fxp+AXjN4Adw5lxNJdLukGzt8t4OP9+2cc7XIf8b9h1v+dwvHP3s84+eMTzscRyUL4JK2l1TmM6YlFUg6fN3Dcs7VFMULiylZWrJyuvEm3mawrpJ0nqQmselfSiqITbeQdFs60Tnn1sacxXNWPnDnLzv/hd5te6cckYPK14B/ALTJTEgqktQ9trwV8GhVBOacS5+ZMf7E8Xz+y89ZMmFJ2uG49e8qoEVs+kWgc2y6KXBqtUbknFsnf3zpj8xYNIM+G/bhkoGXpB2Oi1S2AK4KpsuaV/YGpTMlTZS0VNJHkravIP0QSeMlLZE0WdL1krJWyUn6mySTdENi/lvR/PjrjkSaLpKel7RY0kxJ/5LknVZdvTL15qnMemoWi75axPJZ/rCdeiCXPN45V0s8M/YZHvnqkZUP3PFRT2qO9VGgzLl1v6QjgKHAacBHwBDgZUm9zGxmlvRHA1cDg4H3gc2A+6J9npNIux2hpubLMnZ/F/CP2PTi2Lr5wPPAdGBnoCPwALACuCDX9+dcbTb/w/lMOHcCAJtetyktd2yZckTOOedyNXvxbE57/jQAztv5PLbvXG79pqtmaXfCPAe4y8zuNbMxhIL4YkIBO5udgffM7BEzm2hmrxCavKx2VElqDjwMnAz8XMa2FpvZ9NirKLZsb6APcKyZjTKzF4GLgDMl+SP/XJ23Ys4Kxhw+BlthbHjYhnT+Q+eKV3LOOVdjvPDtC8xcNNObntRQa1MD3kdSh+h/Ab2jAi9Azo/Eiwqy/QltDgEws1JJrwFlPRf1feBYSdub2cdR+/P9gAcT6W4Fnjez1yT9vYxtHSPpWEIt9wjgcjPL1ILvBHxlZjNi6V8Gbgf6Ap/n+j6dq22s1Bh73FiWTV5Gk55N6HV3LyRviVCP/F7Swuj/BsAgSbOj6RZlrOOcq2GO3+p4Ni7cmMKCQgoa+LCxNc3aFMBfZ/V2gc9Ffy2an2sTlLZAPjAjMX8GkLWLrpk9Iqkt8K5CiaABcIeZ/TOTRtKRwDbAduXs+xFgEvATsCVwDdALOCRa3qGMuDLLsopGC4gf5X6ycrXO5H9PZu6Lc8lrnEffp/rSoNC7PtQjPxLuHGZMB47LksY5Vwvs3m33tENwZajsmbXbeokiR5IGEtpgn0FoM94DuFHSRWZ2uaSNgRuBvcxsaVnbMbNhscmvJE0DXpe0qZlNWIcQzwcuXof1nUvdhodtyKwnZ9Hp9E4037J5xSu4OsPMuqYdg3Nu3Vw18iqO+MURdG/dveLELjWVKoCb2aSK0kj6RY6bmw2UAO0T89sTal2yuRx40Mzujqa/ktQMGCbpSkKTlnbAZ7Fb5vnArpLOAgrMrCS5UUJhHkKBfkK0/2RvhUycZcUGoTnN0Nh0C2BKOemdq3GadG3C1u9v7Y+Zd865WubpMU9zwRsXcM171/DD2T/Quok/NK2mqpIzbPRwhlMkfQx8kcs6ZrYc+BTYM7advGj6gzJWawqUJuZlCtQiNI/ZAugXe31C6JDZr4zCN1E6gGnR3w+ALSS1i6XZCygCxpTznpaZWVHmBSwoK61zNUlpcSnzRs5bOe2F7/pJ0k6SDkjMO17SD9FwrMPiD+ZxztUcsxbN4vTnTwfgrO3P8sJ3DbdOZ1lJu0q6n1BwPRd4A9ixEpsYCpws6QRJmxM6OTYD7o22/4Ckq2LpRwCnSzpSUjdJexFqxUeYWYmZLTCzr+MvYBEwJ/ofSZtKukhSf0ldJf2GMMTgO2aWGbLwFUJB+0FJW0naB7gCuNXMlq3NZ+VcTTbxoomM2nUUE6+YmHYoLl3/IHQ0B0DSFsD/Aa8RhoA9kNDUzjlXw5z14lnMWjyLX7T7BRftelHa4bgKVLp3VTQCyiDgJKAQeILQ8fDgaCjBnJnZ45I2BC4jdG4cBfw6NvpIF1av8b6C0MnzCsLT2WYRCuUXVmK3y4FfEcYcbwZMBp6OtpmJqySqBbqdUBu+CLif1ccNd65OmP3cbH68OvSra9qracrRuJT1Iwy5mnEk8JGZnQwgaTJwKXBJ9YfmnCvLU2Oe4onRT5CvfO476D4f9aQWqFQBXNIIYFfCQ2qGAC9FhdXT1jYAM7sFuKWMZQMT08WEzP/SSmw/uY3JwG45rDeJMMShc3XWkolLGHf8OAA6/6Ez7Q5rV8Earo5rzeojQO1GeBx9xv+Ajas1IudcuWYtmsUZz58BwPm7nE//Tv1TjsjlorJNUPYl3I682MyeL6dNtXOuhitdVsqYw8dQ/HMxLbZvwabXbZp2SC59M4hGu4qe1bAN8GFseQvCE4GdczXEte9dyXR/kgAAIABJREFUy6zFs9ii3RZctJs3PaktKtsEZRdC05NPJY0lPADnsSqPyjm33k04dwIL/reABq0b0PeJvuQ18o6XjheAqyX9FTiY8GTikbHlWxJGinLO1RBX7HEFBQ0KOGTzQ2iU7w/rri0qdcY1sw+jtoAdgTsJ7QN/irazlyR/8IxztUDRJ0VMvWUqAJs/uDmNN2mcckSuhrgIKAbeJjyQ55RoxKqMwYRO6s65GqKgQQFX7HEF23TcJu1QXCXILNcHV5axAakXoVb8OKAV8KqZ/aYKYqv1JBUC8+fPn09hYWHa4Ti3mukPTWfpD0vpelHXtENxWRQVFdGyZUuAltGwptVGUktgYbKZoaQNgAVmVquboXje7Go7M+OpMU9xcO+DaZjfMO1w6pWqypvX+RnTZjYeOE/S+cABhBoS51wN1+HYDmmH4GoYSfckpstK6vm8cyl6YvQTHPn0key40Y6MPHEkDfLWuTjnqlllR0G5p+JUzFnLWJxz65GZMeWGKbQ/tj2NNvR2gi6rQcAk4HPCw82cczXMjIUzOPOFMwHYu/veXviupSr7rQ2i4sx53dq0OOfWi+n3TGfCOROYcsMUth+7PflN89MOydU8twNHEUZCuRd4yMzmphuScy7DzDjjhTOYs2QOW7Xfigt3rcxjUFxNUtkCuGfOztVCC0Yt4JszvwGg0+mdvPDtsjKzMyWdAxxCaGZylaTnCcPPvmLr2mnIObdOHh/9OMPHDqdBXgPuO/g+H/WkFqvsKChnEkZAuZbwSOLJkp6QtI/KaSzonEtP8fxiRh86GltmtDmgDV3O65J2SK4GM7NlZvaome0F9AFGA7cBEyU1Tzc65+qvGQtncNYLZwHw9wF/p1+HfilH5NZFpQf+9czZudrDzBg3eBxLJyylYJMCet/fG+X5tbLLWSmhWaEAv23iXIqGvDyEOUvm0K9DPy4YcEHa4bh1tK5P3vDM2bkabMqNU5g9fDZqKPo+0ZeGG/hwVa58kgokHSXpVeAbYAvgLKCL/X97dx5fRXn9cfxzkpAAgQRkhwAiiAhqBfcVrIrWnyvVWpe2uNTdqrR1rd20aqsV0dK6tW4VtbVqS91wKYqiKIoLBJSdsO8JW0hy7/n9MZNwiUlIILmT3Hzfr9d9JXfmmZkzIZx78swzz7hvjDY6kebrV0N/xdG9j+bx0x7X1IMpoM63zppZFtvGBx4J/JcgOb/m7vH6DU9Edla8NM6yR5YB0PfevuQcrPmOpWZm9meCB6wVAH8DznH31dFGJSIAAzoOYOKPJtY0Pag0IXWdhlDJWaSJSGuRxpAPhrD88eX0uLJH1OFI03AZsAiYBwwFhlb1Ye/uI5Icl0iz5O7MWj2LvTvtDdQ4N780MXXtAVdyFmlCMnIyyPtJXtRhSNPxJJpKVqTReGb6M/zgxR9w4xE38rtjfxd1OFKP6lqAKzmLNHKLfr+ItFZp9Li6h3pLpE7cfWTUMYhIYPnG5Vz96tXEPU7LjJZRhyP1rE4FuJKzSOO27q11zLt5HsQhe59s2n+7fdQhiYhIHbk7l/33MtZuWcuQbkO48cgbow5J6tmuzoIiIo3E1qVbyT83H+LQ9cKuKr5FRJqocV+O499f/ZsWaS0060mKUgEukgLiZXHyv59P6cpSsvfLZs8/7Rl1SCIishOWbVjG1a9eDcAvh/6SfbvsG3FE0hBUgIukgPm3zKdwUiHpbdMZ9Pwg0ltpWn4RkabG3bn0v5eyrngdQ7oN4YYjbog6JGkgKsBFmrjV/1lNwR8KABjw2ABa79k64ohEAmZ2tJmNN7OlZuZmdvoO2h9pZu+b2Roz22Jms8zsuhra3xju9776j14kGqfudSodW3fU0JMUF3kBbmZXmtkCMys2sylmdvAO2l9rZl+FybnAzEabWZW3B1eVnM1sNzN7IGEfi8zsfjPLrbStV/H6fv2ctUj92VqwFdIh79o8On23U9ThiCTKBj4Hrqxl+03An4Cjgb2B24HbzeySyg3N7CDgUuCL+glVJHpmxsVDLmbhtQs19CTF1flJmPXJzM4G7iWYX3wKcC3wupnt5e4rq2h/LnAXwVM4JwP9gccJpkYcValtdcm5e/j6GZAP9AYeDJedWantBcBrCe/X1/UcRRpajyt70PbgtrT5VpuoQxHZjru/CrwKtXuAiLtPA6YlLFpgZiOAo4CHyxeaWRvgaeDHwC/qMWSRSLg7m0s3k52ZDUDrFrqSmeqi7gEfBTzi7o+5ez5BIb6ZoMCuyuHA++4+zt0XuPsE4Blgu17zSsl5XeI6d5/u7t919/HuPtfd3wZuAU4xs8p/kKx39+UJr+JdPWGR+hIviVd8n3NQDmmZUf93FqlfZjaYIO+/U2nVWOBld38z+VGJ1L+nvniKvcfuzYS5E6IORZIksk9sM8sEDgAqEqi7x8P3h1Wz2WTggPJhKma2B3AS8EqldnVNzrlAkbuXVd6Pma02s4/M7ELTU02kkVgxbgVTB09lU/6mqEMRqXdmttjMtgJTgbHu/mjCuu8DQ4Cb6rC/LDPLKX8Bbes9aJGdtHTDUq557RoKigqYunRq1OFIkkQ5BKUjkA6sqLR8BTCgqg3cfZyZdQTeC4vhDOBBd7+jvE1Ccj6oNkGE+7uVhMuboV8CbxP0yA8H/gy0Ae6vYV9ZQFbCIiV5qXebZm7iq0u+Ir4pzsp/rKTPr/tEHZJIfTuKIN8eCtxlZnPc/Rkz6wmMAY6v4xXJm4BfNUCcIrvE3blk/CWsL17Pgd0P5Pojro86JEmSSMeA15WZDQNuBq4gGDPeDxhjZre6+211Tc5hT8jLBGPBf524zt1vS3g7zcyygZ9TQwGOkrw0sNimGDPOnEF8U5x2327H7rfuHnVIIvXO3eeH335pZl0I8vMzBFdNOwOfJlyQTAeONrOrgCx3j1WxyzsJ7jcq1xZY3AChi9TJk58/ycuzXyYzPZPHT3ucjLQmVZbJLojyX3o1EAO6VFreBVhezTa3AU8lXI78MiyMHzaz31GH5GxmbQlusNwAnOHupTuIdwpwq5llufvWatooyUuDcXe+vvxrNudvJrNbJgPHDcTSNSpKUl4a264svgVUnhriMWAW8Ptqim/CnF2RtzWaUBqDJUVLuOa1awD49dBfM6jzoIgjkmSKrAB39xIz+wQ4FngJwMzSwvd/qmaz1kC80rLyhGvUMjmHPd+vEyTkU2t5KXN/YF0NxbeSvDSoZY8uY8VTKyAdBj47kMwumVGHJFKj8Ib4fgmL+pjZ/sBad19kZncCPdz9h2H7K4FFBDkbgukIf0Z45dHdNwDTKx1jE7DG3bdbLtKYuTuX/PcSCrcWclD3g/j5ET+POiRJsqivddwLPGFmU4GPCKYhzCYomjGzJ4El7l5+s814YJSZTWPbEJTbgPFhcb3D5BwW3xMIivnzgfKbcgBWuXvMzE4h6In/ECgGjicY+nJP/f8IRHZsw7QNzL56NgB7/G4P2h3dLuKIRGrlQOB/Ce/LrxA+AYwEugG9EtanEVxJ7AOUAXOBG4CHGjpQkWQqLiumTWabYOjJ6Rp60hxF+i/u7s+ZWSfgt0BX4DPgRHcvvzGzF9v3eN9OMOf37UAPYBVBUX5LHQ47BDgk/H5OpXV9gAVAKcGDI0YT9KzPIZwysQ7HEak3mV0zyTkkh4ycDHr+vGfU4YjUirtPJMih1a0fWen9A8ADdTzGsJ0ITSRSrVq04rkzn+PrNV/Tv0P/qMORCJi7Rx1Dygp71gsLCwvJycnZYXuRmsTL4sS3xMloq56S5qCoqIjc3FyAXHcvijqeVKLcLFFxdw1PbeLqKzfryR0ijdjmOZsrvk/LSFPxLSLShD3+2eOc9c+zWLnpGw/7lmZGBbhII1X4fiEf7/0xs38ym3hZ5XuPRUSkKVlctJhrX7+W5/Of5+kvno46HImYCnCRRqhkVQkzzp6Blzmlq0o13aCISBPm7vx4/I8p2lrEIT0O4SeH/CTqkCRiKsBFGhmPOTPPm0nJkhJaD2hN/4f7a8ygiEgT9thnj/HanNfISs/i8dMfJz0tPeqQJGIqwEUamYW3L2TdG+tIa53GoOcHady3iEgTVlBYwHWvXwfAbcfcxoCOAyKOSBoDFeAijcjaN9ay4DcLAOj/YH+yB2VHG5CIiOy0xKEnh+YdyqjDRkUdkjQSKsBFGomyDWXMPG8mOHT7cTe6/qBr1CGJiMguWLphKfmr8slKz+Kx0x7T0BOpoGvbIo1ERtsM+j/YnyUPLKHf/f12vIGIiDRqPXJ6MP2K6Xy4+EMNPZHtqAAXaUQ6jehExzM66qZLEZEUkZOVw/C+w6MOQxoZDUERidi6ievYumRrxXsV3yIiTduTnz/Jw588jJ42LtVRD7hIhLbM28L006eT1iKN/d/Zn+yBuulSRKQpW1S4iKteuYoNJRvIzcrl7H3OjjokaYTUAy4SkVhxjBlnzSBWGKPVnq1otWerqEMSEZFd4O5c/J+L2VCygcN7Hs6ZA8+MOiRppFSAi0RkzrVz2PjpRjI6ZDDwuYGktdB/RxGRpuyRTx/hjXlv0DKjpWY9kRrpE18kAiueXsGyh5aBwcCnB9KyZ8uoQxIRkV2wcP1CfjrhpwDc8e076N+hf8QRSWOmAlwkyTblb+KrS74CoPetvdnthN0ijkhERHaFu3Px+IvZWLKRI3oewU8O+UnUIUkjpwJcJMkW/GoB8c1x2h/Xnt1/uXvU4YiIyC76aMlHvD3/bVpltNLQE6kVzYIikmQDnhhAVs8set3YC0vXlIMiIk3dIXmH8P6F7zNn7Rz27LBn1OFIE2Cao7LhmFkOUFhYWEhOTk7U4YhIE1JUVERubi5ArrsXRR1PKlFuFpGdVV+5WUNQRJJgwycbWHjXQjyuP3hFRFLFizNfZOaqmVGHIU2QhqCINLDSdaXMOGsGxfOLwaH3Tb2jDklERHbR/HXz+cGLP6AsXsb7F77PAd0PiDokaULUAy7SgNydWRfMonh+MS37tKT75d2jDklERHZR3ONc9J+L2FS6iYN7HMzgboOjDkmamMgLcDO70swWmFmxmU0xs4N30P5aM/vKzLaYWYGZjTazKidRNrMbzczN7L5Ky1ua2VgzW2NmG83sX2bWpVKbXmb2spltNrOVZna3memKgdRJwR8LWPPvNVimMeifg2jRrkXUIYmIyC56cOqD/G/B/2iV0Yq/nfY30izyckqamEh/Y8zsbOBe4DfAEOBz4HUz61xN+3OBu8L2ewMXAWcDd1TR9iDgUuCLKnY1GjgFOAsYCnQHXkjYNh14GcgEDgd+BIwEflv3s5Tmav1765l34zwA+o3pR9sD2kYckYiI7Kr56+Zz/RvXA3DXcXfRb7d+EUckTVHUf7KNAh5x98fcPR+4DNgMXFhN+8OB9919nLsvcPcJwDPAdr3mZtYGeBr4MbCu0rpcgsJ9lLu/7e6fABcAh5vZoWGz4cBA4Hx3/8zdXwVuBa40s8xdP21JdSUrS8g/Ox9i0PncznS/VENPRESaurjHufA/F7KpdBNH9z6aqw6+KuqQpImKrAAPC9kDgDfLl7l7PHx/WDWbTQYOKB+mYmZ7ACcBr1RqNxZ42d3f5JsOAFpUOu4sYFHCcQ8DvnT3FQnbvQ7kAINqOKcsM8spfwHq8mymiqYUUbq6lNZ7t6b/Q/0x03zfIiJN3XPTn2Pigom0btGav52qoSey86Ic09wRSAdWVFq+AhhQ1QbuPs7MOgLvWVDRZAAPunvFEBQz+z7BcJaDqjluV6DE3ddXcdyuCW2qiouENlW5CfhVDeulmeh4SkcGvz+Y9NbpZLTRrQMiIqngrEFnsWD9Atq3ak/f3fpGHY40YU2qMjCzYcDNwBXAFKAfMMbMbnX328ysJzAGON7diyMI8U6CMe3l2gKLI4hDIuLuFb3dOQfqAR8iIqkkIy2Dm466KeowJAVEee1kNRADulRa3gVYXs02twFPufuj7v6lu79IUJDfZGZpBMNLOgOfmlmZmZUR3GT5k/B9erjvTDNrV8Nxl1cTF1QfG+6+1d2Lyl/AhuraSuopXlzMp4d9yoZP9c8uIpJKPij4gK1lW6MOQ1JIZAW4u5cAnwDHli8Li+hjgQ+q2aw1EK+0LFa+OfAWsC+wf8JrKsENmfu7eyw8Zmml4+4F9Eo47gfAvpVmYzkeKALy63Ke0jzES+Pkfy+fDVM2MPuq2bjriZciIqlg7tq5HPfUcQx5eAjLN1bbBydSJ1EPQbkXeMLMpgIfAdcC2cBjAGb2JLDE3cuv94wHRpnZNLYNQbkNGB8W1xuA6YkHMLNNwBp3nw7g7oVm9lfgXjNbS1BUPwB84O4fhptNICi0nzKz6wnGfd8OjHV3/Qks3zDvhnkUfVBEem46e/99b910KSKSAspnPdlcupnO2Z3pnF3lLMkidRZpAe7uz5lZJ4L5tbsCnwEnJsw+0ovte7xvBzz82gNYRVCU31LHQ18X7vdfQBbBDCdXJMQVM7OTgb8Q9IZvAp4AflnH40gzsOqFVSweHQz1H/D4AFrt0SriiEREpD6M/Wgs7y58l+wW2fz11L9q1hOpN6ZL5Q0nnIqwsLCwkJwc3ZCXijbP2cwnB3xCrChG3k/z6HePHsgg9aOoqIjc3FyA3PCeEqknys1SG3PXzmW/B/djc+lmxp40lisOumLHG0nKq6/crD/lRHZSbEuM/LPyiRXFyDkihz3u3CPqkEREpB4kDj05ZvdjuOzAy6IOSVKMCnCRnRTfGiezayYtOrVg0HODSGuh/04iIqlAQ0+koUV9E6ZIk9WiXQv2fXlfiucXk9UjK+pwRESknpzc/2RenPUiZw08iz7t+0QdjqQgFeAidVS6vpQW7VoAYGlGq7666VJEJJX0ad+HN3/4ZtRhSArTNRWROijbWMa0w6Yx68JZxDbHdryBiIg0Gcs2LKv4Ps3SNPREGox+s0Rqyd35+pKv2TxrM2snrCW2SQW4iEiqmL1mNv0e6McVL1+hp15Kg1MBLlJLSx9cyspnVkI6DHpuEJmdMqMOSURE6kEsHuOCf1/A5tLNzF47m8x05XdpWCrARWqhaGoRc66dA0Df3/cl94jciCMSEZH6cv+U+3m/4H3aZrbl0VMe1dOMpcGpABfZgdJ1peSflY+XOB1P70jeqLyoQxIRkXry9ZqvufntmwG4Z/g99G7XO+KIpDlQAS6yA19d+BXFC4ppuUdL9npsL/WMiIikiPKhJ8VlxRy3x3H8eMiPow5JmgkV4CI70P2y7mTlZTHo+UEV0w+KyI6Z2dFmNt7MlpqZm9npO2h/pJm9b2ZrzGyLmc0ys+sqtbnJzD42sw1mttLMXjKzvRr2TCRVjZkyhskFkzX0RJJO84CL7MBuJ+zGIXMOIS1Lf6+K1FE28DnwN+CFWrTfBPwJ+CL8/kjgITPb5O4Ph22GAmOBjwk+w+4AJpjZQHffVM/xS4rLy8ljt1a7ceexd2roiSSVuXvUMaQsM8sBCgsLC8nJyYk6HKmDkhUlxDbFaLWHHrIj0SgqKiI3Nxcg192Loo5nV5mZA2e4+0t13O4FYJO7/6Ca9Z2AlcBQd3+3lvtUbpYKqzevpkOrDur9llqpr9ysLj2RSjzm5J+bz9QhU1n7xtqowxFptsxsMHA48E4NzcqnJNJ/Vqm1snhZxfcdW3dU8S1JpwJcpJIFv17A+rfX42VOVl5W1OGINDtmttjMtgJTgbHu/mg17dKA+4D33X16DfvLMrOc8hfQtkEClyZh1upZ9Lu/Hy/OfDHqUKQZUwEukmDNa2tYePtCAPZ6ZC+y986OOCKRZuko4EDgMuBaMzunmnZjgX2A7+9gfzcBhQmvxfUUpzQx5bOeLCxcyEOfPISG4UpUdBOmSKi4oJiZ588EoPvl3elyTpeIIxJpntx9fvjtl2bWBfg18ExiGzP7E3AycLS776igvhO4N+F9W1SEN0ujPxzNh4s/JCcrh0dOeURDTyQyKsBFgHhJnPzv5VO2pow2B7Sh3+h+UYckIoE0oGIsmAUV0wPAGcCwhGK9Wu6+FdiasI8GCFMau1mrZ/GLt38BwL3D76Vnbs+II5LmTAW4NFsec9ZPWk/JshIKJxVS9GERGe0yGPTPQZpyUKQemFkbIPGv2T5mtj+w1t0XmdmdQA93/2HY/kpgETArbH808DPg/oR9jAXOBU4DNphZ13B5obtvabizkaasfOjJ1thWTux3IhcOvjDqkKSZUwEuzdKqF1Yx55o5bF1c0SlGWnYa3S7rRqs+mnpQpJ4cCPwv4X35MJAngJFAN6BXwvo0guEifYAyYC5wA/BQQpvLw68TKx3rAuDxXQ9ZUtG9H9yroSfSqETezWdmV5rZAjMrNrMpZnbwDtpfa2ZfhU9JKzCz0WbWMmH95Wb2hZkVha8PzOw7Cet3D5/IVtXrrIR2Va3f0Y0+0gSsemEVM86csV3xDRDfHKfg9wWsemFVRJGJpBZ3n+juVsVrZLh+pLsPS2j/gLvv4+7Z7p7r7kPc/S/uHk9oU9X+zN0fT/oJSpMxf30wUum+E+4jLycv4mhEIn4Qj5mdDTxJcKf7FOBa4CxgL3dfWUX7cwmeqHYhMBnoT9Dj8ay7jwrbnALEgNmAAT8Cfg4MdvcZZpYOdKq060vCNt3cfWO4HyfoUXktod16dy+uw/npYQ+NjMecD3f/8BvFdwWDrLwsDp1/KJauHhKJTqo9iKcxUW5unj5c/CGH9DhEvd+yS+orN0c9BGUU8Ii7PwZgZpcB/0dQYN9VRfvDCeZ7HRe+X2BmzwCHlDdw9/GVtrnFzC4HDgVmuHsMWJ7YwMzOAP5RXnwnWO/uy5GUsX7S+uqLbwCHrQVbWT9pPe2HtU9eYCIi0qAOzTs06hBEKkQ2BMXMMoEDgDfLl4WXGd8EDqtms8nAAeXDVMxsD+Ak4JVqjpEeDhvJBj6ops0BwP7AX6tYPdbMVpvZR2Z2oe3gz2Y97KFx2/jlRubfusMJEwAoWVbSwNGIiEhDyl+Vz+nPns7iIs04KY1PlD3gHYF0YEWl5SuAAVVt4O7jzKwj8F5YDGcAD7r7HYntzGxfgoK7JbAROMPd86uJ4yJgprtPrrT8l8DbwGZgOPBnoA3b341f2U3Ar2pYX63EGTkyu2XS7qh2GgJRTzzuTB8xnTX/XlPrbTK7ZTZgRCIi0pDK4mWMfGkkHy/9mJYZLXn2zGejDklkO1EPQakTMxsG3AxcQTBmvB8wxsxudffbEpp+RdCrnQucCTxhZkMrF+Fm1opgOqvEbQGotL9pZpZNME68pgJ8px72UNWMHFl5WfQb049OIyoPV5facPeKcX6WZmTkZkAadBzRkcJ3CyldVQpV3f4QjgFvd1S75AYsIiL15p7J9/Dx0o/Jzcrlj8P/GHU4It8Q5Swoqwlulqz8uMEuVBqjneA24Cl3f9Tdv3T3FwkK8pvMrOJc3L3E3ee4+yfufhPwOXBNFfs7E2hNcCPojkwB8swsq7oG7r7V3YvKX8CGHe20uhk5ti7ZyowzZ2hGjjqKFcdY+vBSPhrwERunbxvSv/tvdufgrw5mn3/uQ/+/9A8WVr7AEL7vd18/XX0QEWmiZqycwa8mBhejx5w4hh45PSKOSOSbIivA3b0E+AQ4tnxZWEQfSzXjtQmK5XilZbHyzWs43HZPUktwEfAfd69Nlbs/sC58olq98Jgz55o5VffEhsvmXDsHj0U3U01TUbqulIV3LOTD3T/k60u/ZsvXW1jywJKK9a12b0Xrfq0B6DSiE4OeH0RWj+1/JbLyshj0/CBddRARaaLK4mWM/PdISmIl/N+e/8cPv/XDqEMSqVLUQ1DuJRgeMhX4iGAawmygfFaUJ4ElYS82wHhglJlNY9sQlNuA8eHsJoRPVnuV4GlqbQmGmAwDTkg8sJn1I3jK2kmVgwqnMuwCfAgUA8cT9LTfU0/nDWhGjvpQXFDM4vsWs+zhZcQ2Bn+LZfXKoueonnS9qGu123Ua0YmOp3XUuHsRkRRy9/t3M3XpVNq1bMfDpzysKQel0Yq0AHf358ysE/BboCvwGXCiu5ffmNmL7Xu8byfoG74d6AGsIijKb0lo05lgSEk3oBD4AjjB3d+odPgLCcZnT6gitFLgSmA0Qc/6HMIpE3fqRKtR25k2NCNH1TzmfHrYp5QsCX4+2ftl0+v6XnT6XifSWuz44o6lm/6wEZFGKxaPMWnRJJZtWEa3tt04qtdRpKelRx1Wo1UWL2Pc9GCW4jEnjqF72+4RRyRSvUgfxJPqdvSwh3UT1/H5MZ/vcD/f+t+3VCgS3FhZ9EEROYfmYGlBr8bC3y1k3Vvr6Hl9T3Y7YTf1dkjK0IN4Gk5TeBDPCzNf4JrXrtluCr28nDzGnDiGEXuPiDCyxm1z6WbGfTmOiwZfpM8DaRD1lZsjfxR9c9buqHZk5WXVPHod2PDxBjzefP9Q8piz6oVVfHrYp0w7Yhprxm+bTrDXTb3Y/+396XBiByVbEUkJL8x8gTP/ceY35q9eUrSEM/9xJi/MfCGiyBq/1i1ac/GQi/V5II2eCvAIWbrRb0y/8E3lldu+nXf9PBbetjBpcTUWFTOa7P0RM747gw1TNmBZxpZ5WyralPeEi4ikglg8xjWvXYNXcXd++bJrX7uWWDz2jfXN1fSV0xn9wWj9TKRJifomzGavfEaOquYB7zu6L2Vrylh01yK6X958xrLFS+MU3FPA4jGLKV1RCkBG+wx6XNmDHlf1ILOLHpIjIqlp0qJJNT650XEKigqYtGgSw3YflrzAGqnSWCkjXxrJJ8s+YdXmVdxx7B073kikEVAB3gjsaEaOriO7kpa57WLFimdW0GlEJ9KyUvMChmUYq/61itIVpWT1zCJvVB7dLu5GRhv9uopIaiqNlTJlyRTun1LTs962mbZsGkN7D232Qy1+//7v+WTZJ7Rv2Z6rD7466nBEak03YTaghrjUINvPAAAXPUlEQVTRZ8W4Fcw8byZtBrdh4DMDab1X63rZb5Q2Tt/IkjFL6HtP3+CJlcDaN9ZSsqKEzmd3rtWMJiKpRjdhNpzGcBOmuzNn7RwmzJ3AhHkT+N/8/7GhZIfPbttO79zeDO87nAsHX8iheYc2UKSN1xcrvuDAhw+kNF7KU2c8xfn7nR91SNIM1FduVpdiE5PRLoOMDhlsnLaRqUOmsucDe9L1gq5NrhfE3SmcVMiiPyxi7ctrAWjVvxW9ft4LgN2O3y3K8ERE6l1xWTEtM1pWvD/68aNZvnHbg587tOrAt/t8mzfnvcn64vVVjgMHyErPIhaPsbBwIY98+giH5h1aUYAv3bCU2Wtmc1jPw8hMT93heuVDT0rjpZy616mct+95UYckUicqwJuYDid14KAvDmLmD2ay/u31fHXRV6ydsJb+D/anRbsWUYe3Qx5zVv97NYv+sIgNU8LeHoNO3+1E+29rqkURSR0lsRI+KPiACXMn8Ma8N1hctJjFoxaTZmmYGSfveTJz181leN/hHL/H8QzuNpg0S6uYBcWw7YpwC+/OH/fdcQzvO5x3F77LG3PfYHjf4RVt/jHjH1z3+nVkt8hm2O7DOH6P4xnedzgDOg5och01NbnrvbuYtnwa7Vu258H/ezClzk2aBw1BaUANeZnTY86iuxcx/xfzIQZZvbMY+OxAcg/Nrdfj1Kf41jhTh0xlc/5mACzL6HZBN/JG5dF6z6Y/lEakPmkISsOpS26u68Nw5qydw8tfv8wb895g4oKJbCrdtN36GVfMYGCngUBwJbC6wrGqecB75vTkvhPvq3Ee8D9O/iN/mPwHVm5aud3yHm17MLzvcO449g66tqn+KcFNwdota+k1uhebSjfx9zP+znn7qfdbkqe+crMK8AaUjHGGRVOKyD8nn+L5xez32n7sdkLjGroR2xwjvfW2D6v8c/JZ+9paul/Znbyr8zSjiUg1VIA3nNrm5to8DGfVplXkZOWQlZEFwA1v3MAfJv+hon3n7M4ct8dxDN9jOMftcRw9cnrUOs6dfRJm3ON8seIL3pj7BhPmTWDSwklsjW0lMz2TdTeso3WLoMPjldmvkJmeyZG9jtxuaExT8Pnyz3lm+jPceeyd6v2WpFIB3gQk60afssIy1ryyhi7ndKlYFi+Lk5YR3c2LxQXFLL5vMcseXcYBHx1QcbPo1mVbSW+TTkZbjX4SqYkK8IZTm9xcPgyk8jjs8mEhpw84nYXrFzJt+TReOfcVvrPndwB4Z8E73D7pdobvMZzj+x7Pfl32I82ivZF8S+kW3lv0HnPWzuHygy6vWD74ocF8tvwzWma05OjeR1fEvG/nfVXUilRDBXgTENWd9lvmb+HzYz+n77196XR6p6QdF4IZTQruLmDluJV4WfC71fsXvelzW5+kxiHS1KkAbzg7ys2xeIzdx+xe43zcie469i5uOPKGeo6yYcXiMS4efzGvz3mdZRuXbbeuS3YXztnnHEafODqi6Ko2Y+UMSmIlDO42OOpQpBnTLChSrUW/X0Tx/GJmnDGD7pd3p+8f+5LeaseXLXdWVTOaALQb1o6e1/dktxMb17AYEZGa7OhhOOVuPvJmrj7k6iY5pjo9LZ3HTnsMdyd/VX7FjaLvLHyHFZtWsHrL6oq2cY/zy//9kiN7HcnRvY+uGMKSTKWxUs5/8Xymr5zO0yOe5nuDvpf0GETqkwrwFLTn/XuSkZNBwd0FLP3LUta/u56Bzw6kzT5tGuR48eI408+YTtnaMjDoOKIjvX7ei5xDoplfV0RkVyzbsGzHjYB9Ou/TJIvvRGbGoM6DGNR5ENcddh1by7YyuWAyuS233dD/5Yov+d2k3wFUjBkvn11l/677J2WIzR2T7uCz5Z/RoVUHhvYe2uDHE2loesJJCkrLTKPvH/qy3+v70aJLCzbP2MynB33Kkj8voT6GHMWKY6x4ZkXFvtJbpdPzZz3pdmk3Dv7qYPZ5fh8V3yLSZHVr261e2zUlWRlZHNPnGIZ0G7LdsosHX0yv3F6UxEp4e/7b3PTWTRzw8AF0uacL474c16Axfbb8M26fdDsAfzrpT3Rp02UHW4g0fuoBT2G7Dd+Ng744iFkjZ7H21bXMvnI2aVlpdLto5z40SteXsvTBpSwZs4SS5SVk5GbQ4aQOAPS+qXd9hi4iEpmjeh1FXk4eS4qWVPkwHMPIy8njqF5HRRBd8g3oOIBHTn0Ed2f22tkVw1Xenv82qzevpnN254q2kxZO4p/5/+T4PY5n2O7DaJvVdpeOXRIrYeRLIymLlzFi7xGcPejsXT0dkUZBBXiKy+ycyb7/3ZfF9y9m5bMr6XL+N3sOPOasn7SekmUlZHbLpN1R7bD0bXfAFy8OZzR5aBmxjTEAsnpmEd8ST9p5iIgkS3paOmNOHFPjw3DuO/G+Wk0JmErMjP4d+tO/Q3+uOvgqSmOlTFkyhQO7H1jR5oWZL/DARw/wwEcPkJGWwWF5h1U8aOjA7gfW6meWOP3i63Nf5/MVn9OhVQf+fNKfNTuLpAzNgtKAopoFpToe84rCOl4WZ/lfl5PRPoO5P53L1sVbK9pl5WXRb0w/2g9vz+yrZrPy6W0zmmTvk03P63vS+fudSWuhEUwiDUWzoDScXZkHvDYPw2nO3pr3Fs/nP8+EeROYt27eduvat2zPl5d/WeNc6FX9zAFGHTqKP57wxwaJWaQuNA1hE9DYCvBEC36zgAW/XlD1yrCDYeA/B7LwtwvZ9MWm7WY0UQ+ESMNTAd5wGvJJmLLNvHXzKh4G9Pb8t8lukU3BdQUVnyE/m/AzNpVsYnjf4RzT5xjenv92lXOvQ3Dl4fnvPa8/fCRyKsCbgMZcgK94bgUzz5lJFXkuYEFP+IAnBpCenU7OwY0rfpFUpwK84TTm3JyqyuJlFBQW0Kd98EyIuMfpfHdn1mxZAwQFdov0FpTESqrcvnzc/fxr5usPIIlUfeVmjSFopjK7ZFZffAM4bC3YCoaKbxER2SUZaRkVxTcEBfhjpz3GVQddxV4d9sLxaotvAMcpKCpg0qJJyQhXpMFFXoCb2ZVmtsDMis1sipkdvIP215rZV2a2xcwKzGy0mbVMWH+5mX1hZkXh6wMz+06lfUw0M6/0erBSm15m9rKZbTazlWZ2t5mlzE2rJcuqT3Q7005ERKS2MtIyOGWvU3jgpAeYddUs7j/x/lptV9s52kUau0gLSjM7G7gXuAyYAlwLvG5me7n7yiranwvcBVwITAb6A48T9OWOCpstBm4EZhOMZv4R8G8zG+zuMxJ29wjwy4T3mxOOkw68DCwHDge6AU8CpcDNu3TSjURmt8x6bSciIrKz9u2yb63apeLc69I8Rd0DPgp4xN0fc/d8gkJ8M0GBXZXDgffdfZy7L3D3CcAzQEWvubuPd/dX3H22u3/t7rcAG4FDK+1rs7svT3gljuMZDgwEznf3z9z9VeBW4EozS4mKtN1R7cjKy6q44fIbLJhqsN1R7ZIal4iIND/lc69bNR9KhtEzp2ezmXtdUl9kBXhYyB4AvFm+zN3j4fvDqtlsMnBA+TAVM9sDOAl4pZpjpJvZ94Fs4INKq88zs9VmNt3M7jSz1gnrDgO+dPcVCcteB3KAQTWcU5aZ5ZS/gF17AkEDsnSj35h+4ZvKK4Mv/e7rt9184CIiIg2hfO514BtFeHOee11SV5Q94B2BdGBFpeUrgK5VbeDu4wiGjbxnZqXAXGCiu9+R2M7M9jWzjcBW4EHgjLCHvdw44HzgGOBO4AfA3xPWd60mrvJ11bkJKEx4La6hbeQ6jejEoOcHkdUja7vlWXlZDHp+EJ1GdIooMhERaW5G7D2C57/3/DfmCc/LydMUhJJymtRNhWY2jGAM9hUEY8b7AWPM7FZ3vy2h6VfA/kAucCbwhJkNLS/C3f3hhLZfmtky4C0z6+vuc3chxDsJxrSXa0sTKMI7ntaxxidhioiIJMOIvUdw2l6nae51SXlRFuCrgRhQ+dnoXQhufqzKbcBT7v5o+P5LM8sGHjaz34VDWHD3EmBO2OYTMzsIuAa4tJr9Tgm/9iPoVV9OwrjyhLioITbcfStBrztAk3lgjaUb7Ye1jzoMERER0tPSGbb7sKjDEGlQkQ1BCYvkT4Bjy5eZWVr4vvJ47XKtgXilZbHyzWs4XBqQVcP6/cOv5fMbfQDsa2adE9ocDxQBiUNZRERERETqJOohKPcSDA+ZCnxEMA1hNvAYgJk9CSxx95vC9uOBUWY2jW1DUG4Dxrt7LNzmTuBVYBHBEJBzgWHACeH6vuGyV4A1wH7AaOBdd/8iPM4EgkL7KTO7nmDc9+3A2LCXW0RERERkp0RagLv7c2bWCfgtQZH7GXBiwuwjvdi+x/t2gjm/bwd6AKsIivJbEtp0JpizuxvBjZBfACe4+xvh+hLgOLYV+wXAv8J9lscVM7OTgb8Q9IZvAp5g+3nDRURERETqzNxreh657IpwKsLCwsJCcnL0OHcRqb2ioiJyc3MBcis9p0B2kXKziOys+srNUQ9BaRaKivTZKSJ1o7zR8PQzFpG6qq+8oR7wBmRmPWjk0xCKSKOX5+5Log4ilSg3i0g92KXcrAK8AVkwD2F3YEMtNymfNzyvDtukAp138znv5njOsPPn3RZY6krU9Uq5udZ03s3nvJvjOUOEuVlDUBpQ+A9T67+OEuYN39CcxnzqvJvPeTfHc4ZdOu9m8zNKJuXm2tF5N5/zbo7nDNHm5igfRS8iIiIi0uyoABcRERERSSIV4I3LVuA3JDzOvpnQeTcfzfGcofmed6porv9+Ou/mozmeM0R43roJU0REREQkidQDLiIiIiKSRCrARURERESSSAW4iIiIiEgSqQBvBMzsJjP72Mw2mNlKM3vJzPaKOq5kMrMbzczN7L6oY2loZtbDzP5uZmvMbIuZfWlmB0YdV0Mys3Qzu83M5ofnPNfMbrWESVhTgZkdbWbjzWxp+Pt8eqX1Zma/NbNl4c/hTTPbM6p4pWbKzcrNys2poTHmZhXgjcNQYCxwKHA80AKYYGbZkUaVJGZ2EHAp8EXUsTQ0M2sPvA+UAt8BBgI/BdZFGVcS3ABcDlwF7B2+vx64OsqgGkA28DlwZTXrrwd+AlwGHAJsAl43s5bJCU/qSLlZuVm5OTU0utysWVAaITPrBKwEhrr7u1HH05DMrA3wKXAF8AvgM3e/NtqoGo6Z3QUc4e5HRR1LMpnZf4EV7n5RwrJ/AVvc/fzoIms4ZubAGe7+UvjegKXAH939nnBZLrACGOnuz0YWrNSKcrNyc6pRbo4uN6sHvHHKDb+ujTSK5BgLvOzub0YdSJKcCkw1s3+Gl7SnmdmPow4qCSYDx5pZfwAz+xZwJPBqpFElVx+gK1Dxu+7uhcAU4LCogpI6UW5OXcrNKDeXL0hGbs5oqB3LzjGzNOA+4H13nx51PA3JzL4PDAEOijqWJNqD4HLfvcAdBOd+v5mVuPsTkUbWsO4CcoBZZhYD0oFb3P3paMNKqq7h1xWVlq9IWCeNlHJzylNuVm5Oam5WAd74jAX2IfgLNGWZWU9gDHC8uxdHHU8SpQFT3f3m8P00M9uHYNxZKif57wHnAecCM4D9gfvMbGmKf7hJ6lBuTm3KzcrNSaUhKI2Imf0JOBk4xt0XRx1PAzsA6Ax8amZlZlZGcMPTT8L36dGG12CWAfmVls0EekUQSzLdDdzl7s+6+5fu/hQwGrgp4riSaXn4tUul5V0S1kkjpNys3JzClJsjys0qwBuBcPqbPwFnAN929/lRx5QEbwH7Evy1Xf6aCjwN7O/usQhja0jvA5WnMesPLIwglmRqDcQrLYvRvHLQfIJkfmz5AjPLIbjj/oOogpLqKTcrN6Pc3BxEkps1BKVxGEtw+ec0YIOZlY85KnT3LdGF1XDcfQOw3ThKM9sErEnx8ZWjgclmdjPwD+Bg4JLwlcrGA7eY2SKCy5yDgVHA3yKNqp6FM0f0S1jUx8z2B9a6+6JwLuVfmNlsgqR/G8Hd9y8lP1qpBeVmlJsjjarhKTdHlJs1DWEjEE6JU5UL3P3xZMYSJTObSIpPdQVgZicDdwJ7EvxHv9fdH4k2qoZlZm0JEtoZBJe3lwLPAL9195IoY6tPZjYM+F8Vq55w95HhdFe/IfhQbwe8B1zh7l8nL0qpLeXmgHJz6lJuji43qwAXEREREUmi5jTGR0REREQkcirARURERESSSAW4iIiIiEgSqQAXEREREUkiFeAiIiIiIkmkAlxEREREJIlUgIuIiIiIJJEKcBERERGRJFIBLgKY2UgzW18P+5kYPtI2cmb2azP7LOo4RER2lnKzpCoV4CKB54D+UQdRz+4Bjo06CBGRXaDcLCkpI+oARBoDd98CbIk6jvrk7huBjVHHISKys5SbJVWpB1xSkpmdbGbrzSw9fL+/mbmZ3ZXQ5lEz+3v4/XaXOcsvEZrZD8xsgZkVmtmzZtY2oU22mT1pZhvNbJmZ/bSKONqHbdaZ2WYze9XM9gzXmZmtMrMzE9p/ZmbLEt4faWZbzax1Nec5zMw+MrNN4fm+b2a9E88hoa1X8VqQsH6fML6NZrbCzJ4ys451/NGLiFRLuVm5WQIqwCVVTQLaAoPD90OB1cCwhDZDgYk17KMvcDpwcvgaCtyYsP7ucNlpwPBw30Mq7eNx4EDgVOAwwIBXzKyFuzvwbnlMZtYe2BtoZWYDEmL82N03Vw7OzDKAl4B3gP3C/T8MeDXn0y3h1Q+YEx4fM2sHvA1MC+M9EegC/KO6H46IyE5Qbv4m5eZmSENQJCW5e2HYwzAMmBp+HQ38yszaALkEie6dGnaTBox09w0AZvYUwbi9W8J9XASc7+5vhet/BCwu3zjsTTkVOMLdJ4fLzgMKCD48/knwIXNpuMnRBEl2eRjvrPBrdTHmhOfxX3efGy6bWcPPZHkYgwH/AgoTjn0VMM3db06I/0KgwMz6u/vX1e1XRKS2lJur/JkoNzdD6gGXVPYOMCxMakcBLxAkwSMJei+WuvvsGrZfUJ7gQ8uAzuH3fYFMYEr5SndfC3yV0H5voKxSmzVhm70TYhxoZp3Y1uszMYy7BXA41fQEhcd7HHjdzMab2TVm1q2G8yl3B0GPzGnh+EqAbwHHhJc4N5rZRoIPmfJzFRGpL8rNVVNubkZUgEsqm0iQ0L8FlLr7rHDZMIKEWlMPC0BppfdO/f+f+RJYG8aTmOSHAgcBLYDJ1W3s7hcQJOzJwNnA12Z2aHXtzex84DrgDHdfkrCqDTAe2L/Sa0/CS6EiIvVkIsrN21Fubn5UgEsqKx9reB3bEvpEgiQ/jJrHGO7IXIIPgUPKF4TjBBOny5pJMMwrsU0HYC8gHyAcaziJYKziIOA94Asgi+AS5FR331RTIO4+zd3vdPfDgenAuVW1M7PDgEeBS939w0qrPw2Pv8Dd51R61Xh8EZE6Um5OoNzcPKkAl5Tl7usIEuZ5bEvo7xLcjNOfHfey1LTvjcBfgbvN7Ntmtg/BJcd4QpvZwL+BR8I75r8F/B1YEi4vNxE4B/jM3Te6ezyM87yaYjSzPmZ2p5kdZma9zWw4Qa/IN8YamllX4EXgWYLLol3DV6ewyVhgN+AZMzvIzPqa2Qlm9piFsxWIiNQH5ebt2io3N1MqwCXVvQOkEyb5cGxePrDc3b+qYbva+DlBD8l44E2CHpJPKrW5IFz2X+ADgjvtT3L3xEuo28UYmljFsso2AwMIbtr5muAu+7HAQ1W0HUBw5/yPCMZLlr8+BnD3pcAR4TEnEFx+vQ9YT8IHl4hIPVFuDig3N1MWXGUREREREZFkUA+4iIiIiEgSqQAXEREREUkiFeAiIiIiIkmkAlxEREREJIlUgIuIiIiIJJEKcBERERGRJFIBLiIiIiKSRCrARURERESSSAW4iIiIiEgSqQAXEREREUkiFeAiIiIiIkmkAlxEREREJIn+HzSjoLH/zB6NAAAAAElFTkSuQmCC\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "minimum avg mae over window sizes = 0.8353306033873166 at window size = 3 and mse = 1.3127908019585943\n",
            "minimum avg mse over window sizes = 1.3066071470052691 at window size = 5 and mae = 0.837623648531389\n"
          ]
        }
      ],
      "source": [
        "avg_mae_list = [0.8371292235590698,0.8353306033873166,0.837623648531389,0.8539131697407135]\n",
        "avg_mse_list = [1.3096931666826137,1.3127908019585943,1.3066071470052691,1.3919414789888864]\n",
        "window_size_list = [2, 3, 5, 10]\n",
        "print(f'avg mae over window size = [2, 3, 5, 10]: {avg_mae_list}')\n",
        "print(f'avg mse over window size = [2, 3, 5, 10]: {avg_mse_list}')\n",
        "    \n",
        "plt.rcParams.update({'figure.figsize':(7.5,3.5), 'figure.dpi':100})\n",
        "fig, axes = plt.subplots(1, 2)\n",
        "axes[0].plot(window_size_list, avg_mae_list, 'm--o')\n",
        "axes[0].set_title('Avg MAE over varying window size')\n",
        "axes[0].set_xlabel('window size')\n",
        "axes[0].set_ylabel('MAE')\n",
        "\n",
        "axes[1].plot(window_size_list, avg_mse_list, 'g--o')\n",
        "axes[1].set_title('Avg MSE over varying window size')\n",
        "axes[1].set_xlabel('window size')\n",
        "axes[1].set_ylabel('MSE')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "index = 0\n",
        "min = 100\n",
        "min_index = 0\n",
        "for i in avg_mae_list:\n",
        "  if i<min:\n",
        "    min = i\n",
        "    min_index = index\n",
        "  index+=1\n",
        "print(f'minimum avg mae over window sizes = {min} at window size = {window_size_list[min_index]} and mse = {avg_mse_list[min_index]}')\n",
        "\n",
        "index = 0\n",
        "min = 100\n",
        "min_index = 0\n",
        "for i in avg_mse_list:\n",
        "  if i<min:\n",
        "    min = i\n",
        "    min_index = index\n",
        "  index+=1\n",
        "print(f'minimum avg mse over window sizes = {min} at window size = {window_size_list[min_index]} and mae = {avg_mae_list[min_index]}')"
      ],
      "id": "9K2Htd_bGwep"
    },
    {
      "cell_type": "markdown",
      "source": [
        "sliding window size = 5"
      ],
      "metadata": {
        "id": "uyvusHXzSV8t"
      },
      "id": "uyvusHXzSV8t"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WTf958i9Dec7"
      },
      "source": [
        "Varying batch size"
      ],
      "id": "WTf958i9Dec7"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jKoULtIrG6TO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04eec56b-933f-406a-da44-d6614bb3021b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "wordId_dict finished\n",
            "load reviews finished\n",
            "load data: 3.652s\n",
            "506 2581\n",
            "15\n",
            "shape (19930, 300)\n",
            "18880 18880 18880\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/training/rmsprop.py:188: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "epoch0 train time: 21.082s test time: 0.863  loss = 74.465 val_mse = 6.420 mse = 6.477 mae = 2.357\n",
            "epoch1 train time: 5.793s test time: 0.731  loss = 65.597 val_mse = 1.563 mse = 1.499 mae = 0.903\n",
            "epoch2 train time: 5.804s test time: 0.732  loss = 60.813 val_mse = 1.569 mse = 1.504 mae = 0.884\n",
            "epoch3 train time: 5.870s test time: 0.734  loss = 51.605 val_mse = 1.551 mse = 1.492 mae = 0.879\n",
            "epoch4 train time: 5.884s test time: 0.738  loss = 40.422 val_mse = 1.529 mse = 1.476 mae = 0.875\n",
            "epoch5 train time: 6.077s test time: 0.740  loss = 30.952 val_mse = 1.506 mse = 1.460 mae = 0.873\n",
            "epoch6 train time: 5.947s test time: 0.741  loss = 23.342 val_mse = 1.484 mse = 1.444 mae = 0.871\n",
            "epoch7 train time: 5.979s test time: 0.747  loss = 17.321 val_mse = 1.462 mse = 1.427 mae = 0.869\n",
            "epoch8 train time: 5.999s test time: 0.752  loss = 12.638 val_mse = 1.438 mse = 1.411 mae = 0.867\n",
            "epoch9 train time: 6.020s test time: 0.759  loss = 9.068 val_mse = 1.414 mse = 1.394 mae = 0.866\n",
            "epoch10 train time: 6.041s test time: 0.752  loss = 6.409 val_mse = 1.389 mse = 1.378 mae = 0.864\n",
            "epoch11 train time: 6.079s test time: 0.773  loss = 4.484 val_mse = 1.363 mse = 1.361 mae = 0.864\n",
            "epoch12 train time: 6.111s test time: 0.781  loss = 3.138 val_mse = 1.338 mse = 1.346 mae = 0.863\n",
            "epoch13 train time: 6.138s test time: 0.767  loss = 2.236 val_mse = 1.315 mse = 1.332 mae = 0.862\n",
            "epoch14 train time: 6.170s test time: 0.768  loss = 1.664 val_mse = 1.294 mse = 1.321 mae = 0.859\n",
            "epoch15 train time: 6.169s test time: 0.770  loss = 1.325 val_mse = 1.275 mse = 1.310 mae = 0.854\n",
            "epoch16 train time: 6.181s test time: 0.775  loss = 1.142 val_mse = 1.255 mse = 1.300 mae = 0.850\n",
            "epoch17 train time: 6.197s test time: 0.774  loss = 1.053 val_mse = 1.238 mse = 1.292 mae = 0.849\n",
            "epoch18 train time: 6.237s test time: 0.777  loss = 1.012 val_mse = 1.251 mse = 1.305 mae = 0.831\n",
            "epoch19 train time: 6.290s test time: 0.789  loss = 0.989 val_mse = 1.214 mse = 1.284 mae = 0.839\n",
            "epoch20 train time: 6.458s test time: 0.791  loss = 0.969 val_mse = 1.243 mse = 1.310 mae = 0.827\n",
            "epoch21 train time: 6.322s test time: 0.789  loss = 0.951 val_mse = 1.227 mse = 1.303 mae = 0.827\n",
            "epoch22 train time: 6.332s test time: 0.792  loss = 0.936 val_mse = 1.208 mse = 1.290 mae = 0.829\n",
            "epoch23 train time: 6.312s test time: 0.789  loss = 0.923 val_mse = 1.187 mse = 1.276 mae = 0.835\n",
            "epoch24 train time: 6.338s test time: 0.786  loss = 0.913 val_mse = 1.198 mse = 1.288 mae = 0.832\n",
            "epoch25 train time: 6.280s test time: 0.786  loss = 0.903 val_mse = 1.183 mse = 1.281 mae = 0.835\n",
            "epoch26 train time: 6.280s test time: 0.788  loss = 0.893 val_mse = 1.198 mse = 1.294 mae = 0.827\n",
            "epoch27 train time: 6.260s test time: 0.786  loss = 0.884 val_mse = 1.181 mse = 1.281 mae = 0.833\n",
            "epoch28 train time: 6.271s test time: 0.780  loss = 0.880 val_mse = 1.190 mse = 1.293 mae = 0.830\n",
            "epoch29 train time: 6.304s test time: 0.785  loss = 0.872 val_mse = 1.176 mse = 1.288 mae = 0.837\n",
            "epoch30 train time: 6.295s test time: 0.788  loss = 0.866 val_mse = 1.183 mse = 1.290 mae = 0.831\n",
            "epoch31 train time: 6.328s test time: 0.782  loss = 0.862 val_mse = 1.176 mse = 1.292 mae = 0.837\n",
            "epoch32 train time: 6.310s test time: 0.798  loss = 0.857 val_mse = 1.185 mse = 1.293 mae = 0.831\n",
            "epoch33 train time: 6.313s test time: 0.804  loss = 0.853 val_mse = 1.201 mse = 1.303 mae = 0.826\n",
            "epoch34 train time: 6.497s test time: 0.784  loss = 0.849 val_mse = 1.185 mse = 1.294 mae = 0.831\n",
            "epoch35 train time: 6.315s test time: 0.783  loss = 0.847 val_mse = 1.196 mse = 1.306 mae = 0.830\n",
            "epoch36 train time: 6.317s test time: 0.783  loss = 0.843 val_mse = 1.182 mse = 1.298 mae = 0.835\n",
            "epoch37 train time: 6.282s test time: 0.779  loss = 0.840 val_mse = 1.191 mse = 1.304 mae = 0.829\n",
            "epoch38 train time: 6.277s test time: 0.778  loss = 0.836 val_mse = 1.184 mse = 1.293 mae = 0.832\n",
            "epoch39 train time: 6.326s test time: 0.788  loss = 0.834 val_mse = 1.192 mse = 1.304 mae = 0.831\n",
            "epoch40 train time: 6.287s test time: 0.791  loss = 0.832 val_mse = 1.181 mse = 1.299 mae = 0.835\n",
            "epoch41 train time: 6.307s test time: 0.783  loss = 0.829 val_mse = 1.192 mse = 1.307 mae = 0.830\n",
            "epoch42 train time: 6.298s test time: 0.784  loss = 0.827 val_mse = 1.182 mse = 1.297 mae = 0.834\n",
            "epoch43 train time: 6.289s test time: 0.791  loss = 0.827 val_mse = 1.190 mse = 1.306 mae = 0.831\n",
            "epoch44 train time: 6.300s test time: 0.786  loss = 0.824 val_mse = 1.181 mse = 1.301 mae = 0.834\n",
            "epoch45 train time: 6.316s test time: 0.788  loss = 0.822 val_mse = 1.189 mse = 1.307 mae = 0.831\n",
            "epoch46 train time: 6.290s test time: 0.786  loss = 0.820 val_mse = 1.184 mse = 1.299 mae = 0.835\n",
            "epoch47 train time: 6.315s test time: 0.792  loss = 0.820 val_mse = 1.190 mse = 1.306 mae = 0.832\n",
            "epoch48 train time: 6.319s test time: 0.786  loss = 0.818 val_mse = 1.180 mse = 1.300 mae = 0.837\n",
            "epoch49 train time: 6.313s test time: 0.794  loss = 0.816 val_mse = 1.186 mse = 1.307 mae = 0.833\n",
            "epoch50 train time: 6.299s test time: 0.791  loss = 0.814 val_mse = 1.180 mse = 1.300 mae = 0.836\n",
            "epoch51 train time: 6.316s test time: 0.790  loss = 0.814 val_mse = 1.187 mse = 1.307 mae = 0.833\n",
            "epoch52 train time: 6.312s test time: 0.797  loss = 0.812 val_mse = 1.178 mse = 1.301 mae = 0.836\n",
            "epoch53 train time: 6.339s test time: 0.791  loss = 0.811 val_mse = 1.185 mse = 1.307 mae = 0.834\n",
            "epoch54 train time: 6.317s test time: 0.784  loss = 0.810 val_mse = 1.182 mse = 1.300 mae = 0.836\n",
            "epoch55 train time: 6.313s test time: 0.789  loss = 0.809 val_mse = 1.189 mse = 1.306 mae = 0.832\n",
            "epoch56 train time: 6.297s test time: 0.784  loss = 0.808 val_mse = 1.189 mse = 1.308 mae = 0.836\n",
            "epoch57 train time: 6.284s test time: 0.790  loss = 0.808 val_mse = 1.184 mse = 1.305 mae = 0.833\n",
            "epoch58 train time: 6.321s test time: 0.783  loss = 0.806 val_mse = 1.183 mse = 1.303 mae = 0.836\n",
            "epoch59 train time: 6.319s test time: 0.793  loss = 0.804 val_mse = 1.189 mse = 1.309 mae = 0.833\n",
            "epoch60 train time: 6.307s test time: 0.844  loss = 0.804 val_mse = 1.182 mse = 1.302 mae = 0.836\n",
            "epoch61 train time: 6.824s test time: 0.799  loss = 0.803 val_mse = 1.183 mse = 1.303 mae = 0.834\n",
            "epoch62 train time: 6.321s test time: 0.783  loss = 0.802 val_mse = 1.181 mse = 1.303 mae = 0.836\n",
            "epoch63 train time: 6.303s test time: 0.787  loss = 0.801 val_mse = 1.187 mse = 1.307 mae = 0.833\n",
            "epoch64 train time: 6.295s test time: 0.789  loss = 0.801 val_mse = 1.182 mse = 1.302 mae = 0.837\n",
            "epoch65 train time: 6.302s test time: 0.791  loss = 0.799 val_mse = 1.192 mse = 1.312 mae = 0.834\n",
            "epoch66 train time: 6.309s test time: 0.788  loss = 0.800 val_mse = 1.183 mse = 1.302 mae = 0.836\n",
            "epoch67 train time: 6.317s test time: 0.858  loss = 0.798 val_mse = 1.190 mse = 1.310 mae = 0.833\n",
            "epoch68 train time: 6.369s test time: 0.787  loss = 0.798 val_mse = 1.184 mse = 1.304 mae = 0.836\n",
            "epoch69 train time: 6.310s test time: 0.790  loss = 0.796 val_mse = 1.186 mse = 1.306 mae = 0.833\n",
            "epoch70 train time: 6.293s test time: 0.786  loss = 0.796 val_mse = 1.182 mse = 1.301 mae = 0.836\n",
            "epoch71 train time: 6.311s test time: 0.782  loss = 0.796 val_mse = 1.188 mse = 1.304 mae = 0.834\n",
            "epoch72 train time: 6.305s test time: 0.786  loss = 0.795 val_mse = 1.184 mse = 1.304 mae = 0.837\n",
            "epoch73 train time: 6.295s test time: 0.789  loss = 0.794 val_mse = 1.192 mse = 1.312 mae = 0.833\n",
            "epoch74 train time: 6.306s test time: 0.791  loss = 0.794 val_mse = 1.185 mse = 1.301 mae = 0.835\n",
            "epoch75 train time: 6.274s test time: 0.785  loss = 0.792 val_mse = 1.186 mse = 1.305 mae = 0.833\n",
            "epoch76 train time: 6.281s test time: 0.779  loss = 0.793 val_mse = 1.187 mse = 1.306 mae = 0.836\n",
            "epoch77 train time: 6.311s test time: 0.782  loss = 0.792 val_mse = 1.187 mse = 1.307 mae = 0.833\n",
            "epoch78 train time: 6.298s test time: 0.785  loss = 0.792 val_mse = 1.186 mse = 1.304 mae = 0.835\n",
            "epoch79 train time: 6.316s test time: 0.798  loss = 0.791 val_mse = 1.189 mse = 1.309 mae = 0.833\n",
            "epoch80 train time: 6.289s test time: 0.784  loss = 0.790 val_mse = 1.187 mse = 1.306 mae = 0.836\n",
            "epoch81 train time: 6.328s test time: 0.788  loss = 0.789 val_mse = 1.193 mse = 1.314 mae = 0.833\n",
            "epoch82 train time: 6.306s test time: 0.787  loss = 0.790 val_mse = 1.185 mse = 1.304 mae = 0.836\n",
            "epoch83 train time: 6.321s test time: 0.787  loss = 0.788 val_mse = 1.188 mse = 1.308 mae = 0.834\n",
            "epoch84 train time: 6.328s test time: 0.793  loss = 0.788 val_mse = 1.186 mse = 1.306 mae = 0.836\n",
            "epoch85 train time: 6.311s test time: 0.789  loss = 0.787 val_mse = 1.191 mse = 1.311 mae = 0.833\n",
            "epoch86 train time: 6.334s test time: 0.794  loss = 0.787 val_mse = 1.186 mse = 1.303 mae = 0.835\n",
            "epoch87 train time: 6.355s test time: 0.802  loss = 0.786 val_mse = 1.186 mse = 1.306 mae = 0.834\n",
            "epoch88 train time: 6.330s test time: 0.790  loss = 0.786 val_mse = 1.185 mse = 1.304 mae = 0.836\n",
            "epoch89 train time: 6.303s test time: 0.798  loss = 0.785 val_mse = 1.188 mse = 1.306 mae = 0.834\n",
            "epoch90 train time: 6.322s test time: 0.783  loss = 0.785 val_mse = 1.191 mse = 1.310 mae = 0.835\n",
            "epoch91 train time: 6.299s test time: 0.785  loss = 0.785 val_mse = 1.186 mse = 1.305 mae = 0.833\n",
            "epoch92 train time: 6.307s test time: 0.787  loss = 0.784 val_mse = 1.190 mse = 1.306 mae = 0.835\n",
            "epoch93 train time: 6.300s test time: 0.788  loss = 0.783 val_mse = 1.189 mse = 1.305 mae = 0.833\n",
            "epoch94 train time: 6.319s test time: 0.786  loss = 0.783 val_mse = 1.188 mse = 1.306 mae = 0.834\n",
            "epoch95 train time: 6.300s test time: 0.783  loss = 0.782 val_mse = 1.189 mse = 1.310 mae = 0.834\n",
            "epoch96 train time: 6.321s test time: 0.785  loss = 0.781 val_mse = 1.187 mse = 1.306 mae = 0.835\n",
            "epoch97 train time: 6.302s test time: 0.788  loss = 0.781 val_mse = 1.189 mse = 1.309 mae = 0.833\n",
            "epoch98 train time: 6.319s test time: 0.788  loss = 0.781 val_mse = 1.187 mse = 1.306 mae = 0.835\n",
            "epoch99 train time: 6.314s test time: 0.789  loss = 0.778 val_mse = 1.194 mse = 1.313 mae = 0.833\n",
            "epoch100 train time: 6.334s test time: 0.789  loss = 0.780 val_mse = 1.186 mse = 1.302 mae = 0.835\n",
            "epoch101 train time: 6.313s test time: 0.790  loss = 0.778 val_mse = 1.187 mse = 1.304 mae = 0.833\n",
            "epoch102 train time: 6.317s test time: 0.790  loss = 0.777 val_mse = 1.182 mse = 1.303 mae = 0.836\n",
            "epoch103 train time: 6.324s test time: 0.787  loss = 0.777 val_mse = 1.188 mse = 1.309 mae = 0.834\n",
            "epoch104 train time: 6.348s test time: 0.793  loss = 0.777 val_mse = 1.188 mse = 1.306 mae = 0.835\n",
            "epoch105 train time: 6.343s test time: 0.789  loss = 0.776 val_mse = 1.194 mse = 1.314 mae = 0.834\n",
            "epoch106 train time: 6.286s test time: 0.785  loss = 0.776 val_mse = 1.186 mse = 1.306 mae = 0.835\n",
            "epoch107 train time: 6.298s test time: 0.789  loss = 0.774 val_mse = 1.186 mse = 1.306 mae = 0.834\n",
            "epoch108 train time: 6.314s test time: 0.797  loss = 0.775 val_mse = 1.188 mse = 1.307 mae = 0.835\n",
            "epoch109 train time: 6.317s test time: 0.787  loss = 0.775 val_mse = 1.189 mse = 1.306 mae = 0.833\n",
            "epoch110 train time: 6.332s test time: 0.793  loss = 0.773 val_mse = 1.190 mse = 1.306 mae = 0.834\n",
            "epoch111 train time: 6.312s test time: 0.789  loss = 0.772 val_mse = 1.191 mse = 1.312 mae = 0.834\n",
            "epoch112 train time: 6.303s test time: 0.787  loss = 0.773 val_mse = 1.186 mse = 1.305 mae = 0.835\n",
            "epoch113 train time: 6.317s test time: 0.788  loss = 0.771 val_mse = 1.185 mse = 1.305 mae = 0.834\n",
            "epoch114 train time: 6.304s test time: 0.790  loss = 0.771 val_mse = 1.188 mse = 1.307 mae = 0.835\n",
            "epoch115 train time: 6.334s test time: 0.789  loss = 0.770 val_mse = 1.186 mse = 1.304 mae = 0.834\n",
            "epoch116 train time: 6.315s test time: 0.788  loss = 0.770 val_mse = 1.185 mse = 1.304 mae = 0.834\n",
            "epoch117 train time: 6.324s test time: 0.784  loss = 0.769 val_mse = 1.185 mse = 1.307 mae = 0.834\n",
            "epoch118 train time: 6.329s test time: 0.790  loss = 0.769 val_mse = 1.194 mse = 1.310 mae = 0.834\n",
            "epoch119 train time: 6.308s test time: 0.814  loss = 0.768 val_mse = 1.187 mse = 1.303 mae = 0.834\n",
            "MAE 0.8512053771461526\n",
            "MSE 1.3595100656998025\n"
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    print(sys.path[0])\n",
        "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
        "    word_latent_dim = 300\n",
        "    latent_dim = 15\n",
        "    max_doc_length = 300\n",
        "    num_filters = 50\n",
        "    window_size = 5\n",
        "    v_dim = 50\n",
        "    learning_rate = 0.001\n",
        "    lambda_1 = 0.01 #normally we set from [0.05, 0.01, 0.005, 0.001]\n",
        "    drop_out = 0.4\n",
        "    batch_size_list = [100, 200, 250, 300, 500]\n",
        "    batch_size = 100\n",
        "    epochs = 120\n",
        "    avg_mae_list = []\n",
        "    avg_mse_list = []\n",
        "    \n",
        "    # loading data\n",
        "    firTime = time()\n",
        "    dataSet = Dataset(max_doc_length, sys.path[0], \"dataPreprocessingWordDict.out\")\n",
        "    word_dict, user_reviews, item_reviews, train, valRatings, testRatings = dataSet.word_id_dict, dataSet.userReview_dict, dataSet.itemReview_dict, dataSet.trainMtrx, dataSet.valRatings, dataSet.testRatings\n",
        "    secTime = time()\n",
        "\n",
        "    num_users, num_items = train.shape\n",
        "    print(\"load data: %.3fs\" % (secTime - firTime))\n",
        "    print(num_users, num_items)\n",
        "    print(latent_dim)\n",
        "\n",
        "    #load word embeddings\n",
        "    word_embedding_mtrx = ini_word_embed(len(word_dict), word_latent_dim)\n",
        "    # word_embedding_mtrx = word2vec_word_embed(len(word_dict), word_latent_dim,\n",
        "    #                                           \"Directory of pretrained WordEmbedding.out\",\n",
        "    #                                           word_dict)\n",
        "\n",
        "    print( \"shape\", word_embedding_mtrx.shape)\n",
        "\n",
        "    # get train instances\n",
        "    user_input, item_input, rateings = get_train_instance(train)\n",
        "    print (len(user_input), len(item_input), len(rateings))\n",
        "\n",
        "    # get test/val instances\n",
        "    user_vals, item_vals, user_input_val, item_input_val, rating_input_val = get_test_list(200, valRatings, user_reviews, item_reviews)\n",
        "    user_tests, item_tests, user_input_test, item_input_test, rating_input_test = get_test_list(200, testRatings, user_reviews, item_reviews)\n",
        "\n",
        "    #train & eval model\n",
        "    # train_model()\n",
        "    \n",
        "    # batch_size = 200\n",
        "    # #train & eval model\n",
        "    # train_model()\n",
        "\n",
        "    # batch_size = 250\n",
        "    # #train & eval model\n",
        "    # train_model()\n",
        "\n",
        "    # batch_size = 300\n",
        "    # #train & eval model\n",
        "    # train_model()\n",
        "\n",
        "    batch_size = 500\n",
        "    #train & eval model\n",
        "    train_model()"
      ],
      "id": "jKoULtIrG6TO"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S3-fy_y0G6WA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 426
        },
        "outputId": "407816dc-1504-440c-bc5e-aa0206e7afd9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "avg mae over batch size = [100, 200, 250, 300, 500]: [0.8694882972184632, 0.8395612463259544, 0.840025205422845, 0.8466306726586323, 0.8512053771461526]\n",
            "avg mse over batch size = [100, 200, 250, 300, 500]: [1.3378884361893557, 1.3199179432485686, 1.3143156646765504, 1.3098875162285837, 1.3595100656998025]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 750x350 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "minimum avg mae over batch sizes = 0.8395612463259544 at batch size = 200 and mse = 1.3199179432485686\n",
            "minimum avg mse over batch sizes = 1.3098875162285837 at batch size = 300 and mae = 0.8466306726586323\n"
          ]
        }
      ],
      "source": [
        "avg_mae_list = [0.8694882972184632,0.8395612463259544,0.840025205422845,0.8466306726586323,0.8512053771461526]\n",
        "avg_mse_list = [1.3378884361893557,1.3199179432485686,1.3143156646765504,1.3098875162285837,1.3595100656998025]\n",
        "batch_size_list = [100, 200, 250, 300, 500]\n",
        "print(f'avg mae over batch size = [100, 200, 250, 300, 500]: {avg_mae_list}')\n",
        "print(f'avg mse over batch size = [100, 200, 250, 300, 500]: {avg_mse_list}')\n",
        "    \n",
        "plt.rcParams.update({'figure.figsize':(7.5,3.5), 'figure.dpi':100})\n",
        "fig, axes = plt.subplots(1, 2)\n",
        "axes[0].plot(batch_size_list, avg_mae_list, 'm--o')\n",
        "axes[0].set_title('Avg MAE over varying batch size')\n",
        "axes[0].set_xlabel('batch size')\n",
        "axes[0].set_ylabel('MAE')\n",
        "\n",
        "axes[1].plot(batch_size_list, avg_mse_list, 'g--o')\n",
        "axes[1].set_title('Avg MSE over varying batch size')\n",
        "axes[1].set_xlabel('batch size')\n",
        "axes[1].set_ylabel('MSE')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "index = 0\n",
        "min = 100\n",
        "min_index = 0\n",
        "for i in avg_mae_list:\n",
        "  if i<min:\n",
        "    min = i\n",
        "    min_index = index\n",
        "  index+=1\n",
        "print(f'minimum avg mae over batch sizes = {min} at batch size = {batch_size_list[min_index]} and mse = {avg_mse_list[min_index]}')\n",
        "\n",
        "index = 0\n",
        "min = 100\n",
        "min_index = 0\n",
        "for i in avg_mse_list:\n",
        "  if i<min:\n",
        "    min = i\n",
        "    min_index = index\n",
        "  index+=1\n",
        "print(f'minimum avg mse over batch sizes = {min} at batch size = {batch_size_list[min_index]} and mae = {avg_mae_list[min_index]}')"
      ],
      "id": "S3-fy_y0G6WA"
    },
    {
      "cell_type": "markdown",
      "source": [
        "batch size = 300"
      ],
      "metadata": {
        "id": "IY1P63-aqBj5"
      },
      "id": "IY1P63-aqBj5"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dpNnRxy_HCbg"
      },
      "source": [
        "Varying learning rate"
      ],
      "id": "dpNnRxy_HCbg"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bWsKTRlgHDAc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b82750f3-a7b5-4d0c-9971-fdca01d3ad07"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "wordId_dict finished\n",
            "load reviews finished\n",
            "load data: 1.667s\n",
            "506 2581\n",
            "15\n",
            "shape (19930, 300)\n",
            "18880 18880 18880\n",
            "epoch0 train time: 4.684s test time: 0.585  loss = 40.838 val_mse = 12.745 mse = 12.870 mae = 3.399\n",
            "epoch1 train time: 4.284s test time: 0.484  loss = 39.208 val_mse = 11.053 mse = 11.165 mae = 3.150\n",
            "epoch2 train time: 4.470s test time: 0.481  loss = 36.511 val_mse = 9.086 mse = 9.181 mae = 2.836\n",
            "epoch3 train time: 4.335s test time: 0.488  loss = 33.386 val_mse = 6.984 mse = 7.057 mae = 2.469\n",
            "epoch4 train time: 4.383s test time: 0.489  loss = 30.201 val_mse = 4.898 mse = 4.944 mae = 2.028\n",
            "epoch5 train time: 4.376s test time: 0.492  loss = 27.152 val_mse = 3.081 mse = 3.093 mae = 1.581\n",
            "epoch6 train time: 4.367s test time: 0.489  loss = 24.542 val_mse = 1.875 mse = 1.849 mae = 1.171\n",
            "epoch7 train time: 4.379s test time: 0.493  loss = 22.720 val_mse = 1.545 mse = 1.486 mae = 0.936\n",
            "epoch8 train time: 4.401s test time: 0.488  loss = 21.675 val_mse = 1.554 mse = 1.491 mae = 0.895\n",
            "epoch9 train time: 4.422s test time: 0.491  loss = 20.836 val_mse = 1.552 mse = 1.489 mae = 0.894\n",
            "epoch10 train time: 4.428s test time: 0.493  loss = 20.027 val_mse = 1.550 mse = 1.488 mae = 0.893\n",
            "epoch11 train time: 4.428s test time: 0.494  loss = 19.245 val_mse = 1.547 mse = 1.486 mae = 0.892\n",
            "epoch12 train time: 4.428s test time: 0.494  loss = 18.489 val_mse = 1.545 mse = 1.484 mae = 0.891\n",
            "epoch13 train time: 4.444s test time: 0.507  loss = 17.757 val_mse = 1.542 mse = 1.482 mae = 0.891\n",
            "epoch14 train time: 4.461s test time: 0.492  loss = 17.050 val_mse = 1.539 mse = 1.480 mae = 0.890\n",
            "epoch15 train time: 4.446s test time: 0.485  loss = 16.366 val_mse = 1.537 mse = 1.479 mae = 0.890\n",
            "epoch16 train time: 4.467s test time: 0.494  loss = 15.705 val_mse = 1.534 mse = 1.477 mae = 0.889\n",
            "epoch17 train time: 4.477s test time: 0.499  loss = 15.066 val_mse = 1.532 mse = 1.475 mae = 0.888\n",
            "epoch18 train time: 4.485s test time: 0.509  loss = 14.449 val_mse = 1.529 mse = 1.473 mae = 0.888\n",
            "epoch19 train time: 4.476s test time: 0.506  loss = 13.853 val_mse = 1.527 mse = 1.472 mae = 0.887\n",
            "epoch20 train time: 4.482s test time: 0.492  loss = 13.278 val_mse = 1.524 mse = 1.470 mae = 0.887\n",
            "epoch21 train time: 4.467s test time: 0.512  loss = 12.723 val_mse = 1.522 mse = 1.468 mae = 0.886\n",
            "epoch22 train time: 4.482s test time: 0.509  loss = 12.188 val_mse = 1.520 mse = 1.466 mae = 0.886\n",
            "epoch23 train time: 4.692s test time: 0.493  loss = 11.672 val_mse = 1.517 mse = 1.464 mae = 0.885\n",
            "epoch24 train time: 4.474s test time: 0.495  loss = 11.175 val_mse = 1.515 mse = 1.463 mae = 0.885\n",
            "epoch25 train time: 4.460s test time: 0.487  loss = 10.696 val_mse = 1.512 mse = 1.461 mae = 0.885\n",
            "epoch26 train time: 4.462s test time: 0.488  loss = 10.235 val_mse = 1.510 mse = 1.459 mae = 0.884\n",
            "epoch27 train time: 4.456s test time: 0.500  loss = 9.791 val_mse = 1.507 mse = 1.457 mae = 0.884\n",
            "epoch28 train time: 4.464s test time: 0.493  loss = 9.365 val_mse = 1.505 mse = 1.456 mae = 0.883\n",
            "epoch29 train time: 4.472s test time: 0.493  loss = 8.954 val_mse = 1.502 mse = 1.454 mae = 0.883\n",
            "epoch30 train time: 4.465s test time: 0.489  loss = 8.560 val_mse = 1.500 mse = 1.452 mae = 0.882\n",
            "epoch31 train time: 4.448s test time: 0.493  loss = 8.182 val_mse = 1.497 mse = 1.450 mae = 0.882\n",
            "epoch32 train time: 4.491s test time: 0.498  loss = 7.819 val_mse = 1.495 mse = 1.448 mae = 0.882\n",
            "epoch33 train time: 4.469s test time: 0.481  loss = 7.470 val_mse = 1.492 mse = 1.447 mae = 0.881\n",
            "epoch34 train time: 4.481s test time: 0.493  loss = 7.136 val_mse = 1.490 mse = 1.445 mae = 0.881\n",
            "epoch35 train time: 4.472s test time: 0.493  loss = 6.816 val_mse = 1.487 mse = 1.443 mae = 0.880\n",
            "epoch36 train time: 4.483s test time: 0.485  loss = 6.509 val_mse = 1.485 mse = 1.441 mae = 0.880\n",
            "epoch37 train time: 4.476s test time: 0.491  loss = 6.216 val_mse = 1.482 mse = 1.439 mae = 0.880\n",
            "epoch38 train time: 4.475s test time: 0.494  loss = 5.935 val_mse = 1.480 mse = 1.438 mae = 0.879\n",
            "epoch39 train time: 4.473s test time: 0.525  loss = 5.667 val_mse = 1.477 mse = 1.436 mae = 0.879\n",
            "epoch40 train time: 4.477s test time: 0.496  loss = 5.411 val_mse = 1.475 mse = 1.434 mae = 0.879\n",
            "epoch41 train time: 4.503s test time: 0.499  loss = 5.166 val_mse = 1.472 mse = 1.432 mae = 0.878\n",
            "epoch42 train time: 4.478s test time: 0.492  loss = 4.932 val_mse = 1.470 mse = 1.430 mae = 0.878\n",
            "epoch43 train time: 4.469s test time: 0.658  loss = 4.710 val_mse = 1.467 mse = 1.429 mae = 0.877\n",
            "epoch44 train time: 4.569s test time: 0.495  loss = 4.497 val_mse = 1.465 mse = 1.427 mae = 0.877\n",
            "epoch45 train time: 4.486s test time: 0.490  loss = 4.295 val_mse = 1.462 mse = 1.425 mae = 0.877\n",
            "epoch46 train time: 4.471s test time: 0.491  loss = 4.103 val_mse = 1.460 mse = 1.423 mae = 0.876\n",
            "epoch47 train time: 4.474s test time: 0.497  loss = 3.920 val_mse = 1.457 mse = 1.421 mae = 0.876\n",
            "epoch48 train time: 4.460s test time: 0.488  loss = 3.746 val_mse = 1.455 mse = 1.420 mae = 0.876\n",
            "epoch49 train time: 4.467s test time: 0.495  loss = 3.581 val_mse = 1.452 mse = 1.418 mae = 0.875\n",
            "epoch50 train time: 4.481s test time: 0.501  loss = 3.425 val_mse = 1.449 mse = 1.416 mae = 0.875\n",
            "epoch51 train time: 4.461s test time: 0.495  loss = 3.276 val_mse = 1.447 mse = 1.414 mae = 0.875\n",
            "epoch52 train time: 4.452s test time: 0.493  loss = 3.136 val_mse = 1.444 mse = 1.412 mae = 0.874\n",
            "epoch53 train time: 4.457s test time: 0.492  loss = 3.003 val_mse = 1.442 mse = 1.411 mae = 0.874\n",
            "epoch54 train time: 4.460s test time: 0.496  loss = 2.877 val_mse = 1.439 mse = 1.409 mae = 0.874\n",
            "epoch55 train time: 4.437s test time: 0.490  loss = 2.759 val_mse = 1.436 mse = 1.407 mae = 0.873\n",
            "epoch56 train time: 4.455s test time: 0.499  loss = 2.647 val_mse = 1.434 mse = 1.405 mae = 0.873\n",
            "epoch57 train time: 4.479s test time: 0.491  loss = 2.542 val_mse = 1.431 mse = 1.403 mae = 0.873\n",
            "epoch58 train time: 4.489s test time: 0.520  loss = 2.442 val_mse = 1.429 mse = 1.402 mae = 0.872\n",
            "epoch59 train time: 4.485s test time: 0.496  loss = 2.349 val_mse = 1.426 mse = 1.400 mae = 0.872\n",
            "epoch60 train time: 4.468s test time: 0.501  loss = 2.261 val_mse = 1.424 mse = 1.398 mae = 0.871\n",
            "epoch61 train time: 4.483s test time: 0.494  loss = 2.179 val_mse = 1.421 mse = 1.396 mae = 0.871\n",
            "epoch62 train time: 4.460s test time: 0.492  loss = 2.102 val_mse = 1.418 mse = 1.395 mae = 0.871\n",
            "epoch63 train time: 4.455s test time: 0.495  loss = 2.030 val_mse = 1.416 mse = 1.393 mae = 0.870\n",
            "epoch64 train time: 4.451s test time: 0.492  loss = 1.962 val_mse = 1.413 mse = 1.391 mae = 0.870\n",
            "epoch65 train time: 4.491s test time: 0.486  loss = 1.899 val_mse = 1.411 mse = 1.390 mae = 0.870\n",
            "epoch66 train time: 4.464s test time: 0.502  loss = 1.840 val_mse = 1.408 mse = 1.388 mae = 0.869\n",
            "epoch67 train time: 4.468s test time: 0.493  loss = 1.785 val_mse = 1.405 mse = 1.386 mae = 0.869\n",
            "epoch68 train time: 4.465s test time: 0.497  loss = 1.734 val_mse = 1.403 mse = 1.384 mae = 0.869\n",
            "epoch69 train time: 4.487s test time: 0.493  loss = 1.687 val_mse = 1.400 mse = 1.383 mae = 0.868\n",
            "epoch70 train time: 4.485s test time: 0.494  loss = 1.643 val_mse = 1.398 mse = 1.381 mae = 0.868\n",
            "epoch71 train time: 4.482s test time: 0.500  loss = 1.603 val_mse = 1.395 mse = 1.379 mae = 0.867\n",
            "epoch72 train time: 4.449s test time: 0.503  loss = 1.565 val_mse = 1.392 mse = 1.378 mae = 0.867\n",
            "epoch73 train time: 4.439s test time: 0.495  loss = 1.530 val_mse = 1.390 mse = 1.376 mae = 0.867\n",
            "epoch74 train time: 4.493s test time: 0.510  loss = 1.498 val_mse = 1.387 mse = 1.374 mae = 0.866\n",
            "epoch75 train time: 4.458s test time: 0.493  loss = 1.469 val_mse = 1.385 mse = 1.373 mae = 0.866\n",
            "epoch76 train time: 4.461s test time: 0.491  loss = 1.442 val_mse = 1.382 mse = 1.371 mae = 0.866\n",
            "epoch77 train time: 4.466s test time: 0.491  loss = 1.418 val_mse = 1.379 mse = 1.369 mae = 0.865\n",
            "epoch78 train time: 4.482s test time: 0.498  loss = 1.395 val_mse = 1.377 mse = 1.368 mae = 0.865\n",
            "epoch79 train time: 4.448s test time: 0.487  loss = 1.375 val_mse = 1.374 mse = 1.366 mae = 0.864\n",
            "epoch80 train time: 4.468s test time: 0.502  loss = 1.356 val_mse = 1.371 mse = 1.365 mae = 0.864\n",
            "epoch81 train time: 4.470s test time: 0.492  loss = 1.339 val_mse = 1.369 mse = 1.363 mae = 0.864\n",
            "epoch82 train time: 4.475s test time: 0.494  loss = 1.323 val_mse = 1.366 mse = 1.361 mae = 0.863\n",
            "epoch83 train time: 4.488s test time: 0.513  loss = 1.309 val_mse = 1.364 mse = 1.360 mae = 0.863\n",
            "epoch84 train time: 4.463s test time: 0.499  loss = 1.297 val_mse = 1.361 mse = 1.358 mae = 0.863\n",
            "epoch85 train time: 4.452s test time: 0.513  loss = 1.285 val_mse = 1.358 mse = 1.356 mae = 0.862\n",
            "epoch86 train time: 4.491s test time: 0.503  loss = 1.274 val_mse = 1.356 mse = 1.355 mae = 0.862\n",
            "epoch87 train time: 4.467s test time: 0.510  loss = 1.265 val_mse = 1.353 mse = 1.353 mae = 0.861\n",
            "epoch88 train time: 4.461s test time: 0.507  loss = 1.256 val_mse = 1.350 mse = 1.352 mae = 0.861\n",
            "epoch89 train time: 4.475s test time: 0.510  loss = 1.248 val_mse = 1.348 mse = 1.350 mae = 0.860\n",
            "epoch90 train time: 4.459s test time: 0.494  loss = 1.241 val_mse = 1.345 mse = 1.349 mae = 0.860\n",
            "epoch91 train time: 4.447s test time: 0.494  loss = 1.234 val_mse = 1.343 mse = 1.347 mae = 0.860\n",
            "epoch92 train time: 4.495s test time: 0.493  loss = 1.228 val_mse = 1.340 mse = 1.346 mae = 0.859\n",
            "epoch93 train time: 4.471s test time: 0.497  loss = 1.222 val_mse = 1.337 mse = 1.344 mae = 0.859\n",
            "epoch94 train time: 4.458s test time: 0.492  loss = 1.216 val_mse = 1.335 mse = 1.343 mae = 0.858\n",
            "epoch95 train time: 4.460s test time: 0.489  loss = 1.211 val_mse = 1.332 mse = 1.341 mae = 0.858\n",
            "epoch96 train time: 4.470s test time: 0.492  loss = 1.205 val_mse = 1.330 mse = 1.340 mae = 0.857\n",
            "epoch97 train time: 4.464s test time: 0.509  loss = 1.200 val_mse = 1.327 mse = 1.338 mae = 0.857\n",
            "epoch98 train time: 4.477s test time: 0.493  loss = 1.195 val_mse = 1.325 mse = 1.337 mae = 0.857\n",
            "epoch99 train time: 4.454s test time: 0.496  loss = 1.190 val_mse = 1.322 mse = 1.335 mae = 0.856\n",
            "epoch100 train time: 4.461s test time: 0.505  loss = 1.184 val_mse = 1.320 mse = 1.334 mae = 0.856\n",
            "epoch101 train time: 4.478s test time: 0.509  loss = 1.179 val_mse = 1.317 mse = 1.332 mae = 0.855\n",
            "epoch102 train time: 4.465s test time: 0.500  loss = 1.174 val_mse = 1.315 mse = 1.331 mae = 0.855\n",
            "epoch103 train time: 4.460s test time: 0.494  loss = 1.169 val_mse = 1.312 mse = 1.329 mae = 0.854\n",
            "epoch104 train time: 4.462s test time: 0.485  loss = 1.164 val_mse = 1.310 mse = 1.328 mae = 0.854\n",
            "epoch105 train time: 4.473s test time: 0.493  loss = 1.159 val_mse = 1.307 mse = 1.327 mae = 0.853\n",
            "epoch106 train time: 4.470s test time: 0.493  loss = 1.154 val_mse = 1.305 mse = 1.325 mae = 0.853\n",
            "epoch107 train time: 4.476s test time: 0.493  loss = 1.149 val_mse = 1.302 mse = 1.324 mae = 0.853\n",
            "epoch108 train time: 4.469s test time: 0.496  loss = 1.143 val_mse = 1.300 mse = 1.323 mae = 0.852\n",
            "epoch109 train time: 4.468s test time: 0.508  loss = 1.138 val_mse = 1.298 mse = 1.321 mae = 0.852\n",
            "epoch110 train time: 4.476s test time: 0.491  loss = 1.133 val_mse = 1.295 mse = 1.320 mae = 0.851\n",
            "epoch111 train time: 4.470s test time: 0.508  loss = 1.128 val_mse = 1.293 mse = 1.319 mae = 0.851\n",
            "epoch112 train time: 4.467s test time: 0.496  loss = 1.123 val_mse = 1.290 mse = 1.317 mae = 0.850\n",
            "epoch113 train time: 4.480s test time: 0.521  loss = 1.118 val_mse = 1.288 mse = 1.316 mae = 0.850\n",
            "epoch114 train time: 4.472s test time: 0.502  loss = 1.113 val_mse = 1.286 mse = 1.315 mae = 0.849\n",
            "epoch115 train time: 4.476s test time: 0.496  loss = 1.108 val_mse = 1.283 mse = 1.313 mae = 0.849\n",
            "epoch116 train time: 4.479s test time: 0.513  loss = 1.103 val_mse = 1.281 mse = 1.312 mae = 0.848\n",
            "epoch117 train time: 4.478s test time: 0.484  loss = 1.098 val_mse = 1.279 mse = 1.311 mae = 0.848\n",
            "epoch118 train time: 4.464s test time: 0.492  loss = 1.093 val_mse = 1.276 mse = 1.310 mae = 0.847\n",
            "epoch119 train time: 4.460s test time: 0.508  loss = 1.088 val_mse = 1.274 mse = 1.309 mae = 0.847\n",
            "MAE 0.9583892102254641\n",
            "MSE 1.7321942511073358\n",
            "epoch0 train time: 4.834s test time: 0.502  loss = 35.809 val_mse = 2.167 mse = 2.152 mae = 1.258\n",
            "epoch1 train time: 4.482s test time: 0.491  loss = 26.173 val_mse = 1.542 mse = 1.487 mae = 0.932\n",
            "epoch2 train time: 4.483s test time: 0.522  loss = 19.407 val_mse = 1.521 mse = 1.473 mae = 0.924\n",
            "epoch3 train time: 4.482s test time: 0.514  loss = 12.807 val_mse = 1.501 mse = 1.458 mae = 0.918\n",
            "epoch4 train time: 4.481s test time: 0.521  loss = 8.230 val_mse = 1.480 mse = 1.442 mae = 0.911\n",
            "epoch5 train time: 4.482s test time: 0.491  loss = 5.202 val_mse = 1.458 mse = 1.426 mae = 0.904\n",
            "epoch6 train time: 4.486s test time: 0.499  loss = 3.309 val_mse = 1.435 mse = 1.409 mae = 0.896\n",
            "epoch7 train time: 4.483s test time: 0.497  loss = 2.206 val_mse = 1.411 mse = 1.393 mae = 0.889\n",
            "epoch8 train time: 4.514s test time: 0.498  loss = 1.620 val_mse = 1.388 mse = 1.377 mae = 0.882\n",
            "epoch9 train time: 4.489s test time: 0.490  loss = 1.348 val_mse = 1.366 mse = 1.363 mae = 0.875\n",
            "epoch10 train time: 4.485s test time: 0.502  loss = 1.238 val_mse = 1.344 mse = 1.349 mae = 0.868\n",
            "epoch11 train time: 4.484s test time: 0.493  loss = 1.188 val_mse = 1.321 mse = 1.336 mae = 0.861\n",
            "epoch12 train time: 4.474s test time: 0.494  loss = 1.149 val_mse = 1.300 mse = 1.325 mae = 0.855\n",
            "epoch13 train time: 4.468s test time: 0.506  loss = 1.111 val_mse = 1.275 mse = 1.312 mae = 0.854\n",
            "epoch14 train time: 4.461s test time: 0.497  loss = 1.074 val_mse = 1.264 mse = 1.309 mae = 0.843\n",
            "epoch15 train time: 4.475s test time: 0.506  loss = 1.039 val_mse = 1.233 mse = 1.294 mae = 0.857\n",
            "epoch16 train time: 4.466s test time: 0.490  loss = 1.011 val_mse = 1.258 mse = 1.313 mae = 0.831\n",
            "epoch17 train time: 4.494s test time: 0.517  loss = 0.981 val_mse = 1.213 mse = 1.288 mae = 0.846\n",
            "epoch18 train time: 4.479s test time: 0.490  loss = 0.959 val_mse = 1.218 mse = 1.295 mae = 0.840\n",
            "epoch19 train time: 4.469s test time: 0.513  loss = 0.942 val_mse = 1.211 mse = 1.290 mae = 0.838\n",
            "epoch20 train time: 4.463s test time: 0.498  loss = 0.931 val_mse = 1.208 mse = 1.295 mae = 0.836\n",
            "epoch21 train time: 4.478s test time: 0.505  loss = 0.917 val_mse = 1.201 mse = 1.286 mae = 0.843\n",
            "epoch22 train time: 4.454s test time: 0.496  loss = 0.910 val_mse = 1.206 mse = 1.301 mae = 0.844\n",
            "epoch23 train time: 4.487s test time: 0.516  loss = 0.898 val_mse = 1.195 mse = 1.290 mae = 0.843\n",
            "epoch24 train time: 4.460s test time: 0.506  loss = 0.892 val_mse = 1.201 mse = 1.296 mae = 0.838\n",
            "epoch25 train time: 4.463s test time: 0.520  loss = 0.882 val_mse = 1.245 mse = 1.330 mae = 0.830\n",
            "epoch26 train time: 4.485s test time: 0.503  loss = 0.875 val_mse = 1.189 mse = 1.300 mae = 0.837\n",
            "epoch27 train time: 4.469s test time: 0.512  loss = 0.869 val_mse = 1.194 mse = 1.301 mae = 0.835\n",
            "epoch28 train time: 4.470s test time: 0.492  loss = 0.865 val_mse = 1.182 mse = 1.297 mae = 0.844\n",
            "epoch29 train time: 4.487s test time: 0.500  loss = 0.857 val_mse = 1.195 mse = 1.290 mae = 0.837\n",
            "epoch30 train time: 4.478s test time: 0.492  loss = 0.859 val_mse = 1.216 mse = 1.321 mae = 0.829\n",
            "epoch31 train time: 4.471s test time: 0.500  loss = 0.851 val_mse = 1.195 mse = 1.300 mae = 0.843\n",
            "epoch32 train time: 4.484s test time: 0.505  loss = 0.849 val_mse = 1.181 mse = 1.301 mae = 0.839\n",
            "epoch33 train time: 4.489s test time: 0.509  loss = 0.841 val_mse = 1.192 mse = 1.300 mae = 0.846\n",
            "epoch34 train time: 4.468s test time: 0.510  loss = 0.843 val_mse = 1.175 mse = 1.297 mae = 0.838\n",
            "epoch35 train time: 4.465s test time: 0.513  loss = 0.840 val_mse = 1.185 mse = 1.302 mae = 0.845\n",
            "epoch36 train time: 4.491s test time: 0.495  loss = 0.838 val_mse = 1.173 mse = 1.294 mae = 0.841\n",
            "epoch37 train time: 4.446s test time: 0.506  loss = 0.833 val_mse = 1.187 mse = 1.301 mae = 0.844\n",
            "epoch38 train time: 4.494s test time: 0.488  loss = 0.834 val_mse = 1.169 mse = 1.292 mae = 0.840\n",
            "epoch39 train time: 4.473s test time: 0.495  loss = 0.828 val_mse = 1.182 mse = 1.299 mae = 0.843\n",
            "epoch40 train time: 4.456s test time: 0.495  loss = 0.827 val_mse = 1.174 mse = 1.301 mae = 0.848\n",
            "epoch41 train time: 4.468s test time: 0.492  loss = 0.827 val_mse = 1.190 mse = 1.296 mae = 0.844\n",
            "epoch42 train time: 4.479s test time: 0.491  loss = 0.827 val_mse = 1.170 mse = 1.291 mae = 0.840\n",
            "epoch43 train time: 4.447s test time: 0.495  loss = 0.823 val_mse = 1.193 mse = 1.305 mae = 0.845\n",
            "epoch44 train time: 4.460s test time: 0.499  loss = 0.821 val_mse = 1.170 mse = 1.294 mae = 0.841\n",
            "epoch45 train time: 4.470s test time: 0.494  loss = 0.819 val_mse = 1.191 mse = 1.299 mae = 0.845\n",
            "epoch46 train time: 4.474s test time: 0.498  loss = 0.819 val_mse = 1.168 mse = 1.296 mae = 0.839\n",
            "epoch47 train time: 4.482s test time: 0.502  loss = 0.817 val_mse = 1.192 mse = 1.305 mae = 0.849\n",
            "epoch48 train time: 4.478s test time: 0.510  loss = 0.816 val_mse = 1.168 mse = 1.296 mae = 0.842\n",
            "epoch49 train time: 4.476s test time: 0.505  loss = 0.814 val_mse = 1.188 mse = 1.301 mae = 0.848\n",
            "epoch50 train time: 4.472s test time: 0.489  loss = 0.816 val_mse = 1.173 mse = 1.290 mae = 0.837\n",
            "epoch51 train time: 4.468s test time: 0.493  loss = 0.813 val_mse = 1.190 mse = 1.306 mae = 0.849\n",
            "epoch52 train time: 4.458s test time: 0.490  loss = 0.812 val_mse = 1.169 mse = 1.287 mae = 0.837\n",
            "epoch53 train time: 4.514s test time: 0.493  loss = 0.806 val_mse = 1.200 mse = 1.307 mae = 0.846\n",
            "epoch54 train time: 4.482s test time: 0.496  loss = 0.809 val_mse = 1.167 mse = 1.294 mae = 0.841\n",
            "epoch55 train time: 4.461s test time: 0.500  loss = 0.805 val_mse = 1.194 mse = 1.307 mae = 0.847\n",
            "epoch56 train time: 4.459s test time: 0.495  loss = 0.805 val_mse = 1.169 mse = 1.294 mae = 0.841\n",
            "epoch57 train time: 4.471s test time: 0.496  loss = 0.805 val_mse = 1.185 mse = 1.300 mae = 0.844\n",
            "epoch58 train time: 4.470s test time: 0.492  loss = 0.802 val_mse = 1.172 mse = 1.300 mae = 0.842\n",
            "epoch59 train time: 4.463s test time: 0.519  loss = 0.803 val_mse = 1.191 mse = 1.305 mae = 0.844\n",
            "epoch60 train time: 4.463s test time: 0.485  loss = 0.803 val_mse = 1.174 mse = 1.297 mae = 0.841\n",
            "epoch61 train time: 4.452s test time: 0.494  loss = 0.803 val_mse = 1.195 mse = 1.308 mae = 0.844\n",
            "epoch62 train time: 4.493s test time: 0.512  loss = 0.798 val_mse = 1.165 mse = 1.295 mae = 0.843\n",
            "epoch63 train time: 4.470s test time: 0.495  loss = 0.801 val_mse = 1.197 mse = 1.307 mae = 0.846\n",
            "epoch64 train time: 4.464s test time: 0.503  loss = 0.800 val_mse = 1.169 mse = 1.301 mae = 0.843\n",
            "epoch65 train time: 4.471s test time: 0.496  loss = 0.800 val_mse = 1.195 mse = 1.304 mae = 0.846\n",
            "epoch66 train time: 4.465s test time: 0.490  loss = 0.797 val_mse = 1.169 mse = 1.295 mae = 0.839\n",
            "epoch67 train time: 4.486s test time: 0.499  loss = 0.800 val_mse = 1.194 mse = 1.304 mae = 0.846\n",
            "epoch68 train time: 4.485s test time: 0.507  loss = 0.799 val_mse = 1.171 mse = 1.305 mae = 0.838\n",
            "epoch69 train time: 4.482s test time: 0.494  loss = 0.797 val_mse = 1.192 mse = 1.305 mae = 0.845\n",
            "epoch70 train time: 4.486s test time: 0.497  loss = 0.799 val_mse = 1.170 mse = 1.294 mae = 0.841\n",
            "epoch71 train time: 4.473s test time: 0.540  loss = 0.797 val_mse = 1.194 mse = 1.307 mae = 0.847\n",
            "epoch72 train time: 4.476s test time: 0.494  loss = 0.798 val_mse = 1.173 mse = 1.296 mae = 0.842\n",
            "epoch73 train time: 4.463s test time: 0.490  loss = 0.796 val_mse = 1.194 mse = 1.306 mae = 0.845\n",
            "epoch74 train time: 4.452s test time: 0.495  loss = 0.796 val_mse = 1.170 mse = 1.292 mae = 0.839\n",
            "epoch75 train time: 4.456s test time: 0.494  loss = 0.796 val_mse = 1.190 mse = 1.305 mae = 0.846\n",
            "epoch76 train time: 4.446s test time: 0.491  loss = 0.795 val_mse = 1.177 mse = 1.297 mae = 0.837\n",
            "epoch77 train time: 4.454s test time: 0.491  loss = 0.797 val_mse = 1.190 mse = 1.302 mae = 0.845\n",
            "epoch78 train time: 4.473s test time: 0.498  loss = 0.793 val_mse = 1.169 mse = 1.300 mae = 0.837\n",
            "epoch79 train time: 4.478s test time: 0.494  loss = 0.794 val_mse = 1.199 mse = 1.303 mae = 0.845\n",
            "epoch80 train time: 4.487s test time: 0.491  loss = 0.794 val_mse = 1.168 mse = 1.295 mae = 0.839\n",
            "epoch81 train time: 4.482s test time: 0.496  loss = 0.792 val_mse = 1.193 mse = 1.308 mae = 0.846\n",
            "epoch82 train time: 4.493s test time: 0.497  loss = 0.793 val_mse = 1.179 mse = 1.294 mae = 0.837\n",
            "epoch83 train time: 4.479s test time: 0.494  loss = 0.794 val_mse = 1.193 mse = 1.303 mae = 0.844\n",
            "epoch84 train time: 4.473s test time: 0.500  loss = 0.791 val_mse = 1.174 mse = 1.295 mae = 0.838\n",
            "epoch85 train time: 4.478s test time: 0.509  loss = 0.793 val_mse = 1.189 mse = 1.299 mae = 0.844\n",
            "epoch86 train time: 4.483s test time: 0.488  loss = 0.791 val_mse = 1.179 mse = 1.298 mae = 0.839\n",
            "epoch87 train time: 4.484s test time: 0.485  loss = 0.793 val_mse = 1.197 mse = 1.300 mae = 0.840\n",
            "epoch88 train time: 4.482s test time: 0.487  loss = 0.789 val_mse = 1.174 mse = 1.300 mae = 0.841\n",
            "epoch89 train time: 4.487s test time: 0.509  loss = 0.789 val_mse = 1.194 mse = 1.299 mae = 0.841\n",
            "epoch90 train time: 4.460s test time: 0.496  loss = 0.787 val_mse = 1.175 mse = 1.296 mae = 0.836\n",
            "epoch91 train time: 4.459s test time: 0.502  loss = 0.787 val_mse = 1.191 mse = 1.304 mae = 0.845\n",
            "epoch92 train time: 4.463s test time: 0.514  loss = 0.786 val_mse = 1.176 mse = 1.298 mae = 0.836\n",
            "epoch93 train time: 4.466s test time: 0.492  loss = 0.786 val_mse = 1.191 mse = 1.307 mae = 0.842\n",
            "epoch94 train time: 4.457s test time: 0.496  loss = 0.783 val_mse = 1.172 mse = 1.296 mae = 0.838\n",
            "epoch95 train time: 4.500s test time: 0.490  loss = 0.785 val_mse = 1.193 mse = 1.307 mae = 0.844\n",
            "epoch96 train time: 4.481s test time: 0.491  loss = 0.785 val_mse = 1.174 mse = 1.300 mae = 0.839\n",
            "epoch97 train time: 4.488s test time: 0.493  loss = 0.784 val_mse = 1.199 mse = 1.305 mae = 0.843\n",
            "epoch98 train time: 4.480s test time: 0.491  loss = 0.784 val_mse = 1.173 mse = 1.294 mae = 0.839\n",
            "epoch99 train time: 4.487s test time: 0.485  loss = 0.783 val_mse = 1.188 mse = 1.301 mae = 0.844\n",
            "epoch100 train time: 4.495s test time: 0.510  loss = 0.783 val_mse = 1.175 mse = 1.294 mae = 0.835\n",
            "epoch101 train time: 4.486s test time: 0.504  loss = 0.784 val_mse = 1.189 mse = 1.307 mae = 0.843\n",
            "epoch102 train time: 4.490s test time: 0.492  loss = 0.781 val_mse = 1.170 mse = 1.302 mae = 0.839\n",
            "epoch103 train time: 4.468s test time: 0.494  loss = 0.783 val_mse = 1.195 mse = 1.300 mae = 0.841\n",
            "epoch104 train time: 4.478s test time: 0.512  loss = 0.782 val_mse = 1.171 mse = 1.297 mae = 0.839\n",
            "epoch105 train time: 4.484s test time: 0.490  loss = 0.781 val_mse = 1.191 mse = 1.299 mae = 0.840\n",
            "epoch106 train time: 4.478s test time: 0.490  loss = 0.781 val_mse = 1.176 mse = 1.300 mae = 0.838\n",
            "epoch107 train time: 4.472s test time: 0.503  loss = 0.784 val_mse = 1.196 mse = 1.300 mae = 0.840\n",
            "epoch108 train time: 4.491s test time: 0.484  loss = 0.783 val_mse = 1.176 mse = 1.299 mae = 0.838\n",
            "epoch109 train time: 4.468s test time: 0.492  loss = 0.783 val_mse = 1.191 mse = 1.303 mae = 0.841\n",
            "epoch110 train time: 4.482s test time: 0.495  loss = 0.781 val_mse = 1.172 mse = 1.292 mae = 0.835\n",
            "epoch111 train time: 4.486s test time: 0.506  loss = 0.780 val_mse = 1.192 mse = 1.305 mae = 0.840\n",
            "epoch112 train time: 4.485s test time: 0.515  loss = 0.781 val_mse = 1.166 mse = 1.294 mae = 0.837\n",
            "epoch113 train time: 4.452s test time: 0.508  loss = 0.780 val_mse = 1.191 mse = 1.305 mae = 0.841\n",
            "epoch114 train time: 4.480s test time: 0.495  loss = 0.779 val_mse = 1.171 mse = 1.297 mae = 0.836\n",
            "epoch115 train time: 4.461s test time: 0.488  loss = 0.781 val_mse = 1.186 mse = 1.301 mae = 0.841\n",
            "epoch116 train time: 4.464s test time: 0.516  loss = 0.779 val_mse = 1.173 mse = 1.299 mae = 0.836\n",
            "epoch117 train time: 4.467s test time: 0.502  loss = 0.779 val_mse = 1.191 mse = 1.303 mae = 0.840\n",
            "epoch118 train time: 4.460s test time: 0.502  loss = 0.780 val_mse = 1.170 mse = 1.293 mae = 0.837\n",
            "epoch119 train time: 4.457s test time: 0.500  loss = 0.780 val_mse = 1.188 mse = 1.302 mae = 0.841\n",
            "MAE 0.8499268625805789\n",
            "MSE 1.317287691183217\n",
            "epoch0 train time: 4.837s test time: 0.501  loss = 28.202 val_mse = 1.683 mse = 1.613 mae = 0.928\n",
            "epoch1 train time: 4.469s test time: 0.498  loss = 16.376 val_mse = 1.688 mse = 1.646 mae = 0.914\n",
            "epoch2 train time: 4.471s test time: 0.510  loss = 1.920 val_mse = 1.535 mse = 1.547 mae = 1.029\n",
            "epoch3 train time: 4.448s test time: 0.503  loss = 1.360 val_mse = 1.470 mse = 1.462 mae = 0.863\n",
            "epoch4 train time: 4.462s test time: 0.493  loss = 1.234 val_mse = 1.473 mse = 1.518 mae = 1.013\n",
            "epoch5 train time: 4.525s test time: 0.497  loss = 1.180 val_mse = 1.373 mse = 1.398 mae = 0.852\n",
            "epoch6 train time: 4.474s test time: 0.491  loss = 1.127 val_mse = 1.326 mse = 1.368 mae = 0.929\n",
            "epoch7 train time: 4.483s test time: 0.504  loss = 1.083 val_mse = 1.349 mse = 1.387 mae = 0.844\n",
            "epoch8 train time: 4.477s test time: 0.491  loss = 1.027 val_mse = 1.260 mse = 1.325 mae = 0.863\n",
            "epoch9 train time: 4.473s test time: 0.493  loss = 0.982 val_mse = 1.270 mse = 1.350 mae = 0.866\n",
            "epoch10 train time: 4.461s test time: 0.485  loss = 0.960 val_mse = 1.237 mse = 1.302 mae = 0.856\n",
            "epoch11 train time: 4.476s test time: 0.507  loss = 0.941 val_mse = 1.248 mse = 1.328 mae = 0.842\n",
            "epoch12 train time: 4.460s test time: 0.500  loss = 0.937 val_mse = 1.247 mse = 1.290 mae = 0.853\n",
            "epoch13 train time: 4.441s test time: 0.512  loss = 0.933 val_mse = 1.245 mse = 1.309 mae = 0.836\n",
            "epoch14 train time: 4.454s test time: 0.495  loss = 0.919 val_mse = 1.234 mse = 1.309 mae = 0.840\n",
            "epoch15 train time: 4.436s test time: 0.492  loss = 0.919 val_mse = 1.291 mse = 1.340 mae = 0.837\n",
            "epoch16 train time: 4.442s test time: 0.488  loss = 0.918 val_mse = 1.238 mse = 1.293 mae = 0.844\n",
            "epoch17 train time: 4.430s test time: 0.493  loss = 0.916 val_mse = 1.235 mse = 1.312 mae = 0.856\n",
            "epoch18 train time: 4.442s test time: 0.506  loss = 0.918 val_mse = 1.242 mse = 1.299 mae = 0.850\n",
            "epoch19 train time: 4.446s test time: 0.490  loss = 0.920 val_mse = 1.243 mse = 1.331 mae = 0.855\n",
            "epoch20 train time: 4.430s test time: 0.492  loss = 0.916 val_mse = 1.231 mse = 1.303 mae = 0.843\n",
            "epoch21 train time: 4.432s test time: 0.491  loss = 0.914 val_mse = 1.250 mse = 1.328 mae = 0.845\n",
            "epoch22 train time: 4.474s test time: 0.491  loss = 0.908 val_mse = 1.233 mse = 1.293 mae = 0.847\n",
            "epoch23 train time: 4.458s test time: 0.512  loss = 0.916 val_mse = 1.244 mse = 1.334 mae = 0.844\n",
            "epoch24 train time: 4.491s test time: 0.490  loss = 0.902 val_mse = 1.240 mse = 1.303 mae = 0.839\n",
            "epoch25 train time: 4.468s test time: 0.489  loss = 0.919 val_mse = 1.249 mse = 1.326 mae = 0.848\n",
            "epoch26 train time: 4.458s test time: 0.500  loss = 0.907 val_mse = 1.225 mse = 1.298 mae = 0.844\n",
            "epoch27 train time: 4.464s test time: 0.510  loss = 0.907 val_mse = 1.236 mse = 1.316 mae = 0.842\n",
            "epoch28 train time: 4.444s test time: 0.501  loss = 0.908 val_mse = 1.214 mse = 1.289 mae = 0.840\n",
            "epoch29 train time: 4.439s test time: 0.509  loss = 0.907 val_mse = 1.229 mse = 1.307 mae = 0.849\n",
            "epoch30 train time: 4.503s test time: 0.511  loss = 0.907 val_mse = 1.205 mse = 1.277 mae = 0.846\n",
            "epoch31 train time: 4.458s test time: 0.493  loss = 0.904 val_mse = 1.214 mse = 1.303 mae = 0.845\n",
            "epoch32 train time: 4.458s test time: 0.489  loss = 0.898 val_mse = 1.219 mse = 1.284 mae = 0.845\n",
            "epoch33 train time: 4.449s test time: 0.531  loss = 0.905 val_mse = 1.207 mse = 1.298 mae = 0.844\n",
            "epoch34 train time: 4.452s test time: 0.508  loss = 0.894 val_mse = 1.208 mse = 1.288 mae = 0.847\n",
            "epoch35 train time: 4.455s test time: 0.502  loss = 0.908 val_mse = 1.202 mse = 1.295 mae = 0.840\n",
            "epoch36 train time: 4.475s test time: 0.501  loss = 0.903 val_mse = 1.212 mse = 1.281 mae = 0.842\n",
            "epoch37 train time: 4.477s test time: 0.492  loss = 0.911 val_mse = 1.206 mse = 1.310 mae = 0.843\n",
            "epoch38 train time: 4.499s test time: 0.496  loss = 0.903 val_mse = 1.210 mse = 1.285 mae = 0.841\n",
            "epoch39 train time: 4.485s test time: 0.525  loss = 0.903 val_mse = 1.206 mse = 1.303 mae = 0.842\n",
            "epoch40 train time: 4.459s test time: 0.498  loss = 0.894 val_mse = 1.210 mse = 1.284 mae = 0.841\n",
            "epoch41 train time: 4.454s test time: 0.503  loss = 0.900 val_mse = 1.192 mse = 1.309 mae = 0.840\n",
            "epoch42 train time: 4.466s test time: 0.494  loss = 0.895 val_mse = 1.208 mse = 1.287 mae = 0.837\n",
            "epoch43 train time: 4.460s test time: 0.488  loss = 0.902 val_mse = 1.228 mse = 1.315 mae = 0.832\n",
            "epoch44 train time: 4.449s test time: 0.499  loss = 0.904 val_mse = 1.235 mse = 1.314 mae = 0.830\n",
            "epoch45 train time: 4.457s test time: 0.486  loss = 0.896 val_mse = 1.249 mse = 1.330 mae = 0.833\n",
            "epoch46 train time: 4.483s test time: 0.494  loss = 0.900 val_mse = 1.246 mse = 1.305 mae = 0.828\n",
            "epoch47 train time: 4.453s test time: 0.496  loss = 0.898 val_mse = 1.234 mse = 1.319 mae = 0.832\n",
            "epoch48 train time: 4.463s test time: 0.515  loss = 0.897 val_mse = 1.230 mse = 1.307 mae = 0.835\n",
            "epoch49 train time: 4.492s test time: 0.511  loss = 0.901 val_mse = 1.222 mse = 1.333 mae = 0.839\n",
            "epoch50 train time: 4.456s test time: 0.521  loss = 0.899 val_mse = 1.201 mse = 1.283 mae = 0.850\n",
            "epoch51 train time: 4.452s test time: 0.500  loss = 0.896 val_mse = 1.314 mse = 1.393 mae = 0.833\n",
            "epoch52 train time: 4.452s test time: 0.500  loss = 0.888 val_mse = 1.202 mse = 1.293 mae = 0.841\n",
            "epoch53 train time: 4.465s test time: 0.509  loss = 0.898 val_mse = 1.196 mse = 1.305 mae = 0.833\n",
            "epoch54 train time: 4.464s test time: 0.501  loss = 0.886 val_mse = 1.232 mse = 1.313 mae = 0.836\n",
            "epoch55 train time: 4.459s test time: 0.493  loss = 0.894 val_mse = 1.209 mse = 1.313 mae = 0.833\n",
            "epoch56 train time: 4.438s test time: 0.498  loss = 0.885 val_mse = 1.221 mse = 1.302 mae = 0.833\n",
            "epoch57 train time: 4.446s test time: 0.498  loss = 0.892 val_mse = 1.222 mse = 1.321 mae = 0.833\n",
            "epoch58 train time: 4.484s test time: 0.488  loss = 0.890 val_mse = 1.218 mse = 1.307 mae = 0.829\n",
            "epoch59 train time: 4.465s test time: 0.501  loss = 0.890 val_mse = 1.205 mse = 1.305 mae = 0.834\n",
            "epoch60 train time: 4.466s test time: 0.493  loss = 0.888 val_mse = 1.195 mse = 1.284 mae = 0.847\n",
            "epoch61 train time: 4.455s test time: 0.493  loss = 0.891 val_mse = 1.203 mse = 1.325 mae = 0.832\n",
            "epoch62 train time: 4.462s test time: 0.488  loss = 0.893 val_mse = 1.185 mse = 1.281 mae = 0.852\n",
            "epoch63 train time: 4.448s test time: 0.482  loss = 0.889 val_mse = 1.254 mse = 1.348 mae = 0.831\n",
            "epoch64 train time: 4.441s test time: 0.492  loss = 0.888 val_mse = 1.182 mse = 1.290 mae = 0.849\n",
            "epoch65 train time: 4.427s test time: 0.489  loss = 0.884 val_mse = 1.194 mse = 1.304 mae = 0.844\n",
            "epoch66 train time: 4.443s test time: 0.497  loss = 0.885 val_mse = 1.181 mse = 1.289 mae = 0.844\n",
            "epoch67 train time: 4.469s test time: 0.492  loss = 0.886 val_mse = 1.161 mse = 1.294 mae = 0.850\n",
            "epoch68 train time: 4.455s test time: 0.501  loss = 0.885 val_mse = 1.180 mse = 1.311 mae = 0.837\n",
            "epoch69 train time: 4.466s test time: 0.495  loss = 0.885 val_mse = 1.179 mse = 1.303 mae = 0.835\n",
            "epoch70 train time: 4.452s test time: 0.512  loss = 0.882 val_mse = 1.188 mse = 1.312 mae = 0.835\n",
            "epoch71 train time: 4.448s test time: 0.498  loss = 0.884 val_mse = 1.188 mse = 1.319 mae = 0.833\n",
            "epoch72 train time: 4.453s test time: 0.491  loss = 0.886 val_mse = 1.184 mse = 1.299 mae = 0.832\n",
            "epoch73 train time: 4.468s test time: 0.498  loss = 0.881 val_mse = 1.171 mse = 1.301 mae = 0.852\n",
            "epoch74 train time: 4.447s test time: 0.488  loss = 0.887 val_mse = 1.197 mse = 1.313 mae = 0.839\n",
            "epoch75 train time: 4.435s test time: 0.497  loss = 0.888 val_mse = 1.185 mse = 1.305 mae = 0.834\n",
            "epoch76 train time: 4.437s test time: 0.520  loss = 0.877 val_mse = 1.181 mse = 1.319 mae = 0.838\n",
            "epoch77 train time: 4.479s test time: 0.491  loss = 0.883 val_mse = 1.189 mse = 1.314 mae = 0.833\n",
            "epoch78 train time: 4.434s test time: 0.495  loss = 0.877 val_mse = 1.187 mse = 1.315 mae = 0.831\n",
            "epoch79 train time: 4.479s test time: 0.501  loss = 0.878 val_mse = 1.173 mse = 1.309 mae = 0.840\n",
            "epoch80 train time: 4.454s test time: 0.497  loss = 0.884 val_mse = 1.187 mse = 1.301 mae = 0.840\n",
            "epoch81 train time: 4.472s test time: 0.495  loss = 0.885 val_mse = 1.169 mse = 1.296 mae = 0.846\n",
            "epoch82 train time: 4.435s test time: 0.494  loss = 0.878 val_mse = 1.190 mse = 1.317 mae = 0.834\n",
            "epoch83 train time: 4.462s test time: 0.497  loss = 0.882 val_mse = 1.225 mse = 1.327 mae = 0.835\n",
            "epoch84 train time: 4.469s test time: 0.513  loss = 0.883 val_mse = 1.180 mse = 1.298 mae = 0.832\n",
            "epoch85 train time: 4.448s test time: 0.498  loss = 0.878 val_mse = 1.186 mse = 1.317 mae = 0.837\n",
            "epoch86 train time: 4.454s test time: 0.495  loss = 0.877 val_mse = 1.176 mse = 1.324 mae = 0.838\n",
            "epoch87 train time: 4.443s test time: 0.502  loss = 0.879 val_mse = 1.172 mse = 1.295 mae = 0.851\n",
            "epoch88 train time: 4.444s test time: 0.504  loss = 0.879 val_mse = 1.164 mse = 1.304 mae = 0.840\n",
            "epoch89 train time: 4.448s test time: 0.500  loss = 0.876 val_mse = 1.193 mse = 1.314 mae = 0.835\n",
            "epoch90 train time: 4.454s test time: 0.488  loss = 0.882 val_mse = 1.192 mse = 1.315 mae = 0.833\n",
            "epoch91 train time: 4.444s test time: 0.491  loss = 0.876 val_mse = 1.196 mse = 1.322 mae = 0.838\n",
            "epoch92 train time: 4.432s test time: 0.497  loss = 0.882 val_mse = 1.169 mse = 1.300 mae = 0.843\n",
            "epoch93 train time: 4.457s test time: 0.492  loss = 0.875 val_mse = 1.173 mse = 1.303 mae = 0.856\n",
            "epoch94 train time: 4.445s test time: 0.493  loss = 0.883 val_mse = 1.174 mse = 1.308 mae = 0.837\n",
            "epoch95 train time: 4.437s test time: 0.491  loss = 0.878 val_mse = 1.186 mse = 1.308 mae = 0.839\n",
            "epoch96 train time: 4.444s test time: 0.502  loss = 0.876 val_mse = 1.178 mse = 1.303 mae = 0.836\n",
            "epoch97 train time: 4.461s test time: 0.508  loss = 0.879 val_mse = 1.195 mse = 1.313 mae = 0.837\n",
            "epoch98 train time: 4.442s test time: 0.493  loss = 0.877 val_mse = 1.180 mse = 1.303 mae = 0.838\n",
            "epoch99 train time: 4.465s test time: 0.495  loss = 0.883 val_mse = 1.173 mse = 1.300 mae = 0.854\n",
            "epoch100 train time: 4.462s test time: 0.491  loss = 0.878 val_mse = 1.167 mse = 1.299 mae = 0.846\n",
            "epoch101 train time: 4.465s test time: 0.506  loss = 0.878 val_mse = 1.177 mse = 1.306 mae = 0.843\n",
            "epoch102 train time: 4.456s test time: 0.517  loss = 0.876 val_mse = 1.166 mse = 1.309 mae = 0.846\n",
            "epoch103 train time: 4.442s test time: 0.491  loss = 0.879 val_mse = 1.178 mse = 1.315 mae = 0.844\n",
            "epoch104 train time: 4.463s test time: 0.490  loss = 0.875 val_mse = 1.176 mse = 1.295 mae = 0.839\n",
            "epoch105 train time: 4.450s test time: 0.494  loss = 0.880 val_mse = 1.177 mse = 1.306 mae = 0.851\n",
            "epoch106 train time: 4.431s test time: 0.505  loss = 0.879 val_mse = 1.176 mse = 1.297 mae = 0.842\n",
            "epoch107 train time: 4.503s test time: 0.494  loss = 0.878 val_mse = 1.178 mse = 1.313 mae = 0.852\n",
            "epoch108 train time: 4.455s test time: 0.489  loss = 0.884 val_mse = 1.180 mse = 1.297 mae = 0.842\n",
            "epoch109 train time: 4.477s test time: 0.491  loss = 0.881 val_mse = 1.173 mse = 1.307 mae = 0.846\n",
            "epoch110 train time: 4.451s test time: 0.497  loss = 0.874 val_mse = 1.211 mse = 1.362 mae = 0.840\n",
            "epoch111 train time: 4.453s test time: 0.493  loss = 0.879 val_mse = 1.193 mse = 1.307 mae = 0.837\n",
            "epoch112 train time: 4.441s test time: 0.493  loss = 0.880 val_mse = 1.187 mse = 1.310 mae = 0.838\n",
            "epoch113 train time: 4.451s test time: 0.495  loss = 0.878 val_mse = 1.181 mse = 1.310 mae = 0.839\n",
            "epoch114 train time: 4.430s test time: 0.510  loss = 0.877 val_mse = 1.201 mse = 1.343 mae = 0.837\n",
            "epoch115 train time: 4.438s test time: 0.510  loss = 0.877 val_mse = 1.189 mse = 1.315 mae = 0.839\n",
            "epoch116 train time: 4.471s test time: 0.486  loss = 0.880 val_mse = 1.200 mse = 1.321 mae = 0.837\n",
            "epoch117 train time: 4.439s test time: 0.492  loss = 0.878 val_mse = 1.177 mse = 1.316 mae = 0.853\n",
            "epoch118 train time: 4.441s test time: 0.494  loss = 0.878 val_mse = 1.178 mse = 1.315 mae = 0.849\n",
            "epoch119 train time: 4.432s test time: 0.495  loss = 0.880 val_mse = 1.183 mse = 1.317 mae = 0.858\n",
            "MAE 0.8467357758208358\n",
            "MSE 1.321252338789283\n",
            "epoch0 train time: 4.775s test time: 0.488  loss = 95283.060 val_mse = 491.457 mse = 1430.224 mae = 12.175\n",
            "epoch1 train time: 4.383s test time: 0.501  loss = 1035.174 val_mse = 913.051 mse = 974.388 mae = 21.078\n",
            "epoch2 train time: 4.418s test time: 0.491  loss = 1731.577 val_mse = 670.254 mse = 1800.920 mae = 18.321\n",
            "epoch3 train time: 4.433s test time: 0.493  loss = 908.413 val_mse = 876.854 mse = 849.641 mae = 19.203\n",
            "epoch4 train time: 4.423s test time: 0.512  loss = 1324.793 val_mse = 872.677 mse = 982.105 mae = 22.460\n",
            "epoch5 train time: 4.430s test time: 0.496  loss = 779.830 val_mse = 764.649 mse = 821.918 mae = 19.980\n",
            "epoch6 train time: 4.436s test time: 0.501  loss = 364.995 val_mse = 299.768 mse = 296.351 mae = 12.347\n",
            "epoch7 train time: 4.423s test time: 0.497  loss = 1190.446 val_mse = 1113.454 mse = 1008.470 mae = 22.112\n",
            "epoch8 train time: 4.413s test time: 0.495  loss = 1023.558 val_mse = 745.025 mse = 868.319 mae = 17.434\n",
            "epoch9 train time: 4.414s test time: 0.494  loss = 773.145 val_mse = 1239.294 mse = 751.700 mae = 19.980\n",
            "epoch10 train time: 4.426s test time: 0.487  loss = 2329.490 val_mse = 2163.637 mse = 2286.100 mae = 33.891\n",
            "epoch11 train time: 4.407s test time: 0.495  loss = 2166.400 val_mse = 1754.638 mse = 1222.069 mae = 23.281\n",
            "epoch12 train time: 4.397s test time: 0.491  loss = 997.344 val_mse = 1031.937 mse = 14385.656 mae = 26.399\n",
            "epoch13 train time: 4.361s test time: 0.494  loss = 872.369 val_mse = 807.356 mse = 748.669 mae = 20.304\n",
            "epoch14 train time: 4.386s test time: 0.492  loss = 761.753 val_mse = 931.295 mse = 940.129 mae = 22.024\n",
            "epoch15 train time: 4.371s test time: 0.494  loss = 1164.224 val_mse = 1327.951 mse = 1325.869 mae = 27.020\n",
            "epoch16 train time: 4.374s test time: 0.493  loss = 5328.172 val_mse = 1752.144 mse = 1751.338 mae = 30.848\n",
            "epoch17 train time: 4.394s test time: 0.513  loss = 2715.151 val_mse = 3075.765 mse = 2923.180 mae = 39.041\n",
            "epoch18 train time: 4.397s test time: 0.505  loss = 2617.868 val_mse = 1443.793 mse = 1419.715 mae = 25.935\n",
            "epoch19 train time: 4.415s test time: 0.489  loss = 1156.682 val_mse = 1082.539 mse = 1013.845 mae = 21.776\n",
            "epoch20 train time: 4.394s test time: 0.495  loss = 712.197 val_mse = 565.305 mse = 600.869 mae = 15.676\n",
            "epoch21 train time: 4.420s test time: 0.491  loss = 506.095 val_mse = 467.860 mse = 441.378 mae = 14.150\n",
            "epoch22 train time: 4.408s test time: 0.508  loss = 653.326 val_mse = 1195.286 mse = 1804.727 mae = 22.944\n",
            "epoch23 train time: 4.415s test time: 0.486  loss = 8367.244 val_mse = 3112.538 mse = 3012.487 mae = 37.490\n",
            "epoch24 train time: 4.399s test time: 0.491  loss = 2858.152 val_mse = 2218.789 mse = 7984.264 mae = 31.170\n",
            "epoch25 train time: 4.429s test time: 0.510  loss = 3662.564 val_mse = 2911.809 mse = 6314.148 mae = 39.931\n",
            "epoch26 train time: 4.423s test time: 0.487  loss = 3008.386 val_mse = 4299.326 mse = 4700.265 mae = 49.896\n",
            "epoch27 train time: 4.424s test time: 0.496  loss = 3801.122 val_mse = 3977.341 mse = 4371.053 mae = 47.128\n",
            "epoch28 train time: 4.400s test time: 0.499  loss = 1935.905 val_mse = 1186.436 mse = 2224.750 mae = 24.389\n",
            "epoch29 train time: 4.404s test time: 0.494  loss = 2007.655 val_mse = 3214.787 mse = 3935.529 mae = 40.023\n",
            "epoch30 train time: 4.409s test time: 0.493  loss = 2949.802 val_mse = 1762.616 mse = 1653.297 mae = 26.025\n",
            "epoch31 train time: 4.398s test time: 0.484  loss = 20817942.103 val_mse = 4428.373 mse = 4476.281 mae = 46.778\n",
            "epoch32 train time: 4.396s test time: 0.490  loss = 4860.049 val_mse = 5115.717 mse = 6854.496 mae = 53.370\n",
            "epoch33 train time: 4.387s test time: 0.514  loss = 4600.840 val_mse = 4431.195 mse = 5306.894 mae = 49.883\n",
            "epoch34 train time: 4.383s test time: 0.518  loss = 7680.644 val_mse = 5335.858 mse = 5565.062 mae = 52.359\n",
            "epoch35 train time: 4.434s test time: 0.515  loss = 4994.178 val_mse = 4583.649 mse = 5473.452 mae = 51.184\n",
            "epoch36 train time: 4.407s test time: 0.487  loss = 4601.196 val_mse = 4717.178 mse = 5478.452 mae = 52.003\n",
            "epoch37 train time: 4.401s test time: 0.496  loss = 4348.436 val_mse = 3811.283 mse = 4878.981 mae = 48.152\n",
            "epoch38 train time: 4.426s test time: 0.492  loss = 2704.265 val_mse = 1755.468 mse = 1745.610 mae = 27.250\n",
            "epoch39 train time: 4.406s test time: 0.490  loss = 8142384.158 val_mse = 3823.536 mse = 4035.071 mae = 43.086\n",
            "epoch40 train time: 4.405s test time: 0.496  loss = 3661.661 val_mse = 3797.309 mse = 4549.788 mae = 47.849\n",
            "epoch41 train time: 4.422s test time: 0.488  loss = 11734.389 val_mse = 3226.906 mse = 3818.502 mae = 41.254\n",
            "epoch42 train time: 4.409s test time: 0.497  loss = 24660.539 val_mse = 3199.749 mse = 3531.699 mae = 41.592\n",
            "epoch43 train time: 4.410s test time: 0.492  loss = 4501.856 val_mse = 2968.984 mse = 3116.398 mae = 40.271\n",
            "epoch44 train time: 4.423s test time: 0.489  loss = 3992.356 val_mse = 3797.630 mse = 3956.384 mae = 43.325\n",
            "epoch45 train time: 4.422s test time: 0.497  loss = 4169.946 val_mse = 2264.208 mse = 2708.050 mae = 35.057\n",
            "epoch46 train time: 4.396s test time: 0.501  loss = 7148.042 val_mse = 4119.191 mse = 4080.521 mae = 45.726\n",
            "epoch47 train time: 4.443s test time: 0.499  loss = 4461.304 val_mse = 5208.849 mse = 5313.384 mae = 52.501\n",
            "epoch48 train time: 4.426s test time: 0.501  loss = 4492.605 val_mse = 1763.593 mse = 1788.144 mae = 30.311\n",
            "epoch49 train time: 4.407s test time: 0.500  loss = 3012.672 val_mse = 4082.127 mse = 3862.058 mae = 43.550\n",
            "epoch50 train time: 4.414s test time: 0.500  loss = 4728.872 val_mse = 4351.568 mse = 4278.191 mae = 47.136\n",
            "epoch51 train time: 4.414s test time: 0.498  loss = 2497.130 val_mse = 29399.865 mse = 1112.383 mae = 22.250\n",
            "epoch52 train time: 4.411s test time: 0.499  loss = 1797.321 val_mse = 3412.155 mse = 3766.004 mae = 38.554\n",
            "epoch53 train time: 4.423s test time: 0.504  loss = 3209.943 val_mse = 3429.004 mse = 3288.297 mae = 39.269\n",
            "epoch54 train time: 4.423s test time: 0.504  loss = 2520.314 val_mse = 1328.145 mse = 1324.626 mae = 23.279\n",
            "epoch55 train time: 4.407s test time: 0.502  loss = 1522.178 val_mse = 1750.293 mse = 509.094 mae = 15.348\n",
            "epoch56 train time: 4.440s test time: 0.494  loss = 1103.871 val_mse = 744.756 mse = 651.987 mae = 16.405\n",
            "epoch57 train time: 4.412s test time: 0.509  loss = 838.707 val_mse = 415.147 mse = 946.040 mae = 14.060\n",
            "epoch58 train time: 4.416s test time: 0.514  loss = 856.538 val_mse = 862.134 mse = 798.801 mae = 18.698\n",
            "epoch59 train time: 4.409s test time: 0.494  loss = 967.668 val_mse = 696.077 mse = 757.719 mae = 18.210\n",
            "epoch60 train time: 4.406s test time: 0.486  loss = 1150.606 val_mse = 1089.087 mse = 939.462 mae = 20.583\n",
            "epoch61 train time: 4.412s test time: 0.491  loss = 1019.090 val_mse = 680.103 mse = 607.685 mae = 15.700\n",
            "epoch62 train time: 4.417s test time: 0.493  loss = 811.653 val_mse = 1161.759 mse = 909.406 mae = 18.373\n",
            "epoch63 train time: 4.406s test time: 0.502  loss = 1317.958 val_mse = 585.437 mse = 575.842 mae = 16.968\n",
            "epoch64 train time: 4.397s test time: 0.504  loss = 1137.212 val_mse = 2243.625 mse = 2296.327 mae = 31.247\n",
            "epoch65 train time: 4.412s test time: 0.536  loss = 2394.541 val_mse = 1731.665 mse = 2269.643 mae = 28.313\n",
            "epoch66 train time: 4.404s test time: 0.484  loss = 1336.171 val_mse = 323.078 mse = 335.540 mae = 13.072\n",
            "epoch67 train time: 4.406s test time: 0.494  loss = 492.912 val_mse = 221.026 mse = 176.841 mae = 10.428\n",
            "epoch68 train time: 4.420s test time: 0.487  loss = 480.804 val_mse = 71735.272 mse = 751.862 mae = 17.563\n",
            "epoch69 train time: 4.419s test time: 0.502  loss = 1775.594 val_mse = 1319.750 mse = 1090.936 mae = 21.169\n",
            "epoch70 train time: 4.404s test time: 0.495  loss = 2091.271 val_mse = 1087.440 mse = 1049.318 mae = 21.500\n",
            "epoch71 train time: 4.393s test time: 0.491  loss = 1833.032 val_mse = 3523.914 mse = 3055.894 mae = 34.382\n",
            "epoch72 train time: 4.402s test time: 0.486  loss = 2520.202 val_mse = 1437.304 mse = 1656.926 mae = 25.516\n",
            "epoch73 train time: 4.408s test time: 0.499  loss = 1872.068 val_mse = 1304.740 mse = 1300.862 mae = 23.615\n",
            "epoch74 train time: 4.423s test time: 0.518  loss = 1443.523 val_mse = 1286.888 mse = 1424.396 mae = 25.302\n",
            "epoch75 train time: 4.417s test time: 0.499  loss = 1854.162 val_mse = 2312.158 mse = 1965.080 mae = 29.221\n",
            "epoch76 train time: 4.409s test time: 0.486  loss = 1873.765 val_mse = 1073.800 mse = 1129.604 mae = 21.430\n",
            "epoch77 train time: 4.401s test time: 0.492  loss = 1108.643 val_mse = 549.202 mse = 553.883 mae = 15.431\n",
            "epoch78 train time: 4.398s test time: 0.509  loss = 553.413 val_mse = 276.837 mse = 255.743 mae = 12.454\n",
            "epoch79 train time: 4.433s test time: 0.494  loss = 1183.984 val_mse = 1784.399 mse = 1661.033 mae = 26.072\n",
            "epoch80 train time: 4.409s test time: 0.512  loss = 1005.199 val_mse = 699.848 mse = 675.375 mae = 15.651\n",
            "epoch81 train time: 4.396s test time: 0.494  loss = 2006.227 val_mse = 1213.163 mse = 1225.982 mae = 22.200\n",
            "epoch82 train time: 4.393s test time: 0.503  loss = 1722.544 val_mse = 1433.955 mse = 1202.588 mae = 22.792\n",
            "epoch83 train time: 4.395s test time: 0.494  loss = 1666.985 val_mse = 1488.888 mse = 1414.959 mae = 24.923\n",
            "epoch84 train time: 4.426s test time: 0.518  loss = 1701.870 val_mse = 939.997 mse = 1223.305 mae = 19.971\n",
            "epoch85 train time: 4.404s test time: 0.496  loss = 1062.206 val_mse = 1536.091 mse = 1359.119 mae = 24.195\n",
            "epoch86 train time: 4.412s test time: 0.509  loss = 2965.324 val_mse = 3038.035 mse = 3012.722 mae = 36.392\n",
            "epoch87 train time: 4.396s test time: 0.505  loss = 6441.055 val_mse = 3675.167 mse = 3281.109 mae = 41.814\n",
            "epoch88 train time: 4.408s test time: 0.492  loss = 3251.828 val_mse = 3741.122 mse = 3318.666 mae = 41.294\n",
            "epoch89 train time: 4.406s test time: 0.498  loss = 3704.615 val_mse = 2557.460 mse = 2272.690 mae = 34.326\n",
            "epoch90 train time: 4.412s test time: 0.507  loss = 3019.853 val_mse = 1937.771 mse = 2282.032 mae = 32.516\n",
            "epoch91 train time: 4.391s test time: 0.492  loss = 2184.477 val_mse = 595.156 mse = 714.189 mae = 16.650\n",
            "epoch92 train time: 4.418s test time: 0.497  loss = 753.109 val_mse = 424.691 mse = 25810.723 mae = 16.266\n",
            "epoch93 train time: 4.437s test time: 0.489  loss = 1184.419 val_mse = 1033.027 mse = 1046.355 mae = 19.834\n",
            "epoch94 train time: 4.408s test time: 0.490  loss = 1224.452 val_mse = 769.714 mse = 1360.547 mae = 18.563\n",
            "epoch95 train time: 4.406s test time: 0.496  loss = 1043.871 val_mse = 2024.969 mse = 2006.258 mae = 29.053\n",
            "epoch96 train time: 4.398s test time: 0.499  loss = 1647.282 val_mse = 1250.083 mse = 1262.121 mae = 23.410\n",
            "epoch97 train time: 4.415s test time: 0.507  loss = 1557.873 val_mse = 1888.236 mse = 1804.194 mae = 26.792\n",
            "epoch98 train time: 4.427s test time: 0.513  loss = 1825.392 val_mse = 1031.369 mse = 1207.273 mae = 22.732\n",
            "epoch99 train time: 4.402s test time: 0.492  loss = 1113.778 val_mse = 741.646 mse = 10784.609 mae = 20.062\n",
            "epoch100 train time: 4.405s test time: 0.511  loss = 9652.030 val_mse = 4171.693 mse = 3751.131 mae = 40.990\n",
            "epoch101 train time: 4.409s test time: 0.508  loss = 3886.050 val_mse = 5234.588 mse = 5192.904 mae = 47.874\n",
            "epoch102 train time: 4.417s test time: 0.489  loss = 4105.909 val_mse = 2737.775 mse = 3150.526 mae = 38.932\n",
            "epoch103 train time: 4.389s test time: 0.504  loss = 2798.499 val_mse = 3292.556 mse = 3271.948 mae = 38.581\n",
            "epoch104 train time: 4.428s test time: 0.490  loss = 3943.816 val_mse = 4472.769 mse = 4886.161 mae = 51.389\n",
            "epoch105 train time: 4.398s test time: 0.503  loss = 4310.666 val_mse = 1646.520 mse = 1894.179 mae = 28.662\n",
            "epoch106 train time: 4.408s test time: 0.486  loss = 1375.426 val_mse = 489.244 mse = 507.052 mae = 14.683\n",
            "epoch107 train time: 4.408s test time: 0.488  loss = 714.945 val_mse = 839.812 mse = 798.125 mae = 18.790\n",
            "epoch108 train time: 4.403s test time: 0.493  loss = 1476.736 val_mse = 773.363 mse = 27027.992 mae = 21.403\n",
            "epoch109 train time: 4.408s test time: 0.493  loss = 3183.913 val_mse = 5409.225 mse = 5915.237 mae = 50.223\n",
            "epoch110 train time: 4.407s test time: 0.493  loss = 4483.253 val_mse = 3091.950 mse = 3344.680 mae = 37.674\n",
            "epoch111 train time: 4.393s test time: 0.491  loss = 56191.684 val_mse = 3402.781 mse = 3456.658 mae = 38.648\n",
            "epoch112 train time: 4.441s test time: 0.509  loss = 4386.441 val_mse = 2455.562 mse = 2995.930 mae = 34.544\n",
            "epoch113 train time: 4.386s test time: 0.494  loss = 3472.299 val_mse = 2871.485 mse = 3113.479 mae = 37.622\n",
            "epoch114 train time: 4.401s test time: 0.513  loss = 3254.713 val_mse = 3686.911 mse = 3728.880 mae = 41.326\n",
            "epoch115 train time: 4.413s test time: 0.494  loss = 4296.735 val_mse = 4966.672 mse = 5181.608 mae = 47.445\n",
            "epoch116 train time: 4.395s test time: 0.497  loss = 10401.201 val_mse = 4701.828 mse = 4640.368 mae = 49.473\n",
            "epoch117 train time: 4.395s test time: 0.487  loss = 4596.360 val_mse = 3397.010 mse = 3105.216 mae = 40.796\n",
            "epoch118 train time: 4.393s test time: 0.493  loss = 3452.995 val_mse = 3302.787 mse = 3046.976 mae = 40.244\n",
            "epoch119 train time: 4.385s test time: 0.494  loss = 3458.331 val_mse = 2765.724 mse = 2377.651 mae = 34.969\n",
            "MAE 29.959858588270755\n",
            "MSE 2962.279483567318\n",
            "epoch0 train time: 4.849s test time: 0.490  loss = 11207027764.824 val_mse = 407983438.746 mse = 433780211.000 mae = 15442.787\n",
            "epoch1 train time: 4.473s test time: 0.513  loss = 259422385014.127 val_mse = 2253109084.510 mse = 2471496351.721 mae = 28876.959\n",
            "epoch2 train time: 4.464s test time: 0.492  loss = 104526000480.444 val_mse = 6465609168.212 mse = 5054027895.577 mae = 58294.600\n",
            "epoch3 train time: 4.480s test time: 0.483  loss = 2968658041.651 val_mse = 144879727.412 mse = 153542232.407 mae = 10819.230\n",
            "epoch4 train time: 4.480s test time: 0.494  loss = 54536827948.952 val_mse = 790182851.014 mse = 830096336.735 mae = 25469.146\n",
            "epoch5 train time: 4.479s test time: 0.502  loss = 25458364008.857 val_mse = 4578430683.632 mse = 4115463283.555 mae = 39630.367\n",
            "epoch6 train time: 4.494s test time: 0.487  loss = 534005972.492 val_mse = 21408277.435 mse = 35080935.384 mae = 4347.934\n",
            "epoch7 train time: 4.493s test time: 0.504  loss = 31065035289.937 val_mse = 129759225.472 mse = 124005590.090 mae = 9161.232\n",
            "epoch8 train time: 4.484s test time: 0.497  loss = 467235163000.476 val_mse = 7717100469.959 mse = 8590856333.153 mae = 83068.902\n",
            "epoch9 train time: 4.504s test time: 0.493  loss = 2221119972.571 val_mse = 103353995.661 mse = 104377186.311 mae = 9953.312\n",
            "epoch10 train time: 4.476s test time: 0.498  loss = 42336016382.159 val_mse = 357831083.736 mse = 341068944.507 mae = 13726.439\n",
            "epoch11 train time: 4.476s test time: 0.493  loss = 1062876056286.857 val_mse = 17155760954.560 mse = 17321708691.488 mae = 113514.200\n",
            "epoch12 train time: 4.478s test time: 0.495  loss = 4696597529.905 val_mse = 219914937.122 mse = 263550917.912 mae = 13576.280\n",
            "epoch13 train time: 4.483s test time: 0.508  loss = 37127841730.794 val_mse = 414583647.098 mse = 350909024.173 mae = 17310.394\n",
            "epoch14 train time: 4.464s test time: 0.510  loss = 16432500190.857 val_mse = 136199119.537 mse = 218867785.831 mae = 9656.169\n",
            "epoch15 train time: 4.472s test time: 0.492  loss = 14555148984.063 val_mse = 411250146.067 mse = 438954223.826 mae = 12953.080\n",
            "epoch16 train time: 4.473s test time: 0.492  loss = 270159438215.810 val_mse = 105112892657.675 mse = 113238836229.695 mae = 184473.592\n",
            "epoch17 train time: 4.457s test time: 0.488  loss = 566699709016.381 val_mse = 654179932313.808 mse = 552394299719.278 mae = 382356.863\n",
            "epoch18 train time: 4.498s test time: 0.486  loss = 142537585521.778 val_mse = 6208715405.428 mse = 8259805075.585 mae = 52011.383\n",
            "epoch19 train time: 4.480s test time: 0.502  loss = 26885749840.254 val_mse = 2044781321.910 mse = 2629743622.729 mae = 33491.223\n",
            "epoch20 train time: 4.480s test time: 0.490  loss = 46336245265.079 val_mse = 370318169.933 mse = 368086828.174 mae = 13936.717\n",
            "epoch21 train time: 4.501s test time: 0.490  loss = 224692593986.476 val_mse = 2236184958144.829 mse = 1918515811685.318 mae = 747445.109\n",
            "epoch22 train time: 4.482s test time: 0.495  loss = 1290493545520.762 val_mse = 621444664811.791 mse = 429695793287.065 mae = 312111.003\n",
            "epoch23 train time: 4.479s test time: 0.502  loss = 237148630048.508 val_mse = 29653513602.442 mse = 31884142697.011 mae = 91148.071\n",
            "epoch24 train time: 4.477s test time: 0.499  loss = 15631127781.587 val_mse = 2253544591.872 mse = 1838884156.047 mae = 24484.338\n",
            "epoch25 train time: 4.480s test time: 0.486  loss = 1914505914.540 val_mse = 175054731.460 mse = 171208022.717 mae = 7831.972\n",
            "epoch26 train time: 4.473s test time: 0.511  loss = 8395917832.294 val_mse = 23039183.664 mse = 26615012.444 mae = 3440.986\n",
            "epoch27 train time: 4.479s test time: 0.513  loss = 8963278728.849 val_mse = 41923779.690 mse = 41192808.963 mae = 4707.614\n",
            "epoch28 train time: 4.466s test time: 0.487  loss = 18548127093.571 val_mse = 279080341.405 mse = 269170067.083 mae = 10817.188\n",
            "epoch29 train time: 4.472s test time: 0.492  loss = 28123708917.802 val_mse = 106982947688.155 mse = 112464837961.432 mae = 209998.983\n",
            "epoch30 train time: 4.475s test time: 0.492  loss = 4929292981.508 val_mse = 32849322.264 mse = 32044249.915 mae = 4148.483\n",
            "epoch31 train time: 4.472s test time: 0.491  loss = 242757663744.873 val_mse = 1971790564.606 mse = 2055524996.183 mae = 27069.234\n",
            "epoch32 train time: 4.479s test time: 0.490  loss = 92690807114.159 val_mse = 8261545094.953 mse = 8941413953.796 mae = 54316.388\n",
            "epoch33 train time: 4.470s test time: 0.498  loss = 38430566856.317 val_mse = 93043833631.219 mse = 84398550699.313 mae = 144197.002\n",
            "epoch34 train time: 4.472s test time: 0.494  loss = 453978882284.698 val_mse = 986359010.552 mse = 957626147.254 mae = 19842.242\n",
            "epoch35 train time: 4.451s test time: 0.513  loss = 69625526298.794 val_mse = 6178871599.124 mse = 5384117088.314 mae = 40047.297\n",
            "epoch36 train time: 4.467s test time: 0.515  loss = 53684951562.603 val_mse = 196036479.598 mse = 171997725.556 mae = 8934.387\n",
            "epoch37 train time: 4.484s test time: 0.512  loss = 373551282504.476 val_mse = 6554533112.395 mse = 6074122196.818 mae = 53090.423\n",
            "epoch38 train time: 4.482s test time: 0.487  loss = 2139159275868.889 val_mse = 4983601354575.602 mse = 5699632545431.314 mae = 1222466.462\n",
            "epoch39 train time: 4.473s test time: 0.490  loss = 2377177645771.175 val_mse = 272869283401.501 mse = 243810542889.955 mae = 253281.203\n",
            "epoch40 train time: 4.468s test time: 0.499  loss = 120994775342.730 val_mse = 27145548044.249 mse = 18550713712.549 mae = 69956.951\n",
            "epoch41 train time: 4.516s test time: 0.484  loss = 293868239843.556 val_mse = 301443161514.211 mse = 354119953631.676 mae = 313555.077\n",
            "epoch42 train time: 4.491s test time: 0.485  loss = 101412128800.508 val_mse = 5812531432.344 mse = 10558957665.892 mae = 54477.384\n",
            "epoch43 train time: 4.490s test time: 0.510  loss = 877412056177.778 val_mse = 360539210651.647 mse = 389904436244.512 mae = 309438.716\n",
            "epoch44 train time: 4.464s test time: 0.486  loss = 126993135217.778 val_mse = 8991585034.644 mse = 10169437582.225 mae = 57018.969\n",
            "epoch45 train time: 4.453s test time: 0.510  loss = 4111634341.587 val_mse = 1022208193.564 mse = 1083301046.161 mae = 18532.255\n",
            "epoch46 train time: 4.497s test time: 0.502  loss = 20376244748.444 val_mse = 226160535.033 mse = 259872016.373 mae = 11229.092\n",
            "epoch47 train time: 4.488s test time: 0.511  loss = 75422915468.952 val_mse = 3813392062.403 mse = 4704022853.196 mae = 32950.905\n",
            "epoch48 train time: 4.471s test time: 0.508  loss = 3431387866.222 val_mse = 36138919.696 mse = 45855055.430 mae = 4180.828\n",
            "epoch49 train time: 4.484s test time: 0.503  loss = 37573073528.825 val_mse = 11767973640.671 mse = 10339875657.442 mae = 66745.224\n",
            "epoch50 train time: 4.484s test time: 0.500  loss = 23615953225.397 val_mse = 468204531.669 mse = 598248998.238 mae = 14966.225\n",
            "epoch51 train time: 4.470s test time: 0.494  loss = 144571305424.000 val_mse = 3894850080.557 mse = 4542149796.304 mae = 39201.200\n",
            "epoch52 train time: 4.476s test time: 0.485  loss = 83668623912.635 val_mse = 66182058639.385 mse = 72544319758.307 mae = 164850.768\n",
            "epoch53 train time: 4.467s test time: 0.487  loss = 53592989266.921 val_mse = 26463515211.677 mse = 24137407204.645 mae = 104705.974\n",
            "epoch54 train time: 4.458s test time: 0.491  loss = 46547207168.000 val_mse = 3069317559.080 mse = 3108521839.040 mae = 35473.449\n",
            "epoch55 train time: 4.483s test time: 0.505  loss = 25092426405.714 val_mse = 53162916454.944 mse = 45012503608.781 mae = 118936.291\n",
            "epoch56 train time: 4.454s test time: 0.510  loss = 2282948780935.111 val_mse = 1978761280229.630 mse = 1534336564819.881 mae = 637504.681\n",
            "epoch57 train time: 4.454s test time: 0.503  loss = 1661020041150.984 val_mse = 661982930088.973 mse = 537454025474.385 mae = 363870.123\n",
            "epoch58 train time: 4.466s test time: 0.507  loss = 579853494402.032 val_mse = 250725784254.041 mse = 214917943733.918 mae = 201437.602\n",
            "epoch59 train time: 4.456s test time: 0.503  loss = 141959296877.714 val_mse = 36879303889.809 mse = 26225877694.756 mae = 79539.279\n",
            "epoch60 train time: 4.440s test time: 0.497  loss = 91272021552.762 val_mse = 15379139360.075 mse = 13005113732.728 mae = 70831.611\n",
            "epoch61 train time: 4.477s test time: 0.496  loss = 73690897630.222 val_mse = 14082604641.852 mse = 12510884627.886 mae = 67353.913\n",
            "epoch62 train time: 4.481s test time: 0.490  loss = 2278226432.889 val_mse = 2658880973.761 mse = 3199831941.360 mae = 28959.859\n",
            "epoch63 train time: 4.471s test time: 0.493  loss = 7522958554.000 val_mse = 23502772488.242 mse = 30905562223.703 mae = 89122.745\n",
            "epoch64 train time: 4.487s test time: 0.495  loss = 131101705664.889 val_mse = 5241867982.637 mse = 5344158314.472 mae = 43504.715\n",
            "epoch65 train time: 4.481s test time: 0.487  loss = 21052769255.619 val_mse = 1295386336.516 mse = 1611158412.482 mae = 26285.527\n",
            "epoch66 train time: 4.488s test time: 0.496  loss = 64908239245.206 val_mse = 112565882026.429 mse = 102566860254.708 mae = 182893.676\n",
            "epoch67 train time: 4.473s test time: 0.515  loss = 11888031557.206 val_mse = 730658777.493 mse = 601901538.573 mae = 16711.295\n",
            "epoch68 train time: 4.491s test time: 0.512  loss = 17338076196.952 val_mse = 1844089002.037 mse = 1472055807.183 mae = 23959.371\n",
            "epoch69 train time: 4.476s test time: 0.516  loss = 105308632755.492 val_mse = 9622608066.414 mse = 10747033679.221 mae = 57489.019\n",
            "epoch70 train time: 4.504s test time: 0.503  loss = 212427574846.984 val_mse = 37029081392.634 mse = 34509261971.362 mae = 107473.469\n",
            "epoch71 train time: 4.474s test time: 0.492  loss = 16383776381.460 val_mse = 683490231.838 mse = 854528216.631 mae = 15249.394\n",
            "epoch72 train time: 4.489s test time: 0.491  loss = 53404274279.111 val_mse = 1366782042.956 mse = 1309310788.022 mae = 25194.197\n",
            "epoch73 train time: 4.497s test time: 0.492  loss = 6987315639.556 val_mse = 1624700406.631 mse = 1563289966.145 mae = 24668.587\n",
            "epoch74 train time: 4.478s test time: 0.497  loss = 78166846149.333 val_mse = 1079018245.875 mse = 961274207.862 mae = 20066.493\n",
            "epoch75 train time: 4.458s test time: 0.496  loss = 14870957075.619 val_mse = 2719216484.533 mse = 2927959699.902 mae = 36836.364\n",
            "epoch76 train time: 4.514s test time: 0.497  loss = 330833819787.429 val_mse = 77114430049.650 mse = 61306012379.421 mae = 133635.396\n",
            "epoch77 train time: 4.470s test time: 0.491  loss = 31479175319.365 val_mse = 2092924932.979 mse = 1896413169.189 mae = 26071.781\n",
            "epoch78 train time: 4.472s test time: 0.497  loss = 12203110036.063 val_mse = 824847672.876 mse = 782116571.411 mae = 18414.751\n",
            "epoch79 train time: 4.479s test time: 0.497  loss = 702313584809.651 val_mse = 426617991372.091 mse = 329437326956.860 mae = 321603.131\n",
            "epoch80 train time: 4.464s test time: 0.495  loss = 232336850700.190 val_mse = 105311196942.935 mse = 110705917581.096 mae = 206845.443\n",
            "epoch81 train time: 4.451s test time: 0.514  loss = 30997207887.238 val_mse = 2127863417.213 mse = 2185710325.666 mae = 29868.587\n",
            "epoch82 train time: 4.484s test time: 0.502  loss = 26182571386.413 val_mse = 11456670103.218 mse = 11833712789.851 mae = 69344.428\n",
            "epoch83 train time: 4.460s test time: 0.485  loss = 547110566322.794 val_mse = 110342898293.852 mse = 115826458294.523 mae = 198672.708\n",
            "epoch84 train time: 4.494s test time: 0.483  loss = 38804607752.127 val_mse = 6579781190.922 mse = 7700421404.274 mae = 46668.253\n",
            "epoch85 train time: 4.487s test time: 0.497  loss = 76339090016.254 val_mse = 17286232132.571 mse = 16191751836.242 mae = 89242.814\n",
            "epoch86 train time: 4.480s test time: 0.496  loss = 59475442599.619 val_mse = 308568363718.046 mse = 268905228393.064 mae = 313886.578\n",
            "epoch87 train time: 4.484s test time: 0.491  loss = 33340584054.857 val_mse = 1502531497.079 mse = 1540770509.740 mae = 25932.499\n",
            "epoch88 train time: 4.483s test time: 0.492  loss = 34226539449.397 val_mse = 7173481566.703 mse = 6664157652.953 mae = 51648.819\n",
            "epoch89 train time: 4.463s test time: 0.488  loss = 77787465833.143 val_mse = 301980113160.670 mse = 363178356663.157 mae = 291036.961\n",
            "epoch90 train time: 4.454s test time: 0.497  loss = 21919393260.190 val_mse = 697323228.912 mse = 801564047.273 mae = 16783.020\n",
            "epoch91 train time: 4.493s test time: 0.498  loss = 1532360574943.492 val_mse = 853359588214.463 mse = 526348719796.075 mae = 383309.560\n",
            "epoch92 train time: 4.467s test time: 0.508  loss = 426248072370.794 val_mse = 116785872779.510 mse = 60394476062.276 mae = 135169.608\n",
            "epoch93 train time: 4.454s test time: 0.491  loss = 54719375232.000 val_mse = 21782065395.572 mse = 25171235556.472 mae = 93313.535\n",
            "epoch94 train time: 4.467s test time: 0.512  loss = 7192753803.683 val_mse = 1506747456.561 mse = 1547825953.139 mae = 21943.326\n",
            "epoch95 train time: 4.472s test time: 0.509  loss = 110133905416.762 val_mse = 349176273106.714 mse = 312825931227.912 mae = 298852.083\n",
            "epoch96 train time: 4.452s test time: 0.491  loss = 52581848201.651 val_mse = 3221521212.186 mse = 3782321577.851 mae = 34407.141\n",
            "epoch97 train time: 4.452s test time: 0.491  loss = 31512133994.159 val_mse = 1610118501.798 mse = 1589504339.390 mae = 24666.774\n",
            "epoch98 train time: 4.491s test time: 0.498  loss = 31832789447.873 val_mse = 24336050520.599 mse = 28230038548.113 mae = 99893.740\n",
            "epoch99 train time: 4.515s test time: 0.492  loss = 23672129948.698 val_mse = 7736037046.594 mse = 8364230316.462 mae = 55237.429\n",
            "epoch100 train time: 4.482s test time: 0.491  loss = 179378965595.429 val_mse = 67339053524.045 mse = 85003837350.434 mae = 167146.484\n",
            "epoch101 train time: 4.485s test time: 0.498  loss = 18025930347.683 val_mse = 55270398128.026 mse = 91625238886.479 mae = 153532.119\n",
            "epoch102 train time: 4.479s test time: 0.522  loss = 32652339207.111 val_mse = 3154635248.889 mse = 3800042427.781 mae = 38361.611\n",
            "epoch103 train time: 4.490s test time: 0.490  loss = 15732687410.095 val_mse = 346692474642.477 mse = 459403298182.605 mae = 330651.200\n",
            "epoch104 train time: 4.482s test time: 0.511  loss = 65007563666.286 val_mse = 4931023680.440 mse = 5420089034.590 mae = 43746.124\n",
            "epoch105 train time: 4.470s test time: 0.509  loss = 213790325145.905 val_mse = 18985145133.940 mse = 17047586290.772 mae = 74651.276\n",
            "epoch106 train time: 4.496s test time: 0.485  loss = 310760164641.143 val_mse = 1638521781657.850 mse = 1427821575519.983 mae = 612332.992\n",
            "epoch107 train time: 4.472s test time: 0.514  loss = 1000848511041.016 val_mse = 248144447010.389 mse = 303348241479.047 mae = 280843.745\n",
            "epoch108 train time: 4.487s test time: 0.493  loss = 165574319201.524 val_mse = 18467869552.750 mse = 19951229392.375 mae = 73428.273\n",
            "epoch109 train time: 4.485s test time: 0.503  loss = 9413060121.905 val_mse = 6615307333.643 mse = 6657852529.296 mae = 48803.678\n",
            "epoch110 train time: 4.491s test time: 0.492  loss = 21408509024.508 val_mse = 6435020664.535 mse = 6870991822.123 mae = 51625.041\n",
            "epoch111 train time: 4.478s test time: 0.500  loss = 24730781380.063 val_mse = 1906473053.709 mse = 1888997039.337 mae = 29558.576\n",
            "epoch112 train time: 4.474s test time: 0.494  loss = 144209461029.333 val_mse = 14897787578.659 mse = 12071425595.406 mae = 55165.086\n",
            "epoch113 train time: 4.467s test time: 0.493  loss = 9268674531.810 val_mse = 1002468104.722 mse = 1023914076.172 mae = 19493.170\n",
            "epoch114 train time: 4.469s test time: 0.507  loss = 34519333385.365 val_mse = 5258181637.990 mse = 4646289465.052 mae = 42601.948\n",
            "epoch115 train time: 4.484s test time: 0.487  loss = 673171125636.063 val_mse = 1036317623073.339 mse = 927523801580.774 mae = 520532.281\n",
            "epoch116 train time: 4.491s test time: 0.496  loss = 768609101954.032 val_mse = 246533767996.372 mse = 243131337245.627 mae = 255887.060\n",
            "epoch117 train time: 4.462s test time: 0.486  loss = 198599937820.444 val_mse = 39322689604.460 mse = 36819621023.411 mae = 103281.490\n",
            "epoch118 train time: 4.472s test time: 0.498  loss = 43202042351.746 val_mse = 54606570165.000 mse = 37989654662.123 mae = 110933.200\n",
            "epoch119 train time: 4.474s test time: 0.490  loss = 211442853349.587 val_mse = 151331415325.439 mse = 149728485546.060 mae = 227994.993\n",
            "MAE 117780.3260837661\n",
            "MSE 157164938194.77863\n"
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    print(sys.path[0])\n",
        "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
        "    word_latent_dim = 300\n",
        "    latent_dim = 15\n",
        "    max_doc_length = 300\n",
        "    num_filters = 50\n",
        "    window_size = 2\n",
        "    v_dim = 50\n",
        "    learning_rate_list = [0.0001, 0.001, 0.01, 0.1, 1]\n",
        "    learning_rate = 0.0001\n",
        "    lambda_1 = 0.01 #normally we set from [0.05, 0.01, 0.005, 0.001]\n",
        "    drop_out = 0.6\n",
        "    batch_size = 300\n",
        "    epochs = 120\n",
        "    avg_mae_list = []\n",
        "    avg_mse_list = []\n",
        "    \n",
        "    # loading data\n",
        "    firTime = time()\n",
        "    dataSet = Dataset(max_doc_length, sys.path[0], \"dataPreprocessingWordDict.out\")\n",
        "    word_dict, user_reviews, item_reviews, train, valRatings, testRatings = dataSet.word_id_dict, dataSet.userReview_dict, dataSet.itemReview_dict, dataSet.trainMtrx, dataSet.valRatings, dataSet.testRatings\n",
        "    secTime = time()\n",
        "\n",
        "    num_users, num_items = train.shape\n",
        "    print(\"load data: %.3fs\" % (secTime - firTime))\n",
        "    print(num_users, num_items)\n",
        "    print(latent_dim)\n",
        "\n",
        "    #load word embeddings\n",
        "    word_embedding_mtrx = ini_word_embed(len(word_dict), word_latent_dim)\n",
        "    # word_embedding_mtrx = word2vec_word_embed(len(word_dict), word_latent_dim,\n",
        "    #                                           \"Directory of pretrained WordEmbedding.out\",\n",
        "    #                                           word_dict)\n",
        "\n",
        "    print( \"shape\", word_embedding_mtrx.shape)\n",
        "\n",
        "    # get train instances\n",
        "    user_input, item_input, rateings = get_train_instance(train)\n",
        "    print (len(user_input), len(item_input), len(rateings))\n",
        "\n",
        "    # get test/val instances\n",
        "    user_vals, item_vals, user_input_val, item_input_val, rating_input_val = get_test_list(200, valRatings, user_reviews, item_reviews)\n",
        "    user_tests, item_tests, user_input_test, item_input_test, rating_input_test = get_test_list(200, testRatings, user_reviews, item_reviews)\n",
        "\n",
        "    #train & eval model\n",
        "    train_model()\n",
        "    \n",
        "    learning_rate = 0.001\n",
        "    #train & eval model\n",
        "    train_model()\n",
        "\n",
        "    learning_rate = 0.01\n",
        "    #train & eval model\n",
        "    train_model()\n",
        "\n",
        "    learning_rate = 0.1\n",
        "    #train & eval model\n",
        "    train_model()\n",
        "\n",
        "    learning_rate = 1\n",
        "    #train & eval model\n",
        "    train_model()"
      ],
      "id": "bWsKTRlgHDAc"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bvkY1zg4HDVI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 426
        },
        "outputId": "9f1e0ad2-be6b-4063-ede7-2aa00a28044f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "avg mae over learning rate = [0.0001, 0.001, 0.01]: [0.9583892102254641, 0.8499268625805789, 0.8467357758208358]\n",
            "avg mse over learning rate = [0.0001, 0.001, 0.01]: [1.7321942511073358, 1.317287691183217, 1.321252338789283]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 750x350 with 2 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAtwAAAFUCAYAAAAXoutBAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAAPYQAAD2EBqD+naQAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOzdeXxcVf3/8dcnaZNuSYCWlrSllFJ2lFUE5AtFRHD5CRSwKiIIoiAiiH7li8oqAm4oiBsoLbggi4CyiaBWEKRQZF+kLS2UtnShbdJ0SZrk8/vj3ElvpzPJTDqTe5O8n4/HPNp759x7P3NncuYz5557jrk7IiIiIiJSHhVJByAiIiIi0pcp4RYRERERKSMl3CIiIiIiZaSEW0RERESkjJRwi4iIiIiUkRJuEREREZEyUsItIiIiIlJGSrhFRERERMpICbeIiIiISBkp4RZJgJnNM7NpCR3bzeySJI5djCTPkYgIgJlNN7PpCR27V9SBSZ6j3kQJdxYz+2KUkMxIMIZTohjczA7O8byZ2fzo+Xvz7GMLM1sXldk1T5lpseNkP9aV+nWJ9BdmNsTMLjGzSUnH0pf19vrazIaZ2aVm9qKZrTazd8zsWTO7xsxGx8pd0kld7Wa2TU+8VpG+yMy+YWbHlPs4A8p9gF7oRGAesL+ZTXT32QnGsg74FPCvrPWHAmOB5k62PQFw4G3Ca/pWnnLNwOdyrG8rKlIp1s5Ae9JBpFxvPkdDgIuj/09PMI6+rtfW12Y2EHgE2AW4CfgJMAzYPdrPXcDCrH2dCTTlOPbKzYxd8vtg0gH0Ar39HH0DuAO4u5wHUcIdY2bbAwcBk4FfEirzSxMM6X7gBDP7sru3xtZ/CngaGNHJtp+Otn8jKp8v4W5199+WItikmJkBg9x9bYIxDHH3NYWWd/fOfiz1OWY2AKhw95ZCt0nTOepO/FJefaC+PgbYGzjR3X8ff8LMBgFVOY5xh7svK2HMPa7YurIMxx/q7qsLLd/f/ubNrAKocveCr3Kn6Rx1J/6eoi4lGzsRWAHcR/i1c2LmCTMbaGbLzWxq9kZmVht13/hBbN12Zvbn6DLhEjP7kZkdGV3+m1RgPLcAw4EjYvutAo4Hfp9vIzMbB/wP8Ifosb2ZHVTgMQtmZkPN7IfR5dJmM/uvmX0tSoAzZV40s3/k2LbCzBaY2R1Z6841s5ei87nYzH5pZltmbTvPzO6NzudMYC3whTwxXmdmTWY2JMdzt5jZ22ZWGS0fbWb3mdnC6PXMMbMLM8/Htpseva59zewRM1sDXGFmN5nZsqjlKvtYfzWz/2a9hmmx5cxl6feZ2dVmtjT67NxlZlvnOHeXRHGuMbN/mNlu2fsshpmNMbMbo3PeHL0Hp2aVqTKzy8zsaTNriOJ71MwOyyo3PnotX4vezzmE1r3dbMOl8YkWujStjPY1Nfs9SuocdRF/l+fAzMYDS6PFi23DZf9LYmV2MbM7LNQp68xsppl9rIu3STbW2+vrHaJ/H8t+wt3XuXtjgcftkpkNiOqyOdHf9zwzu8LMqmNl7jWz1/Ns/28LdW183aejv4O10bn+g5ltm1UmZ12Z5xhfi873djmeu9LMWiz6LjCz/zGz283szej1zI/es8FZ202zUP/vYGb3m9kq4HcWuvGsz643om2uj+qlQbHXMD32/KQozo+b2TfN7K3o8/Q3M5uYY39nmdnr0Xl6Moq9232eLXQX/bFt+N6dbWbnW0g0s8/n4xa6Ka2N3qvjc+zPLXxPnmhmLxHquqOsuPo2sXOUL/5Cz4GZOTAUONk21NXTYs93+d1YMHfXI3oArwC/iv7/P4QuGe+JPf9rQgVflbXdZ6Ky+0XLQ4E5wBrgSuAcYAbwbFRuUhdxnJLZH6Eyvjn23NGE7h6jCZdS782x/fnAKmBwtDwb+GmOctMIlydH5HjUdhGjAX8jXPK/ATgL+HMU949i5S6M4t0ma/tDorLHx9bdAKwHrick0FdF8T0JDIyVmwfMApZH5/cL+c5p7H08IWv9kGjf18XW3QXcCnwNOAO4Ldr2+1nbTgcWAUuAa4HPR+/LB6LyH80qvw3QClyY9Rqm5XjP/xOd1y8BP4i2uzVrf9+Nyv45Ou/XA/MJSd60XOcha3sHLoktj4q2fzN6v84A/hSVOzdWbgThEvcPozL/C7wKtAB7xcqNj7Z9ifB3cD5wLjAOuCT2Ov9IuER+Q7Tuu1lxJnKOuoi/y3NA+Ps/I9rHnYSrTZ8G3h09vzuhC8BLwNej+P5J+Fs6Nul6sLc86OX1NfDJaLsLAeviGJm/m53YtK7eooBzNS3a/nbgi4QuLA7cFStzUvY5jNZvF63/WmzdN6PP6x8If8MXRX9bc+PxkKeuzBPjuGif/5vjuTlZ5+5awg+tC6J9/opQD9ye43WvI3wHTiN8V5wETIxe05eyylcRvld+nfUapseWJ7GhHppJqBsuBlYDM7L2d2ZU9hHgbEK98U4Uz/Rc5yFr+3lsXAcOAZ4DlgHfiV7PTdF5+3HWtvOBnxLql68QPtMOfCSrnAMvR+/RRdHnYy+Kq2+TPEc54y/0HBDq5nXR8TN19YHRcwV9NxZcZ3W3sutrD2Df6CR+IFq26ET/OFbmg+ROqO4D5sSWz4vKHR1bN4jwBVFsBX4W0MiG5Pk24O+xP8ZcCffzwG9jy98hVIYDsspNi46T6/GXLmI8Oir3zaz1txP++HeIlncid8X2Uzb+UXBwVO5TWeWOzF4fvW4HjizgfTXgLcKl2Pj6TB/3/4mtG5xj+18QKonq2Lrp0bZfyCpbEX1m/pC1/ivROdk+6zVMy/GeP0Tsyxe4mlDB1UXLowg/Su7KOsbF0fbTcp2HrLLOxgn3rwhJ5PCscrcQEsPMe1TJpsnLFoT7BOJfUOOjYzQAW2eVvyR67tdZ6+8ElmWtS+QcdRF/oedgRPZ5jj33MOFvNP6ZMkKy9lpX758efaO+BgYTfqx59NxU4FRgZI5jZP5ucj1e7SK+PaNyN2St/360/rBouZaQePwgq9z/EuqvcdHydtHf2zeyyu0R/d19I7ZuOjnqyk5ifRyYmbXuPdE+Toqfuxzb/l88zmjdtGjbK/Mc64msdcdmv+fkTyZfJlYXAF+O1u8RLVcREuMniX33AidH5abnOgdZ8cxj4zrwW4SGoh2zyl0ZvSfb5jtHwEDgBeBvWeud8KNwtzyf607r2xSco5zxF3kOmsjxvUCB342FPtSlZIMTgcXAPwA8nNVbgU/Yhi4Ffyd8OKZkNooucR0Rlc04ClhAaF0j2t86QktesW4jVMwfNbMa4KN03p3k3cC7CB+IjFsICcCROTZZF8Wf/fi/LuL6MOFDfm3W+h8Svvw+BODurxFaiuLnrJJwmfUe39Dv+gRCgvOQmY3IPAh9H5uAjbotAHPd/cEuYsy8j7cDHzazYbGnphDeo3/Fynb0ATezmuj4jxJaFXbJ2nUz4Qsyfqx24HfAx6L3KuNE4HF3n9tVvMD1UcwZjxKSvO2i5cMJ9178LGu7nxSw702YmQHHAfdEi/Fz/yBQB+wD4O5tHvXVs9BlY6solpmZMln+6O5Lc6yH8EMm7lFguJnVFhB2T52jTeLvxjnYSFT+/YS/65rYuR5OON87mtmYIuPsj3p9fR3VN+8lJL4QEpxfA4vM7CcW6+4Rcxyb1tWf7SKmD0f/Xp21/ofRvx+J4mkEHgA+HtULGVMIiemb0fJkQuPCbVn1xduEK4/ZdfUmdWUnbgX2NbMdYuumRPv4U2ZFVl09NDr+44Tvnr1z7PfnOdbdDLw361gnEn64/bOAWKf6xn2XH43+nRD9ux/h7/oG37hP/+8IV16644ToOCuyzv3DhDrwkEzBrHO0JaEuf5Tc9dQ/3f3lPMfsqr7tTE+do5zxF3kONlLMd2OhlHDTkQB+glB5b2+hf+lEwuWHUYQvcKIPxB+Bo2OV4WTCr6Z4Bb4doQUl/iGFcImkKNEX/sOEG28mEz7od3SyyacJLbKvx17HOsIv5RNzlG9z94dzPJ7tIrTtgIXuvipr/Sux5zNuBd4XSyQmASPZ+JztSPgALyG0xscfw6LycYUkr/HjDwY+BmEoLsKX0O3x98jMdo/6pzUQWqmWApkbSuuy9rnAc98ocnN0rGOjfe5MaI37TYGxvpm1nKl0Mv3YM+d1o8+Suy+ne5X41oQW2s+z6XnPfEl2nHszO9nMnid8pt6Jyn2ETc8PdP4edfU6O9NT5yhn/EWeg2wTCUnBt9n0fGdu+Mv+rEtMX6qv3b3B3b/u7uMJV1ZOA/5LuHx/YY5NHslRV/+7i7C2I7T8Zv89vE1opcuuq7cFDgSIktF92bSuNkJynf0Z3pVNP7/56spcMldIp0THN0KS+YDH+rSb2TgL/bOXExpklrIhSc7+O2wlXOXMdishkT8x2mcd4QfS73J8FnLpbj3USvg+7o4dCT8Qs8/7w9Hz8br6o2b2hIUhfpdH5c6kd9TVxZ6jfHV1MecgW1HfjYXQKCXB+4F6QiX+iRzPnwj8Nfr/Hwj9pj5EGELm44RLes+VMb7fE1pbtiFUPDmHgIoqp08S+iTm+rU60syGuXuuYaXK6VbCJa8TgB8TzlkD8JdYmQpCsp3rRwFsuAkto+ARSdz9CTObFx3398D/IyTFHV8iZrYFocJuJPQDm0NIqPYh9AfO/nGa8/ju/rKZPU344XNz9G8LoeWrEPmGY7Q86zdX5nX9ltAXMJfnIdwkRbhEezehVW4JId4L2HADWFxn79HmvM6eOkebxN+Nc5Atc75/QGglySXJoe16gz5RX2dz9zeAG83sLuB1Oh/OtTsKSSLvIfRl/zihxfjjhAT49liZimhfHyL332L290sxdfVCM3s0Ou4VwAGEvt3nZ8pEP7geArYi1M2vEhqZxhD+NrPr6ubo6mP2sVZYGBf9ROAywlXXajY0snSlp+tqCK/tIeB7eZ5/DcJNpYQrNo8Q+jQvInT3+Szhx2C2vlhXF3sOshX83VgoJdzBiYQvzrNyPDcZONbMzoguTzxCeOOmmNm/CJX/d7K2eYMwooFl/VLe5O7cAt1FGPbqAGKXR3M4lDDe60VsaGnO2JJw49gxFF6hdOYN4ANmVpPVyr1L7HkA3H2umT1JOGfXEc7p3b7xsG9zCDcdPublGd7vNuCcqMvCFGCeuz8Re34S4dLWZHd/JLPSwtBjxboZuNrM6gl/2Pe5e3cvIWbLnNeJxH7Vm9lwCmtxyLaU0Je+0t0f7qLs8YREYHLWlYEkh2LLpdTnKK7Qc5AvucmMArG+gPMtufWV+jqnKBGcQ+gTXQpvEJKHHYl9L5jZKEILXryuXh0loSeY2XmE+B919/h44HMICdPcqMtgqd0K/Cy6OjiF8APgntjz7yLcG3Syu98cez1HULybgT+Z2XsIn6tn3P2lbke+sXg99I/MSgtDjI6nyGQtMgcYVkDdcRyhwejI+PesmXXV/ainleMcZRRzDnLV18V8Nxak33cpsTCM0GTCzSx3ZD+A64Aaou4I0S/lOwitpCcRfrTcmrXbBwm/tjuG+bIwxNDp3YkxapE+k3DjzD2dFM10J/l+jtdyA+ESYL4W5GLdT7hc+qWs9V8hfHgfyFp/K+EL6FRCf/Lsc3ZbtL9NLqNaGNJqi82M91ZC68XJhEty2S3OmV/i8SENqwi/jIt1C+EcXEPoq1bKcc7/RrhEembW+uz3oSDu3ka47H6cmW3yBW8bD/+U6xy9l+jyc4qU9BxlKfQcZMYZ3uhz6+5LCDcYfSH6QbYRyzFMmWzQl+prM9sz6g+avX47YDdC15JSuD/699ys9edF/96Xtf5WwqgqnyPccJl9vu4k/B1cnNXXGwuGb2a8f4z2/0nCVdF7feNxs3P9DRphdJliPUDo538+ocGqlHX1TEKXs9OjBDLjRLr/w/824EAz2+R+LAvDBWaO00b4DqqMPT+e0OCWJuU4RxnFnIPVbFpXF/PdWBC1cIdKtobYDTNZniD80jmRDRXPrYThay4FXnD37NbkXxK+3G8xs2sILSwnEn5tQWGX9jbi7vkuaQAQ9VE8DnjI8w/4/mdCK+/I6IsfYEB0mTyXuzz/BAH3EH6Rfif6ED9HGBXgaMJIAXOyyt9GuIz+A0Jfqo1+Mbr7P83sl8AFZrYX4ZLwekKrzAmEyrSzvuudcvf/mNlsQutWNZt+iTxO6F92k5ldS3RXPN249OXuS83sL1HcK9n0C63b3H1x9Jn6qpn9mdAtZ0/C5d1ldOOzRbhB9jBghpndQOiOtBWhO80Hov8D3EtIdu4ys/uA7QnDJL1M6GefCmU6RxkFnQN3X2tmLxNaVl8jfOZfdPcXCS2z/wJeiM7364S+xwcSrlDtuRnx9XV9or6OHAFcGn1GnyB0xZhAaJSoJiTs2Y43s1xdAh9y98V5YnnOzG4CPh/rOrc/ofHhbnf/R9Ym9xNa9n5ASFr+mLW/OWb2LUI3wfFmdndUfnvCvSvXR9t2i7svsTB3w3mE9zq7rn6V0NL7Awv3BTUSvvuKTtDcfb2Z/YHw/rex8WADm8XdWyyMvf8T4O9mdhuh1fYUQvzdqYe+T/gbuNfCWNFPE7qQvotw9W08oY67j3D+/mJmvyf0NT6L0F3t3d18SSVXpnOUUcw5eJpwxf48wqgkc919BoV/NxbGixjSpC8+CBX3WmBIJ2WmEvrhDo+WjXAzgJM1LF5sm+0JX85rCJc/f0D4onbgvV3EdEpUbr8uys0jGmYqtu9TOyl/aFTmy9HyNPIPNeXA+C6OP4xw5/uC6Py8RhjDOueYsoQkw8kaniqrzOmEX71rCBXp84R+evW5XneR7/Xl0fFn5Xn+IODf0bEXRMfNDC02KVZuOiF56uxYmWEHf9nJezetq/ecDcMrxY9fSehzuCiK9W+ErjzLgJ8XcB6crOHqCJXRddHnuiXa98PA6bEyRuirPI+QjPyHcLPgNEIXnUy58dExvpbj2JdEz43I85kfn/Q56iL+gs5BVPZAwme5OfucExKrm6L4Wgg3dd0DHFfs57o/Pegj9XXsmJcS6pzFhAaGJVEch2Vtm/m7yfeY1MWxBxC6Gr4enZs3CX2kq/OU/22034c62edkwogPTdHjFUIdslOszHS6qCvz7Ptz0fEbCbMIZz+/K6Ev8yrCD6zrCUmUA6fEyk0Dmro4VmbYwQfzPD+d3EPeHZ9Vbnz28aP1Z7OhvphB+J6ZSejf39V5mEfWcHWE790rCFesm6PX/xjwVTaer+JUwnfyuui9OSXzOcranxObj6KrzzW569skz1HO+Is8BzsTfoiuifY3LfZcl9+NhT4s2qH0ADM7F/gRMNbdFyQdj5SPmR1NuEnrEHd/tKvyJTjeFoQW+m+5e3YfVUHnSIqj+rp/MLM9CUPXfsbdCx1NanOOV0FIku909251W+rr+uo56vd9uMvFNp1idhDhbvlZqrz7hdMJrUn/6qpgsbI/W5FM/8zppT5eb6RzJMVQfd2vnU5oob+z1Ds2s0HZ/dwJM51uheohoH+dI/XhLp87zexNwi/nOsINjbtQupsWJYXM7BOES5sfAc7x8lxCmmJmpxD6WjYRZun8JPBXd3+sDMfrjXSOpBiqr/sZM/t/hJtTP0/okpDvfqXNcQDwIzO7nXBz4D6EsdZfZOOhFvuzfnOO1KWkTKLLkZ8j9FmqJHS2/567Z98AIn2ImTkhwbsVOMM3nj2rVMfYhzAO616E6ZgXE25s+pb3/BjrqaRzJMVQfd3/WJibYRRhlJqTfNNJ3EpxjPGE2Zj3J7TYLic0Avyfbxi4oF/rT+dICbeIiIiISBmpD7eIiIiISBkp4RYRERERKSPdNJlDdMfsaMIYnyIi5VIDLCzTzbV9nupqEekhm11XK+HObTRhIgoRkXIbS5hoSYqnulpEespm1dVKuHNbBTB//nxqa2uTjkVE+qDGxka23XZbUOvs5lBdLSJlVaq6Wgl3J2pra1WJi4iknOpqEUk73TQpIiIiIlJGiSfcZnaWmc0zs3VmNsPM9u+k7EAzu8jM5kTlnzOzo3KUG2NmvzWzd8xsrZm9YGb7lfeViIiIiIhsKtGE28ymAFcDlxKm83wOeNDMRubZ5HLgC8DZhClZfwHcZWZ7x/a5JfAYsB74UFTuq8CKMr0MEREREZG8km7hPg+4wd2nuvvLwBnAGuDUPOVPAq5w9/vd/XV3/zlhCtCvxsqcD8x398+6+5PuPtfd/+ruc8r5QkREREREckks4TazKmBf4OHMOndvj5YPzLNZNbAua91a4ODY8seAmWZ2u5ktMbNnzOz00kW+KW9zVkxfweJbFrNi+gq8TUPqioikTVt7G9PnTeeWF25h+rzptLW3JR2SiPQTSY5SMgKoBBZnrV8M7JJnmweB88zsEWAOcDgwOdpPxgTgTEJXlSuA9wDXmlmLu9+Ua6dmVk1I5jNqCn0RS+9cyuxzZtP8VnPHuuqx1Uy8ZiJbT9660N2IiEgZ3fnKnZzzl3N4q3HDsN1ja8dyzVHXMHnXyQlGJiL9QdJdSop1DjALeBVoAa4DpgLtsTIVwH/c/Rvu/oy7Xw/cQOiuks8FQEPsUdBECkvvXMpLx7+0UbIN0LygmZeOf4mldy4t7FWJiEjZ3PnKnRx/2/EbJdsACxoXcPxtx3PnK3cmFJmI9BdJJtzLgDZgVNb6UcDbuTZw96XufgwwFNiO0BLeBLweK7YIeDlr01eAcZ3EciVQF3uM7Sp4b3NmnzMbcvUeidbNPne2upeIiCSorb2Nc/5yDp6jss6sO/cv56p7iYiUVWIJt7u3AE8TuoUAYGYV0fK/u9h2nbsvIHSJOQ74U+zpx4CdszbZCXijk/01u3tj5kEBswmtfHTlJi3bG+8Umuc3s/LRlV3tSkREyuTRNx/dpGU7znHmN87n0Tcf7cGoRKS/SXqmyauBm8xsJvAkcC6h9XoqgJndDCxw9wui5fcCY4Bno38vIfxo+F5snz8CHjezbwC3AfsDn48eJdOyqKWk5UREpPQWrVpU0nIiIt2RaMLt7rea2dbAZcA2hET6KHfP3Eg5jo37Zw8ijMU9gdCV5H7gJHdfGdvnU2Z2LKGbyEXAXOBcd/9dKWOvqq8qaTkRESm9+pr6kpYTEekOc1cf42xmVgs0NDQ0UFtbm7OMtzlPjH+C5gXNuftxWxit5IC5B2CVVtZ4RaT3aWxspK6uDqAu6somRSqkrm5rb2P8NeNZ0LggZz9uwxhbO5a558ylsqIyxx5EpD8rVV3d20YpSQ2rNCZeMzFayH4y/DPxxxOVbIuIJKiyopJrjroGCMl1XGb5x0f9WMm2iJSVEu7NsPXkrdn9jt2pHlO90frqsdXsfsfuGodbRCQFJu86mTs+fgdjasdstH5s7Vju+PgdGodbRMpOXUpyKOQyZZy3Oa+e9iqLb1rMVh/dinfd/S61bItIp9SlZPMVW1e3tbdx83M3c+qfT6WmqoYV569Qy7aIdEpdSlLEKo2699WF/2NKtkVEUqiyopKjdzkagFUtq1jfvj7hiESkv0h6WMA+Y9jewxh9xmiG7Tss6VBERCSPLQdtyal7ncqoYaNY37aeQQMGJR2SiPQDSrhLpHa/Wmr36/qSpoiIJMfM+PXRv046DBHpZ9SlRERERESkjJRwl1BrQyurX15N29q2pEMREZE81rWuY+6KuZpdUkR6jBLuEnpqz6d4avenaHquKelQREQkj/MfOp8J107g2hnXJh2KiPQTSrhLqHp0GI+7ZUFLwpGIiEg+mWncFzYtTDgSEekvlHCXUGYCnOaFzQlHIiLSs8zsEDO7x8wWmpmb2TFdlJ8Wlct+vFTuWEfXjAZg4Sol3CLSM5Rwl1DV6CoAmhco4RaRfmco8BxwVoHlzwHqY49tgeXA7WWJLkYJt4j0NA0LWEKZFm51KRGR/sbdHwAegDD0XgHlG4CGzHLUIr4lMLVMIXbIJNy6aVJEeopauEuoo4VbXUpERIp1GvCwu79R7gNlEu4V61awdv3ach9OREQJdyl19OFWlxIRkYKZ2WjgQ8CvuihXbWa1mQdQ053j1VXXMXjAYAAWNamVW0TKT11KSmjwjoMZfeZoBu8wOOlQRER6k5OBlcDdXZS7ALh4cw9mZpz1nrOoqqzqSLxFRMrJ3D3pGFInajlpaGhooLZW07WLSOk1NjZSV1cHUOfujUnHU0pm5sCx7t5VAo2FDt+vAfe6+1e6KFsNVMdW1QBvqa4WkXIpVV2tFm4REUnSocBE4NddFXT3ZqCjz14hN2eKiKSBEu4Sa21opfmtZqrqqxi41cCkwxER6RFmNoyQOGdsb2Z7Acvd/U0zuxIY4+6fydr0NGCGu7/YU7ECNLc2s3DVQiqsgu222K4nDy0i/ZBumiyxFye/yFN7PMU7972TdCgiIj1pP+CZ6AFwdfT/y6LlemBcfAMzqwOOo4DW7VK7dsa1TLh2At/6x7d6+tAi0g+phbvEOsbiXqixuEWk/3D36UDePh7ufkqOdQ3AkPJFlZ/G4haRnqQW7hLT0IAiIumn2SZFpCcp4S4xTX4jIpJ+9TX1gBJuEekZSrhLTNO7i4ikX6aFu6G5gdUtqxOORkT6OiXcJdbRwq0uJSIiqVVTVcPQgUMBzTYpIuWnhLvEOlq4F7Xg7ZpUSEQkjcxMN06KSI/RKCUlVrVNFaPPHE31mGp8vWPVmphBRCSNTtv7NFa1rGKbYdskHYqI9HGa2j0HTe0uIuXWl6d27ymqq0Wk3EpVV6tLiYiIiIhIGalLSRm0NrbSPL+ZAVsM6OjTLSIi6dLc2syCVQtY37aenUfsnHQ4ItKHpaKF28zOMrN5ZrbOzGaY2f6dlB1oZheZ2Zyo/HNmdlQn5f/PzNzMflye6Dc15+tzeGqPp1h4vcZ3FRFJqz++8kd2uHYHzrjvjKRDEZE+LvGE28ymAFcDlwL7AM8BD5rZyDybXA58ATgb2A34BXCXme2dY9/vico+X4bQ86oerendRUTSTrNNikhPSTzhBs4DbnD3qe7+MnAGsAY4NU/5k4Ar3P1+d3/d3X8O3A98NV7IzIYBvwNOB1aULfocNL27iEj6KeEWkZ6SaMJtZlXAvsDDmXXu3h4tH5hns2pgXS16ec8AACAASURBVNa6tcDBWet+Ctzn7g/Tw6rGaHp3EZG0qx8WpndvamliVfOqhKMRkb4s6ZsmRwCVwOKs9YuBXfJs8yBwnpk9AswBDgcmR/sBwMw+Qeie8p5CgjCzakIin1FTyHb5dHQp0fTuIiKpVVNdQ01VDataVrGoaRE11ZtV9YuI5JWGLiXFOgeYBbwKtADXAVOBdgAz2xa4BjjR3bNbwvO5AGiIPd7anAAzXUrWL1tPe3P75uxKRETKqL4mtHKrW4mIlFPSCfcyoA0YlbV+FPB2rg3cfam7HwMMBbYjtIQ3Aa9HRfYFRgL/MbNWM2sFDgW+HC1X5tjtlUBd7DF2c17UgK0GdMww2bxI3UpERNJK/bhFpCck2qXE3VvM7GlCt5C7AcysIlq+rott1wELzGwgcBxwW/TU34B3ZRWfSmgR/667t+XYVzPQkRmbbd507GbGuP8dR8WgCiqH5MrvRUQkDT65xyc5eNuD2W3r3ZIORUT6sKT7cEMYEvAmM5sJPAmcS2i9ngpgZjcDC9z9gmj5vcAY4Nno30sILfXfA3D3VcCL8QOY2WrgHXffaH05bf/t7XvqUCIi0k2f3/fzSYcgIv1A4gm3u99qZlsDlwHbEBLpo9w9cyPlOKL+2ZFBhLG4JxC6ktwPnOTuK3suahERERGRwiSecAO4+3Xk6ULi7pOylv9JmPCmmP1P6rJQibWuamXdvHVUVFcwZKchPX14EREpQEtbC/Mb5rN6/WrePerdSYcjIn1U0jdN9lkLf7mQme+eybxL5yUdioiI5DHjrRlM/MlEjr/t+KRDEZE+TAl3mWSGBtT07iIi6RUfFtDdE45GRPoqJdxlkpn8RtO7i4ikV2a2ydXrV7OqRbNNikh5KOEuk/j07mo1ERFJp6FVQ6mrrgM0FreIlI8S7jLJtHC3r26nrXGTob9FRCQlNNukiJSbEu4yqRxSyYAtwiAw6lYiIpJemdkmF61alHAkItJXKeEuo6rRG7qViIhIOml6dxEpt1SMw91XjT5jNG2r2hg0flDSoYiISB5H73w04+vGc9C2ByUdioj0UaYb+jZlZrVAQ0NDA7W1tUmHIyJ9UGNjI3V1dQB17t6YdDy9kepqESm3UtXV6lIiIiIiIlJGSrjLqG1NG00vNLHqGY3tKiKSVq3trcxePpsZb81IOhQR6aOUcJfR8r8uZ+a7Z/LaGa8lHYqIiOTxZsOb7PiTHZl00yTNmyAiZaGEu4wyY3FrencRkfTKzDa5rnUdDc0NCUcjIn2REu4yqh4TTe++qBlvU6uJiEgaDR44mC0HbQloaEARKQ8l3GU0cNTAcIbboGWpWrlFRNJKY3GLSDkp4S6jigEVVI0Kk9+0LFDCLSKSVpreXUTKSQl3mXV0K9FskyIiqaUWbhEpJyXcZdYxvfsCJdwiImk1epgSbhEpH03tXmajPjWK2gNqqX2vZkETEUmrD0z4AAMrB3LwuIOTDkVE+iAl3GU2csrIpEMQEZEuHD7hcA6fcHjSYYhIH6UuJSIiIiIiZaSEu8zam9tper6JlY+uTDoUERHJo93bmfXOLP4575+abVJESk5dSspszaw1zNxzJgO2GsDB76hvoIhIGrW0tbDTdTsB8M7X32GrwVslHJGI9CVq4S6zzLCArctbaVvblnA0IiKSy6ABgzqSbI1UIiKlpoS7zAZsMYCKQeE0tyzS5DciImmlsbhFpFyUcJeZmVE1RmNxi0jfZmaHmNk9ZrbQzNzMjilgm2oz+46ZvWFmzWY2z8xO7Yl4c8kk3ItWLUoqBBHpo9SHuwdUj6lm3Zx1tCxUC7eI9FlDgeeAG4E7C9zmNmAUcBowG6gnwYYgtXCLSLko4e4B1aOj6d3Vwi0ifZS7PwA8AOHKXlfM7CjgUGCCuy+PVs8rV3yF0GyTIlIu6lLSAzq6lCxUwi0iEvkYMBP4upktMLPXzOwHZjY4qYDqa+oBWNikhFtESkst3D1g+EeGM3DEQOreV5d0KCIiaTEBOBhYBxwLjAB+BgwHPptrAzOrBqpjq2pKGdBB2x7ERYdcxN71e5dytyIiSrh7wpaHbcmWh22ZdBgiImlSAThwors3AJjZecAdZvZFd1+bY5sLgIvLFdA+9fuwT/0+5dq9iPRjqehSYmZnRXenrzOzGWa2fydlB5rZRWY2Jyr/XNQXMF7mAjN7ysxWmdkSM7vbzHYu/ysREZECLQIWZJLtyCuAAWPzbHMlUBd75CsnIpIqiSfcZjYFuBq4FNiHcJf7g2Y2Ms8mlwNfAM4GdgN+AdxlZvFrgIcCPwUOAI4ABgJ/NbOhZXkRXfA2p+n5Jt75yzuaMlhEJHgMGG1mw2LrdgLagbdybeDuze7emHkAq0odVGZ697XrczWwi4h0T+IJN3AecIO7T3X3l4EzgDVAvrFYTwKucPf73f11d/85cD/w1UwBdz/K3ae5+0vu/hxwCjAO2LecLySf9vXtzNxzJi986AVaV7YmEYKISFmZ2TAz28vM9opWbR8tj4uev9LMbo5t8nvgHWCqme1mZocA3wduzNOdpEe878b3MemmScxaPiupEESkD0o04TazKkIS/HBmnbu3R8sH5tmsmnCTTdxaws03+WTuVlye68lo8oXazIMS34hTOaiSAVuF7vIaGlBE+qj9gGeiB4Qrl88Al0XL9YSGDwDcvYlwBXILwmglvwPuAb7cQ/HmpLG4RaQckr5pcgRQCSzOWr8Y2CXPNg8C55nZI8Ac4HBgcrSfTZhZBfBj4DF3fzHPPst6Iw6EyW9al7eGyW/2KOeRRER6nrtPJ/S/zvf8KTnWvUpIulOjvqae5xY/p4RbREoqDV1KinUOMAt4FWgBrgOmEvr95fJTQor7iU72WfYbcapGa3p3EZG0y0x+o+ndRaSUkk64lwFthKl940YBb+fawN2XuvsxhGmEtyO0hDcBr2eXNbPrgI8Ch7l7zptwon2W/UaczGyTmt5dRCS91KVERMoh0YTb3VuApwndQoCOLiCHA//uYtt17r6A0C3mOOBPsX1YlGwfC7zf3eeWIfyiVI/R9O4iImnXkXBrtkkRKaGk+3BDuLHmJjObCTwJnEtovZ4KEN3VvsDdL4iW3wuMAZ6N/r2E8MPhe7F9/hT4FHA0sMrMtonWNyR197u6lIiIpF8m4VaXEhEppcQTbne/1cy2JtzJvg0hkT7K3TM3Uo5j4/7ZgwhjcU8gdCW5HzjJ3VfGypwZ/Ts963CfBaaVMv5C1R1cx/ZXbs+wdw/rurCIiCTi3aPezcWHXszOwzVXmoiUjmkilk1FQwM2NDQ0UFtbm3Q4ItIHNTY2UldXB1AX3TsiRVJdLSLlVqq6OumbJkVERERE+jQl3D2o6YUm3nngHVpXabZJEZG0mrN8DtPnTWf52pxzpYmIFE0Jdw96/kPP88KHX2DNq2uSDkVERPI4/vbjOeymw3jirSeSDkVE+ggl3D1IQwOKiKSfxuIWkVJTwt2DNPmNiEj6abZJESk1Jdw9qGqMxuIWEUk7tXCLSKkp4e5BmS4lauEWEUmv+pp6QLNNikjpKOHuQZkuJWrhFhFJL7Vwi0ipKeHuQR1dShYq4RYRSSsl3CJSaolP7d6fDN19KBO+O4FBEwYlHYqIiOQxYcsJXHzoxWxbu23SoYhIH6Gp3XPQdMEiUm6a2n3zqa4WkXLT1O4iIiIiIr2AEu4etvqV1bxz3zvqxy0ikmLzVs7jH3P/wYLGBUmHIiJ9gBLuHjbri7N44aMvsPKfK5MORURE8vjyA1/m/Te/n/tn3Z90KCLSByjh7mFVo8NIJRqLW0QkveqHRWNxa6QSESkBJdw9LDP5jcbiFhFJLw0NKCKlpIS7h2VauJVwi4ikV0fCrdkmRaQElHD3ME3vLiKSfpmEe9GqRQlHIiJ9QVEJt5ntb2aVnTxfbWYf3/yw+i51KRGRNDCzr5vZ4Njy+8ysOrZcY2Y/Sya65KlLiYiUUrEt3P8GhmcWzKzRzCbEnt8CuKUUgfVVHV1KFjajSYdEJEFXAjWx5QeAMbHlIcAXejSiFKmvCTdNLl69mNb21oSjEZHertip3a2L5XzrJFI9upoJ350QEu92IO/1AhGRsiqkPu+3th6yNZdOupT6YfW0e3vS4YhIL1dswl0INdt2oqKqgnFfH5d0GCIi0onKikouOvSipMMQkT5CN02KiIiIiJRRd1q4dzOzbaL/G7CLmQ2LlkeUJqy+be2ctax5dQ2DJw5myM5Dkg5HRPqvz5lZU/T/AcApZrYsWq7Js02/Mb9hPrOWz2Lb2m3ZcfiOSYcjIr1YdxLuv7FxX797o389Wq8uJV1486o3WfSrRYy/bDzjLxyfdDgi0j+9CZweW34bOClHmX7rikev4BdP/4KLDrmISw+7NOlwRKQXKzbh3r4sUfQzmvxGRJLm7uOTjiHtNDSgiJRKUQm3u7/RVRkz26P74fQPmvxGRCT9NNukiJRKSW6ajCZI+LyZPQk8V4p99mVq4RaRpJnZgWb20ax1nzGzuWa2xMyuj0+E0x9lxuJWC7eIbK7NSrjN7BAzuwlYBHwN+DtwQCkC68s026SIpMBFwO6ZBTN7F/Br4GHgKuD/ARckE1o6qEuJiJRK0TdNRiOUnAKcBtQCtwHVwDHu/nJJo+ujqkeHhHv9kvW0r2+nYqBGZxSRHrcXcGFs+RPADHc/HcDM5gOXApf0fGjpkEm4l6xewvq29QysHJhwRCLSWxWV6ZnZPcB/gXcD5wKj3f3szQ3CzM4ys3lmts7MZpjZ/p2UHWhmF5nZnKj8c2Z21Obss6cN3HogNsDAoeVt9eMWkURsCSyOLR9KmN494ylg2x6NKGVGDBnBgIrQLrV49eIuSouI5FdsC/eHgGuBn7v7rFIEYGZTgKuBM4AZhET+QTPb2d2X5NjkcuDThOGsXgWOBO4ys4Pc/Zlu7rNHWYUx8ZqJDKgbwIC6ckz2KSLSpcWEkafmm1kVsA9wcez5GmB9EoGlRYVV8N0PfJfa6lqGVQ3regMRkTzMvfBhs83sAEJXkinAK8BvgD8Q+nDv2Z0uJWY2A3jK3b8ULVcA84GfuPtVOcovBL7j7j+NrfsjsNbdP92dfeY4Ri3Q0NDQQG1tbbEvSUSkS42NjdTV1QHUuXtjTx/fzH4O7AmcDxwDnEy4atkSPX8icK67v6enYyuU6moRKbdS1dVFdSlx9yei/n31wC8Jff4WRvs5wsyKmpksalXZl3CTTuYY7dHygXk2qwbWZa1bCxy8GfsUEelvLgRagX8Srhh+PpNsR04F/ppEYCIifU23+jO4+2rgRuBGM9uZ0Or9f8BVZvaQu3+swF2NACrZuB8h0fIuebZ5EDjPzB4B5gCHA5Oj/XRrn9HQV/Hhr8o+pfG6N9fR9HwTVSOrqN1fLTMi0rPcfRlwiJnVAU3u3pZV5ARgVc9Hli4LVy3klaWvMHzIcPbaZq+kwxGRXmqzOxC7+3+Br5vZBcBHCa0i5XQOcAOh/7YTku6pm3ncC9i472LZLf79YuZeMJdRJ42i9mYl3CLSs8zsxqzlfEXLXaen2o3P3MiF/7iQU/c6lV8f/eukwxGRXqqohDu7gs7jnSJ2uQxoA0ZlrR8FvJ1rA3dfChxjZoOA4YQuLVcBr3d3n8CVhJssM2qAtwp7Cd3TMRb3Qo3FLSKJOAV4A3gGyJtt93eabVJESqHYFu5T6LqCLvguTHdvMbOnCd1C7oaOGxwPB67rYtt1wAIzGwgcRxgPvFv7dPdmoCPz7aSlp2Q6pndfoGEBRSQRPwc+SRipZCrwW3dfnmxI6ZNJuBetWpRwJCLSmxWbcJejgr4auMnMZgJPEobwGxrtHzO7GVjg7hdEy+8FxgDPRv9eQrhp83uF7jMNOqZ3Vwu3iCTA3c8ys/MI98CcClxpZvcRZpv8qxczhFUfptkmRaQUih2l5CzCCCXfI0z7O9/MbjOzI62bzcLufithWvjLCEn0XsBR7p656XFcdMyMQYSxuF8G7gIWAAe7+8oi9pm4TAt3W2MbrU2tCUcjIv2Ruze7+y3ufgSwG/AS8DNgnpkVNfC0mR1iZveY2UIzczM7povyk6Jy2Y9tuv+KSi+TcC9ds5SWNl2RFJHuKfqmyaj7xS3ALWa2HaGbyc+AAWa2u7s3dWOf15G/u8ekrOV/Er4Yur3PNBhQM4DKmkraVrXRsrCFATtpAhwRSVQ7oUugsWHUp2IMBZ4jjGB1ZxHb7QzEx7ZNfHKyuOGDhzOwYiDr29fzdtPbjKsbl3RIItILFdXCncPmVtD9mrqViEiSzKzazD5pZg8BrwHvAr4EjCu28cTdH3D3b7n7XUWGscTd34492ovcvqzMjPqacJFV3UpEpLuKblaNxqzO9Pk7GLiXUEH/JW0VZdptf+n2eJszdNehSYciIv2Mmf2MMHnZfEKr9Cejsbl72rPR98qLwCXu/li+gknMmQBw8aEX4+6M32J8TxxORPqgYocFTEsF3SeMnDIy6RBEpP86A3iTMKTqocChuW7FcffJZTr+oiiGmYQk+nPAdDN7r7v/J882PT5nAsCpe/frochFpASKbeFOuoIWEZHSuJkihnEttWjStP/GVj1uZjsAXwFOyrNZj8+ZICJSCsUm3IlW0H1Ny+IWGp9spGJQBVsdsVXS4YhIP+LupyQdQw5PEroq5pTEnAkAS1Yv4fnFzzNk4BAO2vagHjmmiPQtRSXcKa2ge62GfzXw0vEvUXtQrRJuEZEwhGvqZph5YNYDnPKnU/jgDh/kwU8/mHQ4ItILaSy6BFWNiUYpWaBRSkSkd4vG7Z4YW7W9me0FLHf3N83sSmCMu38mKn8uMJcw9vcgQh/u9wMf7NnIu6ZRSkRkcynhTlD16Gh694UtuHuPXR4VESmD/YB/xJYzfa1vIszXUE+YyCyjCvghYcbgNcDzwAfcPb6PVNBskyKyuZRwJ6iqPrRw+3pn/bL1VG1dlXBEIiLd4+7TCXMy5Hv+lKzl7xFmLU69TMK9fO1ymlubqR5Q3cUWIiIb29yJb2QzVAysYODIgYC6lYiIpNWWg7akujIk2YuaUtfFXER6ASXcCases6FbiYiIpI+ZqVuJiGwWJdwJyyTcauEWEUmvzI2Ti1aphVtEiqc+3Akbc/YYRn5yJLUH1iYdioiI5PGVA77CaXufxr6j9006FBHphZRwJ2yrD2r8bRGRtDt+t+OTDkFEejF1KRERERERKSO1cCestaGVlY+spH1dOyNPGJl0OCIiksOKtSt4etHTtHs7H9whdXPziEjKKeFO2Lp563jxYy8ycOuBSrhFRFLq6UVPc8RvjmC3rXfjpS++lHQ4ItLLqEtJwjLTu69fup72lvaEoxERkVwywwJqlBIR6Q4l3AkbOHwgVhUmZ2tZpLG4RUTSKJNwr1i3grXr1yYcjYj0Nkq4E2ZmVI/WWNwiImlWV13HoAGDAM02KSLFU8KdAlWjQ7eS5oVKuEVE0kizTYrI5lDCnQId07svUJcSEZG0UsItIt2lhDsF1MItIpJ+unFSRLpLwwKmwKgTR1F7QC3D9hyWdCgiIpLH5/b+HEfucCQHbXtQ0qGISC+jhDsFat9TS+17apMOQ0REOnHEDkckHYKI9FLqUiIiIiIiUkZKuFOgvbmdZX9exsLrdSOOiEhaNbU08dCch7jrlbuSDkVEehl1KUmB9vXtvHj0iwCM/MRIBtTqbRERSZu3Gt/ig7/9IHXVdRy767FJhyMivYhauFNgwLABVNZWAhqpREQkrTKjlDQ0N7C6ZXXC0YhIb6KEOyU0FreISLrVVNUwdOBQQLNNikhxlHCnRCbhVgu3iEg6xWeb1FjcIlKMxBNuMzvLzOaZ2Tozm2Fm+3dR/lwz+6+ZrTWz+Wb2IzMbFHu+0sy+bWZzozJzzOxCM7Pyv5ru65j8ZoESbhGRtNJskyLSHYnenWdmU4CrgTOAGcC5wINmtrO7L8lR/lPAVcCpwOPATsA0wIHzomLnA2cCJwMvAfsBU4EG4NoyvpzN0tGlZKG6lIiIpFV9TT2ghFtEipN0C/d5wA3uPtXdXyYk3msICXUuBwGPufvv3X2eu/8VuAXYP6vMn9z9vqjMHcBfs8qkjlq4RUTSb/QwtXCLSPESa+E2sypgX+DKzDp3bzezh4ED82z2OPBpM9vf3Z80swnAh4HfZJX5vJnt5O6vmdmewMFsaAHPFUs1UB1bVdOtF7UZtjpyK3a9ZVeG7DKkpw8tIiIFOmH3E9hj5B7sU79P0qGISC+SZJeSEUAlsDhr/WJgl1wbuPvvzWwE8K+oT/YA4BfufkWs2FVALfCqmbVFx/imu/+uk1guAC7u3ssojSE7DmHIjkq2RUTS7ICxB3DA2AOSDkNEepmku5QUxcwmAd8AvgjsA0wGPmJmF8aKfRw4EfhUVOZk4GtmdnInu74SqIs9xpY8eBERERHpl5Js4V4GtAGjstaPAt7Os823gd+4+6+i5RfMbChwvZl9x93bge8DV7n7H2JltiO0Yt+Ua6fu3gx0dJ5OakCTZfcuo3l+M6NOHKXZJkVEUqilrYXp86bzdtPbfGbPzyQdjoj0Eom1cLt7C/A0cHhmnZlVRMv/zrPZEKA9a11bZvMuyqS+Nf+1019j1hdnsXb22qRDERGRHJpbmznyt0dy8t0ns6p5VdLhiEgvkXQSejVwupmdbGa7Aj8HhhKG8cPMbjazK2Pl7wHONLNPmNn2ZnYEodX7Hndvi5X5ppl9xMzGm9mxhBsm7+qpF9VdVWOikUo0+Y2ISCrVVNcwrGoYoNkmRaRwifZbcPdbzWxr4DJgG+BZ4Ch3z9xIOY6NW6svJ4y5fTkwBlhKlGDHypxNSMJ/BowEFgK/jI6RatWjq2l6uknTu4uIpNjomtG89s5rLFy1kJ2G75R0OCLSCyTeUdjdrwOuy/PcpKzlVuDS6JFvf6sIE+icW7ooe4amdxcRSb94wi0iUoiku5RITEeXEk1+IyKSWpreXUSKpYQ7RapHR9O7q0uJiEhqZWabXLRKfbhFpDBKuFNEXUpERNKvo4W7SS3cIlKYxPtwywbD9h7GrrfsyuDtBycdioiI5HHkxCOZOngqu2+9e9KhiEgvoYQ7RapGVjHqE9nzAImISJrstvVu7Lb1bkmHISK9iLqUiIiIiIiUkRLulFn+8HIW/GwBa+dptkkRkTRydx6c/SDTnp3GmvVrkg5HRHoBdSlJmTcue4OGRxvY7dbdGDxefblFRNLGzJhyxxQamhs4YOwB7DJil6RDEpGUUwt3ylSN1ljcIiJplxmpREMDikghlHCnTGZowJaFGotbRCSt6mvqAU1+IyKFUcKdMpnJb9TCLSKSXpptUkSKoYQ7ZTS9u4hI+mVmm1TCLSKFUMKdMh3Tu6tLiYj0ImZ2iJndY2YLzczN7Jgitn2fmbWa2bPljLGUOrqUaLZJESmAEu6U6ZjefUEz7p5wNCIiBRsKPAecVcxGZrYFcDPwt3IEVS7qUiIixdCwgClTPbaa3W7drWO0EhGR3sDdHwAegDBsXhF+AfweaAMKbhVP2gFjD2Da0dOYuNXEpEMRkV5ACXfKVFRXMPLjI5MOQ0Sk7Mzss8AE4NPAtwooXw1Ux1bVlCm0Lo2rG8fJe52c1OFFpJdRlxIREelxZrYjcBXwaXdvLXCzC4CG2OOtMoUnIlJSSrhTqOHxBhb8dAGr/rMq6VBERErOzCoJ3UgudvfXitj0SqAu9hhbhvAK9ve5f+fGZ25kyeolSYYhIr2AEu4UWnTDImZ9aRbL/7I86VBERMqhBtgPuC4anaQVuAjYM1p+f66N3L3Z3RszDyDRVomzHzib0/58Gs8vfj7JMESkF1DCnUIai1tE+rhG4F3AXrHHL4D/Rv+fkVxohdNIJSJSKN00mUIai1tEehszGwbEh+zY3sz2Apa7+5tmdiUwxt0/4+7twItZ2y8B1rn7RuvTLJNwL1q1KOFIRCTtlHCnUHwsbhGRXmI/4B+x5aujf28CTgHqgXE9HFNZabZJESmUEu4U6uhSslAJt4j0Du4+Hcg7ALe7n9LF9pcAl5QypnLr6FKi2SZFpAvqw51CHV1K3m7B2zTbpIhIGqlLiYgUSgl3ClWNqgrvTBu0LFE/bhGRNKqvqQfUpUREuqYuJSlklcYed+7BgOEDGLjVwKTDERGRHHYdsSs3HXMT29Zum3QoIpJySrhTasTRI5IOQUREOrHl4C35zJ6fSToMEekF1KVERERERKSM1MKdUqueXUXDvxoYsvMQtjpiq6TDERGRHB578zH++85/OXS7Q9lhqx2SDkdEUkot3Cm1/P7lzD57NktuWZJ0KCIiksdlj1zGaX8+jX+9+a+kQxGRFEs84Tazs8xsnpmtM7MZZrZ/F+XPNbP/mtlaM5tvZj8ys0FZZcaY2W/N7J2o3Atmtl95X0lpVY3W9O4iImmn6d1FpBCJdikxsymE2cjOAGYA5wIPmtnO7r5J066ZfQq4CjgVeBzYCZgGOHBeVGZL4DHCjGcfApYCOwIryvxySioz26SmdxcRSa/6YRoaUES6lnQf7vOAG9x9KoCZnQF8hJBQX5Wj/EHAY+7++2h5npndArw3VuZ8YL67fza2bm7JIy8zTe8uIpJ+mm1SRAqRWJcSM6sC9gUezqxz9/Zo+cA8mz0O7JvpdmJmE4APA/fHynwMmGlmt5vZEjN7xsxOL8drKKdMl5LWFa20rW1LOBoREclFXUpEpBBJ9uEeAVQCi7PWLwa2ybVB1LJ9EfAvM1sPzAGmu/sVsWITgDOBWcCRwM+Ba83s5HyBmFm1mdVmHkBNN19TyQyoG0DFkPD2qFuJiEg6aXp3ESlE4jdNFsPMJgHfAL4I7ANMBj5iZhfGilUA/3H3b7j7M+5+PXADoZ94PhcADbHHW2UIvyhmRvVodSsREUmzeAu3uyccjYikVZJ9uJcBbcCorPWjgLfzbPNtbdZphgAAHeNJREFU4Dfu/qto+QUzGwpcb2bfibqkLAJeztruFeC4TmK5knDzZkYNKUi6d7phJyqqKhj6rqFJhyIiIjnUD6vn5mNupr6mnnZvp9Iqkw5JRFIosYTb3VvM7GngcOBuADOriJavy7PZEKA9a12mg7NF/z4G7JxVZifgjU5iaQY6mpHNLF/RHrXlpC2TDkFERDoxsHIgJ+15UtJhiEjKJT1KydXATWY2E3iSMCzgUCAzasnNwAJ3vyAqfw9wnpk9QxhGcCKh1fsed88k3j8CHjezbwC3AfsDn48eIiIiIiI9KtGE291vNbOtgcsIN0o+Cxzl7pkbKcexcYv25YQxty8HxhDG2L4H+GZsn0+Z2bGEbiIXEYYEPNfdf1fml1Nya2avYflfljNw+EBGfTK7542IiKTBUwue4vnFz7Pv6H3Za5u9kg5HRFIo6RZu3P068nQhcfdJWcutwKXRo7N93gvcW6IQE9P0nyZmnz2buv+pU8ItIpJSv5j5C2589kYuP+xyJdwiklOvGqWkv9HkNyIi6aexuEWkK0q4Uywz+U3zgmYNNyUiklIdY3E3aSxuEclNCXeKZcbh9mandUVrwtGIiEguauEWka4o4U6xiuoKBgwP3ezVrUREJJ3qa+oBJdwikp8S7pTLtHJrencRkXSKdylp9+ypIkRElHCnnm6cFBFJt1FDR2EYre2tLFuzLOlwRCSFEh8WUDo3/tvj2e7C7Riyy5CkQxERkRwGVg7kluNuYcSQEdRW1yYdjoikkBLulKvdT5W3iEjaTdljStIhiEiKqUuJiIiIiEgZqYU75ZoXNbP0j0vBYezZY5MOR0REcnh+8fM8ueBJdhq+E4dsd0jS4YhIyqiFO+Wa32pm9tmzmXvhXFZMX4G3aQIcEZG0uePlOzj9ntO55YVbkg5FRFJICXeKLb1zKS8e/SIAbQ1tPHfYczwx/gmW3rk04chERCSuY/KbJo3FLSKbUsKdUkvvXMpLx79Ey6KNx99uXtDMS8e/pKRbRCRFNNukiHRGfbhTyNuc2efMhly9RxwwmH3ubEYcPYLGpxppXd5KRXUFVm1UDKqgonrDo3pcNWYWNnXv+L+IiJSOEm4R6YwS7hRa+ehKmt/qZKIbh+b5zax8dCXzvzef5Q8sz1v00LZDIcqxX57yMsvuWkbFoCg5r67YKEHf+7G9qRxSCcBb175Fw6MNmybx0f/HfnXs/2/v3qOkqM/8j7+f7rkPc2GAkYEBhjsEiSB4Q3FBMUeNG5UgxLgxGk1+JiHrJXIS40kkcWOyu8ZAYn6bGONqEt3EJUTNJupPE9kfq2iOqPEC4hW5CAgIDLcZmO7v/lE1PdU9XdDMTNPdw+d1Tp/prnq6ur7T1U8/9a1vVVPUx9t8dj27i5a3W9IuM1IWoWJ8BZES72BKbH8M4nixRTrAIiK9Q3vBvWXPFmLxGNFINMdrJCKpYvEYy9ctZ9PuTTRUNTB96PSj9llVwZ2HUoeRHCqufHQ5fT7og2t1xFvjxFvi3t/WuFfYRjp6tOOtcVybI7YnBns6L8+KOmKbn21m65LwYSuDvzI4cX/zv29m012bQmNPXXcqZUPKAHj35nfZ8MMN3owISQW6lRqT/jyJ8pHlAGy6ZxNbHtjSqde+fYdhyI1DKGv0ltv8fDO7n9udtpc/Uhahz6Q+FNV4m3tbcxux3bGkHQQrMvX+i0iX1VfWE7EIMRdj676tDOwzMNerJCIBS1cv5drHrmVD84bEtMbqRhafu5jZ42dn/fVVcOehkoaSjONGLx6d8XLH3TeO2J5YR3EeKNBdq8OKOwrOgZ8bSM3pNUkFfHu8a3VEKjp6pyvGVdB3Vt9OBb9rdcRb4kTKOmLjrfGOFYpDfH+c+P7AtEDNu2/NPnb+eWdoexo+1wD+lRI/fPRD1n5rbWjs5KcnUzOtBoBNv9jE2ze8nRxgJIr+4x86nr4z+gKw9XdbWff9dR3FecoOQuN1jVRNqgJgz8t72PbIttAdhOqTqikdXArAwZ0Had3Q2umIQPvrqPgvHC7m2Ll8Jwc2HaCkoYTa6bVYVO/fsaYoUsSAigFs2buFe168h2lDph3V3jMRCbd09VLmPDgHlzJWd2PzRuY8OIclc5dkvehWwZ2HaqfXUtpYSuvG1vTjuA1KG0upnV57RMstri2muLY4o9i6WXUwK7PlDrl+CEOuH5JR7OgfjWbkv4xMW/DHW+OJghSg/tJ6KidWhu4glAzs2DGpGFfBgDkDQncQotUdX3ou5rAiw7UF/rkO4i1xaEk+KtD6fiu7n98d2p76ufUwybu/e+Vu1n5zbWjsR377ES8e2PHEDlbNXRUaO+7ecQz8rNdDtuOpHaz5/Jq0BX+kNMKgawbR77x+AOx7ax/v/9v7nYb1tMdWn1xN5YRKANp2t7Hnb3vSDgOyUiNaEU0MBZL0ti7dylvXvpU0BKy0sZRRi0cxYPaAHK6ZHG1LVy8lFo8BcPNfbgaObu+ZyLHOOYfDEXdxYvEYcRcnYhGKIkVc+9i1nYptAIfDMK577DouHHthVneQVXDnIYsaoxaP4rU5r3k9vsFtxK8FRy0aVZC9aBY1opVRopWH36irJlUleo8Pp/6Seuovqc8oduiNQxl641BczCUV5+098qVDOor+/p/oT/mI8tAdhIrxFYnY8tHlNHy+Ibnob+mILTmuYwfBokbxgOKOmAPJiSB4tKFtZxstb7eEtqff+f0S91vebWHDHRtCY0f+YGSi4N776l5emv5SaGzTt5to+laTF7tqLy+e+WL6or8swsDPDKThqgYADnxwgHe+8U7aXv5IaYSqKVXUnuntLMZaYux4YkfnXn7/cVFtEcV9M9tJPNrarySUmsPbryQ0YckEFd3HiHzoPZOjpy3exoHYgURRF3OxpCKvX0U/SqJevt/ZspOte7cmYoJxMRdjTL8xVJdWA94Jt29sf6NTTPvjUxpPSQxVemfHO6xYvyLt68dcjI+N/Bij6kYBsGbbGh5e83Cn12+PvXjcxUwZNAWAVVtXcdfKu9KuQ9zFuWziZZw94uxE7C3Lbkm7zLiLc9Xkq5g7YS4Aq7eu5uo/XB36P7tm6jXMP3k+AG9uf5NzfnVO2nbFXZz5J83n1rNuBWDtzrWM/vHoxGun+uLULzJ3wtykYSSpHI71zetZvm45M5pmdHfzCKWCO08NmD2ACUsmpO89W6Tes55gUa8Xt/1E0XTKhpVRNqwso+XVnlFL7RmZHXUYMHtA0nvonMMdcIliPVrVsU61f1fL5Kcnpy3i4y1xaqbXdKzv0DKG3DgkbS9/vDWeGB8P3pj98tHlaYcBgTfEpl1sX4y27W3hbT+zo90Htx9k8y82h8Y2Xt+YiD+49SCvfuLV0NiGzzcw9q6xXuyOgzzd/+m0vfyRsgj9P9Gf4bcOByDeFmfVvFVpC/5IaYTKiZVJO2hb7t+CFVvaq/0U1RVR3tTxf2vb04ZFjTevfTOjKwkV4o6xZC4Wj3W79yxdIWYYlSWViZite7dyMH4wbWFTFCliRN8RidhVW1ex98DetIVNSbSE04eenohd/t5ytu/fnrYYK44WJwomgEfWPML6Xes7vX7cxTGMBacvSMT++uVfs3rr6rTrEHdxfnTejxJD5376/E95dsOznZbZHn//7PspL/Y+g3esuIPH3nostBh79LJHqSuvA+C25bdx/yv3hxZ5T3/uaYbVDgNg4bKFLHp2Ueg6vPzFlzm+/vjEcm9ZdkvoNvHsVc9ySuMpANz9wt0seGJBaOxfLv8LM4fPBODh1x/mS3/6UmjsI596hL8f+/eJ9+2Kh68Ijf3NJ3+TKLhf/eBVvvbk10JjR/QdkSi439v5HoufWxwaO3ng5ETBvW3fNpasWhIaO7NpZuL+3oN7eWb9M6GxW/ZsSdyPuRjv7XovNHbfwX2J+xGL0BYP/26KuzibdoefYxaUaVxXqeDOYwNmD6D/hf01PvQYYGaJgjBVcV1xYvz54VSMrWDkv47MKLb6pGpOeeOUTtOdc7iDLmk8feWESk567aS0vfzxluSe/uL+xQz/7vD0OwitcaqmdBy1sIhRdVJVp4K//XHwXIF4Szz9uH9f1YlVSbHblm4LbfuASwYkCm4Xd6z+h9WhsXXn1fHRP3008fiZ454hvq/z6ycJXEmo/XwA6Z2Wr1ueUe9Z9ferOWPoGTz+D48n5g2+Y3DoZQRPbDiRlV9YmXh86i9O5Z0d76SNHdtvLK/Pfz3xeN6Sebz6Qfod2cbqRtZfvz7xeMETC3hu43NpY/uW9U0quBc9u4in1j6VNrYkWpJUcD/42oP84Y0/pI0FWHTuIqLm7YAsW7uM377229DYA7EDiYL7tQ9e44l3ngiNbW3r6KDavGczq7aGD907GD+Y9LxdrbtCY4O9pxELH2oXsUhSbFlRGTWlNUQsQjQSJWIR775594ujHUfw6srrGN9/fNq4iEWoKev4HhhcPZhZI2YlzQ8+b3B1x4UNhtUO4/ITLk/EJv768eP7j0/EjqwbyU1n3NQppv1504ZMS8SOqhvFnefdmXZ9o5EokwdOTor9/bzfd2pT+3Obaps61rdmGH+9+q+h/7P2HSrwrg604foNSbHBdSiOFLNiw4rQ9yuooaoho7iuUsGd5yxq+sKWo8rMsJLknbpoeZTKj1SGPCNZyYAShn1jWEaxpYNLmfLXKZktt76E094/LW3BH2+NJ43pjxRHGP1/R6ct+OMt8aTi3MUcfc/pm1z0B2KLByQPaUk68fcwMr3ikBSuTHvF9h3cx54DyZeHci7dIRJP6uHxokgRRZGitAVLVWny0LtBVYNobm1OW9gcV3lcUuzE+omYWacCK2qdl3vmsDOpK68LLWyCLhhzAcNrh6eNTS1YPz3x05zYcGLa9Y1alLKijqOMV594NWcNPyu0GOtb3vF9Of/k+Vw87uLQYqyxujERe8NpN3Dl5CvTrmvUoklF3oJpC7j+1Os7xaQ72X3+yfMTQyUOZ97x85h3/LyMYmeNmMWsEZmdaDV10FTuu+i+jGLH9BvDbWffllHsoKpBfPnkL2cUW1tWy0XjLsootry4nJMGn5RRbFGkKGnnIp3pQ6fTWN3IxuaNaY9EGUZjdSPTh07P6DW7yg71gT9WmVk1sGvXrl1UV1fnenVEJI/E9sXY8ecdhxwK0+6Ep04I3WFubm6mpqYGoMY519yza3lsyIdcvWztMmbeN/Owcb+66FfMHD4zqTj4YO8HAKFFXmlRadjiROQItJ9nASQV3eYfyj3UeRY9lat1CQIRkSMQrYjS7/x+lDaWJg27SWJQOuTIryQkhae998xCNgbDGFI9hEsnXtqpJ66+sp76ynr6V/Snb3lfaspq6FPSh4riChXbIj1o9vjZLJm7pNNnsLG68aid1KwhJSIiR6g3X0lIjkw0EmXxuYuZ8+AcDEvbe7bo3EW6HrdIjs0eP5sLx16Ys1+aVA+3iEgXtF9JKHjtePCuJKRLAh5b8qH3TEQOLxqJMqNpBpdOvJQZTTOO6o6wxnCnkQ/jAkWkMHT1lyZ72xhuMzsTWABMARqAi51zDx0i/gzgn4FxQAXwHvAz59wPj+A18ypXx+KxnPWeiUh29FSu1pASEZFu0JWEEiqBvwH3AEsziN8L3Am87N8/A/iZme11zt2VtbXMovbeMxGRVCq4RUSk25xzjwKPAmkvj5Ym/kXgxcCktWY2G5gOFGTBLSISRmO4RUQk58xsMjAN+O9cr4uISE9TD7eIiOSMmW0ABuB9Hy10zt19iNhSIHiWalVYrIhIPlEPt4iI5NJ0YCpwDXCdmV16iNibgF2BW/hvqouI5BH1cB9Cc3PBXzhARPKU8ovHOfeuf/cVMzsOWAj8R0j494A7Ao+rgA36X4pItvRUflHBnV4VwJAhQ3K9HiLS+1UBqhg9EZKHjCRxzrUCre2PzUy5WkSOlm7lahXc6b0PNAK7DxNXhXdIM5PYQqD25De1J391tS1VePmm4JlZH2BUYNJwM5sEfOicW2dm3wMGO+cu9+O/DKwDXvfjzwRuBH50BC+baa6G3rW9Qe9qT29qC6g9+a4r7el2rlbBnYbzfg1o4+HiApe+2t1Lfrii/a7ak4fUnvzVjbYUdLtTTAWeCjxuH/pxH3AF3o/hDA3Mj+ANERkOtAFvA18DfpbpC2aaq6F3bW/Qu9rTm9oCak++62J7ut1uFdwiItJtzrllQOgFuJ1zV6Q8/jHw4+yulYhIftBVSkREREREskgFd/e0At8mcBJPgVN78pvak796U1t6q972HvWm9vSmtoDak+9y0h7zhsCJiIiIiEg2qIdbRERERCSLVHCLiIiIiGSRCm4RERERkSw65gtuM/uyma01sxYze87MTj5M/CVm9rof/4qZnZ8y38zsO2a2ycz2m9mTZjY6JabOzO43s2Yz22lmv/B/NKKg2mJmTf66v+vPf9vMvm1mJd1tSy7akxJbamYvmZnzf7yjYNtjZh/3X2+/me0ws4cKtT1mNsbMHjazbf7n53/MbGYetmW2mf0/M9setg2ZWZmZ/cSP2WNmvzPvp80ljRxtb1nJ1bloj2UxX+cqt/mxytV52B7LUq7OUnuOTr52zh2zN2Ae3lmqVwIfAe4CdgD1IfHT8H6gYQEwHrgVOAAcH4j5GrATuBD4KPAw8A5QFoh5FHgJOAU4A3gTeKDQ2gKcC/w78DFgBPAJYAtwe6G+N4HYxcCfAAdMKtT2AJ8EPgSuAcb4rz23gNvzBvBHf/5o4CfAXmBgnrXlM8C3gKvDtiHg3/B+afEsYAqwAni6u+9Nb7zlcHvr8Vydq/aQpXydq/cmEKtcnZ/t6fFcncX2HJV83a03stBvwHPAnYHHEbxfLft6SPxvgf9KmfYs8FP/vgGbgBsD82uAFuBT/uPx/hs6NRBzLhAHBhVSW0KWuwB4pxDfm8D084DV/oe5p5J4Lra1Iryfr72qJz4vedCe/v77MT0QU+VPm5UvbUmZ3pRuG/LbdgCYE5g2zo89taffr0K/5Wh7y0quzlV7Qpbb7Xydy7agXJ2v7clKrs5Ge1KmN6XbjuihfH3MDinxD6NNAZ5sn+aci/uPTwt52mnBeN/jgfjhwMCUZe7C20DaY04Ddjrnng8s40m8JH5KgbUlnRq8vfQuy2V7/ENEP8fb493XnXYElpmr9pwIDAbiZvaif/jvUTM7vkDbsx1YA1xuZpVmVgT8H+ADYGUetSUTU4DilNd9Ha8H5UiW0+v1plwNvStfK1cDytVHJVdnsT2Z6JF8fcwW3Hh7YFG8Q2pBW/A2pnQGHiZ+YGDaoWI+CM50zrXhJb2w1z2cXLUliZmNAr4C/Ozwq3xIOWmPmRlwL96e7/P0nFy9PyP8vwuBfwIuwDv0tszM6jJc93Ry0h7ndSvMAiYDu/F6VG4AznXO7TiyJiRkoy2ZGAgccM7t7OZyjgW9KVdD78rXytXK1Z1ispSrocDz9bFccEsPMrPBwGPAfzrnfp7r9emir+Ad9vperlekh7R/vr/rnPudc24l3rg3B1ySu9XqGv9L9id4RdB04GTgIeAPZtaQy3UTKSS9IF8rV+cx5er0juWCexsQA1LPMj0O2BzynM2Hid8cmHaomPrgTP9wS90hXvdwctUWAMxsEPAU8AzwhYzXOlyu2nMW3uGhVjNrA97ypz9vZvdlvPad5ao9m/y/q9pnOuda8U5uGZrJiofI5ftzAd44waedcy84574E7Ac+e0Qt6JCNtmRiM1BiZrXdXM6xoDflauhd+Vq5Wrk6XUw2cjUUeL4+Zgtu59wBvLFEZ7dPM7OI/3hFyNNWBON95wTi38X75weXWY033q89ZgVQa2ZTAss4C++9eK7A2tLeU7LMf/0r/fFU3ZLD9vwjcAIwyb+1XzpoHnBz11qT0/asxDube2wgphjvxJD3utQYctqeCv9v6jYWp4u5LEttycRK4GDK647F+3I9kuX0er0pV0PvytfK1YBy9VHJ1dAL8nV3zhYt9Bveh7MFb49rPN5Yth3Acf78XwLfC8RP8//pX8U7Q3Uh6S+XswPvkksT8Q6jpLvU1At4h1lOx7t8Tk9cFvCotgXvJI838U4kGIw3lmkg3bzsTy7fm5R1aKJnLzWVi21tEd7Z7x/DS+Z3440761to7cEbv7cN+B3el+0Y4F/95ZyQZ22po6MQcP5rTCLw2cC7zNR7wEy8k3KeAZ7p7rbWG285/Pz0eK7O4ecnK/k6V+9Nyjo0oVydN+0hS7k6i+05Kvn6qCbNfLwB8/1/Yiter8UpgXnLgHtT4i/BO/u2FXgVOD9lvgHfwdsDbMFLbmNSYuqAB/BOJtgF3AP0KbS2AFf4G2enW6G+NynxTfRQEs/htlYM3I6XuJuBJ4AJBdyeqXhnmG/327MCOC8P2xL22VgYiCnDG+f4Id71aZfSAzurvfWWo+0tK7k6F+05xDbpCq0taV6/CeXqfGtPVnJ1ltoT9tlYGIjpdr42f0EiIiIiIpIFx+wYbhERERGRo0EFt4iIiIhIFqngFhERERHJIhXcIiIiIiJZpIJbRERERCSLVHCLiIiIiGSRCm4RERERkSxSwS0iIiIikkUquCVvmdkyM1uU6/UAMLOFZvZSrtdDRCQfKV+LHJoKbpHM3A6cneuVCGNmM8zMmVltrtdFRCTHlK8l76jglmOamZVkEuec2+Oc257t9UmV6fqJiPR2ytdSyFRwS8Ews1Izu93MNprZXjN7zsxmBOb3M7P/8OfvM7NXzOzSlGUsM7M7zWyRmW0DHg/0NpxtZs/7z33GzMYGnpd0iNLM7jWzh8zsRjPbZGbbzewnZlYciGkwsz+a2X4ze9fMPm1ma83sukO0sX25N5vZ+8Aaf/pn/HXbbWabzewBM6v35zUBT/mL2OG35V5/XsTMbvJff7+Z/c3M5nT1PRARyYTytfK1JFPBLYXkTuA04FPAR4H/BB4zs9H+/DJgJfBx4HjgLuBXZnZyynI+CxwATgeuCUz/LvBVYCrQBtxzmPWZCYz0/34WuMK/tfslMAiYAXwS+AJQn0E7zwbGAucAF/jTioFvAicAFwFNwL3+vPX+8vGf1wBc6z++Cbgcr50TgB8Cvzazv8tgPUREukr5WvlagpxzuumWlzdgGbDIvz8UL6kOSol5ErjtEMv4L+D2lGW+kBIzA3DA2YFp5/vTyvzHC4GXAvPvBdYC0cC0B4Hf+PfH+c+fGpg/yp923SHW915gM1BymP/NVH9ZfVLaUBuIKQX2AqelPPdu4IFcv7+66aZb77kpXx/yf6N8rRtFiBSGiUAUeMPMgtNLge0AZhYFvgHMBQYDJf78fSnLWhnyGi8H7m/y/9YD60LiX3POxVKeM9G/PxbvC+eF9pnOubfMbEfIsoJecc4dCE4wsyl4XyInAH3pODo1FFgVspxRQAXwRMr/rAR4MYP1EBHpCuVr5WtJoYJbCkUfIAZM8f8G7fH/LsA7NHcd8Apeb8EivIQVtDfkNQ4G7jv/76GGXR1MeewOE5+ppPUzs0rgcf92GbAVL3E/Tue2BfXx/34c2Jgyr7UH1lNEJB3la+VrSaGCWwrFi3g9JvXOueUhMacDDzvnfg3eCSjAGMJ7FLJpDd7nazJ+D42ZjcLr7ThS44B+wNedc+v9ZU1NiWnvYYkGpq3CS9RDnXP/3YXXFRHpCuVr5WtJoYJbCoJz7g0zux/4pZl9FS+hD8A7YeVl59wfgTeBOWY2DdgB3AAcRw4SuHPudTN7ErjLzL6I17vyA2A/Hb0xmVqHl6C/YmY/xTvB6JspMe/5y73AzP4E7HfO7Taz24Ef+l9m/wPU4H3RNTvn7uti80REQilfK19LZ7pKiRSSK/HOJP8BXo/EQ8BJdIzZ+ye8MXiP451ss9mPyZXLgS3A/wd+D/wc2A20HMlCnHNb8c6mvwTvy+jrwI0pMRuBW4Dv+695pz/rm8CteGe/rwYewztk+W4X2iMikinla+VrCTDnjnTnTUS6wswa8S4JNcs59+dcr4+IiKSnfC09TQW3SJaY2Vl4J8K8gnet1X/BOxt/jHMu9QQeERHJEeVryTaN4RbJnmLgNmAE3qHJZ4DLlLxFRPKO8rVklXq4RURERESySCdNioiIiIhkkQpuEREREZEsUsEtIiIiIpJFKrhFRERERLJIBbeIiIiISBap4BYRERERySIV3CIiIiIiWaSCW0REREQki1Rwi4iIiIhk0f8CC3ajpCvOWK8AAAAASUVORK5CYII=\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "minimum avg mae over learning rates = 0.8467357758208358 at learning rate = 0.01 and mse = 1.321252338789283\n",
            "minimum avg mse over learning rates = 1.317287691183217 at learning rate = 0.001 and mae = 0.8499268625805789\n"
          ]
        }
      ],
      "source": [
        "avg_mae_list = [0.9583892102254641,0.8499268625805789,0.8467357758208358]\n",
        "avg_mse_list = [1.7321942511073358,1.317287691183217,1.321252338789283]\n",
        "learning_rate_list = [0.0001, 0.001, 0.01]\n",
        "print(f'avg mae over learning rate = [0.0001, 0.001, 0.01]: {avg_mae_list}')\n",
        "print(f'avg mse over learning rate = [0.0001, 0.001, 0.01]: {avg_mse_list}')\n",
        "    \n",
        "plt.rcParams.update({'figure.figsize':(7.5,3.5), 'figure.dpi':100})\n",
        "fig, axes = plt.subplots(1, 2)\n",
        "axes[0].plot(learning_rate_list, avg_mae_list, 'm--o')\n",
        "axes[0].set_title('Avg MAE over varying learning rate')\n",
        "axes[0].set_xlabel('learning rate')\n",
        "axes[0].set_ylabel('MAE')\n",
        "\n",
        "axes[1].plot(learning_rate_list, avg_mse_list, 'g--o')\n",
        "axes[1].set_title('Avg MSE over varying learning rate')\n",
        "axes[1].set_xlabel('learning rate')\n",
        "axes[1].set_ylabel('MSE')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "index = 0\n",
        "min = 100\n",
        "min_index = 0\n",
        "for i in avg_mae_list:\n",
        "  if i<min:\n",
        "    min = i\n",
        "    min_index = index\n",
        "  index+=1\n",
        "print(f'minimum avg mae over learning rates = {min} at learning rate = {learning_rate_list[min_index]} and mse = {avg_mse_list[min_index]}')\n",
        "\n",
        "index = 0\n",
        "min = 100\n",
        "min_index = 0\n",
        "for i in avg_mse_list:\n",
        "  if i<min:\n",
        "    min = i\n",
        "    min_index = index\n",
        "  index+=1\n",
        "print(f'minimum avg mse over learning rates = {min} at learning rate = {learning_rate_list[min_index]} and mae = {avg_mae_list[min_index]}')"
      ],
      "id": "bvkY1zg4HDVI"
    },
    {
      "cell_type": "markdown",
      "source": [
        "learing rate = 0.001"
      ],
      "metadata": {
        "id": "1PRV2-_ucaEU"
      },
      "id": "1PRV2-_ucaEU"
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    print(sys.path[0])\n",
        "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
        "    word_latent_dim = 300\n",
        "    latent_dim = 15\n",
        "    max_doc_length = 300\n",
        "    num_filters = 50\n",
        "    window_size = 5\n",
        "    v_dim = 50\n",
        "    learning_rate_list = [0.0001, 0.001, 0.01, 0.1, 1]\n",
        "    learning_rate = 0.001\n",
        "    lambda_1 = 0.01 #normally we set from [0.05, 0.01, 0.005, 0.001]\n",
        "    drop_out = 0.6\n",
        "    batch_size = 300\n",
        "    epochs = 240\n",
        "    avg_mae_list = []\n",
        "    avg_mse_list = []\n",
        "    \n",
        "    # loading data\n",
        "    firTime = time()\n",
        "    dataSet = Dataset(max_doc_length, sys.path[0], \"dataPreprocessingWordDict.out\")\n",
        "    word_dict, user_reviews, item_reviews, train, valRatings, testRatings = dataSet.word_id_dict, dataSet.userReview_dict, dataSet.itemReview_dict, dataSet.trainMtrx, dataSet.valRatings, dataSet.testRatings\n",
        "    secTime = time()\n",
        "\n",
        "    num_users, num_items = train.shape\n",
        "    print(\"load data: %.3fs\" % (secTime - firTime))\n",
        "    print(num_users, num_items)\n",
        "    print(latent_dim)\n",
        "\n",
        "    #load word embeddings\n",
        "    word_embedding_mtrx = ini_word_embed(len(word_dict), word_latent_dim)\n",
        "    # word_embedding_mtrx = word2vec_word_embed(len(word_dict), word_latent_dim,\n",
        "    #                                           \"Directory of pretrained WordEmbedding.out\",\n",
        "    #                                           word_dict)\n",
        "\n",
        "    print( \"shape\", word_embedding_mtrx.shape)\n",
        "\n",
        "    # get train instances\n",
        "    user_input, item_input, rateings = get_train_instance(train)\n",
        "    print (len(user_input), len(item_input), len(rateings))\n",
        "\n",
        "    # get test/val instances\n",
        "    user_vals, item_vals, user_input_val, item_input_val, rating_input_val = get_test_list(200, valRatings, user_reviews, item_reviews)\n",
        "    user_tests, item_tests, user_input_test, item_input_test, rating_input_test = get_test_list(200, testRatings, user_reviews, item_reviews)\n",
        "\n",
        "    #train & eval model\n",
        "    train_model()"
      ],
      "metadata": {
        "id": "PosS8PqecjKt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07520ad5-fa2a-4c64-95e8-8825d5aa6189"
      },
      "id": "PosS8PqecjKt",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "wordId_dict finished\n",
            "load reviews finished\n",
            "load data: 8.157s\n",
            "506 2581\n",
            "15\n",
            "shape (19930, 300)\n",
            "18880 18880 18880\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/training/rmsprop.py:188: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "epoch0 train time: 18.658s test time: 0.920  loss = 69.261 val_mse = 1.571 mse = 1.523 mae = 1.000\n",
            "epoch1 train time: 5.709s test time: 0.709  loss = 60.372 val_mse = 1.537 mse = 1.482 mae = 0.923\n",
            "epoch2 train time: 5.734s test time: 0.714  loss = 44.389 val_mse = 1.513 mse = 1.464 mae = 0.919\n",
            "epoch3 train time: 5.756s test time: 0.719  loss = 28.626 val_mse = 1.490 mse = 1.447 mae = 0.913\n",
            "epoch4 train time: 5.822s test time: 0.716  loss = 17.675 val_mse = 1.467 mse = 1.430 mae = 0.906\n",
            "epoch5 train time: 5.766s test time: 0.715  loss = 10.447 val_mse = 1.443 mse = 1.413 mae = 0.900\n",
            "epoch6 train time: 5.807s test time: 0.723  loss = 5.946 val_mse = 1.419 mse = 1.396 mae = 0.893\n",
            "epoch7 train time: 5.837s test time: 0.733  loss = 3.349 val_mse = 1.395 mse = 1.380 mae = 0.886\n",
            "epoch8 train time: 6.019s test time: 0.727  loss = 2.004 val_mse = 1.372 mse = 1.365 mae = 0.879\n",
            "epoch9 train time: 5.864s test time: 0.725  loss = 1.412 val_mse = 1.350 mse = 1.351 mae = 0.872\n",
            "epoch10 train time: 5.905s test time: 0.722  loss = 1.208 val_mse = 1.327 mse = 1.339 mae = 0.865\n",
            "epoch11 train time: 5.898s test time: 0.734  loss = 1.146 val_mse = 1.307 mse = 1.328 mae = 0.859\n",
            "epoch12 train time: 5.923s test time: 0.819  loss = 1.107 val_mse = 1.286 mse = 1.317 mae = 0.851\n",
            "epoch13 train time: 5.981s test time: 0.749  loss = 1.069 val_mse = 1.270 mse = 1.310 mae = 0.845\n",
            "epoch14 train time: 5.972s test time: 0.736  loss = 1.035 val_mse = 1.256 mse = 1.305 mae = 0.840\n",
            "epoch15 train time: 5.975s test time: 0.745  loss = 1.003 val_mse = 1.227 mse = 1.290 mae = 0.852\n",
            "epoch16 train time: 5.992s test time: 0.746  loss = 0.982 val_mse = 1.258 mse = 1.314 mae = 0.830\n",
            "epoch17 train time: 6.008s test time: 0.747  loss = 0.958 val_mse = 1.212 mse = 1.288 mae = 0.845\n",
            "epoch18 train time: 6.036s test time: 0.742  loss = 0.938 val_mse = 1.242 mse = 1.312 mae = 0.829\n",
            "epoch19 train time: 6.036s test time: 0.751  loss = 0.922 val_mse = 1.208 mse = 1.290 mae = 0.842\n",
            "epoch20 train time: 6.046s test time: 0.744  loss = 0.916 val_mse = 1.227 mse = 1.307 mae = 0.832\n",
            "epoch21 train time: 6.063s test time: 0.747  loss = 0.907 val_mse = 1.197 mse = 1.287 mae = 0.841\n",
            "epoch22 train time: 6.089s test time: 0.751  loss = 0.895 val_mse = 1.219 mse = 1.303 mae = 0.833\n",
            "epoch23 train time: 6.232s test time: 0.752  loss = 0.887 val_mse = 1.187 mse = 1.292 mae = 0.851\n",
            "epoch24 train time: 6.113s test time: 0.759  loss = 0.883 val_mse = 1.188 mse = 1.297 mae = 0.849\n",
            "epoch25 train time: 6.104s test time: 0.762  loss = 0.873 val_mse = 1.192 mse = 1.296 mae = 0.838\n",
            "epoch26 train time: 6.118s test time: 0.751  loss = 0.868 val_mse = 1.200 mse = 1.303 mae = 0.838\n",
            "epoch27 train time: 6.121s test time: 0.760  loss = 0.864 val_mse = 1.193 mse = 1.299 mae = 0.833\n",
            "epoch28 train time: 6.140s test time: 0.774  loss = 0.859 val_mse = 1.199 mse = 1.299 mae = 0.837\n",
            "epoch29 train time: 6.145s test time: 0.766  loss = 0.855 val_mse = 1.183 mse = 1.296 mae = 0.842\n",
            "epoch30 train time: 6.137s test time: 0.755  loss = 0.850 val_mse = 1.235 mse = 1.331 mae = 0.831\n",
            "epoch31 train time: 6.148s test time: 0.751  loss = 0.845 val_mse = 1.198 mse = 1.300 mae = 0.836\n",
            "epoch32 train time: 6.152s test time: 0.754  loss = 0.847 val_mse = 1.214 mse = 1.321 mae = 0.832\n",
            "epoch33 train time: 6.157s test time: 0.759  loss = 0.839 val_mse = 1.203 mse = 1.306 mae = 0.832\n",
            "epoch34 train time: 6.160s test time: 0.757  loss = 0.837 val_mse = 1.207 mse = 1.311 mae = 0.832\n",
            "epoch35 train time: 6.174s test time: 0.760  loss = 0.836 val_mse = 1.170 mse = 1.290 mae = 0.844\n",
            "epoch36 train time: 6.174s test time: 0.757  loss = 0.834 val_mse = 1.201 mse = 1.306 mae = 0.835\n",
            "epoch37 train time: 6.188s test time: 0.753  loss = 0.834 val_mse = 1.194 mse = 1.298 mae = 0.836\n",
            "epoch38 train time: 6.311s test time: 0.758  loss = 0.831 val_mse = 1.207 mse = 1.314 mae = 0.830\n",
            "epoch39 train time: 6.189s test time: 0.761  loss = 0.826 val_mse = 1.196 mse = 1.298 mae = 0.833\n",
            "epoch40 train time: 6.183s test time: 0.764  loss = 0.829 val_mse = 1.206 mse = 1.311 mae = 0.833\n",
            "epoch41 train time: 6.197s test time: 0.760  loss = 0.827 val_mse = 1.191 mse = 1.297 mae = 0.831\n",
            "epoch42 train time: 6.201s test time: 0.762  loss = 0.823 val_mse = 1.204 mse = 1.310 mae = 0.834\n",
            "epoch43 train time: 6.195s test time: 0.766  loss = 0.821 val_mse = 1.168 mse = 1.287 mae = 0.846\n",
            "epoch44 train time: 6.199s test time: 0.767  loss = 0.821 val_mse = 1.211 mse = 1.314 mae = 0.832\n",
            "epoch45 train time: 6.193s test time: 0.762  loss = 0.817 val_mse = 1.179 mse = 1.290 mae = 0.840\n",
            "epoch46 train time: 6.201s test time: 0.764  loss = 0.821 val_mse = 1.207 mse = 1.315 mae = 0.835\n",
            "epoch47 train time: 6.209s test time: 0.765  loss = 0.813 val_mse = 1.192 mse = 1.297 mae = 0.833\n",
            "epoch48 train time: 6.208s test time: 0.771  loss = 0.819 val_mse = 1.199 mse = 1.313 mae = 0.836\n",
            "epoch49 train time: 6.199s test time: 0.768  loss = 0.811 val_mse = 1.194 mse = 1.295 mae = 0.833\n",
            "epoch50 train time: 6.218s test time: 0.766  loss = 0.814 val_mse = 1.193 mse = 1.308 mae = 0.835\n",
            "epoch51 train time: 6.213s test time: 0.765  loss = 0.810 val_mse = 1.192 mse = 1.295 mae = 0.834\n",
            "epoch52 train time: 6.211s test time: 0.764  loss = 0.811 val_mse = 1.197 mse = 1.308 mae = 0.836\n",
            "epoch53 train time: 6.222s test time: 0.772  loss = 0.806 val_mse = 1.184 mse = 1.294 mae = 0.834\n",
            "epoch54 train time: 6.225s test time: 0.770  loss = 0.809 val_mse = 1.198 mse = 1.303 mae = 0.835\n",
            "epoch55 train time: 6.245s test time: 0.762  loss = 0.802 val_mse = 1.187 mse = 1.299 mae = 0.836\n",
            "epoch56 train time: 6.228s test time: 0.765  loss = 0.806 val_mse = 1.194 mse = 1.304 mae = 0.834\n",
            "epoch57 train time: 6.239s test time: 0.762  loss = 0.803 val_mse = 1.186 mse = 1.293 mae = 0.836\n",
            "epoch58 train time: 6.245s test time: 0.770  loss = 0.805 val_mse = 1.192 mse = 1.310 mae = 0.840\n",
            "epoch59 train time: 6.221s test time: 0.769  loss = 0.801 val_mse = 1.188 mse = 1.297 mae = 0.837\n",
            "epoch60 train time: 6.239s test time: 0.765  loss = 0.803 val_mse = 1.194 mse = 1.308 mae = 0.836\n",
            "epoch61 train time: 6.233s test time: 0.766  loss = 0.798 val_mse = 1.184 mse = 1.297 mae = 0.836\n",
            "epoch62 train time: 6.240s test time: 0.763  loss = 0.802 val_mse = 1.199 mse = 1.311 mae = 0.836\n",
            "epoch63 train time: 6.227s test time: 0.764  loss = 0.795 val_mse = 1.183 mse = 1.291 mae = 0.838\n",
            "epoch64 train time: 6.258s test time: 0.768  loss = 0.798 val_mse = 1.187 mse = 1.307 mae = 0.841\n",
            "epoch65 train time: 6.243s test time: 0.770  loss = 0.795 val_mse = 1.183 mse = 1.288 mae = 0.834\n",
            "epoch66 train time: 6.244s test time: 0.763  loss = 0.798 val_mse = 1.188 mse = 1.312 mae = 0.841\n",
            "epoch67 train time: 6.234s test time: 0.763  loss = 0.794 val_mse = 1.185 mse = 1.300 mae = 0.835\n",
            "epoch68 train time: 6.233s test time: 0.773  loss = 0.797 val_mse = 1.191 mse = 1.308 mae = 0.840\n",
            "epoch69 train time: 6.228s test time: 0.768  loss = 0.793 val_mse = 1.186 mse = 1.296 mae = 0.835\n",
            "epoch70 train time: 6.240s test time: 0.766  loss = 0.796 val_mse = 1.190 mse = 1.306 mae = 0.839\n",
            "epoch71 train time: 6.250s test time: 0.775  loss = 0.791 val_mse = 1.186 mse = 1.294 mae = 0.832\n",
            "epoch72 train time: 6.254s test time: 0.764  loss = 0.796 val_mse = 1.193 mse = 1.309 mae = 0.837\n",
            "epoch73 train time: 6.239s test time: 0.769  loss = 0.792 val_mse = 1.180 mse = 1.292 mae = 0.839\n",
            "epoch74 train time: 6.230s test time: 0.773  loss = 0.792 val_mse = 1.188 mse = 1.310 mae = 0.840\n",
            "epoch75 train time: 6.235s test time: 0.766  loss = 0.791 val_mse = 1.188 mse = 1.297 mae = 0.837\n",
            "epoch76 train time: 6.248s test time: 0.771  loss = 0.792 val_mse = 1.188 mse = 1.314 mae = 0.838\n",
            "epoch77 train time: 6.233s test time: 0.769  loss = 0.791 val_mse = 1.183 mse = 1.287 mae = 0.839\n",
            "epoch78 train time: 6.239s test time: 0.771  loss = 0.793 val_mse = 1.189 mse = 1.309 mae = 0.839\n",
            "epoch79 train time: 6.244s test time: 0.766  loss = 0.787 val_mse = 1.183 mse = 1.295 mae = 0.836\n",
            "epoch80 train time: 6.235s test time: 0.764  loss = 0.791 val_mse = 1.188 mse = 1.311 mae = 0.838\n",
            "epoch81 train time: 6.243s test time: 0.768  loss = 0.789 val_mse = 1.182 mse = 1.296 mae = 0.839\n",
            "epoch82 train time: 6.251s test time: 0.768  loss = 0.791 val_mse = 1.190 mse = 1.310 mae = 0.837\n",
            "epoch83 train time: 6.259s test time: 0.771  loss = 0.787 val_mse = 1.186 mse = 1.287 mae = 0.834\n",
            "epoch84 train time: 6.263s test time: 0.770  loss = 0.790 val_mse = 1.192 mse = 1.307 mae = 0.841\n",
            "epoch85 train time: 6.244s test time: 0.765  loss = 0.787 val_mse = 1.188 mse = 1.292 mae = 0.836\n",
            "epoch86 train time: 6.250s test time: 0.774  loss = 0.790 val_mse = 1.178 mse = 1.304 mae = 0.841\n",
            "epoch87 train time: 6.255s test time: 0.765  loss = 0.785 val_mse = 1.188 mse = 1.294 mae = 0.835\n",
            "epoch88 train time: 6.243s test time: 0.772  loss = 0.789 val_mse = 1.183 mse = 1.307 mae = 0.837\n",
            "epoch89 train time: 6.241s test time: 0.768  loss = 0.787 val_mse = 1.187 mse = 1.292 mae = 0.835\n",
            "epoch90 train time: 6.240s test time: 0.776  loss = 0.787 val_mse = 1.191 mse = 1.307 mae = 0.840\n",
            "epoch91 train time: 6.243s test time: 0.770  loss = 0.783 val_mse = 1.181 mse = 1.297 mae = 0.836\n",
            "epoch92 train time: 6.241s test time: 0.764  loss = 0.787 val_mse = 1.187 mse = 1.307 mae = 0.840\n",
            "epoch93 train time: 6.247s test time: 0.772  loss = 0.784 val_mse = 1.182 mse = 1.293 mae = 0.837\n",
            "epoch94 train time: 6.244s test time: 0.766  loss = 0.786 val_mse = 1.183 mse = 1.305 mae = 0.836\n",
            "epoch95 train time: 6.246s test time: 0.773  loss = 0.783 val_mse = 1.186 mse = 1.294 mae = 0.836\n",
            "epoch96 train time: 6.252s test time: 0.773  loss = 0.787 val_mse = 1.185 mse = 1.302 mae = 0.839\n",
            "epoch97 train time: 6.247s test time: 0.770  loss = 0.781 val_mse = 1.184 mse = 1.291 mae = 0.835\n",
            "epoch98 train time: 6.241s test time: 0.768  loss = 0.786 val_mse = 1.185 mse = 1.311 mae = 0.841\n",
            "epoch99 train time: 6.244s test time: 0.774  loss = 0.782 val_mse = 1.186 mse = 1.292 mae = 0.835\n",
            "epoch100 train time: 6.249s test time: 0.770  loss = 0.784 val_mse = 1.188 mse = 1.308 mae = 0.838\n",
            "epoch101 train time: 6.253s test time: 0.767  loss = 0.780 val_mse = 1.183 mse = 1.294 mae = 0.834\n",
            "epoch102 train time: 6.247s test time: 0.771  loss = 0.782 val_mse = 1.189 mse = 1.310 mae = 0.841\n",
            "epoch103 train time: 6.252s test time: 0.778  loss = 0.781 val_mse = 1.182 mse = 1.295 mae = 0.834\n",
            "epoch104 train time: 6.273s test time: 0.778  loss = 0.782 val_mse = 1.191 mse = 1.305 mae = 0.838\n",
            "epoch105 train time: 6.265s test time: 0.772  loss = 0.778 val_mse = 1.183 mse = 1.292 mae = 0.833\n",
            "epoch106 train time: 6.237s test time: 0.768  loss = 0.782 val_mse = 1.179 mse = 1.305 mae = 0.838\n",
            "epoch107 train time: 6.254s test time: 0.772  loss = 0.777 val_mse = 1.183 mse = 1.293 mae = 0.835\n",
            "epoch108 train time: 6.246s test time: 0.773  loss = 0.781 val_mse = 1.183 mse = 1.306 mae = 0.840\n",
            "epoch109 train time: 6.261s test time: 0.769  loss = 0.778 val_mse = 1.178 mse = 1.293 mae = 0.833\n",
            "epoch110 train time: 6.258s test time: 0.767  loss = 0.781 val_mse = 1.190 mse = 1.307 mae = 0.837\n",
            "epoch111 train time: 6.251s test time: 0.768  loss = 0.778 val_mse = 1.178 mse = 1.294 mae = 0.836\n",
            "epoch112 train time: 6.248s test time: 0.782  loss = 0.779 val_mse = 1.181 mse = 1.306 mae = 0.839\n",
            "epoch113 train time: 6.249s test time: 0.776  loss = 0.777 val_mse = 1.185 mse = 1.295 mae = 0.834\n",
            "epoch114 train time: 6.265s test time: 0.775  loss = 0.779 val_mse = 1.186 mse = 1.308 mae = 0.839\n",
            "epoch115 train time: 6.278s test time: 0.772  loss = 0.777 val_mse = 1.184 mse = 1.297 mae = 0.836\n",
            "epoch116 train time: 6.236s test time: 0.771  loss = 0.779 val_mse = 1.174 mse = 1.299 mae = 0.837\n",
            "epoch117 train time: 6.254s test time: 0.770  loss = 0.776 val_mse = 1.181 mse = 1.294 mae = 0.834\n",
            "epoch118 train time: 6.245s test time: 0.764  loss = 0.776 val_mse = 1.182 mse = 1.310 mae = 0.839\n",
            "epoch119 train time: 6.247s test time: 0.765  loss = 0.775 val_mse = 1.176 mse = 1.293 mae = 0.833\n",
            "epoch120 train time: 6.250s test time: 0.766  loss = 0.775 val_mse = 1.187 mse = 1.308 mae = 0.838\n",
            "epoch121 train time: 6.253s test time: 0.773  loss = 0.778 val_mse = 1.182 mse = 1.294 mae = 0.835\n",
            "epoch122 train time: 6.237s test time: 0.767  loss = 0.779 val_mse = 1.186 mse = 1.307 mae = 0.836\n",
            "epoch123 train time: 6.239s test time: 0.770  loss = 0.774 val_mse = 1.179 mse = 1.293 mae = 0.836\n",
            "epoch124 train time: 6.247s test time: 0.761  loss = 0.777 val_mse = 1.175 mse = 1.302 mae = 0.835\n",
            "epoch125 train time: 6.235s test time: 0.767  loss = 0.774 val_mse = 1.176 mse = 1.292 mae = 0.836\n",
            "epoch126 train time: 6.256s test time: 0.768  loss = 0.776 val_mse = 1.184 mse = 1.311 mae = 0.838\n",
            "epoch127 train time: 6.228s test time: 0.767  loss = 0.774 val_mse = 1.178 mse = 1.294 mae = 0.837\n",
            "epoch128 train time: 6.255s test time: 0.765  loss = 0.776 val_mse = 1.181 mse = 1.304 mae = 0.834\n",
            "epoch129 train time: 6.250s test time: 0.771  loss = 0.773 val_mse = 1.181 mse = 1.296 mae = 0.834\n",
            "epoch130 train time: 6.253s test time: 0.764  loss = 0.776 val_mse = 1.188 mse = 1.307 mae = 0.838\n",
            "epoch131 train time: 6.249s test time: 0.768  loss = 0.774 val_mse = 1.181 mse = 1.293 mae = 0.834\n",
            "epoch132 train time: 6.240s test time: 0.769  loss = 0.775 val_mse = 1.174 mse = 1.302 mae = 0.836\n",
            "epoch133 train time: 6.240s test time: 0.768  loss = 0.772 val_mse = 1.177 mse = 1.290 mae = 0.834\n",
            "epoch134 train time: 6.234s test time: 0.767  loss = 0.776 val_mse = 1.183 mse = 1.306 mae = 0.836\n",
            "epoch135 train time: 6.267s test time: 0.769  loss = 0.775 val_mse = 1.181 mse = 1.293 mae = 0.833\n",
            "epoch136 train time: 6.248s test time: 0.773  loss = 0.776 val_mse = 1.181 mse = 1.303 mae = 0.837\n",
            "epoch137 train time: 6.255s test time: 0.772  loss = 0.772 val_mse = 1.180 mse = 1.294 mae = 0.835\n",
            "epoch138 train time: 6.260s test time: 0.773  loss = 0.775 val_mse = 1.181 mse = 1.304 mae = 0.835\n",
            "epoch139 train time: 6.255s test time: 0.773  loss = 0.773 val_mse = 1.180 mse = 1.296 mae = 0.835\n",
            "epoch140 train time: 6.238s test time: 0.767  loss = 0.774 val_mse = 1.174 mse = 1.304 mae = 0.836\n",
            "epoch141 train time: 6.254s test time: 0.771  loss = 0.772 val_mse = 1.174 mse = 1.292 mae = 0.834\n",
            "epoch142 train time: 6.254s test time: 0.770  loss = 0.775 val_mse = 1.179 mse = 1.305 mae = 0.835\n",
            "epoch143 train time: 6.259s test time: 0.770  loss = 0.772 val_mse = 1.182 mse = 1.294 mae = 0.834\n",
            "epoch144 train time: 6.248s test time: 0.768  loss = 0.776 val_mse = 1.185 mse = 1.308 mae = 0.837\n",
            "epoch145 train time: 6.259s test time: 0.768  loss = 0.772 val_mse = 1.178 mse = 1.292 mae = 0.833\n",
            "epoch146 train time: 6.249s test time: 0.772  loss = 0.774 val_mse = 1.177 mse = 1.307 mae = 0.838\n",
            "epoch147 train time: 6.250s test time: 0.765  loss = 0.773 val_mse = 1.173 mse = 1.285 mae = 0.833\n",
            "epoch148 train time: 6.245s test time: 0.784  loss = 0.775 val_mse = 1.180 mse = 1.303 mae = 0.834\n",
            "epoch149 train time: 6.265s test time: 0.770  loss = 0.773 val_mse = 1.179 mse = 1.293 mae = 0.833\n",
            "epoch150 train time: 6.258s test time: 0.770  loss = 0.774 val_mse = 1.178 mse = 1.303 mae = 0.836\n",
            "epoch151 train time: 6.246s test time: 0.770  loss = 0.772 val_mse = 1.177 mse = 1.291 mae = 0.835\n",
            "epoch152 train time: 6.250s test time: 0.767  loss = 0.775 val_mse = 1.173 mse = 1.304 mae = 0.835\n",
            "epoch153 train time: 6.241s test time: 0.763  loss = 0.772 val_mse = 1.178 mse = 1.291 mae = 0.832\n",
            "epoch154 train time: 6.241s test time: 0.775  loss = 0.774 val_mse = 1.178 mse = 1.302 mae = 0.836\n",
            "epoch155 train time: 6.249s test time: 0.767  loss = 0.771 val_mse = 1.172 mse = 1.293 mae = 0.835\n",
            "epoch156 train time: 6.242s test time: 0.767  loss = 0.774 val_mse = 1.177 mse = 1.305 mae = 0.837\n",
            "epoch157 train time: 6.255s test time: 0.775  loss = 0.771 val_mse = 1.177 mse = 1.289 mae = 0.833\n",
            "epoch158 train time: 6.265s test time: 0.765  loss = 0.776 val_mse = 1.180 mse = 1.306 mae = 0.835\n",
            "epoch159 train time: 6.249s test time: 0.765  loss = 0.771 val_mse = 1.175 mse = 1.291 mae = 0.834\n",
            "epoch160 train time: 6.258s test time: 0.770  loss = 0.774 val_mse = 1.172 mse = 1.301 mae = 0.834\n",
            "epoch161 train time: 6.259s test time: 0.763  loss = 0.774 val_mse = 1.172 mse = 1.292 mae = 0.834\n",
            "epoch162 train time: 6.240s test time: 0.773  loss = 0.773 val_mse = 1.174 mse = 1.301 mae = 0.835\n",
            "epoch163 train time: 6.236s test time: 0.778  loss = 0.772 val_mse = 1.181 mse = 1.293 mae = 0.835\n",
            "epoch164 train time: 6.251s test time: 0.773  loss = 0.774 val_mse = 1.172 mse = 1.304 mae = 0.835\n",
            "epoch165 train time: 6.251s test time: 0.770  loss = 0.771 val_mse = 1.182 mse = 1.295 mae = 0.836\n",
            "epoch166 train time: 6.244s test time: 0.767  loss = 0.775 val_mse = 1.174 mse = 1.302 mae = 0.836\n",
            "epoch167 train time: 6.246s test time: 0.773  loss = 0.773 val_mse = 1.183 mse = 1.293 mae = 0.834\n",
            "epoch168 train time: 6.261s test time: 0.768  loss = 0.776 val_mse = 1.176 mse = 1.301 mae = 0.835\n",
            "epoch169 train time: 6.256s test time: 0.769  loss = 0.772 val_mse = 1.180 mse = 1.294 mae = 0.834\n",
            "epoch170 train time: 6.260s test time: 0.767  loss = 0.774 val_mse = 1.182 mse = 1.303 mae = 0.835\n",
            "epoch171 train time: 6.237s test time: 0.765  loss = 0.772 val_mse = 1.175 mse = 1.291 mae = 0.835\n",
            "epoch172 train time: 6.248s test time: 0.775  loss = 0.774 val_mse = 1.180 mse = 1.301 mae = 0.833\n",
            "epoch173 train time: 6.253s test time: 0.768  loss = 0.772 val_mse = 1.179 mse = 1.293 mae = 0.836\n",
            "epoch174 train time: 6.263s test time: 0.766  loss = 0.774 val_mse = 1.176 mse = 1.301 mae = 0.834\n",
            "epoch175 train time: 6.250s test time: 0.765  loss = 0.772 val_mse = 1.183 mse = 1.294 mae = 0.835\n",
            "epoch176 train time: 6.251s test time: 0.771  loss = 0.773 val_mse = 1.172 mse = 1.302 mae = 0.835\n",
            "epoch177 train time: 6.242s test time: 0.763  loss = 0.770 val_mse = 1.182 mse = 1.294 mae = 0.834\n",
            "epoch178 train time: 6.240s test time: 0.776  loss = 0.774 val_mse = 1.173 mse = 1.300 mae = 0.834\n",
            "epoch179 train time: 6.240s test time: 0.770  loss = 0.771 val_mse = 1.179 mse = 1.293 mae = 0.835\n",
            "epoch180 train time: 6.251s test time: 0.767  loss = 0.774 val_mse = 1.174 mse = 1.300 mae = 0.835\n",
            "epoch181 train time: 6.242s test time: 0.773  loss = 0.771 val_mse = 1.177 mse = 1.295 mae = 0.833\n",
            "epoch182 train time: 6.246s test time: 0.763  loss = 0.773 val_mse = 1.177 mse = 1.300 mae = 0.834\n",
            "epoch183 train time: 6.248s test time: 0.765  loss = 0.772 val_mse = 1.179 mse = 1.291 mae = 0.833\n",
            "epoch184 train time: 6.248s test time: 0.770  loss = 0.773 val_mse = 1.171 mse = 1.300 mae = 0.836\n",
            "epoch185 train time: 6.250s test time: 0.764  loss = 0.771 val_mse = 1.177 mse = 1.290 mae = 0.834\n",
            "epoch186 train time: 6.261s test time: 0.771  loss = 0.773 val_mse = 1.176 mse = 1.304 mae = 0.836\n",
            "epoch187 train time: 6.257s test time: 0.771  loss = 0.770 val_mse = 1.181 mse = 1.291 mae = 0.832\n",
            "epoch188 train time: 6.243s test time: 0.769  loss = 0.773 val_mse = 1.174 mse = 1.300 mae = 0.835\n",
            "epoch189 train time: 6.239s test time: 0.767  loss = 0.771 val_mse = 1.178 mse = 1.293 mae = 0.834\n",
            "epoch190 train time: 6.250s test time: 0.769  loss = 0.774 val_mse = 1.174 mse = 1.303 mae = 0.835\n",
            "epoch191 train time: 6.239s test time: 0.767  loss = 0.771 val_mse = 1.178 mse = 1.289 mae = 0.834\n",
            "epoch192 train time: 6.248s test time: 0.766  loss = 0.772 val_mse = 1.176 mse = 1.303 mae = 0.835\n",
            "epoch193 train time: 6.248s test time: 0.772  loss = 0.770 val_mse = 1.174 mse = 1.287 mae = 0.832\n",
            "epoch194 train time: 6.264s test time: 0.777  loss = 0.773 val_mse = 1.174 mse = 1.302 mae = 0.835\n",
            "epoch195 train time: 6.253s test time: 0.769  loss = 0.769 val_mse = 1.179 mse = 1.288 mae = 0.832\n",
            "epoch196 train time: 6.264s test time: 0.769  loss = 0.773 val_mse = 1.176 mse = 1.301 mae = 0.834\n",
            "epoch197 train time: 6.256s test time: 0.767  loss = 0.771 val_mse = 1.180 mse = 1.296 mae = 0.834\n",
            "epoch198 train time: 6.259s test time: 0.767  loss = 0.773 val_mse = 1.175 mse = 1.305 mae = 0.837\n",
            "epoch199 train time: 6.238s test time: 0.767  loss = 0.769 val_mse = 1.177 mse = 1.293 mae = 0.834\n",
            "epoch200 train time: 6.244s test time: 0.767  loss = 0.772 val_mse = 1.173 mse = 1.300 mae = 0.836\n",
            "epoch201 train time: 6.232s test time: 0.766  loss = 0.769 val_mse = 1.175 mse = 1.288 mae = 0.832\n",
            "epoch202 train time: 6.248s test time: 0.765  loss = 0.771 val_mse = 1.175 mse = 1.304 mae = 0.835\n",
            "epoch203 train time: 6.255s test time: 0.771  loss = 0.769 val_mse = 1.179 mse = 1.293 mae = 0.834\n",
            "epoch204 train time: 6.250s test time: 0.768  loss = 0.772 val_mse = 1.173 mse = 1.304 mae = 0.835\n",
            "epoch205 train time: 6.243s test time: 0.777  loss = 0.769 val_mse = 1.177 mse = 1.293 mae = 0.834\n",
            "epoch206 train time: 6.255s test time: 0.766  loss = 0.770 val_mse = 1.175 mse = 1.304 mae = 0.835\n",
            "epoch207 train time: 6.251s test time: 0.767  loss = 0.768 val_mse = 1.179 mse = 1.292 mae = 0.832\n",
            "epoch208 train time: 6.233s test time: 0.767  loss = 0.770 val_mse = 1.172 mse = 1.301 mae = 0.834\n",
            "epoch209 train time: 6.256s test time: 0.768  loss = 0.768 val_mse = 1.173 mse = 1.291 mae = 0.833\n",
            "epoch210 train time: 6.236s test time: 0.775  loss = 0.770 val_mse = 1.172 mse = 1.301 mae = 0.836\n",
            "epoch211 train time: 6.243s test time: 0.773  loss = 0.768 val_mse = 1.179 mse = 1.295 mae = 0.833\n",
            "epoch212 train time: 6.246s test time: 0.766  loss = 0.770 val_mse = 1.172 mse = 1.301 mae = 0.834\n",
            "epoch213 train time: 6.249s test time: 0.775  loss = 0.766 val_mse = 1.177 mse = 1.295 mae = 0.834\n",
            "epoch214 train time: 6.251s test time: 0.775  loss = 0.767 val_mse = 1.170 mse = 1.298 mae = 0.835\n",
            "epoch215 train time: 6.251s test time: 0.772  loss = 0.767 val_mse = 1.180 mse = 1.292 mae = 0.833\n",
            "epoch216 train time: 6.251s test time: 0.771  loss = 0.768 val_mse = 1.171 mse = 1.300 mae = 0.834\n",
            "epoch217 train time: 6.254s test time: 0.781  loss = 0.766 val_mse = 1.172 mse = 1.291 mae = 0.833\n",
            "epoch218 train time: 6.255s test time: 0.770  loss = 0.766 val_mse = 1.169 mse = 1.302 mae = 0.836\n",
            "epoch219 train time: 6.261s test time: 0.768  loss = 0.765 val_mse = 1.175 mse = 1.290 mae = 0.832\n",
            "epoch220 train time: 6.259s test time: 0.769  loss = 0.767 val_mse = 1.174 mse = 1.298 mae = 0.835\n",
            "epoch221 train time: 6.240s test time: 0.761  loss = 0.765 val_mse = 1.176 mse = 1.289 mae = 0.832\n",
            "epoch222 train time: 6.251s test time: 0.763  loss = 0.766 val_mse = 1.169 mse = 1.299 mae = 0.834\n",
            "epoch223 train time: 6.244s test time: 0.775  loss = 0.763 val_mse = 1.177 mse = 1.292 mae = 0.834\n",
            "epoch224 train time: 6.256s test time: 0.770  loss = 0.765 val_mse = 1.171 mse = 1.303 mae = 0.835\n",
            "epoch225 train time: 6.242s test time: 0.764  loss = 0.764 val_mse = 1.173 mse = 1.293 mae = 0.834\n",
            "epoch226 train time: 6.249s test time: 0.771  loss = 0.765 val_mse = 1.172 mse = 1.298 mae = 0.833\n",
            "epoch227 train time: 6.250s test time: 0.764  loss = 0.763 val_mse = 1.173 mse = 1.288 mae = 0.831\n",
            "epoch228 train time: 6.252s test time: 0.768  loss = 0.766 val_mse = 1.167 mse = 1.303 mae = 0.835\n",
            "epoch229 train time: 7.114s test time: 1.051  loss = 0.763 val_mse = 1.178 mse = 1.291 mae = 0.833\n",
            "epoch230 train time: 6.903s test time: 0.889  loss = 0.765 val_mse = 1.167 mse = 1.302 mae = 0.835\n",
            "epoch231 train time: 6.358s test time: 0.772  loss = 0.761 val_mse = 1.175 mse = 1.290 mae = 0.833\n",
            "epoch232 train time: 6.240s test time: 0.768  loss = 0.764 val_mse = 1.172 mse = 1.301 mae = 0.834\n",
            "epoch233 train time: 6.230s test time: 0.769  loss = 0.762 val_mse = 1.175 mse = 1.289 mae = 0.832\n",
            "epoch234 train time: 6.252s test time: 0.767  loss = 0.764 val_mse = 1.170 mse = 1.299 mae = 0.834\n",
            "epoch235 train time: 6.283s test time: 0.767  loss = 0.762 val_mse = 1.174 mse = 1.290 mae = 0.832\n",
            "epoch236 train time: 6.232s test time: 0.765  loss = 0.764 val_mse = 1.171 mse = 1.303 mae = 0.834\n",
            "epoch237 train time: 6.248s test time: 0.769  loss = 0.762 val_mse = 1.173 mse = 1.291 mae = 0.833\n",
            "epoch238 train time: 6.249s test time: 0.779  loss = 0.763 val_mse = 1.169 mse = 1.298 mae = 0.835\n",
            "epoch239 train time: 6.245s test time: 0.769  loss = 0.762 val_mse = 1.174 mse = 1.288 mae = 0.833\n",
            "MAE 0.8389745144456954\n",
            "MSE 1.3047265003628756\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "ParameterTuning_CARL_GoodReads",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}